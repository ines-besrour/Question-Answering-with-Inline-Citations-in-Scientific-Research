{
  "goldens": [
    {
      "input": "What are the main findings of the research on non-extremal Kerr black holes and their role in enabling ultraenergetic collisions?",
      "expected_output": "The main findings of the research on non-extremal Kerr black holes are that they can enable ultraenergetic collisions near the inner horizon, but not at the event horizon. A critical angular momentum is required for the divergence of energy at the inner horizon, and such collisions can occur with arbitrarily high center of mass energies. This is in contrast to extremal Kerr black holes, which require a specific spin parameter to achieve such high energies. The research shows that non-extremal Kerr black holes can serve as particle accelerators, allowing for ultraenergetic collisions to occur near the inner horizon, making them more practical and realizable than extremal black holes.",
      "context": [
        ". We find that a critical angular momentum is required for the divergence of E_{c.m.} at the horizon. However, with this angular momentum, the particle can never reach the horizon. In relation to Lake's work, we first prove that no divergence of energy occur at the inner horizon for particles with generic angular momentums. A critical angular momentum is required for the divergence. We show with an explicit example that two particles can fall from infinity all the way to the inner horizon and collide with an arbitrarily high center of mass energy. We have shown that such arbitrarily high energies can be obtained in any non-extremal Kerr spacetime, unlike the case in that requires the black hole to be exactly extremal. Since extremal black holes do not exist in nature, the ultraenergetic collisions near Cauchy horizons may be more practical and realizable.\nAcknowledgements: This research was supported by NSFC grants 10605006, 10975016 and by“the Fundamental Research Funds for the Central Universities”.",
        ", allowing them to fall from anywhere outside the black hole. Our analysis shows that infinite energies can only be attained at the horizon and one of the particles must have a critical angular momentum. However, with such an angular momentum, there always exists a potential barrier outside the black hole preventing the particle from approaching the event horizon.Since the event horizon of a non-extremal Kerr black hole cannot serve as a particle accelerator creating infinite collision energies, it is natural to ask whether the inner horizon can make a difference. This issue has recently been explored by Lake . The author claimed in the original version that the center of mass energy for two colliding particles is generically divergent at the inner horizon and no fine tuning is required. Then in the Erratum this claim was withdrawn because physical constraints forbid such collisions. We reexamine this issue in details and arrive at the following conclusions. We first confirm, using different arguments, that a generic divergence at the inner horizon is not possible. We further show that for a critical angular momentum, the center of mass energy diverges at the inner horizon. Such a divergence is similar to that proposed by BSW where a critical angular momentum also is required. The difference is that there is no restriction on the spin parameter a. So in",
        "# arXiv Paper: 1106.2852\n\nNon-extremal Kerr black holes as particle accelerators\nabstract: It has been shown that extremal Kerr black holes can be used as particle accelerators and arbitrarily high energy may be obtained near the event horizon. We study particle collisions near the event horizon (outer horizon) and Cauchy horizon (inner horizon) of a non-extremal Kerr black hole. Firstly, we provide a general proof showing that particles cannot collide with arbitrarily high energies at the outter horizon. Secondly, we show that ultraenergetic collisions can occur near the inner horizon of a Kerr black hole with any spin parameter $a$.\nIntroduction: Whether Kerr black holes can serve as particle accelerators with infinite collision energy has recently been discussed. Bañados, Silk and West showed that particles falling from rest outside an extremal Kerr black hole (a=M) can collide with arbitrarily high center of mass energies when the collision occurs arbitrarily close to the horizon. The BSW mechanism has been further discussed and generalized to different spacetimes (See e.g. - ). Jacobson and Sotiriou pointed out that infinite energies for the colliding particles can only be attained at infinite time. Authors also argued"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between Lorentz force and Casimir force in a medium, and how do the research findings explain their interaction?",
      "expected_output": "The research paper explains that the Lorentz force approach to the Casimir effect, as suggested by Raabe and Welsch, leads to a nonzero force on the medium between two mirrors. According to their formula, the total force on a slab in a material planar cavity consists of a medium-screened Casimir force and a medium-assisted force. The medium-assisted force also describes the force on the cavity medium, implying an atom-mirror interaction of the Casimir-Polder type at large distances and of the Coulomb type at small distances. This interaction is insensitive to the polarizability type of the atom, whether electric or magnetic.",
      "context": [
        " corresponding force. Despite this, a number of approaches to the Casimir effect in material systems lead to the result that the Casimir force on the medium between two mirrors vanishes and that the only existing force is that between the mirrors , , (see also text books , and references therein). To overcome this \"unphysical\" result, usually derived by calculating the Minkowski stress tensor , but also obtained using other methods , , Raabe and Welsch very recently suggested a Lorentz-force approach to the Casimir effect. In their approach the force on a body is obtained by calculating the sum of the vacuum Lorentz forces acting on its constituents. Evidently, this method should lead to a nonzero force on the medium between the mirrors. As an application of their approach, Raabe and Welsch derived a formula for the force on a magnetodielectric slab in a magnetodielectric planar cavity, as depicted in Fig. 1. In this paper we i) demonstrate that, according to the Raabe and Welsch formula, the total force on the slab actually consists of a medium-screened Casimir force and a medium-assisted force and ii) point out a few unexpected results coming from the unusual properties",
        "# arXiv Paper: quant-ph/0502078\n\nScreened Casimir forces\nabstract: We demonstrate that a very recently obtained formula for the force on a slab in a material planar cavity based on the calculation of the vacuum Lorentz force [C. Raabe and D.-G. Welsch, Phys. Rev. A 71, 013814 (2005)] describes a (medium) screened Casimir force and, in addition to it, a medium-assisted force. The latter force also describes the force on the cavity medium. For dilute media, it implies the atom-mirror interaction of the Casimir-Polder type at large and of the Coulomb type at small atom-mirror distances of which the sign is insensitive to the polarizability type (electric or magnetic) of the atom.\n: Screened Casimir forces M. S. Tomaš tomas@thphys.irb.hr Rudjer Bošković Institute, P. O. B. 180, 10002 Zagreb, CroatiaWe demonstrate that a very recently obtained formula for the force on a slab in a material planar cavity based on the calculation of the vacuum Lorentz force [C. Ra",
        " force on a slab in a material planar cavity based on the calculation of the vacuum Lorentz force [C. Raabe and D.-G. Welsch, Phys. Rev. A, 71, 013814 (2005)] describes a (medium) screened Casimir force and, in addition to it, a medium-assisted force. The latter force also describes the force on the cavity medium. For dilute media, it implies the atom-mirror interaction of the Casimir-Polder type at large and of the Coulomb type at small atom-mirror distances of which the sign is insensitive to the polarizability type (electric or magnetic) of the atom.12.20.Ds, 42.50.Nn, 42.60.Da IRB-TH-1/05It is well known that an atom in the vicinity of a body (mirror) experiences the Casimir-Polder force and, at smaller distances, its nonretarded counterpart, the van der Waals force. Consequently, being a collection of atoms, every piece of a medium in front of a mirror should experience the corresponding force. Despite this, a number of approaches to the Casimir effect in material systems lead to the result that the Cas"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does ChPT apply to the research conducted at RHIC, and what are the main findings of this application?",
      "expected_output": "Chiral Perturbation Theory (ChPT) applies to the research conducted at Relativistic Heavy Ion Collisions (RHIC) by providing a framework to describe the meson gas formed after a collision. The main findings of this application include model-independent predictions, such as the temperature dependence of the pion electromagnetic charge radius, and the description of thermal effects of resonances like the ρ meson. ChPT's unitarization programme allows for the extension to finite temperature, making it a promising framework for future RHIC applications.",
      "context": [
        "bf-46a7-41c8-b209-278f355ee58b}}Summarizing, ChPT and its unitarization programme can be extended successfully to finite temperature and provide a promising framework for future applications in the context of Relativistic Heavy Ion Collisions.\nAcknowledgments: This work is supported by the Spanish research projects FPA2000-0956,PB98-0782, BFM2000-1326 and FPA2004-02602.",
        "# arXiv Paper: hep-ph/0408110\n\nChiral Mesons in Hot Matter\nabstract: We review our recent work on thermal meson properties within the Chiral Perturbation Theory framework. We will focus on the pion electromagnetic form factor, stressing its importance for Relativistic Heavy Ion Collisions. We obtain model-independent predictions (based only on chiral symmetry) such as the temperature dependence of the pion electromagnetic charge radius. Imposing unitarity allows to describe the thermal effects of resonances such as the $\\rho$ meson.\nIntroduction: In order to describe the meson gas formed after a Relativistic Heavy Ion Collision (RHIC) for temperatures below Chiral Symmetry Restoration, Chiral Perturbation Theory (ChPT) is the most general theoretical scheme and its predictions are based only on Chiral Symmetry. When applied to the low-{{formula:9c5da419-6690-4a37-88f5-d64b6638a66c}} meson gas, one can analyze static quantities such as the pressure , , quark condensate , , and pion decay constants , . All these quantities point",
        " Our results for the pion charge radius confirm those in based on QCD sum rules.\nThermal unitarity and resonances: In the center of mass frame ({{formula:1473928b-0807-4d2d-bf1c-6c5a756baeda}} , corresponding to back to back dilepton pairs) the ChPT pion form factor satisfies the following perturbative unitarity relation for energies above the two-pion threshold {{formula:f46bd3ac-37b1-42d2-8a03-7ebd1f68b3fd}} : {{formula:205a291d-65b7-43d1-8dce-bfcc522ab040}}with: {{formula:c5e5c31d-17b1-49c8-9977-ddff9ed96a97}}The superscripts {{formula:c2c3fa1e-44de-4409-9947-7508ee6d8481}} and {{formula:89a04cfd-fd59-4966-98a7-31b63094558c}} indicate the LO"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the primary energy source used to fuel the muon and antimuon beams in the MUonE experiment?",
      "expected_output": "The primary energy source used to fuel the muon and antimuon beams in the MUonE experiment is not explicitly stated in the given context, but it is mentioned that the energy of the incoming muons or antimuons is 150 GeV.",
      "context": [
        "cb650fc9-7eef-47ad-b36d-2c2b12cec401}} is the Källen function.In a fixed-target experiment where the electron is initially at rest, {{formula:9f2a4643-2af9-4200-be70-5f05d502a823}} is the energy of the incoming muons or antimuons, and {{formula:64cc0449-8b6b-42be-9b0b-70b8af1fcf4e}} is the electron recoil energy, the variables {{formula:c3579438-e363-4cb1-8f8a-6eb52482d249}} and {{formula:cc9bf303-8e5b-40fc-a5ae-80b45b0717ca}} are given by {{formula:aff740ad-9343-4f8e-bf8b-0e6df69f9826}}Both positive and negative muon beams will be available for the MUonE experiment. For {{formula:b33cb9da-1cd3-40f4",
        "bb6f8dd-8529-4e21-85b6-c612315f450f}} and {{formula:b30e1c8f-01a1-4cef-919f-3dc7df52e690}} . The same pattern is observed at NLO . {{figure:95feb0b9-436c-49a0-890b-8029722c397d}}Figure REF shows that, when the muon/antimuon beam has an energy of 150 GeV, for most of the kinematic region scanned by the squared momentum transfer {{formula:9a678fbd-011a-4989-a271-e629d0e559eb}} the factor {{formula:1f85744d-6726-4b70-a03a-db143fe658a8}} is of order {{formula:0d4b96b0-e5c4-4cbb-984f-bc4f4df5bdc5}} –{{formula:f6664dd6-8cdd-4262-8baf-0483c954",
        " photons.We showed that, in a fixed-target experiment where the electron is initially at rest and the energy of the incoming muons or antimuons is 150 GeV, the corrections to the differential scattering cross sections with respect to {{formula:394245d7-d9a6-4a0c-8c44-df48fb7eb292}} , the square of the difference of the initial and final electron momenta, are of order {{formula:5f3ff3d2-5a4a-4c6a-932d-30d4ecbf5230}} –{{formula:4a44ee86-42c8-4822-b413-b998521b3fa7}} for most of the kinematic region spanned by {{formula:81110962-3b8d-4153-a6f7-0e073d465049}} . These corrections will therefore play a crucial role in the data analysis of future high-precision muon-electron scattering experiments like MUonE, whose goal is to reach a relative precision of order {{formula:1af3b041-591a-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the impact of grading length on triangle inequality, and how do the methods used affect the key results?",
      "expected_output": "The research paper proves that a natural candidate for a homogeneous norm on a graded Lie algebra of length 5 satisfies the triangle inequality, answering Moskowitz's question. The main findings show that the homogeneous norms defined for gradings of lengths 3, 4, and 5 satisfy the triangle inequality. The methods used, including Hölder's inequality and Muirhead's inequality, provide an elementary proof for the most complicated case of length 5, with the same approach applicable to lengths 3 and 4.",
      "context": [
        "# arXiv Paper: 1903.06545\n\nThe triangle inequality for graded real vector spaces of length 5\nabstract: In this paper, we prove that a natural candidate for a homogeneous norm on a graded Lie algebra of length 5 satisfies the triangle inequality which answers Moskowitz's question.\nIntroduction: In , M. Moskowitz extended the classical theorem of Minkowski on lattice points and convex bodies in {{formula:19a9fd2c-5ce3-43aa-ad62-453bd7afd2b8}} to simply connected nilpotent Lie groups {{formula:ded2ab69-a570-4358-9c5b-bcbe070a9d34}} with a {{formula:ab2f5347-3565-4756-98e8-f1c473752d6d}} -structure whose Lie algebra {{formula:413af2d7-90c9-4a24-91cf-5b493ed51114}} admits a length 2 grading. For this purpose, the author proved the triangle inequality for a certain natural homogeneous norm of the Lie algebra associated with the grading of length 2",
        ", the author proved the triangle inequality for a certain natural homogeneous norm of the Lie algebra associated with the grading of length 2. Later, in , the same author extended the homogeneous norms for which the triangle inequality holds to gradings of length 3 and 4. At the end of the paper , the author demonstrated that the method of his proof cannot be carried on to the case of length 5, so it was left as an open problem. However, the arguments in the proof contains some errors. The inequalities (6) and (12) in the proof of Theorem 1 in are not correct for length {{formula:590ca902-b7a3-4596-879d-8981e4adc61c}} or 4.In this paper, we claim that for the grading of lengths 3, 4 and 5, the homogeneous norms defined as in satisfy the the triangle inequality. For the sake of conciseness, the complete proof will be provided only for the most complicated case, the case of length 5. The gradings of length 3 and 4 can be done with the same manner. Our proof is quite elementary. The main tools in our proof are only Hölder's inequality and Muirhead's inequality.",
        " proof is quite elementary. The main tools in our proof are only Hölder's inequality and Muirhead's inequality. We refer the reader to for the latter inequality.A Lie algebra {{formula:3e19bc97-d8b6-40df-8fb3-f696c995fb56}} is said to be graded if there is a finite family of subspaces {{formula:8f5f68be-2ecc-40ec-b6de-2fe975f69457}} with {{formula:88058969-7a14-4ffa-b6a9-9846f593099f}} satisfying {{formula:e002a92d-1f6c-4191-b4cd-eec258861553}} for all {{formula:b251232d-35c2-4709-b085-9f5afebbb31c}} (here, {{formula:8df2a9a4-f93e-46e8-8e88-1e3550ef5bc1}} if {{formula:2162ba75-a0c1-4ed"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does pdeeig use to solve eigenvalue equations and what are its key findings?",
      "expected_output": "The pdeeig function in Matlab's PDE Toolbox uses the Arnoldi Algorithm to solve eigenvalue equations. The key findings include the ability to yield sixty to seventy eigenvalues within minutes for a particular homotopy, and the observation of eigenvalue dynamics for each of the five symmetry families. For example, the eigenvalues get very close together at certain points, and the depiction of the homotopy of Eigenvalue 10 on the circle shows ambiguous behavior at specific times.",
      "context": [
        " to the program in its desired format. To accomplish this task in Matlab's PDE Toolbox for a fixed {{formula:7498bb3e-3362-4e49-9f46-3577f1a2ce16}} , we specify polygons and circles. More specifically, we specify the vertices of the polygon and the center and radius of a circle, with standard coordinates in the plane. Once we input this data, we then fix a closed interval of the real line in which we look for eigenvalues {{formula:f51904a2-27b6-455d-85e8-bc9893c9ce40}} . The program then uses the Arnoldi Algorithm with function pdeeig to solve the eigenvalue equation {{formula:5bfd88b2-d323-4fec-849b-1bfe0f7c6333}} for {{formula:2242f158-c54f-4d68-8e8e-1c4c87199eb8}} in the specified interval. For this particular homotopy, this process yields sixty to seventy eigenvalues within minutes. As our first",
        " specified interval. For this particular homotopy, this process yields sixty to seventy eigenvalues within minutes. As our first approximation to a smooth homotopy using {{formula:44d13296-b7b0-467d-8030-6e294d7eab41}} or {{formula:678fddb1-7ed1-4962-bbd0-a16ee431a0be}} , we solve for eigenfunctions at times {{formula:db6c696d-2752-4db5-ab48-74df7483d74e}} . For now, Matlab's solution method is sufficient. However, other solvers based in different programs may be more effective in the future. For instance, the boundary element method appears much more efficient than the finite element method.In the following diagrams, we show the observed eigenvalue dynamics for each of the five symmetry families ({{formula:41fad489-b104-4dd2-8cd5-4d2d7343a90d}} ). For convenience, we divide all eigenvalues by {{formula:7ce7c8ec-f6f5-4eff",
        "f19e62-5b8a-42d8-845e-743c2294c633}} ) get very close together at {{formula:d02a0113-f2d0-42f8-93cc-07175f404a19}} . Our depiction of the homotopy of Eigenvalue 10 on the circle in Fig. {{formula:0f2acc84-caf0-42f4-815e-0b255363b177}} again shows ambiguous behavior at {{formula:b9123433-d0fb-41c7-8d7b-8d33424f8f4d}} . To make our analysis more precise, we solve the eigenvalue problem for {{formula:64b26880-f588-4957-84e1-6d7bb018f7a0}} for {{formula:37d2d926-03ad-4d85-9679-21ff538e86c0}} . In Fig. {{formula:c95eb7ea-092e-4f9c-9b6f-7458d68aad"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main limitations of data and models in multilingual Grapheme-to-Phoneme systems and how do they impact the accuracy of these systems?",
      "expected_output": "The main limitations of data and models in multilingual Grapheme-to-Phoneme systems are the lack of multilingual pronunciation dictionaries to train multilingual models and the challenge of encoding a huge number of orthographic symbols in neural models due to the wide range of writing systems in world languages. These limitations impact the accuracy of these systems by leaving substantial room for improvement in covering more languages and improving G2P accuracy. For example, the research paper mentions that statistical G2P models are not optimal for every language, such as logogram-based Chinese, where querying from a pronunciation dictionary is more effective, and phonetically regular languages like Spanish and Italian, where rule-based methods could work well.",
      "context": [
        " research , , , , , , . Multilingual models increase efficiency by training a single model that can process multiple languages, rather than training a separate model for each language. While this multilingual training process is more challenging, multilingual pretrained transformers have been closing the gap between multilingual models and monolingual models. Despite the potential benefits, there are relatively few publicly-available tools for multilingual G2P, notably Epitran , Phonetisaurus , and eSpeak-NG . These rule-based or finite-state transducer (FST) based models work well for many languages but they still leave substantial room for improvement in both covering more languages and improving the G2P accuracy.Yet multilingual G2P still faces many non-trivial barriers, both in terms of data and of models. One is the lack of multilingual pronunciation dictionaries to train multilingual models. Moreover, world languages have a wide range of writing systems, how to encode a huge number of orthographic symbols in neural models remains challenging. To tackle multilingual G2P problem, we address both challenges. First, to create a training dataset, we aggregated pronunciation dictionaries previously published or made available in around 100 languages. Second, to encode",
        " create a training dataset, we aggregated pronunciation dictionaries previously published or made available in around 100 languages. Second, to encode diverse writing systems, we trained on 99 languages the sequence-to-sequence ByT5 model that takes raw bytes as processing units. Our results show that byte-level ByT5 outperformed the token-based mT5 models in multilingual G2P with far fewer parameters and the multilingual ByT5 also outperformed most monolingual G2P models with the same architecture. Moreover, multilingual models can perform zero-shot G2P on unseen low-resource languages with seen writing systems. Pretrained weights can also be fine-tuned on low resource languages to speed up convergence and increase performance. Our proposed method represent an efficient strategy for multilingual and low-resource G2P problems and we make our models publicly available to facilitate future research.\nMultilingual Pronunciation Dictionaries: In order to create a large pronunciation dataset, we manually selected and merged several publicly available G2P datasets , and constructed G2P mapping dictionaries for some new languages. To promote future research, we released a detailed catalogue of available G2P data and their licenses at: https://github.com/lingjz",
        " greatly at successive checkpoints and susceptible to overfitting.\nConclusions: Amassing around 100 publicly available grapheme-to-phoneme datasets, we trained large-scale multilingual G2P models based on ByT5. We found that byte-level transformers significantly outperformed the token-based mT5 and monolingual models. The pretrained multilingual G2P model can benefit low resource G2P as it can perform zero-shot prediction on certain unseen languages. We also demonstrate that finetuning monolingual data on pretrained monolingual model generally helps the model converge to lower PER than randomly initialized models.However, it should be noted that statistical G2P models are not the optimal model for every language. For logogram-based Chinese, querying from a pronunciation dictionary is more effective than using a neural G2P model at the word-level, though phrase-level G2P is still needed for Chinese to disambiguate homographs . For phonetically regular languages like Spanish and Italian, rule-based methods could work very well. Yet for the majority of writing systems that are only partially phonetically regular, neural G2P models can effectively generate phonemic transcriptions from texts. In the future,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the QCD(BF) phase structure research, and what methods were used to obtain these results?",
      "expected_output": "default",
      "context": [
        " $m$, two strong scales $\\Lambda_{1},\\Lambda_2$, and two vacuum angles, $\\theta_1, \\theta_2$. In particular, we find that the QCD(BF) vacuum respects the $\\mathbb{Z}_2$ exchange symmetry of two gauge groups. Under the assumption of the adiabatic continuity, our result successfully explains one of the conjectured phase diagrams in the previous literature and also gives positive support for the nonperturbative validity of the large-$N$ orbifold equivalence between QCD(BF) and $\\mathcal{N}=1$ $SU(2N)$ supersymmetric Yang-Mills theory. We also comment on problems of domain walls.\nIntroduction: In this paper, we study the phase structure of Quantum Chromodynamics (QCD) with the bifundamental fermion (QCD(BF)). QCD(BF) is the 4-dimensional SU(N)_1 \\times SU(N)_2 gauge theory with a Dirac fermion in the bifundamental representation, and this theory may be regarded as the flavor-symmetric fundamental QCD with the same color and flavor",
        "2 exchange symmetry and the validity of the large-N orbifold equivalence, while it is logically possible that all these regions could be separated by phase transitions from the four-dimensional QCD(BF) with the small/intermediate fermion mass. All in all, it is plausible to conclude that the QCD(BF) vacuum respects the \\mathbb {Z}_2 exchange symmetry.It would be interesting to directly compare \\mathcal {N} = 1 super Yang-Mills theory with the exchange-symmetric sector (g_1 = g_2 and \\theta _1 = \\theta _2 = \\theta _d) of the QCD(BF). Since the action of the parent theory is reduced to twice the action of the daughter theory, S_{SYM} \\rightarrow 2S, we note that the correspondence between parameters of the parent and daughter theories is given by \\theta _p = 2 \\theta _d . Let us now look into the vacuum structure predicted by the semiclassical descriptions:Daughter theory: SU(N)_1 \\times SU(N)_2 QCD(BF) There are N^2 classical vacua \\lbrace {k",
        " 1 SYM theory, QCD with fundamental quarks, and QCD with 2-index quarks. This approach stands out for its calculability and compatibility with widespread interpretations of quark confinement, particularly the center vortex picture , , , , . In this paper, we apply this semiclassical computational method to find the phase diagram of QCD(BF) and discuss its relation with previous studies.The punchline of our results is the following. Within the validity of the semiclassics (N\\Lambda _{1,2}L\\ll 1 with the torus size L), we obtain the phase diagram not only as a function of \\theta _1 and \\theta _2 but also as the function of the fermion mass m and the strong scales \\Lambda _{1,2}, and our result covers from the chiral limit to the relatively heavy fermion mass. Moreover, it turns out that the \\mathbb {Z}_2 exchange symmetry is unbroken, and our result provides affirmative support for the nonperturbative orbifold equivalence. We also comment on subtleties related to domain walls.The rest of this paper is structured as follows. In Section , we introduce the Q"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does GPR estimate functions from noisy data?",
      "expected_output": "GPR estimates functions from noisy data by assuming a distribution of functions described by a Gaussian process. It uses a kernel to define a prior probability distribution of functions, which is then updated using Bayes' theorem and the observations to obtain a posterior distribution of functions. The predictive mean from this posterior distribution gives the expected value of the estimated function at new input points. For example, in the given context, GPR is used to estimate the function f(x) mapping x_i to y_i, where y_i = f(x_i) + ε_i, and ε_i represents a Gaussian distribution with zero mean and σ_n^2 variance.",
      "context": [
        " function f(x) mapping x_i to y_i. It is also assumed that the observations (y_i) are subject to noise as well, giving y_i=f(x_i)+\\epsilon _i. Here, \\epsilon _i represents a Gaussian distribution with zero mean and \\sigma _n^2 variance. To estimate the underlying function f(x), we start by assuming a distribution of functions (a Gaussian process) from which f(x) can potentially be sampled. Gaussian processes are described by a mean function m(x)=\\operatornamewithlimits{\\mathbb {E}}[f(x)] and a covariance function k(x,x^{\\prime })=\\operatornamewithlimits{\\mathbb {E}}[(f(x)-m(x))(f(x^{\\prime })-m(x^{\\prime }))] (commonly referred to as the kernel). These properties are the same for all f(x) that are generated by a particular Gaussian process. For GPR, m(x) is usually assumed to be zero, by default, and a suitable kernel k(x,",
        " _r=0.3\\sigma .\nGaussian process regression: Gaussian process regression (GPR) is a non-linear, non-parametric regression technique used for interpolating existing data . It is particularly useful when the analytic function describing the underlying process is unknown and the observed data are very noisy. GPR is different from other parametric regression techniques using closed functional forms like linear regression or cubic splines, in the sense that it does not assume any ansatz. It solely relies on large amounts of data (observations) to estimate a function, from which missing data are predicted. This is done by assuming a distribution of functions described by a Gaussian process, that is, a generalization of the Gaussian distribution to function spaces. While Gaussian distributions describe the properties of random variables (scalars or vectors), Gaussian processes describe the properties of functions.Consider that we have a dataset \\lbrace (x_i,y_i)\\rbrace _{i=1}^{n} where x_i are the input points and y_i are the observations. We intend to estimate the function f(x) mapping x_i to y_i. It is also assumed that the observations (y_i",
        " while larger values of l mean that the covariance is stronger for larger distances of n (i.e., f(x) varies more gradually with change in n). The kernel is used to define a prior probability distribution of functions encapsulating our assumptions about the underlying true function, called the Gaussian process prior. Using Bayes' theorem, this prior distribution along with our observations (here: the \\mathbf {T}_n(\\tau ) sampled at different n from particle resolved simulations) is used to estimate a narrower posterior distribution of functions which are likely to fit our observations. This is done by optimizing the parameters of the kernel, namely, l. The parameters are optimized separately for each of the 25 matrix elements. We can then obtain the predictive mean from the posterior distribution which gives the expected value of the estimated function at new input points. In the present calculations, we have found that the predictions for the \\delta _r=0.21\\sigma case were significantly improved when a composite kernel combining k_{\\text{O.U}}(n,n^{\\prime }) and a white noise kernel k_{\\text{white}}(n,n^{\\prime })=\\sigma _n^2\\delta (n"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the research paper use to ensure uniform electron recoils from the Cs source, and what are the main findings related to this process?",
      "expected_output": "The research paper uses a Cs source located approximately 10 cm from the center of the scintillator to ensure uniform distribution of electron recoils along the length of the cylinder. The main findings related to this process are that the organic glass demonstrated significantly higher electron light output compared to EJ-309 and EJ-276, with EJ-309 and EJ-276 being approximately 60% and 40% as bright as the organic glass, respectively.",
      "context": [
        "702-55745e7a5153}}For the EJ-276 and the organic glass scintillators, the two target PMTs were gain-matched using a {{formula:cf2aabb0-63fd-4135-9954-927e9a4dc6ad}} Cs source located approximately 10 cm from the center of the scintillator to ensure uniform distribution of electron recoils along the length of the cylinder. The pulse integral values obtained from each target PMT were combined using the geometric average to obtain the total scintillation light independent of the interaction location .The traditional MeVee electron-equivalent light unit assumes a linear response of the electron light yield and is not a useful quantity for scintillators that exhibit significant nonlinearity. Rather than relying upon the (potentially unjustified ) assumption of a linear electron response over the full range of the measurements, the light unit in this work was determined relative to that of a 477 keV electron using the Compton edge of a 662 keV {{formula:47dcfa27-d623-4dba-81a0-2e9edd04bc4e}} ray from a {{formula:92",
        " of three organic scintillators: EJ-309, a liquid scintillator with good PSD properties and a low flash point, EJ-276, currently the only commercially available PSD-capable plastic scintillator, and an organic glass developed at Sandia National Laboratories. The relative electron light output was obtained by comparing the light output corresponding to the Compton edge of a 662 keV {{formula:0a275e66-71cf-481e-8a3a-a9e5f63a6f57}} ray. The organic glass demonstrated significantly higher electron light output in comparison to the other two materials; EJ-309 and EJ-276 were approximately {{formula:c788e4c7-50d3-49f1-8f22-99dc504e464e}} and {{formula:aa08d0d7-f37e-4d68-833e-2e4a9dc8682f}} as bright as the organic glass, respectively. The proton light yield was measured using a double TOF technique over a broad energy range. The proton light yield of EJ-309 and the organic glass were within {{",
        " the higher light output, though the EJ-276 scintillator demonstrated significantly poorer resolution. Rapid degradation of the light output of EJ-276 was also observed as a function of time. Zaitseva et al. previously measured the electron light output of a fresh EJ-276 sample as comparable to that of EJ-309 . The authors confirmed that after approximately six months the light output of the EJ-276 sample had decreased by {{formula:86904ca6-065f-4f28-92bb-d7936a44e783}} .The potential for bias introduced by the wavelength-dependent photocathode quantum efficiency (QE) and the disparate scintillator emission spectra was investigated. The emission spectrum for EJ-309 was obtained from Eljen Technology . Photoluminescence emission spectra of the EJ-276 and organic glass scintillator were measured using a setup featuring a broad-spectrum XS-433 Xe lamp as excitation source coupled to a SP-2150i monochromator (both from Acton Research Corporation). The emitted light was collected by a SpectraPro-2150i spectrometer and a PIXIS: 100"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the key improvements that WFB-loss brings to face representation, and how does it enhance the accuracy of face recognition models?",
      "expected_output": "The WFB-loss brings key improvements to face representation by enhancing feature discrimination and generalization capability. It improves the accuracy of face recognition models by introducing a new loss term that complements the standard cross-entropy loss. As shown in Table REF, the model with WFB-loss achieves cutting-edge performance, outperforming other methods such as center loss and sphere face loss. For example, the WFB-loss term in Eq. (REF) is set to a specific value, which leads to better results compared to the standard cross-entropy loss alone.",
      "context": [
        " layers due to its good trade-off between prediction accuracy and model complexity. Features extracted from the last pooling layer are adopted as the face representation (512 dimensions).The verification accuracy with different models are listed in Table REF . As shown, for the loss function, we investigated the standard cross entropy, cross entropy plus our WFB-loss term in Eq. (REF ), the center loss in , and the sphere face loss in . For the WFB-loss, we set {{formula:23fb28a7-a69e-4d49-bf84-a33c6986548e}} in Eq. (REF ) as {{formula:54e3faaa-c79b-4693-a025-e6621fecc73d}} . For the center loss, we tried different sets of parameters and found the best performance could be achieved when the balancing coefficient was {{formula:17f3472b-08f9-4ba4-9a04-61ce2f3a4602}} , as reported in the table. For the sphere face , we noticed this paper very recently and only tried limited sets of parameters (there are four parameters to be adjusted together). The parameters",
        " we noticed this paper very recently and only tried limited sets of parameters (there are four parameters to be adjusted together). The parameters reported in the paper can not make the network converge on our dataset. The only parameter set we found to make the network converge leads to worse results, compared with the standard cross-entropy loss. Due to time constraint, for the other methods, we only report the results for some of them referring the numbers stated in the published corresponding papers. Please note that these methods use different datasets and different networks structures.As shown in Table REF , we obtain the face representation model with the cutting-edge performance with the help of our WFB-loss term in Eq. (REF ). We regard our model good enough to let us start to investigate the one-shot learning phase. We also tried different values of {{formula:ea8f4633-d967-4511-89a5-725961e26555}} in Eq. (REF ) and found our method is not sensitive to the choose of {{formula:65996dac-6fd1-4cbc-b700-4b3df3c2b5ca}} , shown in the Figure REF . Larger {{",
        "\nRepresentation Learning: To learn more effective feature representation, we design our face representation model with supervised learning framework considering persons' identities as class labels. Specifically, we propose the loss function as follows: {{formula:2315d432-08bf-46f5-bde6-63d21b624c5f}}where {{formula:6d61755a-5882-4f95-9c83-97758f7f239c}} is the standard cross-entropy loss used for the Softmax layer, while {{formula:de95ee87-4d32-4cb4-a804-735acddf311d}} is the newly proposed loss used to improve the feature discrimination and generalization capability. {{formula:3aa9e99f-7aee-4876-93c4-d3b7944e9d0d}} is the trade-off parameter between two loss functions. More specifically, we recap the first term, cross-entropy {{formula:9b2720f0-6cf6-4001-9293-92affd09acc7}} as follows: {{formula:e42a3"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the polynomial bounds on the mixing time of Thorp shuffles and how do they relate to the overall efficiency of the shuffling process?",
      "expected_output": "According to the research paper, the polynomial bounds on the mixing time of Thorp shuffles are given by the uniform mixing time, which is at most formula:287611bd-2e34-4e93-90f3-aa73639f0ad0. This bound is related to the overall efficiency of the shuffling process, as it implies that the process converges geometrically to the uniform distribution. The paper also mentions that the mixing time is polynomial in formula:302566a8-a42e-4e4c-b210-7eab0094df5e, which suggests that the shuffling process becomes more efficient as the number of cards increases. Additionally, the use of evolving sets, a technique introduced by the author and Peres, helps to bound the mixing times and provides insight into the efficiency of the Thorp shuffle.",
      "context": [
        ":75183d58-4ba7-4a02-a8d7-afc208bc68e4}}\nIndistinguishable cards mix in poly time: Let {{formula:84177539-25a4-4207-ac9c-852436e475b4}} be a set of cards. Then the process {{formula:dc26696f-1959-4fad-90f5-75473ef6a8d1}} is a Markov chain. The following lemma says that the uniform mixing time for this chain is {{formula:137e3d80-c426-435a-8c20-8f8309d98758}} .Lemma 4 There is a universal constant {{formula:389267a0-6fd7-481b-97ab-0c189522290b}} such that if {{formula:f299c53a-bcd4-4e75-9070-3636513d3de6}} then {{formula:8860fe2d-f10a-4358-9fe4-0aa2eb1b0e10}}where we write {{",
        " stage neighboring cards are interchanged with probability {{formula:6a785fed-1df6-4fa7-9df6-aeda4f4bcc57}} . We note that in a recent breakthrough result, Çam showed that the matrix {{formula:fb6dc00f-0e12-4ef8-a9be-173815b57ae5}} has strictly positive entries. This can be viewed as a result about the “diameter” of the Thorp shuffle; after a small number of steps there is a positive probability of being in any given state. However, these probabilities are in general very small so this does not imply a good bound for the mixing time.The main result of this paper is that indeed the mixing time is polynomial in {{formula:302566a8-a42e-4e4c-b210-7eab0094df5e}} . Our proof uses evolving sets, a technique for bounding mixing times that was introduced by the author and Peres in . Another paper that uses some of the same ideas is , in which a variant of evolving sets is used to analyze the exclusion process. Evolving sets are related to the",
        "b00}} , the uniform mixing time for the process {{formula:57a8f10f-d046-48c2-8665-7f17895b042e}} is at most {{formula:287611bd-2e34-4e93-90f3-aa73639f0ad0}} . Thus, using standard facts about geometric convergence and the uniform mixing time, we can conclude that for a universal constant {{formula:3849b3cd-34fe-4d91-924f-a9cd6d313b03}} we have {{formula:eabc5f68-0588-4256-a1f2-6052849d357e}}for all {{formula:575c23a8-da62-4e78-bb6a-e0f4837277b0}} .Truncated Thorp shuffle. Fix {{formula:3605d8f6-89df-478d-a28e-c94b4161c0b9}} . Define the {{formula:311edbda-62a0-412e-8acd-9523f2ff"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "default",
      "expected_output": "The research paper discusses phase fluctuations in a crossover filamentary state and analyzes the correlation length. For alpha values less than 1.15, the correlation length follows the expected BKT scenario. However, for alpha = 1.15, the behavior deviates from the BKT scenario, and short-range superconductivity is observed.",
      "context": [
        " phase fluctuations in this crossover filamentary state, we analyze the correlation length \\xi ^{xy}. In Fig. REF we present \\xi ^{xy} for \\alpha =1,\\,1.05,\\,1.1,\\,1.15 and L=32,\\,64,\\,128,\\,256, as found by fitting the correlation function in Eq. (REF ). For \\alpha <1.15, we find that \\xi ^{xy}\\sim L, thus following the expected BKT scenario, thereby justifying the BKT analysis discussed above. Note that the black dashed lines marks T_\\text{BKT} found from the crossing of J_\\text{s} (Fig. REF ). For \\alpha =1.15, although J_\\text{s} vanishes at L=128, we can still observe BKT-like features of \\xi ^{xy}: the saturation value at low temperatures is in fact increasing with L, with some slowing down for L=256. Again, we mark the temperature T=0.2 with a dashed red line: this temperature corresponds to the maximum of J_\\text{s} found at L=16 and to",
        "-critical behaviour of the random-field Ising model and the correlation length saturates to a finite value that determines the typical size of the clusters.\nShort-range and filamentary superconductivity: We discuss now the survival of a filamentary superconducting cluster and the presence of short-range SC in the polycrystalline charge-ordered side of the phase diagram. Whereas in the case \\alpha =1.1 it was still possible to define a BKT transition, albeit with a certain degree of uncertainty, when the anisotropy parameter is increased up to \\alpha \\ge 1.15 no T_\\text{BKT} can be defined from the crossing point. In particular, at \\alpha =1.15 one can see from Fig. REF (a) that J_\\text{s} vanishes already at L=128. The point \\alpha =1.15 deserves, however, more attention since it displays both a finite critical temperature T_{\\text{CO}} and a short-range coherence of the superconducting cluster, as indicated by the finite value of J_\\text{s} at L=16,\\,32,\\,64. The substantial fraction of superconducting pseud",
        " panels a, b and c), which is lowered to 0.75 for \\alpha =0.9 while the critical temperature is only very slightly decreased. This can be appreciated looking at panels d, e, and f, where we show the relative crossing points with the universal critical line 2T/\\pi , indicating with a vertical line the corresponding T_\\text{BKT}. A first consequence of the random field is indeed visible in the smearing of this crossing at \\alpha =0.9, highlighted in grey.In panels a, b and c of Fig. REF instead we present the superfluid stiffness in the filamentary region of our phase diagram, namely \\alpha =1,\\,1.05,\\,1.1. The suppression of J_s caused by the presence of the correlated disorder emerging is much more visible here. In particular, we highlight the fact that, while for \\alpha =1,\\,1.05 the scaling law still produces efficient results (see panels a and b of Figs. REF and REF ), this does not seem to be the case for \\alpha =1.1 ((see panels a and b of Figs. REF and REF ). However, the extrapolated T"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main factors examined in the projections and how do they impact the research findings?",
      "expected_output": "The main factors examined in the projections are the counting polynomials and their corresponding fractions, which fully project on specific factors such as the 323af964-0021-4fc4-81db-66db239f43fa-factors, 4b15b891-940d-4dc8-acfe-4f2c4afe2481-factors, and 07eee8de-89f8-4389-b04a-ad9792c99726-factors. These factors impact the research findings by determining the number of fractions that are complement of a smaller one, resulting in 252 fractions being discarded, and ultimately concluding that there are 802 fractions remaining.",
      "context": [
        "factors. a counting polynomial is: {{formula:e94d5797-4c2d-4f00-bd5b-34f35130937c}} the fraction fully projects on the {{formula:323af964-0021-4fc4-81db-66db239f43fa}} -factors. 192 have {{formula:cac4873c-fbbd-4328-812a-4ff67dbcbe52}} ; they are the complement of the fractions with {{formula:4db40b8b-c6f9-4796-81e3-21d9001ade0e}} . 60 have {{formula:c1d65452-efb7-48db-9bab-cdd531c5ba08}} ; they are the complement of the fractions with {{formula:c3c7ad00-340a-4d00-bbf8-6ca10a5b6eef}} .See the original paper for more details. Discarding the 192 + 60 = 252 fractions that are complement of a smaller one, in conclusion there are 1054 - 252",
        "0980}} -factors. a counting polynomial is: {{formula:c8b050f6-2e4e-4e86-8550-b9348187b784}} the fraction fully projects on the {{formula:4b15b891-940d-4dc8-acfe-4f2c4afe2481}} -factors and on the {{formula:07eee8de-89f8-4389-b04a-ad9792c99726}} -factors. an counting polynomial is: {{formula:74e1acd2-9be4-45bb-85de-c7991ea2b68d}} the fraction fully projects on the {{formula:f745baa6-fbbd-4077-8fcd-508fc1fec1fe}} -factors and on the {{formula:a5c2386d-a04e-4a3f-9a8e-8e2f644c4125}} -factors. a counting polynomial is: {{formula:e94d5797-4c2d-",
        " on the {{formula:903ddc58-9cd8-4502-a628-242a762e43e8}} -factors, on the {{formula:7a723c4f-b930-4cd9-aa44-6b1be59187a0}} -factors and on the {{formula:5af9cb74-cc8d-4e62-b166-e0cba4954e30}} -factors. 20 have counting polynomial of the form: {{formula:c010101c-b9ac-4a5c-b53b-61409eb3275e}} the corresponding fraction has resolution 3 and projectivity 2; moreover it fully projects on the {{formula:8d04acb7-a3bc-4b15-b82f-3bcff3264d48}} -factors, on the {{formula:9e5156ea-c377-4b68-9aec-0ccf7b640860}} -factors and on the {{formula:f7b5497f-10a3-4a74-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on Frobenius action and how do the methods used impact the key results?",
      "expected_output": "The research paper explores the Frobenius action on supersingular elliptic curves, specifically looking at the exceptional curves and proper preimages associated with certain points. The main findings include the identification of unique invariant curves and points on the curve, which are generated by specific points. The methods used, such as analyzing the Frobenius action and its effects on the curve, impact the key results by providing insight into the structure and properties of supersingular elliptic curves. For example, Corollary 2 discusses the implications of these findings for supersingular elliptic curves, highlighting the importance of understanding the Frobenius action in this context.",
      "context": [
        "c7b-b230-49f9-999b-d15c872502d3}} .For more clarity on what is going on, see Figures REF , REF , where arrows denote the Frobenius action. In the second figure {{formula:a2601376-186e-4dea-8440-8aa4d736a264}} are the exceptional curves associated with the points {{formula:f2a868d2-f44c-426a-b2a7-ff7a919eb76a}} and {{formula:c88a615f-19b5-40c4-bb99-46ef5ed9ad2d}} (resp. {{formula:27f170a9-7f75-4971-a8e3-c13cb6074e24}} ) are the proper preimages of {{formula:463295b8-495d-4522-bacf-0c3417136876}} (resp. {{formula:be3ba258-1211-4238-a8fa-a5f4983c9392}} ) with respect to {{",
        "c6cd3c1dc9d}} see ) and the lines {{formula:14a168db-365e-4518-9993-4caee2d5cf4e}} {{formula:f31b5d2f-9a0b-4669-9cd3-46d4db05e2f5}} resp. {{formula:ec760783-d19e-49e9-b335-7b8545220afa}} are the unique {{formula:c14328ad-30bd-4524-a104-fb04dbbd63d8}} -invariant curves {{formula:992ccd77-3358-4f95-8500-81d367c6b78e}} resp. points{{formula:44f95300-8072-4424-96ae-41461a7aa240}} on {{formula:249a44ec-2bc6-4540-96dd-3ac2078fbee7}} . Moreover, they are {{formula:6e00e59b-c7e7-4382-b7f4-1fc667ee",
        "771-e00b-4357-8f01-c398997dc56a}} is generated, for example, by the points {{formula:ead9bed8-9734-435b-ba90-912d5bf70d5a}} , {{formula:425643c3-ef6d-448b-b978-b226b0843b2c}} . Therefore {{formula:3ba59c65-b641-4bc3-b013-fe60b67626ca}} and thus {{formula:0ae0cd8e-bacc-455f-a0de-1a634c339b55}} for an appropriate {{formula:c0c66169-d16c-4933-9926-8e5f608c3dd0}} .Corollary 2 For {{formula:e4f15db2-3e2f-49c4-80ed-2e3ccb0e7332}} supersingular elliptic curves {{formula:9ec65e50-137c-41b9-9aff-e7588ed7b89d}} i."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do two qubits map to a qutrit in the context of quantum computing research, and what are the key findings related to this mapping?",
      "expected_output": "default",
      "context": [
        " quantum device. To this end, we employ two qubits to describe a plaquette (i.e., a qutrit). The explicit mapping is given as |1\\rangle \\rightarrow |00\\rangle , |3\\rangle \\rightarrow |01\\rangle , and |\\bar{3}\\rangle \\rightarrow |10\\rangle . Namely, we put a pair of qubits in the center of the plaquettes. We define the projectors acting on two qubits asM_1 &=& \\frac{1+Z}{2}\\otimes \\frac{1+Z}{2} , \\\\ M_{3} &=&\\frac{1+Z}{2}\\otimes \\frac{1-Z}{2} , \\\\ M_{\\bar{3}} &=&\\frac{1-Z}{2}\\otimes \\frac{1+Z}{2} , \\\\ M_{\\phi } &=&\\frac{1-Z}{2}\\otimes \\frac{1-Z}{2} .Using them, the Casimir (REF ) is written asM^{(q)}&=2M_3+2M_{\\",
        "=M^{r(q)}-2M_{\\phi }\\otimes M_{\\phi }+2M_{1}\\otimes M_{\\phi }+2M_{\\phi }\\otimes M_{1} \\\\ &=\\frac{1}{2}+\\frac{1}{4} (Z_1+Z_2+Z_3+Z_4) \\\\ &-\\frac{1}{2} (Z_1Z_3+Z_2Z_4)-\\frac{1}{4} (Z_2Z_3+Z_1Z_4).The additional terms do not affect the logical qutrit state |\\Psi _{\\rm trit}\\rangle since they satisfy M_{\\phi }|\\Psi _{\\rm trit}\\rangle =0. Unfortunately, there are still a few nonlocal CZ gates. For the implementation of the remaining nonlocal CZ gates in Eq. (REF ), we rely on optimization of circuits using Qiskit's transpiler .Next, we explain the qubit representation of the plaquette operator. We implement the plaquette operator so that it is still a unitary operator in",
        "rangle =\\sum _{a_1,a_2,a_3,a_4\\in \\lbrace 1,3,\\bar{3}\\rbrace }\\psi (a_1,a_2,a_3,a_4)|a_1,a_2,a_3,a_4\\rangle .The dimension of the Hilbert space is scaled as 3^N with N being the number of plaquettes, and thus the system can be described as a S=1 quantum spin chain i.e., a qutrit chain.In what follows, we derive the explicit spin representation of the q-deformed SU(3)_1 Kogut-Susskind Hamiltonian to implement it on quantum processors. To this end, we follows Ref. , and move to the unitary equivalent basis by applying F-moves to rungs. Here, the F-moves are defined as\\parbox {2.cm}{\\includegraphics [scale=0.3]{fig2_4.pdf}} =\\sum _f[F^{a\\bar{b}c}_d]_{ef}\\parbox {2.cm"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What changes occur in the magnetic structure of Fe4Si2Sn7O16 at a temperature of 1.6 K?",
      "expected_output": "At a temperature of 1.6 K, new low-angle Bragg peaks emerge for Fe4Si2Sn7O16, indicative of 3D long-range ordered magnetism, and the magnetic structure shows no further change down to at least 0.1 K.",
      "context": [
        " REF ). The same peaks are observed at 0.1 K, i.e., the magnetic structure shows no further change down to at least this temperature; and at 1.8 K for Fe{{formula:ff68d7c8-58dd-458e-8e78-2bd69d68000d}} Mn{{formula:603507ab-a666-44f2-9f37-d6f505e783ba}} Si{{formula:844fdbef-641c-4464-8a6b-f1e8a89372be}} Sn{{formula:3d369924-e551-4609-ac28-762277da4989}} O{{formula:ddc94c82-6b6a-415f-9ff4-256d6da7f9d9}} , i.e., the magnetic structure is not fundamentally changed by almost complete substitution of Mn{{formula:66548233-4ade-411a-94d6-2b13fcbd026c}} for Fe{{formula:219ca97a-7f02-42fd-ba89-c2",
        " in a liquid-like rather than a frozen state – down to at least 1.65 K. A heavily Mn-doped sample Fe{{formula:8d782a22-336c-4ff3-a400-a40b6780873a}} Mn{{formula:2c04ef3b-74ec-4f07-9020-e2967cdd49fc}} Si{{formula:b8cfdb63-695b-42ac-be8c-b3b8ec5956ae}} Sn{{formula:9e188fdb-a684-4961-9ae5-3d8ef1f1688f}} O{{formula:d5e06965-71ad-4135-ac15-97cffa44bd17}} has the same magnetic structure. Although the propagation vector {{formula:936b3a94-d5c8-49ab-af19-7df12aef13fd}} breaks hexagonal symmetry, we see no evidence for magnetostriction in the form of a lattice distortion within the resolution of our data. We discuss the relationship to partially frustrated magnetic order on the py",
        ":cd993850-2005-4fb4-9235-a01b6bf70290}} Sn{{formula:d759065b-0741-4b38-b821-85feb2543e02}} O{{formula:12722385-f6d8-4da1-b745-c71417cd0c82}} . Rietveld-refinements of the nuclear structure above {{formula:a1cb93a9-2d8b-4364-ab1a-c5e893b92e9b}} were consistent with our previous work using single-crystal X-ray diffraction and a combination of synchrotron X-ray and neutron powder diffraction. New low-angle Bragg peaks emerge for Fe4Si2Sn7O16 at 1.6 K, below {{formula:c898d6dc-b3f2-4445-a49f-90751c9b63cd}} , indicative of 3D long-range ordered magnetism (Fig. REF ). The same peaks are observed at 0.1 K, i.e., the magnetic structure shows no further change"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between ANK and alternative formulations in the context of axially symmetric spacetimes, and how do these differences impact the research findings?",
      "expected_output": "The main differences between ANK and alternative formulations in the context of axially symmetric spacetimes are the definitions used for the center of mass and angular momentum. The ANK approach uses a specific formula that includes a term missing in alternative formulations, and has a different factor with an opposite sign in front of the second term. These differences impact research findings as they lead to non-vanishing extra terms when octupole data is included, and the ANK definition must be changed when including higher multipole moments. For example, the ANK formulation only defines angular momentum for quadrupole radiation, whereas alternative approaches may have different definitions for higher multipole moments.",
      "context": [
        " axially symmetric spacetimes is a clear indication that the ANK definition must be changed when including higher multipole moments . It is worth mentioning that only for quadrupole radiation both formulae agree. We obtain non vanishing extra terms when octupole data is included (see appendix ).When comparing the relationship between the center of mas worldline, and spin and the geometrical quantities at null infinity like the Bondi mass, momentum, etc., we will only consider quadrupole radiation data.In the ANK approach one has {{formula:803b3119-40bd-4385-a552-f0c8d505f54e}}where {{formula:3304e894-78bf-4677-a96a-551cf1522c82}} is the center of mass worldline. In our formulation, from eq. (REF ) we get {{formula:bb2fdbc7-eaed-4378-9ed3-7e4b8d18d75b}}The main difference between the above equations is the last term in the ANK, which is missing in our equation. Note also a different factor with an opposite sign in front of the second term",
        "01af4d2a2}}The relationship between the mass dipole moment and angular momentum with the asymptotic fields at null infinity are different in both formalisms. These differences are a consequence of the definitions used in both formulations. Whereas in our approach we integrate a two form with values on the BMS algebra, in the ANK approach one directly uses {{formula:1e3b2dce-6ae7-4b44-a6ee-688923f1a09e}} for the definitions.The angular momenta in the ANK formulation is only defined for quadrupole radiation, where most of the definitions available in the literature agree. However, one could forsee potential problems for {{formula:bd62ff68-65e1-415a-8856-407370b50632}} if one considers higher multipole moments in the radiation data and/or spacetimes with symmetry. The fact that {{formula:83da3895-2a53-45b5-8d36-412b59103e55}} is not conserved for axially symmetric spacetimes is a clear indication that the ANK definition must be changed when including higher multipole moments",
        " As a non complete list of authors we could mention Dray and Streubel ,Bramson , Geroch , Helfer Moreschi , Penrose and Winicour . Although a recent living review offers a complete survey of the main results in the field with the main motivations and technical aspects of each definition, the fact that there is no agreement among these alternative approaches reflects the difficulty of the subject. However, there is a common link between them that can be used as a starting point: all the approaches agree for quadrupole radiation.This fact has been used in the Adamo-Newman-Kozameh approach. By restricting themselves to quadrupole radiation data, it is shown that both the center of mass and angular momentum are defined from an asymptotic Weyl scalar whose {{formula:be8252a6-d110-4270-a5c0-cb400c238277}} part of the spherical harmonic decomposition transforms as a 4-dim two form under the action of the homogeneous Lorentz algebra of the BMS group, the available kinematic geometry of null infinity . Moreover, the ANK formulation is the only one that gives equations of motion for both the center of mass and"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research paper on coding theory, specifically regarding the application of the Gilbert-Varshamov bound to FH sequences, and how do the methods used impact the key results?",
      "expected_output": "The research paper establishes a connection between cyclic codes and frequency hopping (FH) sequences, allowing the application of the Gilbert-Varshamov bound from coding theory to FH sequences. The main finding is that the Gilbert-Varshamov bound can be transformed to provide a lower bound for FH sequence sets, which is asymptotically good. This means that as the length of the sequence increases, the bound tends to a constant value, providing a positive relative distance and rate over a fixed alphabet size. The methods used involve reformulating bounds from coding theory, such as the Singleton bound, to derive tighter lower bounds on the maximum Hamming correlation of FH sequence sets. The key results include a new lower bound on the size of FH sequence sets and the implication that most upper bounds on FH sequence sets are not achievable in certain scenarios.",
      "context": [
        "# arXiv Paper: 1810.11757\n\nAsymptotic Gilbert-Varshamov bound on Frequency Hopping Sequences\nabstract: Given a $q$-ary frequency hopping sequence set of length $n$ and size $M$ with Hamming correlation $H$, one can obtain a $q$-ary (nonlinear) cyclic code of length $n$ and size $nM$ with Hamming distance $n-H$. Thus, every upper bound on the size of a code from coding theory gives an upper bound on the size of a frequency hopping sequence set. Indeed, all upper bounds from coding theory have been converted to upper bounds on frequency hopping sequence sets (\\cite{Ding09}). On the other hand, a lower bound from coding theory does not automatically produce a lower bound for frequency hopping sequence sets. In particular, the most important lower bound--the Gilbert-Varshamov bound in coding theory has not been transformed to frequency hopping sequence sets. The purpose of this paper is to convert the Gilbert-Varshamov bound in coding theory to frequency hopping sequence sets by establishing a connection between a special family of cyclic codes (which are called hopping cyclic codes in this paper",
        ", the most important lower bound–the Gilbert-Varshamov bound in coding theory has not been transformed to frequency hopping sequence sets. The Gilbert-Varshamov bound in coding theory is asymptotically good in the sense that the length {{formula:e05a8fe2-e331-48d1-9610-4547e40ff345}} tends to {{formula:0ec1eed7-b72b-4b91-9c89-e0db51d49b39}} and the alphabet size {{formula:d1ed6963-c8d4-41f6-84ae-745ca8b3ac8c}} is fixed. This scenario is interesting from both practical and theoretical points of view for FH sequence sets. In this scenario, most of the upper bounds on FH sequence sets mentioned above are not achievable. The Gilbert-Varshamov bound in coding theory characterizes the existence of good codes with positive relative distance and positive rate over a fixed alphabet size. Considering the strong connection between FH sequences and cyclic codes, one might be curious about whether there exists a similar lower bound for the FH sequence set. We emphasize that the",
        ". Peng and Fan obtained a lower bound on the maximum Hamming correlation of an FH sequence set (or an upper bound on family size). In fact, this bound can be obtained from the Plotkin bound in coding theory. As an FH sequence set gives a nonlinear cyclic code, every upper bound on the size of a code in coding theory can be converted into an upper bound on the size of an FH sequence set. Indeed, Ding et.al presented three upper bounds on the family size of the FH sequence set derived from the classical bounds in coding theory. By reformulating the Singleton bound in , Yang et.al obtained a new and tighter lower bound on the maximum Hamming correlation of an FH sequence set. Liu et.al further improved this bound. Note that a lower bound on maximum Hamming correlation also implies an upper bound on the family size. There are many attempts to construct optimal sets of FH sequences meeting these upper bounds on the family size , , , , .On the other hand, a lower bound from coding theory does not automatically produce a lower bound for frequency hopping sequence sets. In particular, the most important lower bound–the Gilbert-Varshamov bound in coding theory has not been transformed to frequency hopping"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do image vortices and anti-vortices satisfy boundary conditions in the context of this research paper?",
      "expected_output": "In the context of this research paper, image vortices and anti-vortices satisfy boundary conditions by ensuring that any flow at the boundaries is tangential, meaning the normal component of the flow at the boundaries is zero. This is achieved by having an infinite array of image vortices beyond the channel, which creates a flow that cancels out the normal component of the flow at the boundaries, thus satisfying the boundary condition.",
      "context": [
        ".V. Kusmartsev Department of Physics, Loughborough University, Loughborough LE11 3TU, United KingdomConsider a vortex of vorticity {{formula:cade554a-6a7c-4c96-bbbf-65e6817ab8bd}} at an arbitrary position {{formula:978c5cd0-6c24-413a-8d45-a92a6ccbf3d7}} , in the channel. The correct boundary condition on {{formula:e4ec47c1-f824-4891-8983-abf508a9b882}} is that any flow at the boundaries should be tangential, i.e., the normal component of the flow at the boundaries should be zero. This is achieved, in analogy with 2D electrostatics, by having an infinite array of image vortices (each of vorticity {{formula:b9d3442c-d237-4da3-8be5-5f5c5dc9daae}} ) beyond the channel at {{formula:13c66940-0efd-4c6d-b53e-6e",
        " at {{formula:13c66940-0efd-4c6d-b53e-6e8aa25d6a87}} and an infinite array of image anti-vortices (of vorticity {{formula:3b062f83-4b7a-4680-b9b1-7fa2d73270ca}} ) at {{formula:49b9ff1f-ffdd-4234-8ae0-315947824e2a}} , where {{formula:9ec3aaf4-61e7-4071-b260-83909baf2b54}} {{formula:4695866c-7063-4014-a675-b18a9029472d}}The energy required to create the vortex at {{formula:3ced850f-773a-4ab0-8121-4734930676aa}} is the energy with which it is attracted to the boundaries. This can be evaluated by calculating the energy of interaction of the vortex with all its image vortices and anti-vortices. We do this in analogy with 2D hydrodynamics (see",
        " image vortices and anti-vortices. We do this in analogy with 2D hydrodynamics (see, for example, Newton and references therein): consider an infinite array of vortices, at positions {{formula:d0955c69-5cd8-45cc-a162-b94f0b3253b6}} , each of strength (circulation) {{formula:9140c130-f2c5-4151-bb52-e398393eb0b4}} ({{formula:7e6732c4-f0e6-4fc6-9d0c-8a16f75883ae}} is the superfluid density) and an array of anti-vortices each of strength {{formula:9e91df09-a4aa-44c6-8dfc-55e1768a5035}} at positions {{formula:82f77aa2-9fc9-4d48-9869-3184a4d6aae1}} . Then the stream function {{formula:e205316f-c2b7-4554-bc6c-e981"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings related to periodicity in the given formula 380b37e3 and how do the conditions affect these findings?",
      "expected_output": "The main findings related to periodicity in the given formula 380b37e3 show a periodic solution that fulfills the periodicity condition. The periodic orbit exists when the system meets specific conditions, and the number of oscillators affects the period of the periodic wave. For example, despite having a limited number of oscillators, the periodic wave can have a longer period, resulting in a system with more equations to solve.",
      "context": [
        "-ab83-0ccc4c6fc539}} -periodic solution given by {{formula:380b37e3-2678-4b89-8070-9e1a2e9e1635}} where {{formula:ea0191a2-90a3-446f-bcd8-7c8d5248f2be}} with {{formula:468ab9f3-f852-4d52-b6bd-51471579c8c2}} where {{formula:60852da2-683f-4fd0-8a18-77accb22b9f2}} represents a multiple of {{formula:2d5906d2-dd26-4277-b18f-8d2ad25d610b}} . This {{formula:dc546332-0889-4f54-bc99-97d6d0d10d48}} -periodic solution fulfills the periodicity condition {{formula:fb1d2998-f00b-442e-aed5-aaee5802bc14}} Proof Note: Notice that although there are just {{",
        "82ec-1007e1bd310d}} , {{formula:6231042e-2b0f-4ce3-98e5-b000c38229a3}} , then the CML given by {{formula:a11f9252-7177-4ac2-a43d-4d568f3c893f}} shows a {{formula:4f3b7ed0-bf67-41c3-acdd-c7425bd2a760}} -periodic solution given by {{formula:ed1b018c-95a5-466d-98da-42b8552a2892}} where {{formula:ff9b5b3e-64d2-498c-9e74-1301f47f76b4}} with periodic conditions {{formula:029ba2a2-0005-4655-87af-53e2fc5de435}} Proof The periodic orbit given by {{formula:94d14c7f-428b-488f-a098-73261403283b}} will exist when the following system {{formula:238fc0df-",
        "b-442e-aed5-aaee5802bc14}} Proof Note: Notice that although there are just {{formula:f4ddecf5-183c-4d25-a1c3-381929df6028}} oscillators the periodic wave will have period {{formula:3d34d212-932f-4430-b65d-2b0ebbac874b}} , therefore, the system that is dealt with in the proof will have {{formula:6d4a2842-eced-452b-82a6-1ca008bbf9d8}} equations. Let {{formula:84944927-7533-4f75-ae9e-4ed39a374ac2}} oscillators be with the initial conditions given by {{formula:7d595fbe-2b74-4262-9bb5-253392b55957}} Initial conditions are fixed points of {{formula:07ff8a1a-4b4a-46f5-b8f3-0fcea6fb9c01}} (taken one every {{formula:dd"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the research paper propose to mitigate precision explosion in MP, and what are the key findings regarding its effectiveness?",
      "expected_output": "The research paper proposes two methods to mitigate precision explosion in MP: using the MP operation, which involves addition and generates lower-bit output, and having an online training system. The key findings show that MP avoids numerical precision explosion caused by conventional MAC operations and that the quantization error can be mitigated by online training, resulting in lower accuracy loss.",
      "context": [
        " These systems do not offer a complete end-to-end multiplierless computation.Traditionally, energy efficiency can be achieved using a quantization technique instead of approximate computation, like fixed-point precision . However, such linear quantization techniques result in accuracy loss as the data is represented uniformly. To mitigate this error, adaptive quantization can be used as described in . This technique uses a measurement model to estimate the correct quantization for all the parameters of the classification system. Implementation of such a system is not feasible in the case of edge devices as there is the overhead of estimating the quantization levels, compromising the device's energy efficiency. Representation of the entire framework in the bfloat number system can provide similar effects of adaptive quantization . {{figure:86a96f6d-74ce-4390-b14c-91eec626a5da}}{{figure:fb44e5dd-cd28-4aba-b668-402c1a0c59c6}}In , we see an end-to-end multiplierless system using the MP approximation technique for kernel machines. We extend the capabilities of this framework in our current work where we use the feature extraction used as kernel function, as used in and , and implement",
        " generate 20-bit output, and the final output layer will have 44-bit output. If we quantize this output to 8-bit, we may lose a substantial amount of accuracy. In contrast, the MP operation, which involves addition, would generate 18-bit at the output, which would result in lower accuracy loss. Thus, MP avoids numerical precision explosion caused by conventional MAC operations. Moreover, the quantization error can be mitigated by having an online training system, which has been shown in .[subfigure]labelformat=empty {{figure:007a500d-63e7-47ee-902d-a494f6e51afc}}\nMultiplierless Kernel Machine using MP: We develop a classification framework based on multiplierless kernel machine using the MP approximation . Consider a vector {{\\mathbf {x}}} \\in \\mathbb {R}^d, the decision function for kernel machines is given as,f({{\\mathbf {x}}}) = {{\\bf w}}^T{{\\bf K}} + \\mathbf {b}.Here, {\\bf K} is a function of {\\mathbf {x}}. Following the derivations in , we can rewrite eq.(REF ) in MP",
        " of this framework in our current work where we use the feature extraction used as kernel function, as used in and , and implement this kernel in MP approximation using the MP principle in . The resulting framework is a one-of-a-kind digital hardware implementation of a multiplierless acoustic classifier with a feature extractor used as a kernel.\nIn-Filter Computation using Margin Propagation Kernel Machine: In-filter computation described in and , combines the feature extraction and non-linear SVM kernel into a single function as opposed to a traditional SVM as shown in Fig.REF . We leverage this principle, use an FIR filter as the kernel function, and implement this framework using MP-based approximation. MP-based kernel machine has proven to be an energy-efficient system for implementing a classification framework for edge devices .\nPrecision Explosion and MP as Adaptive Quantization: Consider a fully connected single hidden layer network. In the case of a fully connected network, for a 8-bit quantized inputs, the hidden layer output will generate 16-bit output from multiplication. For the 16-dimension input vector, the hidden layer will generate 20-bit output, and the final output layer will have 44-bit output. If we quantize this output to"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How is the 2500Å luminosity calculated from the 2MASS magnitudes in this research paper?",
      "expected_output": "The 2500Å luminosity is calculated by extrapolating the observed 2MASS magnitudes, which probe rest frame 4000-7000Å, using the mean quasar SED. The observed magnitude from 2MASS is used to calculate the observed flux, and then the luminosity in the rest frame wavelengths is calculated. The luminosity at 2500Å is then extrapolated from the observed flux using a specific formula.",
      "context": [
        "ula:acfff429-5884-4222-91a5-a1d249157a34}} Hz{{formula:6e3d8f54-6a1a-4a12-b5be-4f0625b62a01}} for luminosity, {{formula:12d6054e-5851-4dc4-911f-5a55a14a9103}} . All fluxes and luminosities are upper limits, except for the 3{{formula:ca5ef08e-573c-4dea-b5d5-5efd9f8bfc1e}} detection.\nThe 2500Å Luminosity: We calculate the rest frame 2500Å luminosity, {{formula:ff9a085c-94d8-47cf-9c12-6754c3090de2}} , by extrapolating the observed 2MASS magnitudes (probing rest frame 4000–7000Å for {{formula:6e20b6f7-e6b0-4165-97ab-bb8695f18c5f}} objects) using the mean quasar S",
        "c}} is the observed magnitude from 2MASS; {{formula:d436b516-5e59-472e-90f3-ae802a2b7b19}} is the observed flux in the {{formula:75775bc0-207a-42dd-beba-dcb9c0449806}} band; {{formula:1e277e2b-4a78-482f-b78d-e5e0bec57874}} is the luminosity in the rest frame wavelengths of the {{formula:6af58350-99e9-4e5c-91bb-5d99831cd1fa}} band for each object; The last three columns contain the luminosity at 2,500Å as extrapolated from the {{formula:eeaa2480-c451-4157-b005-28210e5d615a}} , {{formula:a185cd58-a2bd-4436-8d3b-d0ac50611664}} , and {{formula:26297617-1ab8-4edf-898a-e0dd93b4cdd7}}",
        "ula:48be3404-614d-4784-b706-c947a4595a94}} , is given by, {{formula:7eb2481c-6851-4704-86fb-d04764bb53e6}}where {{formula:a4d1cb81-b928-496e-ac16-0fee02071ed0}} is a constant found analytically for the 2–10 and 0.2–2 keV ranges.From the simple relation in Equation REF , we interpolate the value for the monochromatic flux density at the observed energy {{formula:29cd6996-99a1-4b7b-b99e-4a2feb22aee7}} using, {{formula:47ccf437-791e-4e6b-a3e0-7c6754d0f684}}We find the 2 keV rest frame luminosity ({{formula:6657a5c6-00bd-4b1d-8912-68a3bf5d6fdf}} ) from the corresponding observed fluxes using, {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does SA reduce model outcome uncertainty, and what methods are used to achieve this reduction in the research paper?",
      "expected_output": "Sensitivity Analysis (SA) reduces model outcome uncertainty by identifying the most influential input variables and quantifying their effects on the model output. In the research paper, the Morris one-at-a-time (MOAT) method, a type of Global Sensitivity Analysis (GSA), is used to achieve this reduction. The MOAT method evaluates the sensitivity of 23 tunable parameters corresponding to seven physical parameterization schemes of the WRF model, and the sensitivity measures are calculated for eleven output variables. By applying the MOAT method, the study aims to recognize the parameters that strongly influence the model performance, thereby reducing the uncertainty in the model outcomes.",
      "context": [
        "umerical or otherwise) under alternative assumptions, owing to the uncertainties in the model inputs, to ascertain the influence of different input variables (i.e., parameters). SA finds its application, among the other things, in robustness evaluation, uncertainty reduction, error search, and model simplification. However, the prime focus of the present study is to identify the model input variables that cause substantial uncertainty in the model outcome, which then can be further researched for improved robustness of the model. Sensitivity analysis employed in the current study comprises three significant steps: (i) selection of an accurate model to be used and the identification of the adjustable model input variables, (ii) quantification of uncertainties (i.e., ranges and probability distributions) in each parameter, (iii) choice of a suitable sensitivity analysis method to evaluate parameter sensitivity.The WRF model applied to the Indian summer monsoon, and a set of 23 parameters, with uniform distribution, corresponding to various physics schemes are considered for the SA in the present study. The ranges and variations of all the model input variables are presented in the next section. The usage of computationally expensive variance-based methods is not feasible as the number of model input variables is too large. At the same time, the application of group",
        " identifies the model input variables that have a considerable impact on the model outcome and later enables the adjustment of parameters to obtain the ideal solution. Sensitivity analysis (SA) is the most frequently utilized statistical tool to recognize the most important parameters when there is uncertainty involved. There are various SA studies on uncertainty quantification of parameters which concentrates on parameters of a particular scheme , , , . A few studies comprehensively considered many parameters across various physics schemes , . But these studies are conducted over small regions. The objective of the present study is to apply the SA methods to parameters corresponding to seven different physics schemes on a larger region (Monsoon core region) when the output variables are evaluated for sensitivities concerning high-intensity precipitation events during the Indian summer monsoon.This paper is organized as follows. Section gives an introduction to the Morris one at a time SA technique. Section gives a detailed description of the parameters, physics schemes, and events considered for the SA. Section lists out the critical results and inferences related to the sensitivity measures.\nSensitivity analysis method: Sensitivity Analysis (SA) is defined as the recalculation of outcomes of the model (numerical or otherwise) under alternative assumptions, owing to the uncertainties in the model inputs, to ascertain the influence of different input",
        "# arXiv Paper: 2003.01353\n\nAssessment of WRF model parameter sensitivity for high-intensity precipitation events during the Indian summer monsoon\nabstract: Default values for many model parameters in Numerical Weather Prediction models are typically adopted based on theoretical or experimental investigations by scheme designers. Short-range forecasts are substantially affected by the specification of parameters in the Weather Research and Forecasting (WRF) model. The presence of a multitude of parameters and several output variables in the WRF model renders appropriate parameter value identification quite challenging. The objective of the current study is to reduce the uncertainty in the model outcomes through the recognition of parameters that strongly influence the model performance using a Global Sensitivity Analysis (GSA) method. Morris one-step-at-a-time (MOAT), GSA method, is used to identify the sensitivities of 23 chosen tunable parameters corresponding to seven physical parameterization schemes of the WRF model. The sensitivity measures (MOAT mean and standard deviation) are evaluated for eleven output variables, out of which some are surface meteorological variables and the remaining are atmospheric variables, which are simulated by the WRF model with different parameters. Twelve high-intensity four day precipitation events during the Indian summer monsoon (ISM)"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between manifold construction and wave properties, and how do the methods used impact the key results?",
      "expected_output": "The research paper presents a method to describe wave properties of particles by constructing a manifold where mass particles move along geodesic lines. The main findings include the equivalence of describing a mass particle on the manifold and a photon in the Minkowski four-space. To achieve this, the paper proposes building an operator similar to the d'Alember operator, known as the de Rham operator, on the constructed manifold. The equation derived from this process is expected to describe the wave properties of mass particles, with the potential being the field potential. For example, the paper mentions the de Broglie hypothesis as a basis for formulating the wave properties of particles, highlighting the importance of manifold construction in understanding particle behavior.",
      "context": [
        " along isotropic geodesic lines of this manifold for any motion.Hence, description of a mass particle on the manifold {{formula:f4a10994-9398-45c6-ba30-3cc47f10d850}} and description of a photon in the Minkowski four-space {{formula:88d2748a-cc32-4eb3-981a-d3b0398c12db}} becomes equivalent. And so, we can formulate the de Broglie hypothesis as follows:to describe wave properties of particles it is necessary - to build the manifold {{formula:daf12e56-a3eb-4d1d-be98-c4cad4f37245}} in which mass particles move along geodesic lines of this manifold for any motions; - to build an operator the similar to that d'Alember on the manifold {{formula:2627cf36-dbd5-4de9-a745-7119e3c0078d}} (so-called the de Rham operator {{formula:8cf2e612-204c-4b88-a1ce-35070c1",
        "-4de9-a745-7119e3c0078d}} (so-called the de Rham operator {{formula:8cf2e612-204c-4b88-a1ce-35070c1ff730}} );to build the manifold {{formula:daf12e56-a3eb-4d1d-be98-c4cad4f37245}} in which mass particles move along geodesic lines of this manifold for any motions;to build an operator the similar to that d'Alember on the manifold {{formula:2627cf36-dbd5-4de9-a745-7119e3c0078d}} (so-called the de Rham operator {{formula:8cf2e612-204c-4b88-a1ce-35070c1ff730}} );then the equation {{formula:5b033fc4-9b50-4236-867f-72e7c3c9fb92}}where {{formula:96e6c1c2-5207-46f0-ad0d-3df3c8468a83}}",
        " {{formula:8cf2e612-204c-4b88-a1ce-35070c1ff730}} ); then the equation {{formula:5b033fc4-9b50-4236-867f-72e7c3c9fb92}} where {{formula:96e6c1c2-5207-46f0-ad0d-3df3c8468a83}} is the {{formula:f0062679-b51e-4c9c-9061-29b709b0f55d}} -field potential should describe the wave properties of mass particles.to describe wave properties of particles it is necessary- to build the manifold {{formula:daf12e56-a3eb-4d1d-be98-c4cad4f37245}} in which mass particles move along geodesic lines of this manifold for any motions; - to build an operator the similar to that d'Alember on the manifold {{formula:2627cf36-dbd5-4de9-a745-7119e3c0078d}} (so-called the de Rham"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of metallic screening on QP corrections in this research paper?",
      "expected_output": "The main findings regarding the impact of metallic screening on QP corrections in this research paper are that the QP corrections are substantially reduced due to the metallic nature of the substrate, which screens the interaction inside the molecule non-locally. For example, the QP corrections are below 0.5 eV for adsorbed PTCDA, compared to 3.0 eV for gas-phase PTCDA. The research also shows that the QP correction turns out to be small, but there is a clear trend from 0.2 eV for a certain distance to 0.5 eV for another distance.",
      "context": [
        "-9e52-4521-b205-b1227251829a}}in which the QP self-energy correction {{formula:5a9e42a4-9842-4821-8106-5fee2d70c4b3}} results from the difference between the correctly screened Coulomb interaction ({{formula:d8e05f37-ddb8-49ab-a324-5f20be0432ff}} ) and an interaction {{formula:d0eb8604-0088-4d9b-bc4e-8fc88375477e}} from hypothetical metallic screening (see Ref. for details). The resulting QP corrections to DFT-LDA differ from standard {{formula:b218a9ec-549c-40cd-8a18-0d57fa6c3518}} QP corrections by less than 20 % , , , . For gas-phase PTCDA, LDA+{{formula:89a58846-a12e-45f2-8911-eeadea5bd1fa}} can easily be checked against a standard {{formula:37ce7287-04f",
        " establishing LDA+{{formula:693995f8-db29-4392-80ae-3e01af287fe3}} as a suitable method. {{figure:3badf173-94d9-413c-b0d5-f053d638433b}}When the molecule is adsorbed on the surface, three features show up in {{formula:b1f4a4c0-c170-4949-ae97-b91d5d069051}} and {{formula:1f2519bf-47a6-4caf-87ec-137b8680e02f}} : (i) Due to the metallic nature of the substrate, which screens the interaction inside the molecule non-locally , , , the {{formula:01424afb-236e-4f5d-afda-59d3df346c9a}} corrections are substantially reduced as compared to gas-phase PTCDA . As shown in Fig. REF a, the QP corrections are below 0.5 eV for {{formula:ebeb4fa1-528e-458d-9252-a430",
        "-phase PTCDA it amounts to 3.0 eV.Results for the effective QP shift {{formula:24611038-d538-4e6f-9569-8d7af547651a}} , the {{formula:4d448c6f-76f9-48e6-b924-e83144e27887}} parameter, and the self-consistent occupation {{formula:e5561c60-cb84-4c48-a279-edab389113f6}} are displayed in Fig. REF a-c. Although the QP correction turns out to be small, there is a clear trend from {{formula:dd7b1ad5-9ff1-4b5a-85c9-dada50c077fe}} eV for {{formula:488ba669-ad9e-4655-b37c-eb90c5c779f9}} = 5 Å to {{formula:649cb4fa-5c4a-4a01-bac6-97a1c682d0c0}} {{formula:c5a457f6"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the topcolor boson mass and the symmetry breaking scale in this research paper?",
      "expected_output": "The topcolor boson mass acts as the cut-off for the effective theory below the symmetry breaking scale, and it remains fixed as the scale at which the topcolor interactions would become strong if topcolor remained unbroken.",
      "context": [
        "}} , this potential comes from the sum of all planar diagrams involving one fermion loop as shown in figure REF , and is {{formula:7665a6ef-f877-414f-b85e-ee07b7e61c16}} . The flavor structure of this class of diagrams insures that the vectorial subgroup, {{formula:4114f530-756f-40bc-a74a-a8515225e6c5}} , remains unbrokenIn fact, for any QCD-like vector gauge theory, this remains true exactly . .Because {{formula:2c18ed63-b7fa-4418-8774-ba3abe7fbd88}} varies with {{formula:288c77d8-3f70-4c0b-95e6-94ab8c1e9444}} , we must be careful about what we mean by the scale of topcolor breaking. It will be most useful to define this scale as the Lagrangian mass of the topcolor gauge boson. This is appropriate because it is this mass which acts as the cut-off for the effective theory below the symmetry breaking scale. In fact",
        "c4371f-5b2d-4eaa-a7ca-012efd09494d}} and are the result of “vacuum-alignment” , , , .\nTopcolor: In top-color models , all or part of electroweak symmetry breaking is due to the presence of a top-condensate. In many models this condensate is driven by the combination of a strong isospin-symmetric top-color interaction and an additional isospin-breaking {{formula:ba7d8f56-6748-4326-8dcd-b6af6bace308}} gauge boson which couples only to the third generation of quarks. These additional interactions are strong, but are spontaneously broken at a scale {{formula:0417924c-3899-4866-b582-3df74d6923fa}} TeV. In the simplest model , the couplings of the third generation of quarks to the new {{formula:3f8cb2c5-ceda-4491-8dd6-a28882903837}} interaction were taken to be proportional to weak hypercharge, and the masses of",
        " appropriate because it is this mass which acts as the cut-off for the effective theory below the symmetry breaking scale. In fact we will soon see that the topcolor boson mass, {{formula:00b1ffa6-d1d2-4e78-ae1f-ce81a9da3348}} , remains fixed as {{formula:67ce327a-4dbf-4bdc-b0fa-fba1242ce8db}} , which means the the vacuum expectation value that is responsible for the breaking must actually grow like {{formula:abc8ec41-aeac-45bf-a50c-1ef4254a4191}} .Let {{formula:6ac4e494-76c2-44b0-9067-111e04005b45}} be the scale at which the topcolor interactions would become strong if topcolor remained unbroken, i.e. it is the analog of {{formula:bf3fa968-b222-4736-90fb-3ef111da9095}} for the ordinary strong interactions. {{formula:531ab231-d46d-42ba-9802-bb"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the research paper use to ensure semantic consistency in ART, and what are the main findings regarding its effectiveness?",
      "expected_output": "The research paper uses a multitask verifier model to ensure semantic consistency in ART, which utilizes semantic parsing and content features to calculate scores for both global consistency and spatial consistency. The main findings regarding its effectiveness are that the jointly optimized verifier model successfully guides the foreground adjustment and improves the global visual consistency, and the ART framework performs well on images with the human body as foregrounds, as demonstrated through experiments on a newly created dataset of 300K images.",
      "context": [
        " real if the foreground objects were put in the wrong location at the very beginning. Zhao et al. present a new image search technique. Given a specific rectangle bounding box on background image, the method returns the most compatible foreground objects to paste from given candidates. However, the model is still unable to find the best location and scale of the foreground, since the bounding box is directly given by human.To overcome these shortcomings, we proposed Auto-Retoucher(ART) framework to generate composite images that are harmonious both in appearance and location. A multitask verifier model will utilize semantic parsing and content features to calculate scores for both the global consistency and the spatial consistency. Then a gradient-based adjustment algorithm based on the surface of our verifier model will adjust the foreground object into the most suitable location. Since no previous work has been done on this specific task, existing datasets is not available. Therefore, we created a new dataset with 300K images for the Auto-Retouching task. Experiments are conducted on this dataset to empirically evaluate the effectiveness of our method. We prove that our model performs well on this dataset.To summarize, the main contributions of our work are four-fold:We proposed a novel multitask verifier model to evaluate the",
        " a challenging task in image editing. Current techniques for generating such images are heavily relied on user interactions with image editing softwares, which is a tedious job for professional retouchers. Some exciting progress on image editing has been made to ease their workload. However, few models focused on guarantee the semantic consistency between the foreground and background. To solve this problem, we propose a framework – ART(Auto-Retoucher)，to generate images with sufficient semantic and spatial consistency from a given image. Inputs are first processed by semantic matting and scene parsing modules, then a multi-task verifier model will give two confidence scores for the current matching and foreground location. We demonstrate that our jointly optimized verifier model successfully guides the foreground adjustment and improves the global visual consistency.\nIntroduction: The goal of most image editing tasks is to come up with an automatic model which could replace the human labor. Consider when someone wants to turn his life photos into tourist photos in Hawaii. This would be a mission impossible for a Photoshop layman. How to replace the background and adjust the foreground location simultaneously so that the retouched image looks consistent and semantically reasonable? Now we call this specific task as Auto-Retouching and propose a Auto-Retoucher(ART",
        "# arXiv Paper: 1901.03954\n\nAuto-Retoucher(ART) - A framework for Background Replacement and Image Editing\nabstract: Replacing the background and simultaneously adjusting foreground objects is a challenging task in image editing. Current techniques for generating such images relies heavily on user interactions with image editing softwares, which is a tedious job for professional retouchers. To reduce their workload, some exciting progress has been made on generating images with a given background. However, these models can neither adjust the position and scale of the foreground objects, nor guarantee the semantic consistency between foreground and background. To overcome these limitations, we propose a framework -- ART(Auto-Retoucher), to generate images with sufficient semantic and spatial consistency. Images are first processed by semantic matting and scene parsing modules, then a multi-task verifier model will give two confidence scores for the current background and position setting. We demonstrate that our jointly optimized verifier model successfully improves the visual consistency, and our ART framework performs well on images with the human body as foregrounds.\nAbstract: Replacing the background and simultaneously adjusting foreground objects is a challenging task in image editing. Current techniques for generating such images are heavily relied on user interactions with image editing softwares"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding marginal couplings and their impact on finiteness in the research paper?",
      "expected_output": "The research paper finds a finite theory with marginal couplings for every independent row, column, and diagonal line. It also proves the existence of additional marginal couplings when certain conditions are met. For example, in the case of the gauge theory, a single gauge group with branes and D5 branes stretched across the box can have at least one marginal operator, impacting the finiteness of the theory. The paper shows that in the limit where certain gauge couplings are zero, the theory has a conformal fixed point at low energy, demonstrating the significance of marginal couplings in determining finiteness.",
      "context": [
        " we find a finite theory, which in addition to the marginal coupling associated with {{formula:9f01018d-d60e-498e-a8a0-2f8fc9715c9d}} has marginal couplings for every independent row, column, and lower-left/upper-right diagonal line passing through the boxes.We will now give some examples of such models.\n{{formula:e3c1f35d-a4e8-4b7e-980d-95c710942bfc}} {{formula:1b16b06b-0c81-4c29-9d09-85e71b2f306e}} gauge theory: First consider {{formula:4a3bab5a-c62f-4dcb-ac94-3e0eaa74b997}} , as shown in figure REF . This choice gives a single gauge group {{formula:81986df6-2e79-49f4-927f-4e8caed47b34}} with {{formula:f0446fce-5389-43fa-ae99-c10dd8c",
        "691-44f1-afa8-99c8fc9798e5}} branes, and with {{formula:4438871f-75a8-4ce7-b5a3-b8b46fc4d8c6}} D5 branes stretched across the box in the {{formula:9f120437-6068-4c03-8516-925b0d937e99}} position, can have at least one marginal operator. We also prove the existence of additional marginal couplings when {{formula:30566222-9a15-45d6-b1a4-868355781881}} for all {{formula:0894f5f6-991d-44fe-99d3-62ff50afee67}} .As discussed in section 2, each box has a gauge group with a gauge coupling {{formula:da3c4479-bcec-4c81-8467-6873e52bd1d5}} , while each corner at which four boxes intersect has two Yukawa couplings {{formula:b1d2652e-26b5-413a-a",
        "f6-c8e6-4e81-8b6e-88733a06e91c}} , {{formula:c2e78389-9e2e-4029-9c00-95a8b2cabc18}} . The matter content is as in Eq. (REF ). The {{formula:5c821979-5263-4150-8220-ced78d2c76e9}} group factor has a positive one-loop beta function, the {{formula:867ca112-4887-4bc5-a260-aeeb8bdf7e9c}} group factor has a negative one-loop beta function, and so the theory is not finite and indeed is non-renormalizable. However, in the limit that the gauge couplings for all groups except {{formula:62dc71fe-a1c9-4d21-ac5e-edbe591e61e8}} are zero, the {{formula:55716ca6-99e0-435a-b35e-267e40f6be49}} theory has a conformal fixed point at low energy . Its matter"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how strain configurations affect the properties of graphene, and what methods were used to obtain these results?",
      "expected_output": "The main findings show that strain configurations significantly affect the properties of graphene. The research used Raman spectroscopy with circular light polarization to analyze strain configurations, including hydrostatic and shear strain components. This method allowed for the determination of local strain and doping levels without assuming the underlying strain configuration. The study found that for samples U1 and U2, the expected slope agreed well with the data points, while sample U3 showed a larger shear strain due to non-ideal loading. The methodology was also applied to complex nanoscale strain configurations, such as graphene suspended on gold nanodimers, where it was able to extract local strain and doping levels with nanoscale resolution.",
      "context": [
        " The expected slope agrees well with the data points extracted for sample U1 and U2 (see dots and squares in the inset of Fig. REF b). For sample U3, the shear strain at a given hydrostatic strain is larger than expected. In this experiment the PMMA beam was deflected several times before the measurement was taken, which led to a non-ideal load resulting in a different strain configuration. Extracting hydrostatic and shear strain components is useful for determining the strain configuration and testing the stress-strain transfer from a substrate to graphene. For samples U1 and U2 we observed good agreement of the uniaxial strain calculated from the beam deflection and the uniaxial strain calculated from the measured hydrostatic strain (Supplementary Fig. S3).\nComplex nanoscale strain configurations: So far, we presented one application of our methodology for strain evaluation. Using circular light polarization turned out to be useful for unraveling the local strain and doping levels, especially for weak strain when no peak splitting is detectable. As a second example, we analyze complex and unknown strain configurations on the nanoscale. Exfoliated flakes of graphene were suspended on multiple lithographically-fabricated pairs of gold nanodisc",
        " a method for analyzing arbitrary strain configurations and simultaneous doping in graphene using Raman spectroscopy. First, the shift due to pure hydrostatic strain is determined through explicit or implicit (circular light) averaging of the G and 2D mode components. Second, the peak shifts induced by hydrostatic strain and doping are separated by a correlation analysis of the G and 2D mean frequencies. This offers the possibility to calculate the local shear strain, hydrostatic strain and doping without any assumption on the underlying strain configuration. We demonstrated this strain analysis for graphene on two different substrates. First, graphene was subjected to uniaxial stress by deflection of a PMMA beam. We demonstrate that circular corotating light polarization is ideal for obtaining the mean frequencies of the G and 2D mode under strain. This approach is even applicable for low strain levels when no peak splitting is visible. As a second example, graphene was suspended on pairs of gold nanoparticles, which induced strong local strain of completely unknown configuration. Surface-enhanced Raman scattering allowed extracting the local strain and doping levels with nanoscale resolution. The two examples serve as model cases for the strain analysis of arbitrary strain configurations in graphene. Our methodology also carries over to other 2D materials, such as Mo",
        " a hydrostatic component ranging from 1% to 2.6% (Fig. REF e). The shear component was calculated from the G splitting using a shear deformation potential of {{formula:df48fbed-8186-46d1-b525-4ac21024f0ef}} . With a magnitude from 0.75% to 1.8% it was weaker than the hydrostatic component (Fig. REF e). This differs from uniaxial strain, where the shear component was twice as large as the hydrostatic component (Fig. REF b inset) and biaxial strain where the shear component vanishes. Shear strain and hydrostatic strain are largely uncorrelated for the nanodimers (Fig. REF e), from which we conclude that the nanodimers induce different strain configurations in the suspended graphene. Our methodology for strain evaluation is even applicable for this complicated case because it does not require any assumption of the strain configuration as an input parameter.Finally, we demonstrate our analysis of local strain and doping for several different graphene flakes covering arrays of gold nanodimers. Representative Raman spectra of three graphene flakes (samples D1-D3) are"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how NLOS, multi-path, and bias affect the accuracy of UWB TDOA, and what methods were used to mitigate these effects in the research?",
      "expected_output": "The main findings show that NLOS, multi-path, and bias significantly affect the accuracy of UWB TDOA measurements. NLOS and multi-path propagation lead to outliers, while bias causes systematic errors. To mitigate these effects, methods such as channel impulse response-based approaches, support vector machines, Gaussian processes, and sparse pseudo-input Gaussian processes have been proposed. Additionally, an online expectation maximization algorithm and semi-definite programming method have been used to address NLOS measurement errors. A neural network-based approach has also been used to learn and compensate for systematic bias, improving localization accuracy.",
      "context": [
        " Non-line-of-sight (NLOS) and multi-path radio propagation, for example, can lead to erroneous, spurious measurements (so-called outliers, Figure REF ). Even line-of-sight (LOS) UWB measurements exhibit error patterns (i.e., bias), which are typically caused by the UWB antenna's radiation characteristic . The ability to effectively (i) remove outliers and (ii) compensate for bias is essential to guarantee reliable and accurate UWB localization performance. {{figure:e9374cbf-8b41-4dbe-b822-d889cffc3260}}Multiple approaches have been proposed for the mitigation of UWB outlier measurements when using the TWR scheme. In , a channel impulse response-based approach detects NLOS propagation from the received UWB waveforms, without the need for prior knowledge of the environment. In , the measurement error caused by NLOS is estimated directly from the received radio waveform using support vector machines (SVM) and Gaussian processes (GP).Concerning the bias of TWR localization, the authors of and model and correct UWB pose-dependent biases using sparse pseudo-input Gaussian processes (SPGP) and demonstrate their approach on",
        "formula:a96c0e1c-a23f-4e8e-9849-eeb856ce7bd1}} is the same for different tags.\nUWB TDOA Outlier Measurements: Beyond the systematic biases observed in the previous section, TDOA measurements are often corrupted by outliers caused by multi-path and NLOS signal propagation. The multi-path effect is the result of the reflection of radio waves, leading to longer ToF and wrong TDOA measurements . In indoor scenarios, metal structures, walls, and obstacles are the major causes of multi-path propagation. NLOS propagation can occur because of the obstacle-penetrating capability of UWB radios, with delayed or degraded signals resulting also leading to outlier measurements , .NLOS and multi-path propagation often result in extremely improbable TDOA measurements, which should be rejected as outliers. In Section , we devise a robust localization framework to reduce the influence of outliers and achieve reliable and robust localization performance.\nUWB TDOA Bias Model Learning with a Neural Network: Knowing that TDOA hyperbolic localization is especially sensitive to measurement bias, we aim to show that compensating for the systematic bias can greatly improve the localization accuracy.As highlighted",
        " and correct UWB pose-dependent biases using sparse pseudo-input Gaussian processes (SPGP) and demonstrate their approach on a quadrotor platform equipped with a Snapdragon Flight computer. An iterative approach to estimate bias in TWR measurements is presented in .As TDOA localization involves three UWB radios instead of two, modeling the measurement error is inherently more challenging. Most existing works focus on mitigating errors caused by NLOS and multi-path propagation. Pioneering research was conducted in , , where an online expectation maximization (EM) algorithm addresses TDOA NLOS measurement errors. In , a semi-definite programming method is applied to the same problem. Much of the research on UWB TDOA localization including , , has been conducted in 2D scenarios and demonstrated using ground robots. In , the authors mention that UWB TDOA measurements are also affected by a systematic, position-dependent bias in line-of-sight conditions. Yet, to the best of our knowledge, no existing work focuses on addressing this spatially varying source of bias.In this work, we propose a framework to improve the accuracy and robustness of TDOA-based localization for resource-constrained mobile robots. We separately tackle the challenges posed by (i) systematic bias and ("
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research paper on LQ cost functionals and how do the methods used impact the key results?",
      "expected_output": "The research paper on LQ cost functionals presents a general class of cost functionals of the form J(u) = ∫[0,∞) [x(t)′Qx(t) + u(t)′Ru(t)]dt, where Q, R, and A are matrices. The paper focuses on distributed LQ control, allowing only distributed diffusive control laws that achieve consensus. The main finding is that the optimal protocol requires global information on the network graph and the initial state of the entire network. For example, the paper shows that the cost functional J(u) is a special case of a more general form with specific matrices, such as Q = I and R = I. The methods used, such as minimizing the cost functional over all control laws of a specific form, impact the key results by providing a solution to the general distributed LQ problem.",
      "context": [
        "360}}A general class of LQ cost functionals are those of the form {{formula:dfe581d5-56d8-4ef8-9970-43b29cfeb6bc}}where {{formula:56061d3c-c555-4111-a7e2-3db1ba22f542}} , {{formula:11625bf6-6882-4e24-8c58-ce5d8607ec86}} and {{formula:43f713a2-3c26-4bd5-b73c-3e1fd7da8681}} and {{formula:6b6ca24c-4589-4da3-9a18-c131afb96679}} .In the context of distributed LQ control we only allow distributed diffusive control laws that achieve consensus, i.e. the controlled trajectories converge to {{formula:666538e2-84c8-4eea-b3f8-d71e2cdd014c}} , the span of the vector of ones. Thus the class of control laws over which we want to minimize (REF ) consists",
        " multi-agent systems, it must necessarily be of the form {{formula:2a95be15-a580-4c9e-ba3c-0da56b434a94}}for some {{formula:013063ee-1249-4137-9e6d-4badc5f381ee}} and {{formula:43221573-8f2f-48ad-858f-3d575d90edee}} . The corresponding distributed LQ problem is to minimize, for the system (REF ) with initial state {{formula:1255e746-eb82-462d-95b7-245f9d934cf4}} , the cost functional (REF ) over all control laws of the form {{formula:02cc910d-4f18-4a7f-bf90-5fb83393ae87}} with {{formula:c3d5d05a-f40f-4cc6-b061-9aa825517e2c}} .As an illustration, we will now provide two important special cases of LQ cost functionals. The first one was studied before in and :",
        "58c3-4dfe-bf62-fb4ba9f66301}} , we conclude that (REF ) is a special case of (REF ) with {{formula:fbb52c91-7819-4d00-ae38-2928ec4581bf}} and {{formula:3464b9f2-8280-4ead-a900-691f8408e83c}} .\nCentralized Optimal Gain: In this section we will briefly give a solution to the general distributed LQ problem with cost functional (REF ) as introduced in Section , thus generalizing the result from to general distributed LQ cost functionals. We will show that, indeed, computation of the optimal protocol requires global information on the network graph and the initial state of the entire network.Consider the cost functional (REF ) together with the dynamics (REF ) with given initial state {{formula:2f64ca2e-b701-4518-936f-d2e8314a0991}} . Since the admissible control laws are given by {{formula:c8edf916-491b-4eca-a19c-e7828f1acaf4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the relationship between grain sizes in nebulae and their corresponding albedo values in this research paper?",
      "expected_output": "The research paper finds that denser reflection nebulae have higher albedo values, likely due to larger average grain sizes or a reduced number of small grains. For example, Kim and Martin's study shows that size distributions of interstellar grains with R_v = 3.1 have higher albedos than standard R_v = 3.1 grains. The paper also notes that the dust albedo in the UV in the diffuse ISM of the Galaxy is lower, about 0.45, compared to the albedo in denser regions, about 0.65.",
      "context": [
        "a360-58054b95e69a}} 10{{formula:764a915c-731f-46dd-8ad6-2d6525790e3e}} cm{{formula:3057a02e-c9bd-476d-ae2e-57798bda8b40}} . The denser reflection nebulae appear to share albedo values in excess of 0.6 in the 130-170nm spectral range, while the present study yields a value definitely lower than that. This difference is most easily understood if we assume that the average grain size is larger in reflection nebulae, or, what would amount to the same, that the number of very small grains (size {{formula:97e2aea1-151c-4a52-8530-9e05eeb16e93}} ) is reduced compared to the number of larger (size {{formula:bef5ab5b-2474-48cf-875d-97ee4241fcdc}} ) grains. This is indeed supported by the fact that in reflection nebulae R{{formula:dcf05c40-1096-4888-82bb",
        "21986dd8bfb7}} pc. Sembach and Savage (1996) have shown that dust grains in such environments are subject to erosive processes which return a portion of their solid substance to the gas phase. Kim and Martin ( ) have shown that size distributions of interstellar grains characterized by R{{formula:44359c00-16f7-4f74-b4e5-38ab89f0e9f2}} 3.1 have higher albedos than the standard R{{formula:bf55ad17-8e82-4475-8a5f-b64fbcab17ee}} = 3.1 grains throughout the entire near-IR to UV spectral range. There are still discrepancies, however, between the observationally derived albedo values for reflection nebulae and the values predicted by Kim & Martin. This may simply reflect the fact that their assumed graphite/silicate composition for the grains does not correspond entirely to reality. Nevertheless, we have for the first time a relatively clear indication that the dust albedo in the UV in the diffuse ISM of the Galaxy is lower than the albedo in denser regions, about 0.45 vs. 0.65,",
        " that in reflection nebulae R{{formula:dcf05c40-1096-4888-82bb-ed4cd629626f}} = A{{formula:7a1071c6-6689-43a6-a65b-ecdf8d195268}} /E(B-V) is usually found to be larger than the \"normal\" galactic value for the diffuse ISM of R{{formula:7d4ce581-f75d-4106-a62d-4fafccf184fb}} = 3.1, which is true also for the regions compared in Table 3. In addition, most of the dust observed in this study is seen at relatively high galactic latitudes, placing it potentially at large z-distances. With our assumed z-distribution, 50% of the dust is located at {{formula:0482bd70-9860-4981-a9c8-327f272785d5}} z{{formula:91875836-6632-4466-ae08-21986dd8bfb7}} pc. Sembach and Savage (1996) have shown that dust grains in such environments"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does neutron density play in the production of Sr, Ba, and Pb, and how do the research findings support this relationship?",
      "expected_output": "According to the research paper, neutron density plays a crucial role in the production of Sr, Ba, and Pb. The paper states that models with higher neutron densities start their production phases of Ba and Pb at higher neutron exposures. This shift shows a slow down of the heavy-element production in terms of integrated neutron exposure in comparison to the s process. For example, the simulation with a typical s-process neutron density of 7.5 x 10^7 cm^-3 is included in Figure REF, which shows the heavy-element abundance patterns. The research findings support this relationship by demonstrating that an increase in neutron exposures is required for the production of heavier isotopes and Pb in particular, as the i-process neutron-capture path moves away from the valley of stability.",
      "context": [
        " post-decay abundances of the stable nuclei. For comparison, a simulation with a typical s-process neutron density of {{formula:67df2e48-f359-49ae-b032-753f89aa4dda}} is also included in Figure REF . To account for the different time scales on which neutron captures happen at different {{formula:847e9eb5-b419-4cb6-a833-9515208f54f0}} , the Sr, Ba and Pb production ratios are shown as a function of the time-integrated neutron exposure {{formula:d43a9f59-c6c7-46af-9c88-63570c0febe1}} . All the simulations show an initial phase of decrease in [Ba/Sr] and [Pb/Ba] until they reach a minimum and begin to rise. For each simulation the rise of [Pb/Ba] starts after the rise in [Ba/Sr]. This shows that the main production of Sr, Ba and Pb follow one after the other in the respective order as expected.The minimum [Ba/Sr] and [Pb/Ba] for",
        " the other in the respective order as expected.The minimum [Ba/Sr] and [Pb/Ba] for the different simulations shows that models with higher {{formula:bc835d18-e4e1-431c-b4cb-1f85978261ef}} start their production phases of Ba and Pb at higher {{formula:28f44ad5-79df-43b3-a830-8dde9beafee8}} . This shift shows a slow down of the heavy-element production in terms of integrated neutron exposure in comparison to the s process. As the i-process neutron-capture path moves away from the valley of stability more neutron-rich isotopes are produced. On average, these have lower neutron-capture cross sections than isotopes with fewer neutrons which are closer to the valley of stability. With a decrease of the average neutron-capture cross section an increase in neutron exposures is required for the production of heavier isotopes and Pb in particular.In more detail, Figure REF also shows the heavy-element abundance patterns of the simulations with {{formula:33b6deeb-d7bc-4133-bf73-e",
        " the simulations with {{formula:33b6deeb-d7bc-4133-bf73-e1697f97d929}} , {{formula:ef2a131a-2a42-4b93-b7b8-b87b250ee7d8}} , and {{formula:1fac4925-3b82-4299-8edb-0877c6126af4}} at the same integrated neutron exposure {{formula:6f7867f8-4b89-40be-912d-2afd9f21598e}} . From the different distributions of the produced heavy elements, it can be seen that the neutron-capture paths have progressed to very different stages, although the same amount of neutrons per neutron-capture cross section have been provided. The simulation with {{formula:a900d7a1-c7e0-4ed8-a378-1a1f9f74cf56}} shows a peak for the elements Xe, Cs, and Ba ({{formula:ed944e34-2915-4fdb-92d2-e63d7"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the plan structure in this research paper support robot introspection and what are the key findings related to this aspect?",
      "expected_output": "The plan structure in this research paper supports robot introspection by being systematic and hierarchical, based on the entity descriptions concept. This structure makes relevant information, such as action outcomes and failures, easily retrievable. The key findings related to this aspect are that the proposed plan structure enables the robot to choose action parameterizations that increase task execution success rates and overall performance of fetch and deliver actions. For example, a PR2 robot equipped with the proposed system was able to achieve higher success rates in a real-world setting.",
      "context": [
        " with this knowledge.Plans have to be well-structured such that relevant information — action outcomes, failures that happened, parameterizations used, order of action execution etc. — is available and easily retrievable to support introspection. We propose systematic and hierarchical plan structure, based on the entity descriptions concept , and demonstrate how such plan structure is advantageous for introspection on the example of fetch and deliver plans.The novel contributions of this paper are:an approach to structure robot plans such that they are easily introspected; a model for fetch and deliver plans and their implementation; fast plan projection mechanism for a mobile manipulation robot as means to generate different behavior for the same symbolically-represented action; mechanisms to incorporate plan projection online into a specific segment of the robot control program.We evaluate our approach by showing how a PR2 robot is able to choose action parameterizations that increase task execution success rates and overall performance of fetch and deliver actions in a real world setting by using our system.\nRelated Work: In cognitive science, predicting how motions affect the future evolution of the environment was found to play an essential role in human manipulation capabilities. An example of this line of thought is Hesslow's simulation theory of cognition , which proposes that thinking is imagined interaction with the environment,",
        "# arXiv Paper: 1812.08224\n\nSpecializing Underdetermined Action Descriptions Through Plan Projection\nabstract: Plan execution on real robots in realistic environments is underdetermined and often leads to failures. The choice of action parameterization is crucial for task success. By thinking ahead of time with the fast plan projection mechanism proposed in this paper, a general plan can be specialized towards the environment and task at hand by choosing action parameterizations that are predicted to lead to successful execution. For finding causal relationships between action parameterizations and task success, we provide the robot with means for plan introspection and propose a systematic and hierarchical plan structure to support that. We evaluate our approach by showing how a PR2 robot, when equipped with the proposed system, is able to choose action parameterizations that increase task execution success rates and overall performance of fetch and deliver actions in a real world setting.\nIntroduction: There have been remarkable demonstrations of autonomous mobile manipulation robots performing everyday activities such as folding clothes or washing dishes . However, the robot control programs for executing these tasks are only applicable in the specific settings that they are implemented for. To enable the robots escape laboratory settings and enter unstructured environments such as human households, they need to be able to autonomously manipulate",
        " perspective, traditional simulation-based approaches are computationally expensive and have a low real-time factor, whereas our plan projection is very fast with respect to the pace of action execution (see Section ).The closest related work that deals with large time span temporal plan projection is by Mösenlechner et al. . The system described in considers simple sequences of actions designed for simulation. Our approach aims at complete plans, which run on a real robot and are, therefore, much more complex than those used in simulation. This requires an approach to structure plans such that they are easily introspected, which is presented in this paper. Additionally, running projection on a real robot during execution and integrating results of projection-based reasoning back into the executive poses another challenge, which has been tackled in this paper.\nPlan Architecture: To enable convenient performance introspection, plans have to be nicely structured, i.e. be modular, explicit and transparent. We have developed plans for fetching and delivering objects that have such a structure. In these plans, the control flow is separated from the reasoning necessary to ground entity descriptions into the environment at hand. The control flow of fetch is illustrated in Figure REF . {{figure:bfb352d3-14c3-4a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does Equation REF's Receiver Operating Characteristic restrict convergence around the point (1,0) for real values of the given formulas, and what are the implications of this restriction on the overall results?",
      "expected_output": "According to the research paper, the Receiver Operating Characteristic (ROC) of Equation REF restricts convergence around the point (1,0) for real values of the given formulas, specifically {{formula:bb87a6da-e759-4e01-b33d-778548dfd12e}} and {{formula:43621f8f-fb4a-41d5-b88c-9fe5438a93ae}}, to the region shown in Fig. REF Right. This restriction implies that the analytic continuation around (1,0) has a limited region of convergence, which is already covered by the formulas presented in Section. The restriction is caused by the {{formula:4a3559f3-afdf-4c22-8b50-c90fc7037ed8}} series, which prevents the analytic continuation from converging around the whole neighborhood of {{formula:a09d714d-efd9-4ad0-86a2-d1740d1d3912}}.",
      "context": [
        " that, for real values of {{formula:bb87a6da-e759-4e01-b33d-778548dfd12e}} and {{formula:43621f8f-fb4a-41d5-b88c-9fe5438a93ae}} , the ROC of the RHS of Eq. (REF ) is restricted to the region shown in Fig. REF Right.Using Eq.(REF ) we get the analytic continuation around (1,0) as {{formula:a3e96263-04a1-46ee-b7dc-ce92984f0ce4}}which now converges, still for real values of {{formula:42481c82-0bc5-4715-88e1-ea012d282b95}} and {{formula:64afdaf5-301e-41ac-9fda-fa9603f115f4}} , in the region shown in Fig.REF . One will note here that the regions shown in Fig.REF Right and Fig.REF are already covered by the formulas presented in Section . {{figure:f43a5dee-44",
        " and Fig.REF are already covered by the formulas presented in Section . {{figure:f43a5dee-4422-4bc8-bdec-e216d57004b1}}Looking at the ROC plots it is clear that the {{formula:4a3559f3-afdf-4c22-8b50-c90fc7037ed8}} series prevents the analytic continuation in Eq.(REF ) (respectively Eq.(REF )) to converge around the whole neighbourhood of {{formula:a09d714d-efd9-4ad0-86a2-d1740d1d3912}} (respectively {{formula:9358439e-9dd0-411e-a852-3f8ed7ae8844}} ). More precisely, the constraint {{formula:cfd9213d-cb70-418f-905d-8e8c6f8fb6df}} (respectively {{formula:63b8e526-2b3f-4754-bbe3-16ac124a3cc7}} ) is responsible for that, which means that by going on with the",
        " -{{formula:28805d8a-120b-403b-baf4-6cd913e848f5}}- | 1x,xy ]+ (c1)(a-b1-b2)(a-b2)(c1-b1)(-x)-b1 F3(b1,b2,b1-c1+1,b2-c2+1,b1+b2-a+1;1x,1y) Substituting the above back in Eq.(REF ) we get Eq.(REF ).The regions of convergence of Eqs.(REF ) and (REF ) are ploted in Fig. REF Left.\nAnalytic Continuation around {{formula:2c8a62ea-e14c-4d48-8555-9809505671ed}}: Looking at the ROC of the second continuation around (0,1), Eq.(REF ), we note that {{formula:ea291514-a6e7-4d1f-8320-44b57689ad95}} converges in the entire neighborhood of {{formula:f0b5543d-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of gapped spin excitation spectrum on thermal conductivity in this research paper?",
      "expected_output": "The main findings regarding the impact of gapped spin excitation spectrum on thermal conductivity in this research paper are that at low temperatures, both spin thermal conductivity and phonon-spin scattering are negligible due to the gapped spin excitation spectrum. However, as the temperature increases, the number of thermally activated spin excitations increases, leading to an increase in thermal conductivity. The paper also highlights that measurements of thermal conductivity in external magnetic fields can provide detailed information on scattering mechanisms limiting thermal conductivity.",
      "context": [
        "352e5}} K taken from neutron scattering experiments , and {{formula:84f389c7-566e-40a1-b5a6-badaeb106a37}} from an ESR study . The value of {{formula:3a03e527-b495-4bae-9680-61f8ca8fd427}} , used in the analysis, is predicted by theory and is confirmed by the inelastic neutron scattering measurements , . Equation (REF ) describes correctly the linear-in-{{formula:1bcfadeb-c677-4e29-b517-21b65778e9b4}} decrease of the energy gap at {{formula:1377c586-18ba-4e28-84a1-eb359c77a4eb}} between 1 and 6 T observed in the ESR experiment , but does not account for deviations at higher fields caused by the influence of the aforementioned transverse staggered field.Within the Boltzmann equation approximation, the spin thermal conductivity parallel to the chain direction is represented by {{formula:143f70db-09f8-49c7-b8a9-04a",
        "6fc-232aa39de44b}} ). The spin excitation spectrum at {{formula:c4819bf5-59b2-41d9-9120-eb8b7743161d}} is gapped with the smallest gap {{formula:9be01d0a-0b0d-4e8f-9d36-cfdd873c181e}} , therefore at {{formula:4e470de3-7fe8-4ec2-99d4-916c3b641cab}} , both spin thermal conductivity and phonon-spin scattering are negligible. With increasing {{formula:5b678540-e34c-4c45-a1be-9df43129c79b}} at a constant temperature, {{formula:c0b6d40d-c90d-448d-8d85-fd0d015e5f39}} decreases and the number of thermally activated spin excitations increases. Therefore, {{formula:965446f9-2e45-464d-8502-6da95e3c02dc}} should increase, while {{formula:c",
        "94-85f1-d2a4e0ad2045}} , {{formula:7de5ef09-0e5b-4a74-a879-e8d25332bf5c}} , as conservation laws prohibit the decay of the heat current. Both extrinsic sources of scattering, such as defects and phonons, and intrinsic spin-spin interactions which are not integrable, render the heat conductivity finite , , , , , , , , .Measurements of the thermal conductivity in external magnetic fields, which can strongly modify the spin excitation spectrum, offer detailed information on scattering mechanisms limiting {{formula:fefa0771-d9f9-4493-8a7e-e9aba707a2b5}} . However, magnetic fields typically available are too weak to noticeably influence the spectrum of spin excitations in the previously investigated , , {{formula:3b8abcb2-5587-4b23-b451-56c9ad99cde7}} chain materials AgVP{{formula:27e0bd06-ccef-4697-b8d3-af1321f87739}} S{{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do symmetries classify fermionic CFTs and what are the main findings of this research in relation to conformal field theory?",
      "expected_output": "The research classifies fermionic CFTs based on symmetries that preserve the affine algebra. The main findings include the use of the generalized Jordan-Wigner transformation to establish a one-to-one correspondence between fermionic and bosonic theories with specific symmetries. The authors identify non-anomalous symmetries for certain modular invariants, allowing for the construction of corresponding fermionic and parafermionic CFTs, thus achieving a complete classification. For example, the ADE classification completes the list of bosonic modular invariants, which is then used to classify fermionic and parafermionic CFTs with affine algebra symmetry.",
      "context": [
        "formula:26a3ed25-93b3-49b7-98bd-ac3be5175681}} symmetry: {{formula:25e1a5a5-5e78-483b-b1e4-e6741254bc19}}On the other hand, the authors of determine the non-anomalous finite symmetries, which preserves the affine algebra, in {{formula:96a0bd5d-f621-4427-ac27-bb95c744e043}} and {{formula:037fe934-dcb2-4dc7-9a3b-8b98ecb86979}} modular invariants. Thus, for each {{formula:8b58304e-6ce9-412b-b479-afbca51660f2}} symmetry of bosonic theories, we can perform the generalized Jordan-Wigner transformation and obtain the classification of fermionic and parafermionic theories.Before proceeding to the detailed construction of each theory, we summarize the classification results of fermionic and parafermionic {{formula:32400a6c-90ca-46",
        "-964f-4031e33aef19}}Note that the theories with and without tilde have the same partition function when the paraspin structure is {{formula:5ecfd0b2-c823-4e68-82fe-96c069442313}} . {{figure:8a9ccf5e-ba8e-493f-a589-bcfc32541055}}\nClassification of {{formula:4878fceb-bc28-4f1e-aadb-ad968a65aa42}} models: In this section, we classify the fermionic and parafermionic CFTs with affine {{formula:c85927b3-065a-4096-8419-e5d6f838db11}} algebra symmetry. As discussed in section , the ADE classification completes the list of bosonic modular invariants , , . Our strategy for classification is based on the generalized Jordan-Wigner transformation of the bosonic modular invariants by non-anomalous {{formula:bb0cb5ce-dafe-4d16-b42d-5d",
        "ization, which establish a one-to-one correspondence between a (para)fermionic theory and a bosonic theory with {{formula:e4430306-eb11-4d04-92e0-42b1bb4a540b}} symmetry. Fermionization and parafermionization require us to ensure the absence of 't Hooft anomaly for the {{formula:ff9ea2e7-8558-4d14-8cd1-1058164fdb72}} symmetry. Notably, the global symmetries without anomaly have been enumerated for {{formula:99c2baa7-753d-485c-9bc9-2090468131b5}} and {{formula:288bf5b9-4fee-489a-9628-8daab70b9e30}} modular invariants, under the assumption that the symmetry group preserves the affine algebra . Based on each identified non-anomalous symmetry, we construct corresponding fermionic and parafermionic CFTs, achieving a complete classification.For {{formula:5183b7b3-a7c4-48d9-bf48-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does avoiding temporal dependency simplify equation solving in the context of this research paper?",
      "expected_output": "Avoiding temporal dependency simplifies equation solving by reducing the number of binary variables and making the problem more tractable. According to the paper, this is achieved by iteratively fixing neighbor decisions and updating the decision at each time step, which decouples the temporal dependency and reduces the problem to a standard combinatorial optimization.",
      "context": [
        "ula:d69fd6c6-63b7-4702-97cb-e6abf17b03e9}} : {{formula:ba78d5a3-7886-4e9c-b733-e544ec152e3b}}Compared to Eq. (REF ), Eq. (REF ) avoids the temporal dependency across multiple time steps and largely reduces the number of binary variables. In practice, solving Eq. (REF ) is more tractable than solving Eq. (REF ).This observation motivates the idea of iteratively fixing the neighbor decisions {{formula:200ffd15-4d88-4af4-a479-1abda5d2929c}} and updating the decision at time step {{formula:cb900431-c008-4356-aab3-86df4b39a241}} for all {{formula:9e1da31f-a6c7-4002-9ad9-3a05a94400ae}} . We use {{formula:0ddb664d-d51e-49c5-9aa3-d97d3848683d}}",
        "b664d-d51e-49c5-9aa3-d97d3848683d}} to initialize all decisions. Then we can iteratively solve Eq. (REF ) with different {{formula:3a22fba3-eb4b-4af6-94f2-bf13acbc3374}} to update the decision {{formula:45280df8-5f74-4fff-8188-311861ff1f8b}} . This method decouples the temporal dependency and reduces the problem to a standard combinatorial optimization of function {{formula:4576dd08-0fd5-404f-a309-b1f64c72407d}} with additional regularization terms. We can use mixed integer linear program or any other approximation algorithms to solve Eq. (REF ).Moreover, we can notice that any improvement made by solving Eq. (REF ) with {{formula:1fdae238-59ba-4169-a14d-184a9bc2f183}} provides the same improvement to Eq. (REF ). This suggests that the optimal decision of Eq. (REF ) is a fixed point",
        "c6-2d5745711b98}} otherwise {{formula:f08e1aeb-db49-42d2-940d-08ad1497ada0}} . Solve Eq. (REF ) with {{formula:4e086b95-8671-496d-a41f-4c2e034e6cbe}} , {{formula:01d2be7b-cefa-4fac-8f98-3a5acb75821d}} to update {{formula:29085f6b-ce65-4358-a348-8e215f8f145f}} .\nDecomposition and Iterative Algorithm: If we fix the assignments {{formula:8ed9164b-03f6-4359-b909-4a80c6c79367}} , finding the optimal solution at time step {{formula:fc941f81-ede9-45b0-89fd-85689c587a82}} reduces to the following problem with {{formula:d69fd6c6-63b7-4702-97cb-e6abf17b"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do nonlinear components contribute to the increase in complexity, and what methods were used to analyze this phenomenon in the research paper?",
      "expected_output": "Nonlinear components contribute to the increase in complexity by requiring piecewise linear overapproximation, which translates into an MIP formulation with more binary variables. The research paper analyzed this phenomenon using examples such as the Tora and Car problems, where the number of timesteps was reduced and the frequency of symbolic queries was increased to handle the added complexity. The paper also compared the compute times of feasibility experiments and reachable set computation problems, showing that the feasibility framing of the problem can be significantly faster, as seen in the TORA problem where the compute time was reduced from over 18 hours to less than 9 minutes.",
      "context": [
        " system includes more nonlinear components, the complexity of the problem increases. Each nonlinear component requires piecewise linear overapproximation, which in turn translates into an MIP formulation with more binary variables. While there is some variability from one problem to another, additional binary variables generally add complexity to the problem. Furthermore, a dynamical system with more nonlinearity may require a more complex control policy; e.g. a deeper neural network, which further increases the complexity of the problem. In order to handle this additional complexity, in the Tora and Car examples, we reduced the number of timesteps and increased the frequency at which symbolic queries were performed to reset the time horizon.fig:cartora shows the reachability results for the Tora T3 example and Car C3 example. The difference between the concrete and hybrid-symbolic approaches is not as significant as in the single pendulum example, in part because we had to concretize the symbolic queries earlier to avoid having queries that were too large. Symbolic sets were calculated for all of the TORA problems (T1, T2, T3) at timesteps 5, 10, and 15. For the car problems (C1, C2, C3), symbolic sets",
        " in the top right plot of fig:cartora. In other words, every trace from the starting set is a counter example.fig:timeb also shows the compute time of the feasibility experiments. The compute times for these experiments are reliably shorter than those of the reachable set computation problems, but generally on the same order of magnitude. The TORA problem is a marked exception, where the feasibility framing of the problem is 1 to 2 orders of magnitude faster for problems T1, T2 and T3. The compute time needed to solve problem T3 was reduced from more than 18 hours to less than 9 minutes. We hypothesize that there is some interaction between the wide layers (100 neurons) of the T3 control policy and solving the optimization problems over state variables that are part of reachable set computation. Solving a feasibility problem involves either finding a single feasible solution or showing that no feasible solution exists for each timestep, whereas solving a reachable set computation problem translates to solving {{formula:8f8bb499-6189-4536-9e64-74704f28a952}} optimization problems, where {{formula:b2d6fb1f-60ec-4051-8eca-1dc5b",
        ", 20, and 25. It can be observed for both S1 and S2 that the hybrid-symbolic approach is capable of proving the safety property, but the naive 1-step concrete sets are too loose, and cannot prove the property. The convex hull of Monte Carlo simulations represent an underapproximation of the true reachable set, and one can observe that the sets generated by OVERT's hybrid approach hug the true system trajectories tightly. This suggests that few spurious counter examples will be generated.While one might observe that performing more and longer symbolic queries would produce even tighter reachable sets, this also adds to the complexity of the MIP to be solved. For example, in the case of S2, the naive one-step reachable set computation took less than 4 minutes, while the hybrid symbolic approach with two 10-step queries, one 5-step query, and 22 one-step queries required just under 12 minutes to solve. {{figure:c69e0ff9-1005-4b75-94bd-87e619329c41}}When the update function of a dynamical system includes more nonlinear components, the complexity of the problem increases. Each nonlinear component requires piecewise linear overapprox"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "default",
      "expected_output": "The research paper discusses the distribution of normalized values H_{q}(λ)/√q for elliptic curves in the Legendre family, where both q and λ vary. The distribution is found to be semicircular, and the moments of this distribution are calculated. For even moments, the limits involve the Catalan numbers, which are defined as C(n) = (2n)! / (n! * (n+1)!).",
      "context": [
        " \\mathbb {F}_q\\setminus \\lbrace 0,1\\rbrace . In , Ono, Saad, and Saikia consider all elliptic curves in the Legendre family and determine the distribution of normalized values H_{q}(\\lambda )/\\sqrt{q} \\in [-2,2], where now both q and \\lambda \\in \\mathbb {F}_q\\setminus \\lbrace 0,1\\rbrace vary. They find the distribution is semicircular, explicitly determined in Corollary 1.2 of , analogous to the classical Sato-Tate conjecture.Let m be a fixed positive integer. Computing the mean of the (H_{q}(\\lambda )/\\sqrt{q})^{m} values over \\mathbb {F}_q and taking the limit as p tends to infinity gives the first example of a hypergeometric moment. The values of this limit are determined by Ono, Saad, and Saikia in thm:oss. When m is even the limits involve the Catalan numbers.Throughout this paper, C(n) = \\frac{(2n)!}{n!(n+1)!} denotes the",
        "frac{1}{p^{(\\frac{m}{2}+1)}} \\sum _{\\lambda \\in \\mathbb {F}_p} H_{p}(\\lambda ^2)^{m} = {\\left\\lbrace \\begin{array}{ll} 0 &\\quad \\text{if m is odd}\\\\ C(n) &\\quad \\text{if m = 2n is even} \\end{array}\\right.}A variation of the moments considered in thm:oss, in the context of McCarthy's p-adic hypergeometric functions, was recently studied by Pujahari and Saikia . These authors establish the semicircular distribution and \\text{SU}(2) moments for two cases of p-adic hypergeometric functions. The \\text{SU}(2) distribution also arises when counting matrix points on Legendre elliptic curves, as shown by Huang, Ono, and Saad .Given a hypergeometric datum that is defined over \\mathbb {Q} and primitive there is no guarantee the moments will converge to a known distribution, such as the \\text{SU}(2) distribution. A further restriction on the hypergeometric datum, requiring",
        "lambda \\in \\mathbb {F}_q\\setminus \\lbrace 0,1\\rbrace , wherea_{\\lambda }^{\\text{Leg}}(q):= q + 1 - |E_{\\lambda }^{\\text{Leg}}({\\mathbb {F}_{q}})|is the trace of Frobenius for the Legendre elliptic curveE_{\\lambda }^{\\text{Leg}}: y^{2} = x(x-1)(x-\\lambda ).In , Beukers, Cohen, and Mellit generalize the point-counting connection of Koike to more general hypergeometric data by connecting the H_{q} function for data defined over \\mathbb {Q} to point-counting on a certain non-singular completion of an explicit toric variety.\nHypergeometric Moments: The key connection of Koike allows us to consider the Sato-Tate distribution of the normalized values H_{q}(\\lambda )/\\sqrt{q} \\in [-2,2] for a fixed \\lambda \\in \\mathbb {F}_q\\setminus \\lbrace 0,1\\rbrace . In , Ono"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the sodium dimer beam play in enabling STIRAP, and how does it contribute to the research findings?",
      "expected_output": "The sodium dimer beam plays a crucial role in enabling STIRAP by providing a well-collimated, cold beam of molecules that can undergo population transfer to the final state via stimulated Raman adiabatic passage. This beam is produced through stimulated Raman photoassociation of cold Na atoms, resulting in translationally cold Na2 molecules in a chosen vib-rotational state. The beam's properties contribute to the research findings by allowing for the transfer of information from the initial state to the photons emitted by the cavities, as shown in Eqs. (REF) and (REF), and enabling the simulation of the system's dynamics.",
      "context": [
        "d8100}} to which the state {{formula:ddaaface-607e-47d1-baf7-6466cb446803}} decays and the cavity has either one or no photon. If the system successfully completes the transfer, then, irrespective of its initial state, it ends up in state {{formula:d055f592-b9c2-47ef-a83f-f14e836f222d}} after a time long compared to all relaxation times, which corresponds to the absence of memory in the system about its initial state. The information about the initial state is transferred to the photons emitted by the corresponding cavities, as per Eqs. (REF ) and (REF ).For numerical simulations of the system's dynamics we have chosen a well-collimated, cold beam of sodium dimers. Such a beam can be produced via stimulated Raman photoassociation of cold Na atoms, thereby obtaining translationally cold Na{{formula:9824d995-e85f-4b98-bec5-3e1525e9b1fb}} molecules in the chosen vib-rotational state of the electronic ground state {{formula:d7fd6bcf-782",
        "ea}} enters both cavities, all other outcomes being idle events. This state is entangled and symmetrized or antisymmetrized, depending on the molecular configuration. Under the Raman-resonance condition, each fragment, passing through the sequence of partially overlapping cavity (quantized) and pump (classical) fields, undergoes population transfer to the final state {{formula:dcc7732f-9d5a-4abb-9146-5800f6ad5fd3}} , via stimulated Raman adiabatic passage (STIRAP) , and adds to the corresponding cavity a single photon at a frequency {{formula:2c5836d8-1f1f-4160-92e2-a556bd7cbcc5}} uniquely determined by the initial internal and translational state. Upon leaving their cavities through the partially transparent front mirrors, the two entangled photons encode the dissociative state: {{formula:88e8f6fe-b199-4e64-9cbb-650ab92f54ee}}where {{formula:48d6712f-2552-48cb-8f28-7f6e64",
        ":b5dd0eb2-e022-4180-8eef-48bee61eff47}}where the primed operators stand for the receiving node, {{formula:86a3f5c6-6287-4c8b-bd8b-bb1ab6b35e77}} describes the fragment and cavity field relaxation processes at both nodes, and the last term provides the unidirectional coupling between the two nodes, in which the output of the cavity at the sending node constitutes the retarded input for the cavity at the receiving node. As expected , our simulations show that, provided the photon wavepacket {{formula:785b8685-dedc-4a48-bb3e-bd56994fd47f}} is completely time-symmetric, the processes at the two nodes are the time reversals of each other. Figure REF (b) illustrates the results of our calculations. The parameters used again correspond to a dissociating sodium dimer and the excited state {{formula:b6cc393e-eea8-41bb-983a-edf4e9585c3b}} corresponds to the state {{formula:e4cf6de"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about the application of gyrokinetic equations in plasma studies, and how do the methods used relate to the key results?",
      "expected_output": "The research paper derives the gyrokinetic equation for guiding centres and the neoclassical drift-kinetic equation, which are crucial in plasma studies. The methods used involve decomposing the perturbation into the Boltzmann response and the gyrophase-independent distribution function. Key results include the derivation of transport equations and the calculation of drift velocities due to fluctuating gyrokinetic potential and guiding-centre drift velocity. For example, the gyroaverage of the Boltzmann response time derivative is used to separate the equation into mean and fluctuating parts.",
      "context": [
        "72eca8-5afe-4d59-bc70-99424fb2150c}} is Maxwellian, and that the perturbation {{formula:5429b5ec-d546-4469-9ff6-7429259c432e}} can be decomposed into the Boltzmann response and the gyrophase-independent distribution function for guiding centres, {{formula:efa3d1d9-3c27-49ef-979e-984dc6dac305}} . At {{formula:d40e15ad-4af5-4b96-8297-e4b45654320f}} we derive the gyrokinetic equation for {{formula:8ac0f3da-1c2b-4a73-90a6-c80469e00821}} , and the neoclassical drift-kinetic equation for {{formula:a537b024-08c0-4919-8cb7-bfe02bb4b921}} . We finish our derivation at {{formula:a2646f0e-4ae8-40c1-887b-5fd",
        " {{formula:d40e15ad-4af5-4b96-8297-e4b45654320f}} we derive the gyrokinetic equation for {{formula:8ac0f3da-1c2b-4a73-90a6-c80469e00821}} , and the neoclassical drift-kinetic equation for {{formula:a537b024-08c0-4919-8cb7-bfe02bb4b921}} . We finish our derivation at {{formula:a2646f0e-4ae8-40c1-887b-5fd7923ff4d8}} , but continuing to {{formula:8cbe1540-9b78-4cd3-91e5-a7ca3498f9b2}} we would obtain the transport equations . Order {{formula:d577a9de-3a49-4099-a874-19e6fefda2d0}} : gyrotropy of {{formula:7c7243b3-2f45-41f6-8724-d353e6848",
        "f9dde7-7d89-480c-b72a-d43c2de0db91}} and {{formula:2a0b4e63-b993-44e8-afb5-5dcff6cc709b}} , we find {{formula:dbb191ca-a5ea-45fa-8b96-30131061e213}}where {{formula:b75ca53d-a85c-4402-bbf4-42720f10dba0}}is the drift velocity due to the fluctuating gyrokinetic potential (REF ), and {{formula:b12016e5-8f54-4f86-838a-933f1a29b63a}}is the guiding-centre drift velocity with terms corresponding to the curvature drift and {{formula:21bdc279-ad34-4cdb-9dd4-71f0cce19f86}} drift respectively. We have also used the gyroaverage of the Boltzmann response time derivative (REF ).We separate (REF ) into mean and fluctuating parts using the turbulent average. This gives an equation for"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of ChPT in predicting 2π⁰ photoproduction, and how do the methods used contribute to the key results?",
      "expected_output": "The main findings of Chiral Perturbation Theory (ChPT) in predicting 2π⁰ photoproduction are that this channel is significantly enhanced compared to double pion final states with charged pions, close to threshold. This enhancement is attributed to pion loops in the 2π⁰ channel. The methods used, such as the TAPS BaF₂ calorimeter at the Mainz Microtron accelerator, contribute to the key results by providing sensitive measurements that agree with ChPT predictions and support the dominance of the Δπ intermediate state over the σN intermediate state in the second resonance region.",
      "context": [
        "# arXiv Paper: nucl-ex/0310023\n\nRecent Results from 2 pi-zero Photoproduction off the Proton\nabstract: The reaction $\\gamma p \\to \\pi^0 \\pi^0 p$ has been measured using the TAPS BaF$_2$ calorimeter at the tagged photon facility of the Mainz Microtron accelerator in the beam energy range from threshold up to 820 MeV. Close to threshold, chiral perturbation theory (ChPT) predicts that this channel is significantly enhanced compared to double pion final states with charged pions. The strength is attributed dominantly to pion loops in the 2\\Piz{} channel - a finding that opens new prospects for the test of ChPT. Our measurement is the first which is sensitive enough for a conclusive comparison with the ChPT calculation and is in agreement with its prediction. The data are also in agreement with a calculation in the unitary chiral approach. In the second resonance region, a recent model interpretation of new GRAAL data claimed a dominance of the $\\Roper \\to \\sigma N$ reaction process. We present very accurate invariant mass distributions of ${\\Piz \\Piz}$",
        "sigma N$ reaction process. We present very accurate invariant mass distributions of ${\\Piz \\Piz}$ and ${\\Piz p}$ systems, which are in contrast to the $\\sigma N$ intermediate state and which show a dominance of the $\\Delta \\pi$ intermediate state."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of defects on monoclinic ZrO2 grain boundary properties, and what methods were used to obtain these results?",
      "expected_output": "The main findings of the study show that the equilibrium structure of the monoclinic ZrO2 grain boundary energetically favors oxygen vacancies (V_O^2+) and substitutional Nb defects (Nb_Zr^2-), while the metastable structure has different preferences. These results were obtained using first-principles simulations, specifically density functional theory (DFT) calculations with the CASTEP 17.2 code, which allowed the researchers to examine the structural and electronic properties of the grain boundary with and without defects.",
      "context": [
        "principles methods, such as the requirement for periodic supercells and the limited system size. However, the majority of the oxide layer in Zr alloys consists of the monoclinic phase, which differs in its oxygen coordination numbers compared to the metastable phases. Specifically, two type of O are present, threefold and fourfold in m-ZrO2, as opposed to just fourfold O in the tetragonal and cubic phases. Thus, we expect a difference in the local atomic environment in monoclinic boundaries. In the present study, we used first-principles simulations to study the interaction of oxygen vacancies, Sn and Nb defects with the \\Sigma 3 []{180}{} (100)^{} [001] grain boundary in monoclinic ZrO2, and the effects of these defects on the structural and electronic properties of the equilibrium boundary structure and a metastable boundary structure, which is expected to be representative of a more general oxide boundary.\nComputational details: We performed first-principle calculations of monoclinic ZrO2 grain boundaries using the CASTEP 17.2 density functional theory (DFT) code . The exchange-correlation functional was approximated",
        "# arXiv Paper: 2311.04667\n\nInteraction of monoclinic ZrO$_2$ grain boundaries with oxygen vacancies, Sn and Nb -- implications for the corrosion of Zr alloy fuel cladding\nabstract: We used density-functional-theory simulations to examine the structural and electronic properties of the $\\Sigma 180{\\deg}(100)[001] $ grain boundary in monoclinic ZrO$_2$, which is a very low-energy (0.06Jm$^{-2}$) twin boundary present in experimental oxide texture maps, with suggested special properties. This equilibrium structure was compared with a metastable structure (with a boundary energy of 0.32Jm$^{-2}$), which was considered to be representative of a more general oxide boundary. The interaction of oxygen vacancies, substitutional Sn and Nb defects (substituting host Zr sites) with both structures - and their effect on the boundary properties - were examined. We found that the equilibrium structure energetically favours V$_\\textrm{O}^{2+}$ and Nb$_\\textrm{Zr}^{2-}$, whereas the metast"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What does the V_N(r,θ) form represent in the context of this research paper and how is it used to achieve the main findings?",
      "expected_output": "The V_N(r,θ) form represents the emitted proton-daughter nucleus nuclear potential, which is a cosh parametrized form used to calculate the interaction between the emitted proton and the daughter nucleus. It is used to achieve the main findings by taking into account the deformed effect of the daughter nucleus, with parameters V_0 and a representing the depth and diffuseness of the nuclear potential, respectively. For example, the values of V_0=57.83 MeV and a=0.857 fm are used, as taken from Ref., to calculate the nuclear potential and ultimately understand the proton-daughter nucleus interaction.",
      "context": [
        "l}(r).The emitted proton-daughter nucleus nuclear potential V_{N}(r,\\theta ) is chosen as a type of cosh parametrized form . It can be expressed asV_N(r,\\theta )=-V_0\\frac{1+\\cosh (R_d(\\theta )/a)}{\\cosh (r/a)+\\cosh (R_d(\\theta )/a)},where V_0 and a are the parameters of the depth and diffuseness of the nuclear potential, respectively. We choose V_0=57.83 MeV and a=0.857 fm, which are taken from Ref. . Taking into account the deformed effect of daughter nucleus, the R_d(\\theta ) is given by R_d(\\theta )=R_d^{^{\\prime }}(1+\\beta _2Y_{20}(\\theta )+\\beta _4Y_{40}(\\theta )+\\beta _6Y_{60}(\\theta )),where \\beta _2, \\beta _4 and \\beta _6 denote the quadrupole, hexadecap",
        "),where \\beta _2, \\beta _4 and \\beta _6 denote the quadrupole, hexadecapole and hexacontatetrapole deformation parameters of the residual daughter nucleus, which are taken from FRDM2012 . Y_{lm}(\\theta ) is a spherical harmonics function. R_d^{^{\\prime }} is the spherical radius of daughter nucleus , which can be expressed asR_d^{^{\\prime }}=1.27A_d^{1/3}.Based on the double folding model, the Coulomb potential for the deformed daughter and emitted proton can be expressed as V_{C}(\\vec{r},\\theta )=\\iint \\frac{\\rho _d(\\vec{r_1})\\rho _p(\\vec{r_2})}{\\vert \\vec{r}+\\vec{r_1}+\\vec{r_2}\\vert }d\\vec{r_1}d\\vec{r_2},where \\vec{r} is the vector between the centers of the emitter proton and daughter nucleus. Meanwhile, \\vec{r_1} and \\vec{r_2}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used to produce GO and what are the main findings of this research?",
      "expected_output": "The methods used to produce GO include treating graphite with an oxidant, such as potassium chlorate, and improving this method by adding sulfuric acid and nitric acid. The main findings of this research are that the bridge site is the most stable position for oxygen atoms on graphene, and the study presents results for different concentrations of C/O, including the DOS and energy band structure of GOs to address the critical issue of how to modulate the band gap of graphene.",
      "context": [
        " Fluorine or Chlorine are some ways for designing the energy band structure of graphene in a specified way. Of these mentioned ways, this paper is primarily concerned with an investigation of the properties of oxygenated graphene, also known as graphene oxide (GO). These GOs, doped by oxygen atom either on one side or both sides, are considered to be useful for a wide variety of device-based applications including supercapacitors , , , , energy storage , , sensors , , , mermistors and water purification .Manufacturing GO commercially at a reasonable cost and in an environmentally favorable manner is another challenge for this material. There exist several methods for producing GO. In 1859, Brodie treated graphite with {{formula:702a71e1-edb5-4926-a657-c8e2c8d9161a}} , an oxidant, to oxidize graphite in {{formula:491affc0-6ddb-43b3-8ce4-c2ac1d05f6cc}} . Staudenmaier improved this method later by adding {{formula:7d41ccfa-dfa5-44a6-9e47-474f",
        "-4058-adb8-050530daa7a6}} . The first Brillouin zone is sampled in a Gamma scheme by {{formula:19d517fa-a161-4207-ba11-29f80d5e8a18}} k points for structure relaxations, and then by {{formula:fc694937-2efc-441b-9eac-c8fec24eadd7}} k points for further calculations on electronic properties. We present our results for different concentrations of C/O. Also, we reported the DOS and energy band structure of GOs to address the critical issue of how to modulate the band gap of graphene. The outline of the rest of our presentation is as follows: In Sec. , we present and discuss our results for the band structure of GO for various adatom dopings, its geometrical structure and DOS. Section is devoted to some concluding remarks.\nResults and Discussions: The stable adsorption position of O atoms on graphene is determined by comparing the total ground state energies, in which the lower value corresponds to higher stability. Among the bridge-, hollow- and top-sites, our calculations show that the bridge is the most stable"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does incomplete wetting play in the formation of 3D islands, and how do the researchers' methods and findings support this phenomenon?",
      "expected_output": "Incomplete wetting plays a crucial role in the formation of 3D islands, as it provides the thermodynamic driving force for coherent 3D islanding. According to the research paper, the energy of the boundary between the 3D islands and the wetting layer is not zero, which means that the 3D islands are not completely wetted by the substrate. This incomplete wetting leads to atomic displacements near the island edges, resulting in stronger adhesion and larger misfit, which in turn drives the formation of 3D islands. The researchers' methods and findings support this phenomenon, as they show that the mean adhesion parameter and the thermodynamic driving force for coherent 3D islanding have similar values to those in the classical SK mode at sufficiently large misfit values.",
      "context": [
        " 3D clusters in SK growth as homoepitaxial growth on an uniformly strained crystal surface. If so, it is not clear what is the thermodynamic driving force for 3D islanding if the islands are coherently strained to the same degree as the underlying wetting layer. This question is closely connected with the structure and energy of the boundary between the 3D islands and the wetting layer. The energy of this boundary is often taken equal to zero . This means a complete wetting of the 3D islands by the substrate (the WL) which rules out the 3D islanding from a thermodynamic point of view. It is also not clear why coherent 3D islands are observed in compressed rather than in expanded overlayers, and at values of the misfit {{formula:5a85b342-ad18-443e-ab4e-2cc87bb2cb89}} that are huge for materials with directional and brittle covalent bonds (InAs/InP (3.2%) , Ge/Si (4.2%) , , InAs/GaAs (7.2%) , CdSe/ZnSe (7.6%) ). The only exception, to the authors' knowledge, of expanded",
        " weaker attractive interatomic forces lead to smaller displacements both lateral and vertical of the end atoms and in turn to stronger adhesion. The latter requires larger misfit in order for the 3D islanding to take place.\nDiscussion: The existence of critical misfit clearly shows that the origin of the 3D islanding in the coherent SK growth is the incomplete wetting which is due to the atomic displacements near the islands edges. As seen in Fig. REF the mean adhesion parameter {{formula:d1faf862-9d06-4fd6-b128-182fb739f4ad}} , or which is the same, the thermodynamic driving force {{formula:d42f3081-78e6-46ef-9a7c-c79f4fcd034f}} for coherent 3D islanding has practically the same values as that in the case of the classical (dislocated) SK mode at sufficiently large values of the misfit. Moreover, the comparison of Fig. REF and Fig. REF shows that at a given misfit and a given thickness, there is a critical lateral size (or a critical volume) beyond which MDs are spontaneously introduced to relieve the strain"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do Frank constants play in determining the equilibrium properties of bent core liquid crystals, and how do they impact the overall structure of these materials?",
      "expected_output": "The Frank constants play a crucial role in determining the equilibrium properties of bent core liquid crystals. They impact the overall structure of these materials by influencing the balance between splay and bend tendencies. According to the research paper, the specific equilibrium will naturally depend on both the dimensions of the region under consideration and on the relative magnitude of the Frank energy constants. For example, in the case of a positively defined bend, the Frank free energy for a 2D bent-core liquid crystal is given by a specific expression that can be deduced by taking the two-dimensional and polarized limit of the polar nematic theory of bent core liquid crystals.",
      "context": [
        " useful also to compatible systems as, for example, the Euclidean precompatibilty equation REF must hold for all director fields in the plane. Recent examples where the present formulation provides further insight into the geometry of the director include the theory of thin nematic elastomers , , and in the implementation of the tensorial conservation laws derived to describe nematic polymers .In many liquid crystalline systems, either boundary conditions or the topology of the system lead to non-trivial order. In the frustrated case, even free boundary problems result in non-trivial solutions that in particular depend on the relative value of the Frank coupling constants. The limit {{formula:7c8d9d57-ef7f-46e3-ac49-f19bf82268e3}} corresponds to the case where splay deviations are infinitely more energetically expensive compared to bending variations. We thus expect that in this limit, whenever possible, the system will completely comply with the local splay tendency and minimize the remaining bending term with respect to all such splay-free configurations. For bent core liquid crystals this implies that we seek to minimize {{formula:bb1e9cd4-532f-487b",
        " order to obtain the equilibrium solution for the incompatible case of bent core liquid crystals. For such systems, the nematogen favors a state of vanishing splay and constant bend. As was previously shown, for example in equation (REF ), such a state cannot exist in the plane. Thus every realized configuration will inevitably contain some compromise between the mutually contradicting bend and splay tendencies. The specific equilibrium will naturally depend on both the dimensions of the region under consideration and on the relative magnitude of the Frank energy constants. For a positively defined bend, {{formula:a9e542c3-f4ee-4d7b-b5b1-12f1a683a23a}} , the Frank free energy for a 2D bent-core liquid crystal is given by This expression for the energy can be deduced by taking the two dimensional and polarized limit of the polar nematic theory of bent core liquid crystals, as for example appears in , and identifying the polar vector with the perpendicular to the director, {{formula:76d51917-1fa0-428c-833b-37d2e93a95e3}} . For non-positively defined {{formula:9e5c1ea3-29"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the parameter count of T-CFT without DS compare to other models in the research paper?",
      "expected_output": "The T-CFT module without downsampling has 149.51M parameters, which is significantly higher than other models such as MambaST with 22.52M parameters, and also higher than the downscaled T-CFT and the original CFT.",
      "context": [
        ". The T-CFT module, without downsampling (w/o DS), has 149.51M parameters and very high GFLOPs (8712.36). The downscaled T-CFT and the original CFT had significantly lower GFLOPs (101.63 and 33.87, respectively). The deformable variant of CFT (D-CFT) requires fewer parameters but has high GFLOPs. In contrast, our MambaST was able to achieve superior detection performance while requiring the lowest number of parameters (22.52M) and GFLOPs (5.43). Our MambaST also has a relatively low inference latency (less than 40ms). {{table:286e0344-0da7-4ef8-8476-cef9c4fb467c}}\nComparison with State-of-the-art Methods: Table REF shows the comparison results against the state-of-the-art fusion methods on the KAIST dataset with “sanitized” annotations. MSDS-RCNN combines a convolution neural network-based multispectral proposal network and a multispectral classification network to perform fusion. GAFF proposes inter-",
        "scaled pedestrians. As shown, the CFT and T-CFT, which use arithmetic addition as a fusion strategy, performed poorly. This is likely due to resolution reduction via average pooling, which removed the fine-grained information for small-scaled objects. Our MambaST kept full resolution before the fusion step and produced superior performance across all settings, with a LAMR of 11.37% overall, 13.56% during the day, and 7.32% at night, and achieving the highest recall rate of 99.56%. This demonstrates the effectiveness of our MambaST approach in detecting small-scale pedestrians. {{figure:4de0969c-2da8-4e75-bac0-3e40a12baacc}}\nEfficiency Evaluation: Table REF reports the number of parameters, gigafloating point operations (GFLOPs), and latency (ms) during inference. Note that in our T-CFT experiments, the feature maps were first downsampled to 8\\times 8 before fusion. This reduces the number of parameters and produced better results empirically. The T-CFT module, without downsampling (w/o DS), has 149.51M parameters and"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in distribution and membership between GCs and OCs, and how do these differences impact the research findings?",
      "expected_output": "The main differences between Globular Clusters (GCs) and Open Clusters (OCs) are their distribution and membership. GCs are distributed throughout the whole Galaxy, contain a large number of members, and retain gravitational connectedness for over 10 Gyr. In contrast, OCs belong to the Galactic disk and contain a smaller number of members. These differences impact the research findings as they suggest that the intersection of the Galactic disk by a GC can affect the evolution of OCs, with some pairs showing a high probability of an evolutionary relationship, such as NGC 104 - Turner 3 and NGC 6341 - NGC 6613, which have a maximal probability of 0.51.",
      "context": [
        "chement within 1 kpc at the time of the intersection the Galactic disk by a globular cluster for the latest 100 Myr: NGC 104 - Turner 3, NGC 104 - NGC 6396, NGC 104 - Ruprecht 127, NGC 5139 - Trumpler 17, NGC 5139 - NGC 6520, NGC 6341 - NGC 6613, NGC 6838 - NGC 6520, NGC 7078 - NGC 7063, NGC 6760 - Ruprecht 127.\nINTRODUCTION: As it is known, the main characteristics of globular and open star clusters in the Galaxy differ significantly. Globular clusters (GCs) are distributed throughout the whole Galaxy, contain {{formula:b225b0d7-815c-404e-9a4d-1f8c6d58b1a6}} members, which for a long time retain gravitational connectedness, on average, for more than 10 Gyr. Open clusters (OCs) belong to the Galactic disk, contain no more than {{formula:49b7e333-b5ab-4c07-8496-2bd49570c883",
        " of both open clusters is very likely to have been caused by the same reason, namely, the intersection of the disk of the Galaxy by the NGC 104 51 million years ago.In the Figures, we display the Galactic orbits of the globular and open clusters, calculated using the total gravitational potential consisting of three component (bulge, disk, halo) axisymmetric potential and non-axisymmetric potentials of bar and spiral density wave. The orbits of the GCs (drawn by gray, red in color version) are integrated on the interval of 200 million years backwards. The OCs orbits (drawn by light, yellow in color version) are integrated on the time interval {{formula:b7d5abde-79c6-4c05-824d-fb3773fc5765}} up to the moment of intersection of the Galactic plane by a corresponding globular cluster. The points in the Figures are obtained using Monte-Carlo simulations and make up the confidence areas corresponding to the level {{formula:c8de3983-1069-4b07-8772-fdbf056e482a}} . The brightest points (yellow in color) correspond to the orbit end points of OC",
        " time of the intersection the Galactic disk by a globular cluster for the latest 100 Myr were obtained. These are the following pairs:NGC 104 – Turner 3,NGC 104 – NGC 6396,NGC 104 – Ruprecht 127,NGC 5139 – Trumpler 17,NGC 5139 – NGC 6520,NGC 6341 – NGC 6613,NGC 6838 – NGC 6520,NGC 7078 – NGC 7063,NGC 6760 – Ruprecht 127.For these pairs, a probability of possible evolutionary relationship was estimated using Monte-Carlo simulation. The probability is maximal, equal to 0.51 for pairs NGC 104 – Turner 3 and NGC 6341 – NGC 6613. Globular cluster NGC 5139 ensures the maximal impact energy in the pairs with open clusters Trumpler 17 and NGC 6520.\nAcknowledgments: The authors are grateful to the anonymous referee for critical remarks that contributed to an improvement of the paper. The authors are thankful to Kirill Maslennikov and Alexander Tsvetkov for their assistance in preparing the text."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the gate voltage affect the resistance of a single-layer graphene field-effect transistor, and what are the key findings related to this modulation?",
      "expected_output": "The gate voltage affects the resistance of a single-layer graphene field-effect transistor by modulating the Fermi energy and consequently the resistance of the component far enough from the metallic contacts. However, the resistance of the component very close to the contacts remains essentially independent of the back gate voltage. Key findings include that the device acts as two resistors in series, and the range of carrier density over which the SLG device shows an insulating behavior matches closely with the impurity number density extracted from the given equation.",
      "context": [
        " unit area of the SLG FET device and {{formula:e7acb99e-282a-4a8c-8a4f-1098766b4129}} is the charge impurity concentration in the SLG device. It should be noted that {{formula:a76a9a4f-be53-488b-8c22-848dff816b53}} is not a contact resistance in the conventional sense – it is the resistance of the part of graphene monolayer very close to the contact leads where the effect of the back gate is screened off by the metallic leads. The device thus effectively acts as two resistors in series – a component far enough from the metallic contacts whose Fermi energy (and consequently resistance {{formula:c9009434-bae6-47f2-bee0-e33b290bf982}} ) can be modulated by the gate voltage {{formula:ac33db1e-1a0e-4d32-86c0-7936f4462482}} , and another component very close to the contacts whose resistance {{formula:dd1c8903-1391-4181-9",
        " component very close to the contacts whose resistance {{formula:dd1c8903-1391-4181-9d04-28b357d5141c}} is essentially independent of the back gate voltage. We have performed extensive three-dimensional electrostatic simulations to extract the potential profile in the conduction channel (on top of SiO{{formula:332a6973-9791-4930-a6f8-a42566942f1e}} dielectric) due to the presence of the metallic contact pads using the Finite Element Method (FEM) – a typical result for the exact geometry of device g28m6 is shown in figure REF . The simulations were performed using COMSOL Multi-physics 3.5 software environment. In the simulations a potential of 10 V was applied between the contact pads and the back gate. It can be seen that the potential is quite small in the region very close to the metallic contact pads. The section of graphene monolayer lying in this region thus practically experiences no gate voltage – consequently the resistance of this section is independent of {{formula:1fb626c1-8003-4b0c-a783-55768b96c05",
        "3c29-4987-b632-40b146d2142d}} and quite low value of {{formula:6fae2081-a09b-4dd5-90d8-45841324c272}} ) the TCR is negative over a much larger gate voltage range. We find in all cases that the range of carrier density over which the SLG device shows an insulating behavior (dR/dT{{formula:2a790f6b-c7f7-4571-a58e-3ab6afe48b12}} 0) matches very closely with the impurity number density extracted from equation REF . This can be understood using the following simple picture: in undoped graphene there are strong bipolar charge density fluctuations , , . This macroscopic charge inhomogeneity leads to random p- and n-type charge clusters in the device leading to a percolating current transport near the Dirac point. Transport in this regime is thus determined by the linkage between the percolating clusters rather than by the properties of the clusters themselves. As the chemical potential moves away from the Dirac point, eventually the number of carriers doped into the system exceeds the number density fluctuations"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does equation (1) relate to the Kardar-Parisi-Zhang equation, and what are the key findings of this mapping in the research paper?",
      "expected_output": "Equation (1) is mapped to the well-known Kardar-Parisi-Zhang (KPZ) equation, as noted in the research paper. The mapping is achieved through a transformation that involves a specific parameter, which is not renormalized under any renormalization procedure. A key finding of this mapping is that it allows for the determination of the OPE coefficients in equation (8) and the calculation of a specific formula in the case of a conservative random force, which is important for the KPZ equation.",
      "context": [
        "f-9366f5c832ca}} allows us to determaine the OPE coefficients in eq.(8), and calculate {{formula:7bd93a96-7b9e-4d82-8ba5-1b66574f6a0e}} in the case where the random force is conservative, which is important for the KPZ-equation, work in this direction is on the way.Acknowledgements: We wish to thank Vahid Karimipour for valuable discussions.",
        "-e69ef748c42a}} maps eq.(1) to the well-known KPZ equation [5], {{formula:06b79a30-587a-4148-9bf9-36978acdfcce}}It is noted that the parameter {{formula:93025ace-e1a7-4b36-bad7-49fbaaf765ec}} that appears in the above transformation is not renormalized under any renormalization procedure [8]. Following Polyakov [1] consider the following generating functional {{formula:9be9900b-a87b-405b-9e2b-7466556e6f73}}Noting that the random force {{formula:7bd9bf6f-4f1b-4938-a34c-dc8837403eb5}} has a Gaussian distribution {{formula:a33d4c03-10e2-48b4-ba43-bd7083d9314e}} satisfies a closed differential equation provided that the viscosity {{formula:86775c30-2a9b-4b1b-888c-ada473817339",
        " {{formula:40ecff48-9a24-43ef-a25e-1f1c54936c22}}which is solved by: {{formula:8290548e-60ac-4da9-bef7-34a41b6c6259}}In principle the parameter {{formula:18052091-7e29-456f-81e6-673b8e170277}} in eq.(15) can be evaluated by means of the exponents of {{formula:14702557-f0d2-4d17-ab4a-59b2b648e4cf}} term in eq.(21) which for {{formula:e1011aef-633c-4888-b2f0-6240ff82c656}} it turns out that {{formula:875be3ed-8f29-4877-8717-50b06cb6aaa8}} . Our exact result for {{formula:06a238b1-8277-4696-850f-9366f5c832ca}} allows us to determaine the OPE coefficients in eq.(8"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the power-law play in modeling the behavior of shear-thinning fluids, and how does it impact the accuracy of the model's predictions?",
      "expected_output": "The power-law model plays a significant role in modeling the behavior of shear-thinning fluids as it describes the rheology of these fluids using a simple two-parameter model. However, its accuracy is impacted by the absence of plateaux at low and high strain rates, which can lead to unrealistic results in these flow regimes. For example, as shown in Figure REF, the bulk rheology of the power-law model for shear-thinning case is presented generically on log-log scales, demonstrating its ability to model shear-thinning behavior. Nevertheless, the model's limitations, such as the lack of plateaux, can affect the accuracy of its predictions.",
      "context": [
        "d94046-d15c-45f4-8723-e38098739711}} Relation for Power-Law Fluids: The power-law, or Ostwald-de Waele, is a widely-used fluid model to describe the rheology of shear thinning fluids. It is one of the simplest time-independent non-Newtonian models as it contains two parameters only. The model is given by the relation , {{formula:bc0315a0-f729-4cc6-98e1-7ad492ba4248}}where {{formula:27e9ba1a-1413-4221-bd81-8c7ebbc9b2fa}} is the fluid viscosity, {{formula:6fe88c40-16a8-464d-93df-048f6230dacd}} is the stress, {{formula:4163b540-3652-4a1d-9e9a-c34655175ccf}} is the strain rate, {{formula:5411f73b-2dd5-4107-ad58-ccc473932a72}} is the",
        "11f73b-2dd5-4107-ad58-ccc473932a72}} is the consistency factor, and {{formula:bc671f5e-a863-4263-9193-afa57f99c098}} is the flow behavior index. In Figure REF the bulk rheology of this model for shear-thinning case is presented generically on log-log scales. Although the power-law is used to model shear-thinning fluids, it can also be used for modeling shear-thickening by making {{formula:d3c04dd8-233f-4210-81fb-bddc5ee4eada}} greater than unity. The major weakness of power-law model is the absence of plateaux at low and high strain rates. Consequently, it fails to produce sensible results in these flow regimes. {{figure:781febbe-4679-4a51-bc89-34df5df92cba}}The volumetric flow rate, {{formula:7bbc992e-7a66-427c-8355-d2655b6c4492}} , of a power",
        "e-7a66-427c-8355-d2655b6c4492}} , of a power law fluid through a circular capillary of constant radius {{formula:c870b790-35fb-468d-8560-39af9dab3825}} with length {{formula:5451738e-134b-4e78-a648-a5960a0475a3}} across which a pressure drop {{formula:4ac0729e-00c3-4b53-9db3-49db50b50b85}} is applied is given by {{formula:c2eaf657-2469-4a05-8ee6-b7a29df9ef78}}A derivation of this relation, which can be obtained from Equation REF , can be found in , . On solving this equation for {{formula:e33edafd-40e7-4d16-b975-23955a1affeb}} the following relation is obtained {{formula:7bc52cd3-848d-4833-a6f5-647d5835898a}}For an infinites"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of ADAM optimization in model training, particularly when using a learning rate of 2e-5, and how does it compare to other optimization methods?",
      "expected_output": "The main finding of ADAM optimization in model training, as used in the SP-Net with a learning rate of 2e-5, is its effectiveness in converging to optimal parameters. ADAM is an adaptive learning rate method that adapts the learning rate for each parameter based on the magnitude of the gradient, which helps in stabilizing the training process. In the context of the paper, ADAM outperforms other optimization methods by providing a more stable and efficient convergence. For instance, the use of ADAM with a learning rate of 2e-5 and a weight decay of 5e-5 leads to better performance compared to other optimization methods. However, the paper does not provide a direct comparison with other optimization methods, but the choice of ADAM indicates its suitability for the task at hand.",
      "context": [
        ".83% decreases). This phenomena gives another strong evidence of negative knowledge transfer.\nTraining Details: Hyperparameters The BERT in SP-Net is the pre-trained uncased BERT-Base . We use ADAM to train the model with a learning rate of 2e-5, a weight decay of 5e-5. And we set VPB as the similarity function for prediction. For the weights assigned to each loss, we set {{formula:a92bec38-1c6f-4506-84e6-6b419760f82d}} , {{formula:4b44da4a-0497-4e56-967e-565774d46250}} , {{formula:bd5565da-5f77-4650-89e4-69aef617bde9}} , and {{formula:9c57f5b2-9de2-4615-bcec-08429f43ecfa}} as 0.2, 0.1, 0.2, and 0.5 respectively. Those hyperparameters mentioned above are derived from the best implement in our experiments. To prevent the impact of randomness, we do",
        " hyperparameters mentioned above are derived from the best implement in our experiments. To prevent the impact of randomness, we do each experiment 10 times with different random seed and report the average results.Data Selection Due to cross validation, each domain is used in turn as a test domain. As such one domain used for training and may be used for testing next time. Therefore, if we set a group of global similarity combination weights {{formula:ee12b517-25ea-4182-8c55-0270ff9b42fc}} , {{formula:d59a6167-311a-48e2-8c78-f88e5043c361}} , and {{formula:80f8b09a-9319-4eea-8819-6733044c5a9a}} according to all experimental results, it must lead to test data leakage. This is unfair for the comparison. To this end, we set {{formula:9fe2b2bf-e534-462d-b0a5-870ebc9d03ba}} , {{formula:17b083d9-43ba-4629-b389-e629322"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the effect of the formula on relic density and how does it relate to the main findings of the research paper?",
      "expected_output": "The effect of the formula on relic density is that it largely dominates the corrections due to the variations of the neutralino-Higgs couplings, especially in the range of large values where the specific condition is fulfilled. This is a key finding of the research paper, which suggests that the quantity in question has a significant impact on the relic density and the scattering cross-section.",
      "context": [
        "}} varies rather slowly with {{formula:8fbf57ab-8b6e-466a-8780-3b64e0b564f0}} , we find that in the range of large {{formula:0b196ba7-2759-4142-a1b5-1064d7ed0939}} where {{formula:9a9dc551-68b0-4380-aaf1-05feb1e7de07}} -{{formula:58e522c0-cae1-4ed0-8993-9a3b6d0db5da}} YU is fulfilled, the effects on the relic density and on {{formula:3e742a7e-ff9f-43ff-aead-5e2a73d834f0}} driven by the quantity {{formula:cb7af5f0-29b2-4016-be87-3e17a94fbc6b}} largely dominate on the corrections due to the variations of the neutralino-Higgs couplings. The reason is that in this range of {{formula:b9428a73",
        "-ed73-4f5f-b48f-2220bbe66c1d}}We now turn to the question of the sensitivity of the overall outlined scenario on the particular chosen values of the couplings {{formula:903a067c-51a3-4d26-bb11-8f98927ccc54}} and {{formula:188224e4-3f50-45c7-a019-2fb803a6603c}} . We focus on the variations induced in {{formula:25765846-a8b9-435e-976b-2f89e2a951fd}} and in {{formula:5fffe9ec-743c-43a4-9e3e-a9b9785b8b35}} , which are directly responsible for the relic density suppression, as well as in the neutralino-neutralino-Higgs couplings, on which both the neutralino relic density {{formula:af422d1b-8a95-4500-89a6-f922a36363e1}} and the scattering cross-section {{formula:d53971",
        "f-73fe890b6a7b}} -{{formula:64d7e17a-f7fe-42f0-9a38-2a927ad5dd4b}} YU, and, at a given {{formula:f9997b7e-5ca4-4d79-99ba-7f020b1d3b22}} , is always above the corresponding {{formula:344f5f39-8a16-450a-adc1-5d0ac0cb26cd}} . For completeness, we also include the curves at fixed values of the lightest neutral Higgs {{formula:bbd2d485-12ec-4e78-a97d-788f99424d8b}} , for which the LEP2 bound gives {{formula:20293ab7-1053-44ca-94c6-e8e725607670}} . We notice that the bound on the Higgs mass, which in the MSSM at large {{formula:f4b2c0d7-a4c4-4a7b-b7ac-83"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main methods used to simplify wave functions in quantum mechanics and what are the key results of this process?",
      "expected_output": "The main methods used to simplify wave functions in quantum mechanics include approximations and projections onto physical bases. According to the research paper, the treatment for the wave function is an approximation to reduce tedious calculations and make the components orthogonal. The effective relative wave function is obtained by projecting onto the physical base and is described by the sum of four Gaussian functions. This process results in a simplified wave function that reasonably contains most of the effect of anti-symmetrization, allowing for more manageable calculations and a better understanding of the system's behavior.",
      "context": [
        " for simplicity, the wave function of {{formula:39f701f3-8fe9-45f2-8c1b-646012d363ce}} can be rewritten as {{formula:7428ae06-a89b-41e4-8b94-9d8449e8e16a}}We should emphasize that our treatment for the wave function of {{formula:2dfb6cfd-124d-4ff1-b8b4-89414b6abc25}} is just an approximation for reducing tedious and almost inoperable calculations and making the two components orthogonal. The obtained effective relative wave function can reasonably contain most of the effect of anti-symmetrization of the wave function of {{formula:65276645-a63c-4402-b9f5-1f7a0f15dc95}} shown in Eq. (REF ). The effective wave function is obtained by projecting onto the physical base, and further described by the sum of four Gaussian functions. {{formula:0c71c7b9-0ad5-494a-aa26-e42b20080f03}}It should",
        "anti-symmetric) wave functions in the spin and isospin spaces, {{formula:68c9f51f-8e9f-4fed-a778-f985606025d0}} the spatial wave function and {{formula:83312034-aabc-444b-b4db-64e8b7ad08dc}} and {{formula:f556599b-f394-4dbd-8e78-1be370f08909}} the Jacobi coordinates. In the same way, the wave function of {{formula:f698b638-35e9-45ad-abbd-e6dc8f08d655}} can be abbreviated to the form {{formula:986d7f83-8c85-4031-be3b-3caeda0dd947}}where {{formula:29eccf06-0c86-4a3c-8d86-f507d1b4259b}} is the total anti-symmetrization operator, {{formula:60580008-c458-4ff3-9f50-e62d5",
        "wave relative wave functions between {{formula:8c9c7e7c-5370-4097-9bfd-1b63b2f9c31a}} and {{formula:70747032-19fe-4bd5-96b0-8d7df65afbe9}} clusters (the {{formula:732d3d1b-0668-42b9-a3e8-c9ae31641cb2}} -wave components are negligibly small), and {{formula:fe23abde-39fb-4eb3-87e7-c68819b687ed}} , {{formula:035cac99-d967-4bd8-a873-ecb60856be38}} stand for the spin-isospin wave functions in the hadronic degrees of freedom in the corresponding channels, respectively. The channel wave function can be defined as {{formula:eedaaead-e677-43a5-b5da-9e339bc6a6ea}}Therefore, for simplicity, the wave function of {{formula:39f701f3-8fe9-45f2-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in how the Bose-Fermi Kondo model interacts with bosonic versus fermionic baths, and how do these interactions impact the model's key results?",
      "expected_output": "The Bose-Fermi Kondo model interacts differently with bosonic and fermionic baths. The fermionic bath of conduction electrons leads to the Kondo effect, where the spin is screened. In contrast, the bosonic bath of spin fluctuations introduces dissipation, which can destroy the Kondo effect for sub-Ohmic dissipation. This destruction occurs when the spectrum of the bosonic bath has a form that leads to decoupling of the spin from the conduction electrons, resulting in a fraction of spins decoupling even for weak coupling to the bosonic bath.",
      "context": [
        " the conduction electron spin density at site {{formula:89c8e879-58c4-4c89-9568-7e3d3abd2084}} , respectively. We concentrate on the paramagnetic phase. Applying the standard procedure to average over disorder in the {{formula:30c9b72c-5a7c-4063-8c96-756d81faf96b}} couplings and taking the limit of infinite coordination {{formula:dd18f3bf-01af-4320-ac36-016167ae495c}} , the local effective action assumes the form {{formula:7e807320-ddff-4cf2-9414-0b57193915ac}}The local action of Eq. (2) describes the so-called Bose-Fermi Kondo model , , which describes a Kondo spin interacting with both a fermionic bath of conduction electrons and a bosonic bath of spin fluctuations. For a disordered Kondo lattice, we must consider an ensemble of such impurity models supplemented by following self-consistency conditions. The bosonic spin bath {{formula:97f",
        " such impurity models supplemented by following self-consistency conditions. The bosonic spin bath {{formula:97f6ed8e-053c-4c90-a826-49c848e887f1}} and the conduction electron bath {{formula:dc0e04d5-e892-4e57-a453-63eff01c3a1b}} are obtained by appropriate disorder averaging , and for simplicity we use a simple semicircular model density of states for conduction electrons.Destruction of the Kondo effect. The presence of RKKY interactions introduces a qualitative modification in the dynamics of the Kondo spins, through the presence of a dissipative bosonic bath of spin fluctuations. This behavior depends crucially on the precise spectral form of the bosonic bath, allowing for the destruction of the Kondo effect in the presence of sub-Ohmic dissipation , . For a spectrum of the form {{formula:b08585a7-295d-4266-af52-3120073a2ee9}}Fermi liquid behavior is recovered for {{formula:cb9263c0-43ec-47f6-b1c4-",
        " is recovered for {{formula:cb9263c0-43ec-47f6-b1c4-7a81a83a068b}} , but for {{formula:577a9209-7da4-45c5-b9c7-71da9a9356d8}} (sub-Ohmic dissipation), and for sufficiently small bare Kondo temperature {{formula:0e816277-5aec-4933-b6d3-929ab02b6cfc}} , the spin decouples from the conduction electrons. Within an electronic Griffiths phase, however, the disordered Kondo lattice has a very broad distribution of local Kondo temperatures {{formula:765700e4-b519-4531-99cc-b5085e4a4218}} . Therefore, for {{formula:7e907b45-2f96-4aeb-8342-9b3c03f4d79f}} and arbitrarily weak coupling to the bosonic bath (i.e. weak RKKY interaction), a fraction of the spins will decouple.To obtain a sufficient condition for decoupling"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does S-FGRM enhance adversarial transferability in the context of this research paper, and what are the key findings related to its methods and results?",
      "expected_output": "The S-FGRM enhances adversarial transferability by combining two strategies: data rescaling and Depth First Sampling. Data rescaling substitutes the sign function to provide a more accurate gradient approximation, while Depth First Sampling stabilizes the update direction by eliminating local fluctuation errors. The key findings are that S-FGRM mitigates overfitting, improves the attack transferability, and is generally applicable to any gradient-based attack. For example, the paper shows that the synthetic directions produced by the sign function are limited, and the proposed method can investigate the model decision boundaries, reducing overfitting and improving transferability.",
      "context": [
        " the minimal values of the gradient. To compensate for this deficiency, we propose our Depth First Sampling Method. By sampling around the input image, the negative influence of small value fluctuations will be alleviated. Unlike typical sampling methods, starting with the second sampling, we sample around the previous sampled image rather than the original input image. This strategy is similar to depth-first search. It not only mitigates the rescaling uncertainty but also investigates the model decision boundaries, which reduces overfitting to some extent and improves the attack transferability. To sum up, we propose the Sampling-based Fast Gradient Rescaling Method (S-FGRM) to further enhance the adversarial transferability by combining the above two strategies.\nFast Gradient Rescaling Method: Typical gradient-based adversarial attack methods, like I-FGSM, MI-FGSM, NI-FGSM, etc., are inextricably linked to the sign function. There is no doubt that the sign function provides sufficient perturbation for the gradient update, allowing crafted adversarial examples to approaching their targeted class more easily. Nevertheless, it is not the optimal or near-optimal solution. The synthetic directions produced by sign are limited, as we have illustrated in Fig.",
        "# arXiv Paper: 2307.02828\n\nSampling-based Fast Gradient Rescaling Method for Highly Transferable Adversarial Attacks\nabstract: Deep neural networks are known to be vulnerable to adversarial examples crafted by adding human-imperceptible perturbations to the benign input. After achieving nearly 100% attack success rates in white-box setting, more focus is shifted to black-box attacks, of which the transferability of adversarial examples has gained significant attention. In either case, the common gradient-based methods generally use the sign function to generate perturbations on the gradient update, that offers a roughly correct direction and has gained great success. But little work pays attention to its possible limitation. In this work, we observe that the deviation between the original gradient and the generated noise may lead to inaccurate gradient update estimation and suboptimal solutions for adversarial transferability. To this end, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM). Specifically, we use data rescaling to substitute the sign function without extra computational cost. We further propose a Depth First Sampling method to eliminate the fluctuation of rescaling and stabilize the gradient update. Our method could be used in",
        "prime } of the black-box model more than rescale. We argue that such inaccuracy of gradient might mislead the optimization path as shown in Fig. REF (b) (the color shade represents the values of the function). The iterative process might fall into suboptimum compared to rescale and weaken the adversarial transferability.Based on the above analysis, we propose a Sampling-based Fast Gradient Rescaling Method (S-FGRM) to have a more accurate gradient approximation and boost the transferability of gradient-based attacks. Specifically, we design a data rescaling method for the gradient calculation, that directly exploits the distribution of gradient values in different dimensions and maintains their disparities. Moreover, as most gradient values are extremely small, as shown in Fig. REF (a), we further introduce a sampling method, dubbed Depth First Sampling, to stabilize the update direction. Our sampling method eliminates the local fluctuation error and further boosts the transferability of the crafted examples. Note that our proposed method is generally applicable to any gradient-based attack and can be combined with a variety of input transformation or ensemble methods to further enhance the adversarial transferability. Extensive experiments and analysis show that our integrated methods mitigate the overfitting issue and further"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings related to the density of Fredholm maps in this research paper, and how do the methods used contribute to these results?",
      "expected_output": "The main findings of the research paper are that the set of Fredholm maps is residual and dense. This means that for any given map, we can find a nearby Fredholm map with nondegenerate equilibria. The methods used to achieve these results involve defining a Fredholm map as a map whose derivative is a Fredholm operator for each point in the domain. The paper also utilizes a theorem of Smale, which generalizes Sard's theorem to Fredholm maps, to show that the set of Fredholm maps is dense. Specifically, the paper claims that the operator is a Fredholm map, and its derivative is a Fredholm operator for each point, contributing to the density of Fredholm maps.",
      "context": [
        " Fredholm maps) {{formula:a4b2ece7-3af2-4753-888b-ba14eeeda31c}} is a residual (i.e., a countable intersection of open dense sets) set. Being residual, it is dense. Therefore, for any {{formula:2e548f42-7335-41ab-903a-14ed4742d05c}} we can find {{formula:7e083515-476c-498a-853d-4188a80b0f0b}} with {{formula:86f1db85-a900-418a-b8d1-684633a65838}} . With this choice of {{formula:c698b23d-c8c8-4a0f-a6dc-f5d3895bdfba}} {{formula:eebcba26-a24f-489d-a07b-921765b8b600}}and {{formula:23fb7de9-dafa-4d40-b0c2-3b4cd2dfa48d}} has nondegener",
        "afa-4d40-b0c2-3b4cd2dfa48d}} has nondegenerate equilibria. This concludes the proof of the density.To see that {{formula:e1e628ae-2456-415a-a605-4ef43c6c6b5c}} is Fredholm, write {{formula:95d266a0-6180-4140-8c09-80abbd464df3}} where {{formula:9401a0bd-053f-4534-8c7f-9deb78c52f93}} and {{formula:07214a9c-7f0f-4a2a-8398-6992ca3a79c2}} are respectively defined by {{formula:920d5af8-be62-4766-8ca8-42bc0e2c28d3}} and {{formula:a3f6c1ce-9b7a-45de-8b37-cd5e3f25eb53}}The operator {{formula:1a74272a-5d4a-4a58-",
        "8b4c-2d645c5e50cf}} if and only if 0 is a regular value of {{formula:499c7b02-f0da-433f-82b7-21e80a8204c8}} or, equivalently, a regular value of {{formula:59fd3999-ac8a-486b-add0-2044ced1cbfc}}We claim that {{formula:e4a2f4c2-a687-4f67-a4f7-753690bd8ee1}} is a Fredholm map. That is, a map whose derivative {{formula:b96df4be-ad87-49f7-8eee-ea321ec43b16}} is a Fredholm operator for each {{formula:d62c01bf-ce02-415c-8f54-823b27a94562}} (see Section for the definition of a Fredholm operator). Hence by a theorem of Smale (1965) generalyzing Sard's theorem to Fredholm maps) {{formula:a4b2ece7-3af2-4753-888b"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do Constant objects enable modification of UFL forms without recompilation, and what are the main findings related to this approach in the research paper?",
      "expected_output": "Constant objects enable modification of UFL forms without recompilation by wrapping constants in Constant objects, allowing changes to be made without recompiling UFL forms. The research paper finds that this approach is useful in subclasses of LowReynolds, where constants like Cmu are defined as attributes in the class by updating the __dict__ dictionary. For example, in the LowReynolds_Coupled class, constants are wrapped in Constant objects using a for loop, enabling easy modification of UFL forms.",
      "context": [
        " objects defining the forms we need a constant like Cmu as an attribute in the class, and this is accomplished by simply updating the __dict___ dictionary. Also note that constants should be wrapped in Constant objects if they enter variational forms written in UFL. That way constants may be changed without the need to recompile UFL forms.Subclasses of LowReynolds define a segregated or a coupled scheme. For example,class LowReynolds_Coupled(LowReynolds):def __init__(self, problem, parameters):LowReynolds.__init__(self, system_composition=[['k','e']],problem=problem, parameters=parameters)def define(self):LowReynolds.define(self)classname = self.prm['time_integration'] + '_ke_' + \\str(self.prm['scheme']['ke'])self.schemes['ke'] = eval(classname)\\(self, self.system_composition[0])Two actions are performed: definition of the subsystems to be solved (here the {{formula:42f8a58b-8dd6-4a5a-89",
        "prm['model']self.model_prm = dict(Cmu=0.09, sigma_e=1.30,sigma_k=1.0, e_nut=1.0, e_d = 0.5, f1 = 1.0)Ce1_Ce2 = dict(LaunderSharma=dict(Ce1=1.44, Ce2=1.92),JonesLaunder= dict(Ce1=1.55, Ce2=2.0),Chien= dict(Ce1=1.35, Ce2=1.80))self.model_prm.update(Ce1_Ce2[model])# wrap in Constant objects:for name in self.model_prm:self.model_prm[name] = Constant(self.model_prm[name])# store model_prm objects as class attributes:self.__dict__.update(self.model_prm)Dictionaries are useful for storing the data, but in the scheme objects defining the forms we need a constant like Cmu as an attribute in the class, and this is accomplished by simply updating",
        ".prm['iteration_type'] == 'Picard':F = self.form(**form_args)self.a, self.L = lhs(F), rhs(F)elif self.prm['iteration_type'] == 'Newton':form_args['k_'], form_args['e_'] = solver.k, solver.eF = self.form(**form_args)ke_, ke = unknown, solver.keF_ = action(F, function=ke_)J_ = derivative(F_, ke_, ke)self.a, self.L = J_, -F_As in the CoupledBase constructor for the NS schemes, we send all attributes in the solver class as keyword arguments to the form methods. Most of these arguments are never used and are absorbed by a final **kwargs argument, but the number of variables needed to define a form is still quite substantial:class Steady_ke_1(KEpsilonCoupled):def form(self, k, e, v_k, v_e, # Trial and TestFunctionsk_, e_, nut_, u"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about the relationship between principal ideals and quadratic fields, and how do the methods used impact the key results?",
      "expected_output": "The research paper explores the relationship between principal ideals and quadratic fields, specifically focusing on imaginary quadratic fields. The main findings include the establishment of a basic property of principal ideals, as stated in Lemma 2.3, which provides insight into the similarity of certain ideals. Additionally, the paper discusses the conditions under which an ideal is not WR, highlighting the importance of principal fractional ideals in quadratic fields. The methods used, such as combining Lemma REF with Lemma REF, enable the derivation of key results, including the conclusion that certain fields are principal ideal domains. For instance, the fields represented by formulas 606cfcf7-a86c-43e9-ba7f-a8251755fd62 and 411b03de-3150-4198-bef6-7706e877c530 are principal ideal domains, demonstrating the significance of the research findings.",
      "context": [
        "89d8-b881f398cca2}} a squarefree integer, so that {{formula:8e01c880-a3e4-47b6-af55-782a8444c0f9}} ). We first consider imaginary quadratic fields, and start by establishing a basic property of principal ideals.Lemma 2.3 Let {{formula:5cec5e37-7ff6-40e2-ad79-cafaab8a911a}} be an imaginary quadratic number field. If {{formula:598d45a4-ada8-415f-bf0b-1d43b286d782}} is a principal ideal and {{formula:4b636041-9830-4247-ae81-661d7afce784}} , {{formula:aa1f14f5-f5e0-4018-af15-fc726cb94fad}} , is a fractional ideal, then {{formula:8081ce1d-1e07-4ab7-a111-8eeb38177d12}} is similar to {{formula:36b12de",
        " squarefree integer and {{formula:cc850291-6c47-403b-8d00-60b3f03da4db}} be a quadratic number field. Let {{formula:3fa4be12-8f0f-4174-852f-0176c6720c33}} be an ideal, then {{formula:78d3f668-66f3-43a6-8e9f-2d99e3954e8c}} .Let {{formula:359ddaf8-8229-4dd9-b9c0-ab18e8693761}} be a squarefree integer, {{formula:da4425d5-6604-4ff5-85d4-44d36c4e7bac}} be a quadratic number field, and {{formula:16d809c5-91e1-44f1-a19b-035593b3dc57}} an ideal. By Lemma REF , {{formula:452a346a-3b87-4b78-973e-75debee68fc0}} unless {{formula",
        "-dadbbe6e2877}} is an imaginary quadratic field {{formula:a259eb06-5a21-4869-93dd-d6f43f112a36}} and {{formula:52048304-a409-4735-b03f-70ed90ace9f7}} is a principal fractional ideal in {{formula:248387eb-0701-4e3f-bdc5-cb46634cfe2a}} , then {{formula:aad58829-750d-4dbf-8a14-36859361738a}} is not WR.Both of the fields {{formula:606cfcf7-a86c-43e9-ba7f-a8251755fd62}} or {{formula:411b03de-3150-4198-bef6-7706e877c530}} are principal ideal domains, and so the first statement follows by combining Lemma REF with Lemma REF . On the other hand, {{formula:24f30511-bff6-4ce6-9681-cf4019930907}} is not WR whenever"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do quantum fluctuations play in inducing electron tunneling between classical states, and how do the researchers measure this phenomenon in their study?",
      "expected_output": "Quantum fluctuations play a crucial role in inducing electron tunneling between classical states in the study. According to the research paper, quantum fluctuation makes the electron tunnel between the two classical states in the wells, splitting the degenerate ground states and creating a two-level qubit system. The researchers measure this phenomenon by estimating the tunneling energy regarding the potential as quadratic and analyzing the effective Hamiltonian of the system. For instance, when the external flux is slightly different from the ideal value, the two minima of the double well are no longer degenerate, and a gap is created, which can be controlled by adjusting the flux through the rhombus.",
      "context": [
        "5dee-4265-beee-497966760211}} . Quantum fluctuation makes the electron tunnel between the two classical states in the wells and that splits the degenerate ground states. The low-energy configuration can be regarded as a two-level qubit system. The effective Hamiltonian is {{formula:9e9125db-df1b-45ec-acfd-fb24d8061a0d}}where {{formula:95e3cd0e-e147-4afc-984d-5a93f154ccdf}} , and {{formula:6df5b65f-4657-473b-8051-b77cc42b78c6}} is the classical action taking the path between the two minima. The value of tunneling energy is estimated regarding the potential as quadratic.Let us see what happens when the condition is slightly changed from the ideal. Notice that the dynamics require that the flux through the rhombus is exactly half a flux quanta. If the external flux is a bit different from the ideal value, {{formula:b9eea21d-0bb2-4f56-8041-52d",
        ", {{formula:b9eea21d-0bb2-4f56-8041-52d00c5cef46}} , we add a first order change to the potential, {{formula:91440200-7d94-47c5-a11d-4779826074bf}}As a result of the slight change of the flux, the two minima of the double well are no longer degenerate but separated by a gap {{formula:f893e007-5bbe-4e59-96c9-595c7e85e5e2}} . Then the effective Hamiltonian is {{formula:d0562be7-98a2-4069-80f8-85ea6f6a7c16}} .This is important for operations. We can control the evolution of the system by adjusting the flux through the rhombus. Below, in the Josephson array building from the basic rhombi, logical qubits are encoded for computation. Some gate operations are done just by the method of tuning the flux of the rhombi along specific paths.\nArray: Now we have got the bricks we need. In this part we show",
        "23577}}where {{formula:16ab9d89-dcc9-4c3a-bf93-204dc2409efc}} indicates the phase difference between the two far ends of the rhombus. Neglecting the kinetic part, this energy has two equivalent classical minima at {{formula:3edf09c4-b51a-4f4d-9c13-d0cddefbaa72}} (as shown in Fig.REF ), that is, the phase difference across each junction is {{formula:84fdc6a6-a15a-42bb-9000-04692caef7b3}} . So the two classical ground states correspond to states in which there are both clockwise and counterclockwise supercurrents in the loop. This is the reason why we call it a persistent-current qubit. We denote the two states as {{formula:79fee30d-7812-4057-a00a-7af417eb45ff}} and {{formula:8a2a8303-5dee-4265-beee-497966760211}} . Quantum fluctuation makes the electron tunnel between"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding simplicial suspension and canonical equivalence, and how do the methods used relate to the key results?",
      "expected_output": "The research paper presents key findings on simplicial suspension and canonical equivalence. The main results show that the unreduced suspension is equivalent to the reduced suspension in a canonical manner. This is achieved through the use of a natural map from the unreduced suspension to the reduced suspension, which is a sectionwise weak equivalence. The paper also establishes a canonical equivalence between the simplicial suspension of a space and the wedge product of the space with itself. For example, the simplicial suspension of a space X is denoted by ΣX, and the wedge product of X with itself is denoted by X ∨ X. The canonical equivalence is given by a map ΣX → X ∨ X, which is a weak equivalence. The methods used to obtain these results involve the use of simplicial presheaves and the study of their properties, such as the wedge product and the simplicial suspension. The key results have important implications for the study of algebraic topology and homotopy theory.",
      "context": [
        "ca3a4ab}} of two pointed spaces {{formula:0632e85c-853e-4040-9903-f539044b5d2c}} and {{formula:3c9f3c48-610b-40d8-afd0-524ea1a4acbb}} is {{formula:279dada1-ca6c-4bb9-b4d5-e5cd5c55950c}} , and that the wedge product {{formula:4e7e621a-66fa-44ed-a6a6-40869a7cf694}} is the disjoint union with the two base points identified. These formulas hold sectionwise for simpllicial presheaves, e.g. {{formula:f4904734-7e36-413b-af2d-3f51fad44323}} . The simplicial suspension {{formula:96f6b544-d382-46b6-9a00-d312afc9ffb1}} of {{formula:b80c3c66-fe74-4a46-9dd0",
        "-870f-fd91c4aaa5d5}} . By , there is a canonical equivalence {{formula:916f3d37-2836-4238-8be8-839e37e6c561}} . Combining with (REF ) produces the desired canonical isomorphism.Corollary 3.2 For any choice of base point of {{formula:7248c88d-74de-4eab-aaac-7199528885d0}} , there is a canonical isomorphism {{formula:f1a8420a-931a-4283-81ca-3a32d15040ef}} in {{formula:214e22cb-aed6-4755-9dfd-ff2b3968e21f}} .Since the natural map from the unreduced suspension to the reduced is a sectionwise weak equivalence, the unreduced suspension is equivalent to the reduced in a canonical manner. Thus we have a canonical equivalence between {{formula:d3e79af6-ff89-4ef2-92de-697c06a8c364}} and {{formula:972395c2",
        "b1}} of {{formula:b80c3c66-fe74-4a46-9dd0-650c7853f023}} in {{formula:458f8637-9f43-41bf-8879-187e333e4257}} is {{formula:e5164bdd-cec7-4bcb-b177-7236ceba27c7}} . Let {{formula:87e1cd2f-f5e7-466e-bed6-b2ed0e2267e4}} denote the unreduced simplicial suspension, {{formula:5f8511a8-92c7-4419-a03e-9c171027d5c4}} , where {{formula:b636a839-8a75-4564-9029-686af111ecc5}} denotes the standard 1-simplex, and {{formula:b221ce7f-2ac6-4cda-83b2-5687ce467e1f}} denotes the equivalence relation defined {{formula:ba5ba562-589f-4540-b214"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how human auditory information loss affects word recognition, and what methods were used to study this relationship?",
      "expected_output": "The main findings show that human auditory information loss affects word recognition negatively, with higher information loss leading to lower word recognition rates. The study used a speech communication model, where words were sent through a noisy channel and received by a human listener, to quantify the information loss. The researchers conducted a closed-vocabulary intelligibility test to assess the word recognition rate of humans and derived upper and lower bounds on the information loss using information theoretic tools. They also compared human performance with optimal classifiers, finding that humans have a higher information loss and lower word recognition rate, especially in low signal-to-noise ratio (SNR) conditions, with the machine classifier outperforming humans by as much as 8 dB.",
      "context": [
        " performance in terms of information loss and word recognition rate. We observe a higher information loss and lower word recognition rate for humans compared to the optimal classifiers. In fact, depending on the SNR, the machine classifier may outperform humans by as much as 8 dB. This implies that for the speech-in-stationary-noise setup considered here, the human auditory system is sub-optimal for recognizing noisy words.\nIntroduction: As an acoustic signal enters the ear, it passes several processing stages until the information it carries is decoded in the brain. There exist numerous works that have studied and modeled some stages of auditory processing, from biophysical to computational models , , , , , , , , , , . Due to the information processing and transmission at each stage, some information loss may occur. In this study, our motivation is to quantify the information loss in the human auditory system, from the eardrum to the speech decoding stage in the brain. This is a first step towards assessing the information loss of the individual components in the human auditory system. We model a speech communication system, where a speaker utters a word from a fixed dictionary and the word waveform passes a noisy communication channel, before it is classified by a human listener.Our key",
        " the useful information needed for identifying the words in the human auditory system. It should be noted that the lower and upper bounds for the relative information loss for humans almost coincide. We also observe that the relative information loss in the optimal classifiers is less than the relative information loss in the human auditory system, which confirms that in this set-up, the human auditory system performs sub-optimally.From Fig. REF , it is seen that there is an inverse relationship between probability of correct decision (or speech intelligibility) and the relative information loss for humans. This monotonic relationship between intelligibility and the amount of information that reaches the brain has also been observed in .\nConclusion: In this paper, we defined and quantified the information loss in the human auditory system. We first considered a speech communication model where words are spoken and sent through a noisy channel, and then received by a listener. For this setup, we defined and bounded the relative information loss in the listener. The relative information loss describes the fraction of speech information that reaches the eardrum of the listener, but which is not used to decode the speech. To obtain the word recognition rate for humans, we conducted a listening test. The results showed that bounds for the relative information loss in the human",
        "# arXiv Paper: 1805.00698\n\nInformation Loss in the Human Auditory System\nabstract: From the eardrum to the auditory cortex, where acoustic stimuli are decoded, there are several stages of auditory processing and transmission where information may potentially get lost. In this paper, we aim at quantifying the information loss in the human auditory system by using information theoretic tools. To do so, we consider a speech communication model, where words are uttered and sent through a noisy channel, and then received and processed by a human listener. We define a notion of information loss that is related to the human word recognition rate. To assess the word recognition rate of humans, we conduct a closed-vocabulary intelligibility test. We derive upper and lower bounds on the information loss. Simulations reveal that the bounds are tight and we observe that the information loss in the human auditory system increases as the signal to noise ratio (SNR) decreases. Our framework also allows us to study whether humans are optimal in terms of speech perception in a noisy environment. Towards that end, we derive optimal classifiers and compare the human and machine performance in terms of information loss and word recognition rate. We observe a higher information loss and lower word recognition rate for humans compared"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how vortex flow affects the propagation of transverse waves in the research paper?",
      "expected_output": "The research paper suggests that vortex flow inside magnetic patches can be a significant driver of transverse waves. The size of the vortex in the chromosphere is estimated to be around a certain value, and the most preferable value of a specific formula lies between two given values, depending on the generation mechanism of waves. The coronal observation is consistent with a particular formula, indicating that the swaying motion of the flux tube is the main driver of transverse waves, but vortex flow can also play a crucial role.",
      "context": [
        "0bf9-4ae1-9d52-8ba81595468f}} is given as a function of density so that {{formula:172a1a6a-96af-482b-92a5-804794b48ba3}} in the photosphere and low chromosphere and {{formula:143d55da-311f-44a7-9d3c-a474c44069ad}} in the high chromosphere and corona. We ignored the dependence of {{formula:35072c59-6657-4816-ad97-ef660acc6c18}} on temperature for simplicity. Because we are interested in the coronal physics, and {{formula:383e7388-08a8-456c-81d6-45711078590b}} (fully ionized) for coronal mass density, our assumption should not affect the numerical results.Following , we assume a two-step super-radial expansion of the flux tube such that {{formula:4ec8a83c-20ad-479e-805f-a6d83a72ea4d}}where {{formula:4747240a-7",
        " with {{formula:8926afb6-a986-475a-8a0c-2abc848fe311}} . Thus, {{formula:fb4e019b-a0d1-4fc9-b18d-13f3696299c1}} is consistent with the photospheric wave driving and coronal wave observation, although the best choice for {{formula:56987d71-e942-473e-9f8e-fa767ab11eac}} should be investigated in detail from both aspects in future works.M.S. is supported by Leading Graduate Course for Frontiers of Mathematical Sciences and Physics (FMSP) and Grant-in-Aid for JSPS Fellows. T.Y. is supported by JSPS KAKENHI Grant Number 15H03640. T.K.S. is supported in part by Grants-in-Aid for Scientific Research from the MEXT of Japan, 17H01105. Numerical calculations were in part carried out on PC cluster at Center for Computational Astrophysics, National Astronomical Observatory of Japan.",
        " the swaying motion of the flux tube is the main driver of transverse waves . However, if the vortex flow inside the magnetic patches is the most energetic driver of transverse waves , {{formula:055ac342-3655-49e0-8688-35861b73f366}} is preferable. Radiation MHD simulations indicate that the vortex size in the chromosphere is around {{formula:5ae8f915-a1e2-4ca3-90e6-9165800c865b}} , . Therefore, the most preferable value of {{formula:07d5eed3-a106-4817-8008-c9edf2cbfc3b}} lies between {{formula:270bcedc-0dc1-426f-8686-34402e9a35d8}} and {{formula:175caa3f-f407-4ee5-8938-b299b54b2543}} , depending on the generation mechanism of waves. As discussed in Section REF , the coronal observation is consistent with {{formula:8926afb6-a986-475a-8a0c-2abc8"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the three key benefits of using a mapless motion planner and how do they improve navigation?",
      "expected_output": "The three key benefits of using a mapless motion planner are: (1) it can be trained end-to-end from scratch through asynchronous deep-RL, allowing for direct application in unseen real environments without fine-tuning, (2) it can generalize to a real nonholonomic differential robot platform without any fine-tuning to real-world samples, and (3) it provides a low-cost solution for indoor service robots with several range sensors. These benefits improve navigation by enabling the robot to navigate in complex environments with minimal prior knowledge, reducing the need for expensive sensors and mapping, and allowing for online learning and adaptation to new environments.",
      "context": [
        " this 10-dimensional information can be replaced by low-cost range sensors.In addition, reinforcement learning methods provide a considerable online learning strategy. The effects of the motion planner can be developed considerably with training in different environments continuously. In this developing procedure, no feature revision or human labeling is needed. On the other hand, the application of the deep neural networks provides a solution for multiple sensor inputs like RGB image and depth. The proposed model has shown the ability to understand different information combinations like range sensor findings and target position.\nConclusion: In this paper, a mapless motion planner was trained end-to-end through continuous control deep-RL from scratch. We revised the state-of-art continuous deep-RL method so that the training and sample collection can be executed in parallel. By taking the 10-dimensional sparse range findings and the target position relative to the mobile robot coordinate frame as input, the proposed motion planner can be directly applied in unseen real environments without fine-tuning, even though it is only trained in a virtual environment. When compared to the low-dimensional map-based motion planner, our approach proved to be more robust to extremely complicated environments.",
        "world environment is the central challenge to transfer the trained model to a real robot directly. In this paper, we only used 10-dimensional sparse range findings as the observation input. This highly abstracted observation was sampled from specific angles of the raw laser range findings based on a trivial distribution. This brings two advantages: the first is the reduction of the gap between the virtual and real environments based on this abstracted observation, and the second is the potential extension to low-cost range sensors with distance information from only 10 directions.We list the main contributions of this paper: (1) A mapless motion planner was proposed by taking only 10-dimensional sparse range findings and target relative information as references. (2) The motion planner was trained end-to-end from scratch through an asynchronous deep-RL method. The planner can output continuous linear and angular velocities directly. (3) The learned planner can generalize to a real nonholonomic differential robot platform without any fine-tuning to real-world samples.\nDeep-Learning-based navigation: Benefiting from the improvement of high-performance computational hardware, deep neural networks show great potential for solving complex estimation problems. For learning-based obstacle avoidance, deep neural networks have been successfully applied on monocular images and",
        " the trained planner is influenced by the environment to some extent. Env-2 is much more aggressive with closer obstacles so that the robot can navigate in the complex real environment successfully.In this paper, we compared our deep learning trained model with the original and low-dimensional map-based motion planner. Compared with the trajectories of Move Base, the path generated from our planner is more tortuous. A possible explanation is that the network has neither the memory of the previous observation nor the long-term prediction ability. Thus LSTM and RNN are possible solutions for that problem. We set this revision as future work.However, we are not aiming to replace the map-based motion planner: it is obvious when the application scenario is a large-scale and complex environment, the map of the environment can always provide a reliable navigation path. Our target is to provide a low-cost solution for an indoor service robot with several range sensors, like a light-weight sweeping robot. The experiments showed that Move Base with sparse range findings can not be adapted to narrow indoor environments. Although we used the range findings from a laser sensor, it is certain that this 10-dimensional information can be replaced by low-cost range sensors.In addition, reinforcement learning methods provide a considerable online"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about polytropic spheres and Palatini f(R) gravity, and how do the methods used lead to the key results that cast doubt on this theory?",
      "expected_output": "The research paper examines static spherically symmetric polytropic spheres in Palatini f(R) gravity and finds that no regular solutions to the field equations exist for physically relevant cases. The methods used involve writing the field equations using quantities built with the Levi-Civita connection of the metric and analyzing the behavior of curvature invariants. The key result is that the tidal forces diverge at the surface of polytropic spheres, casting doubt on the validity of Palatini f(R) gravity as an alternative to General Relativity. For example, the paper shows that for a monatomic isentropic gas or a degenerate electron gas, the curvature invariants diverge, indicating that there are no regular solutions. This finding suggests that Palatini f(R) gravity may not be a viable alternative to General Relativity.",
      "context": [
        "# arXiv Paper: 0801.4852\n\nPolytropic spheres in Palatini f(R) gravity\nabstract: We examine static spherically symmetric polytropic spheres in Palatini f(R) gravity and show that no regular solutions to the field equations exist for physically relevant cases such as a monatomic isentropic gas or a degenerate electron gas, thus casting doubt on the validity of Palatini f(R) gravity as an alternative to General Relativity.\nIntroduction: The quest for theories of gravity which can serve as alternatives to General Relativity (GR) has become more intense due to recent feedback from cosmology. The energy density of the universe appears to be currently dominated by a cosmological constant, or by an unknown form of energy (dark energy) that is mimicking the behaviour of a cosmological constant ( 2001). The problems connected with the inclusion of such a constant in Einstein's equations have triggered research and the proposal of alternatives, one of which could be to modify GR. One of the alternative theories which has been considered is {{formula:40818fe6-e494-4bc8-b662-deff31db762e}} gravity in the Palat",
        " gas) for which no regular solution of the field equations can be found if one adopts the Palatini variational approach, apart from in the special case of GR, casting doubt on whether Palatini {{formula:dab599a1-20a2-4c01-82dd-ab45e1033d93}} gravity can be considered as a viable alternative to GR.\nA no-go theorem for polytropic spheres: the physics behind the proof: To understand in which situations Palatini {{formula:4605276a-d124-4157-816b-754846eeff7d}} gravity behaves differently from GR, let us write the field equations using just quantities built with the Levi-Civita connection of the metric (we denote these with a “tilde”, ) and not with the independent connection. (As already mentioned, these expressions for the connection are in general different in Palatini {{formula:4e2541a9-29a8-40fa-a0f2-519353f1b4b0}} gravity.) One then gets ( 2006b) {{formula:c509d497-2861-4900",
        "ce7c07f}} in Eq. (REF ) become so important that they make the curvature invariants {{formula:3b42c29c-db01-44fb-9d30-752eab612a9c}} and {{formula:1c01bac4-b8b7-4742-afab-2597002b8ee9}} diverge. As such, there exist no regular solutions to the field equations of Palatini {{formula:2a987a2e-01fa-41f2-8131-732211b99692}} gravity for polytropic spheres with {{formula:b970bbe2-a3c9-4b9e-bd31-d926e8bf5b89}} , apart from in the special case of GR (which does not have this kind of problem because only the density, and not its derivatives, enters the field equations). Physically, this means that the tidal forces diverge at the surface of such objects, although the density is exactly zero there.It was recently argued ( 2007) that the singularity which we found would not cast doubt on the viability of Palatini {{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does Meta-RTL's adaptive weights enhance low-resource commonsense reasoning, and what are the key findings of this approach in the research paper?",
      "expected_output": "Meta-RTL's adaptive weights enhance low-resource commonsense reasoning by dynamically estimating target-aware weights of source tasks, allowing for weighted knowledge transfer and fast convergence on limited target data. The key findings of this approach in the research paper are that Meta-RTL consistently outperforms strong baselines by up to 5 points in terms of reasoning accuracy on tasks such as RiddleSense, Creak, and Com2sense, demonstrating the effectiveness of the proposed reinforcement-based meta-transfer learning framework.",
      "context": [
        "monsense reasoning tasks: RiddleSense , Creak , Com2sense . Experiments demonstrate that Meta-RTL consistently outperforms strong baselines by up to 5 points in terms of reasoning accuracy.\nRelated Work: The proposed Meta-RTL is related to both meta learning and commonsense reasoning, which are reviewed below within the constraint of space.\nMeta Learning: Meta learning, or learning to learn, aims to enhance model generalization and adapt models to new tasks that are not present in training data. Recent years have gained an increasing attention of meta learning in NLP . propose an adversarial approach to improving sampling in the meta learning process. Unlike our work, they focus on the same speech recognition task in a multilingual scenario. use adapters to perform meta training on summarization data from different corpora. The most related work to the proposed Meta-RTL is . The significant differences from them are two-fold. First, they focus on different categories under the same task in CV while our interest lies in exploring multiple tasks in commonsense reasoning for the low-resource target task. Second, they simply utilize MLP to estimate weights at each step. In contrast, we use LSTM to encode the long-term information across training iterations to calculate adaptive",
        " at each step. In contrast, we use LSTM to encode the long-term information across training iterations to calculate adaptive weights. To sum up, previous works either mechanically use a fixed task sampling strategy or just take into account the variability of different original tasks. Substantially different from them, we propose a reinforcement-based strategy to adaptively estimate target-aware weights for source tasks in the meta-transfer learning in order to enable weighted knowledge transfer.\nCommonsense Reasoning and Datasets: A wide range of commonsense reasoning datasets have been proposed recently. create COPA for causal inference while present Winogrand Scheme Challenge (WSC), a dataset testing commonsense reasoning in the form of anaphora resolution. Since the size of these datasets is usually small, effective training cannot be obtained until the recent emergence of pre-training methods . On the other hand, large commonsense reasoning datasets have been also curated , , , which facilitate the training of neural commonsense reasoning models. A popular trend to deal with these datasets is using graph neural networks for reasoning with external KGs , , and fine-tuning unified text-to-text QA models . Apart from ConceptNet, Wikipedia and Wiktionary are also used as additional knowledge sources for commonsense",
        " network to predict the weight for each source task. The difference between the task-specific loss and general loss is used as the reward to the policy network. The LSTM nature facilitates the policy network to capture the weight estimation history across meta training iterations. The estimated weights are then incorporated into the meta-transfer learning algorithm to update the meta model. In this way, Meta-RTL is able to learn target-aware source task weights and a target-oriented meta model with weighted knowledge transferred from multiple source tasks.To summarize, our contributions are three-fold:We propose a framework Meta-RTL, which, to the best of our knowledge, is the first attempt to explore reinforcement-based meta-transfer learning for low-resource commonsense reasoning. The adaptive reinforcement learning strategy in Meta-RTL facilitates the meta model to dynamically estimate target-aware weights of source tasks, which bridges the gap between the trained meta model and the target task, enabling a fast convergence on limited target data. We evaluate Meta-RTL with BERT-base and ALBERT-xxlarge being used as the backbone of the meta model on three commmonsense reasoning tasks: RiddleSense , Creak , Com2sense . Experiments demonstrate that Meta-RTL consistently"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does RMT apply to Vellekoop and Mosk's transmission optimization and what are the key findings of this application?",
      "expected_output": "Vellekoop and Mosk applied Random Matrix Theory (RMT) to predict the ideal intensity enhancement in transmission optimization. RMT is a mathematical framework used to study the behavior of complex systems. In this context, RMT was used to model the transmission of light through a disordered medium, such as a TiO2 sheet. The key finding of this application is that the ideal intensity enhancement is given by a formula that depends only on the number of modulated wavefront segments, and is independent of sample properties and system parameters. This means that the enhancement should be the same regardless of the specific characteristics of the sample or the experimental setup.",
      "context": [
        " wire based on random matrix theory (RMT) , , , , , Vellekoop and Mosk predicted that the ideal intensity enhancement is given by {{formula:955935d1-bae7-4593-9975-e14cfbdf3696}}where {{formula:b5503181-04af-4308-ae31-a2ee76f42e5a}} is the number of modulated wavefront segments. Note that Equation REF only depends on the number of modulated wavefront segments and predicts that the enhancement should be independent of sample properties and system parameters .After Vellekoop and Mosk's study, further research on optimized transmission was performed and a wide array of different enhancements was obtained for similar numbers of modulated wavefront segments , , , , , , . Initially these discrepancies were posited to be due to system noise and sample persistence time , , where the persistence time {{formula:eafb39cb-487f-47aa-b7fc-d5e7db56a8e4}} , is defined as the time during which a sample's speckle pattern remains unchanged – typically on the order of hours for solid systems and milisec",
        " by Vellekoop and Mosk in which they focused light transmitted through a TiO{{formula:c0d9d6ec-8451-44dd-b141-a93afd9350db}} sheet into a focal point . In their study they defined the metric of optimization to be the intensity enhancement {{formula:899b6603-6a78-4d3e-a1e3-56ed129093f6}} , which is given by , {{formula:5d2b7f15-6975-4718-92b3-a39dfa5b65bf}}where {{formula:aa7ed809-c8ab-4508-ba65-9ffa6fcf5bd3}} is the intensity in the target area after optimization and {{formula:6d236f70-dccf-416a-936c-f5d7d1e3cc9e}} is the average intensity in the target area before optimization. Using an optical analogue to electron conduction in a disordered wire based on random matrix theory (RMT) , , , , , Vellekoop and Mosk predicted that the",
        "fc694ae5479}} and {{formula:db3c92e6-ef4a-4ff9-a35a-d318377b9263}} depend on the other four parameters.Given the drastic difference between previous models of optimized transmission and the experimentally measured enhancement as a function of the different experimental parameters, we developed an alternative model to describe the observed effects which is based on wave propagation of a Gaussian beam with a random phase front. This model is called the random phase Gaussian beam model (RPGBM). Initially, we modeled the effect of an opaque sample as adding a random phase front to a Gaussian beam, but neglected scattering effects to the beam's intensity profile at the exit surface of the sample, which lead to some discrepancies between the RPGBM and experiments . To correct for these shortcomings we later improved the RPGBM by including RMT modeling to describe the effect of the sample on the incident beams amplitude and phase .While the RPGBM was initially formulated to model how different experimental parameters affect optimization of transmission through opaque media, it should be noted that there is a symmetry between transmission and reflection in which both the transmitted and reflected fields should behave similarly during wave propagation from the sample surface. This symmetry implies that the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on neutron removal and its effect on revealing Mg's structure, and how do the methods used contribute to these findings?",
      "expected_output": "default",
      "context": [
        "# arXiv Paper: 1802.09986\n\nRe-examining the transition into the N=20 island of inversion: structure of $^{30}$Mg\nabstract: Intermediate energy single-neutron removal from $^{31}$Mg has been employed to investigate the transition into the N=20 island of inversion. Levels up to 5~MeV excitation energy in $^{30}$Mg were populated and spin-parity assignments were inferred from the corresponding longitudinal momentum distributions and $\\gamma$-ray decay scheme. Comparison with eikonal-model calculations also permitted spectroscopic factors to be deduced. Surprisingly, the 0$^{+}_{2}$ level in $^{30}$Mg was found to have a strength much weaker than expected in the conventional picture of a predominantly $2p - 2h$ intruder configuration having a large overlap with the deformed $^{31}$Mg ground state. In addition, negative parity levels were identified for the first time in $^{30}$Mg, one of which is located at low excitation energy. The results are discussed in the light of shell-model calculations employing two newly developed approaches with markedly",
        " Department of Physics and Astronomy, James Madison University, Harrisonburg, VA 22807, USA [pa2]Present Address: Division of cancer sciences, University of Manchester, Manchester, M13 9PL, UKIntermediate energy single-neutron removal from {{formula:1b1b1f2e-6bc8-40b1-a41b-1af6bbb2a628}} Mg has been employed to investigate the transition into the N=20 island of inversion. Levels up to 5 MeV excitation energy in {{formula:9ca7c4e6-157f-4a48-bfa6-347c4662569b}} Mg were populated and spin-parity assignments were inferred from the corresponding longitudinal momentum distributions and {{formula:5baf8221-8960-41c4-b169-1db6ba3102a8}} -ray decay scheme. Comparison with eikonal-model calculations also permitted spectroscopic factors to be deduced. Surprisingly, the 0{{formula:c53058f9-9295-43ba-a135-cd5073a42d2d}} level in {{formula:",
        " at low excitation energy. The results are discussed in the light of shell-model calculations employing two newly developed approaches with markedly different descriptions of the structure of $^{30}$Mg. It is concluded that the cross-shell effects in the region of the island of inversion at Z=12 are considerably more complex than previously thought and that $np - nh$ configurations play a major role in the structure of $^{30}$Mg.\n: [5]Physics Letters BRe-examining the transition into the N=20 island of inversion: structure of {{formula:58e8f0aa-f572-4ce2-b0e2-c6d36bc57c1c}} Mg[lpl,lpc,usc]B. Fernández-Domínguez[lpl]B. Pietras[surr]W.N. Catford[lpc]N.A. Orr[lpl,tud,yor]M. Petri[lpl]M. Chartier[lpl,yor]S. Paschalis[surr]N. Patterson[surr]J.S. Thomas[usc"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the parameter play in the complex structure of elliptic curves and how does it impact the main findings of this research paper?",
      "expected_output": "The parameter plays a crucial role in controlling the complex structure of elliptic curves. In the large area limit, the remaining parameter has a clear geometric interpretation, which is essential for understanding the moduli space of F-theory compactifications. According to the research paper, this parameter is identified with the j-invariant of the heterotic elliptic curve, allowing for a consistent mapping between the moduli space of F-theory compactifications and the heterotic string picture. For example, Friedman, Morgan, and Witten's analysis of the moduli space involves flopping a curve and varying the parameter to build an elliptic surface, which can be described in Weierstrass form.",
      "context": [
        " geometric interpretation of the parameter {{formula:f669c4cf-e527-43a1-bc3b-70d658db473b}} could not be expected, since the {{formula:311364bb-b764-40fe-b092-f2bc54206096}} action on {{formula:40b94b59-3b50-4241-98e1-ef8ffa123060}} makes non-geometric identifications between elliptic curves of area {{formula:b68c6c12-5b48-490e-9731-9e613af874fd}} and {{formula:f711f043-eb87-4533-b271-b60a6b204535}} . However, if we consider the large area limit {{formula:98a7218c-5efb-49b7-a4c6-bc444551bd83}} , the remaining parameter {{formula:758df5ff-2975-4f0a-941c-8e3b6b9adc6a}} has a clear geometric interpretation.Clearly as",
        " will control the complex structure of {{formula:85690c9e-fb47-4c3b-b1fe-eb9ee047fffb}} . In the heterotic string interpretation, we have sent the area of the elliptic curve to infinity but we should be left with its complex structure as a modulus which we can vary. Clearly the only way we can consistently map the moduli space of F-theory compactifications to the heterotic string picture is to identify the {{formula:db347829-884d-4b4b-af3a-e64fa25e8ef9}} -invariant of {{formula:83d4befc-0ddf-4ac1-854f-70fbe30254fb}} with the {{formula:efeb9594-cb9a-4994-88ed-3410d8ed304b}} -invariant of the heterotic elliptic curve.Friedman, Morgan and Witten actually go a bit further in this analysis of the moduli space. By flopping {{formula:3135d3fb-5a72-4b57-a94c-645",
        "}} . Now when we let this elliptic curve vary in a family by varying {{formula:8b348cfd-a5e1-4bae-969d-bb1f28a48b58}} , we build an elliptic surface. We can naturally describe the surface, {{formula:0af92d09-a33b-4ea3-8855-7c4e14a50db9}} , in Weierstrass form {{formula:95f9b1e5-e7b6-4679-a69c-111044261c53}}since {{formula:52c8cc13-3bde-45c9-9d90-e0008ced0c2f}} is cubic in {{formula:d618cd3d-cb78-4a29-8144-b9e8e16f53e5}} .To avoid hidden obstructers we choose {{formula:d51b4adc-687b-4083-ade6-01a5968412f0}} and put {{formula:7e7df7c2-2ffa-4081-b9"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the shared neighbors of 2465dc29 and a4ba46c1 and how do they relate to the main findings of this research?",
      "expected_output": "According to the research paper, 2465dc29 and a4ba46c1 share exactly two common neighbors, one of them being ad3bb533. This finding is crucial as it implies that 3ca9b727 and 5eebac7d share no common neighbors. For instance, 9b90ab07 is not adjacent to 1bea4db6, which supports the main conclusion that certain nodes have a specific number of shared neighbors, such as 71f9bd66 and b2fe4b48 sharing exactly two common neighbors: 6cdde6cd and 5b68a79a.",
      "context": [
        " Since {{formula:2465dc29-e20d-4cc0-a95a-6280fbba0c5b}} and {{formula:a4ba46c1-9709-4f0e-8771-fc92bccb9456}} by Lemma REF , {{formula:aa588a0a-0fac-4a8f-b79e-df80a0085f94}} and {{formula:adb8fe0c-6e99-4fcf-ba8e-1e008767b4ff}} share exactly two common neighbors, one of them being {{formula:ad3bb533-a63a-44b9-bc27-ed6664273264}} Hence, {{formula:3ca9b727-cd53-4baa-a15d-1bd8404173a4}} and {{formula:5eebac7d-050f-4d08-816f-39ffe8ed0adf}} share no common neighbors, as {{formula:9b90ab07-0479-48af-b479-32ac",
        "c6eca-0775-497f-83ce-c7f3fe9b9063}} is not adjacent to {{formula:1bea4db6-1254-4a4c-a66b-ae36d2b73153}} and, for all {{formula:2ab5c355-c1aa-47f8-af1f-ddf78ec8c325}} {{formula:71f9bd66-e3a9-4926-9ecc-9af60eb3e51d}} and {{formula:b2fe4b48-b5d3-4b6c-8253-9fce17479251}} share exactly two common neighbors: {{formula:6cdde6cd-e6bd-4b3d-ab7f-bb4fc6079355}} and {{formula:5b68a79a-fe57-4381-8d92-60a232e222fb}}Since {{formula:9f019740-2e75-4416-b632-87698c9da208}} has {{formula:",
        "a679-ba72ebdd3c12}} neighbors shared by both {{formula:f76fcb9e-1119-4f88-a06e-e87c3f6baec1}} and {{formula:915eb071-e393-415c-9b47-9e2c20631dec}} They are {{formula:9bafd1af-0bc8-43bd-9589-ab24c402e304}} {{formula:c9512b3b-88cf-406f-a385-0f547770d060}} ..., and {{formula:195feef0-0686-4957-86d2-26369a706b07}} all falling into {{formula:80070d53-6a89-4757-94f5-9cce72dbd2c3}} {{formula:819759b5-b9f0-4deb-b7c1-7c0a3431b381}} where {{formula:882b6790-2bc9-45af-b0e9-8fbb2710bb24"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on Eventual Increase and how do the methods used contribute to the key results?",
      "expected_output": "The research on Eventual Increase finds that if a value never decreases again once it starts to increase, then Eventual Increase applies. The methods used, such as Fixpoint and acceleration calculus, contribute to the key results by ensuring that certain variables remain constant or increase monotonically. For example, the canonical example of Eventual Increase is a fixpoint iteration, where the value of a variable remains constant when applying the body of the loop. The research also shows that Eventual Increase yields specific formulas, such as the formula 596d5c5a-272b-4943-b362-2eb33c5f465d, which does not cover the case where a variable is negative initially.",
      "context": [
        "03b1}} never decreases again once it starts to increase, then Eventual Increase applies and it suffices that {{formula:ca6510b8-136e-440a-9b4a-785d233d9aac}} increases and {{formula:2b2eb0a0-7bfb-431d-973a-50d496a833b3}} holds. The canonical example is {{formula:127c38d2-5768-4918-b73a-9d5a303bbc30}}Fixpoint can be used for inequations {{formula:b7210171-21c0-41d2-99f7-e10d448dbfc7}} that do not behave (eventually) monotonically. In this case, {{formula:77a85a1d-9e63-4e0d-a182-9e8680841628}} ensures that the value of {{formula:f14ceef0-4594-4e25-8972-d5707a428294}} remains constant when applying the body of the loop. The canonical example is {{formula:81",
        "}} iteration, respectively. The canonical examples where Increase or Decrease applies are {{formula:f53bc219-4a25-4458-bd64-10f459fa9a3b}}respectively. Eventual Decrease applies if {{formula:de75963f-a0aa-495e-8a80-dd2d6fa44167}} never increases again once it starts to decrease. In such cases, it suffices to require that {{formula:bec64364-4ee1-4b90-a25c-65ac06816db8}} holds initially and right before the {{formula:17d8f0d3-8d02-476c-9ce0-527f1ba9f4c6}} iteration. The canonical example is {{formula:d3e42d84-8d08-4b09-acf0-8a352326495a}}If {{formula:f5057455-f7bf-4ed2-8fb8-9179edce03b1}} never decreases again once it starts to increase, then Eventual Increase applies and it suffices that {{form",
        "}} is always monotonically increasing, and hence the resulting transition does not cover runs where {{formula:ff140f09-0021-4847-b187-0320b9be2867}} decreases initially. So for (REF ), Eventual Increase yields the formula {{formula:596d5c5a-272b-4943-b362-2eb33c5f465d}} , which does not cover the case that {{formula:55f02d44-7c13-4545-b3af-6c1c1305f594}} is negative initially. For Fixpoint, the reason is that the resulting transition requires certain variables to remain constant. So for (REF ), it yields the formula {{formula:f061cca5-3daf-4b3f-8181-c0b2c44588bd}} .LoAT uses the acceleration calculus of . It operates on acceleration problems {{formula:768c2c7e-a4ee-4e24-9a74-808245e906d6}} , where {{formula:274e3813-849e-40f3-a5dd-a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do LTT observations support the 1-planet hypothesis and what are the main findings of this research?",
      "expected_output": "LTT observations support the 1-planet hypothesis by showing a clear quasi-sinusoidal signal in the OPTIMA measurements, which is coherent with early optical data and recent observations. The main findings of this research are that the 1-planet solution is relatively well constrained by available optical observations, and the results reinforce recent negative tests of dynamical stability of the 2-planet hypothesis. The study suggests that the 1-planet hypothesis is the simplest model explaining the (O-C) variability, but confirmation of the planetary origin of the LTT signal requires long-term monitoring of the system.",
      "context": [
        "8b4e1323-31fd-4a01-a75c-509845c2433a}} requires an efficient, internal mechanism of the binary period change, or indicates a presence of one more companion. Our findings might be a breakthrough after a few cited works reporting basically only unstable 2-planet models of the HU Aqr system, but these discrepancies add even more ambiguity to the 2-planet hypothesis.However, the results of our experiments show that the 1-planet solution is relatively well constrained by available optical observations selected as a homogeneous data set. Because the early optical data (the white light and V-band measurements) are coherent with an impressive, very clear quasi-sinusoidal signal exhibited by superior-precision OPTIMA measurements, as well as with the recent MONET/N, PIRATE and WFC data, a single-companion hypothesis seems well justified. A confirmation of the planetary origin of the LTT signal still requires long-term monitoring of the system. Due to its very long orbital period, it will take many years to confirm or reject the signal coherence. Such new data would be also very useful to constrain the orbital period by the recent OPTIMA observations alone.Acknowledgements",
        " REF . That may introduce significant systematic errors, particularly if the reduction is performed by different researchers. This issue may be likely resolved by a re-analysis of the entire set of all available light curves, under similar conditions paying particular attention to their origin – the spectral window, an instrument, and even technical and observational circumstances.Another direction still open is a study of the binary interactions, to eventually eliminate or discover astrophysical causes of the LTT variability. The problem is in fact universal and affects other techniques of extrasolar planets detection, such as pulsar timing and radial velocity monitoring of active or evolved stars, as well. It is yet possible that the observed (O-C) signal has both the planetary and unmodeled astrophysical component , making its unique resolution even harder.To the best of our knowledge, possible effects of the red-noise regarding the LTT observations have not been studied in detail. That problem certainly deserves a deep and careful investigation.\nConclusions: Using a new formulation of the LTT model of the (O-C) to the available data of the HU Aqr system, we found that the 2-planet hypothesis by is not likely. Our results reinforce recent negative tests of dynamical stability of that system in the literature",
        " planet c, {{formula:0f2cc56b-6ea8-4413-aa2b-34bd4966801d}} may be {{formula:81e463f5-cc4d-4774-b171-a2f337e04fa6}} day cycle{{formula:a60058b5-fbb5-4185-95e9-0f1c34a65f4a}} , which is difficult to explain by magnetic braking or mass–loss. However, this may indicate a presence of a third companion in an unconstrained orbit.We conclude that these results seem to favour the 1-planet hypothesis as the simplest model explaining the (O-C) variability, particularly in the light of very small rms of the homogeneous OPTIMA set and apparently perfect quasi-sinusoidal fit illustrated in Fig. REF . {{figure:6789c01d-b406-41e7-b094-7091aec2db19}}\nRed noise and/or systematic errors?: Analysis of the LTT observations has much in common with pulsar timing, planetary transits, and precision radial velocity observations, which are modelled with least"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the 165-day orbit of IGR J11215-5952 affect its emission patterns and what are the implications of this orbital period on our understanding of the object's behavior?",
      "expected_output": "The 165-day orbit of IGR J11215-5952 affects its emission patterns by causing a dramatic drop in the mass accretion rate when the neutron star is far away from periastron. This results in undetectable X-ray emission during certain periods of its orbit. The implications of this orbital period suggest that IGR J11215-5952 could be a missing link between SFXTs and Be X-ray binaries, which have similar high eccentricity and elongated orbits but are often detected in X-rays, including pulsations, during time intervals away from periastron.",
      "context": [
        "0-ecce82da10be}} 5952 is known to have a long and eccentric orbit which would naturally cause the mass accretion rate to drop dramatically when the neutron staris far away from periastron. As the orbital period of the source is {{formula:6a8a7d13-cf1b-4328-a01a-ace9633befd4}} 165 days and the last accurately observed outburst occurred on 2016 February 14, we conclude that the XMM-Newton observation took place about 60 days after the closer expected outburst, likely in a region of the orbit where the mass accretion rate is too low to give rise to a detectable X-ray emission. Although we only have one deep upper limit so far, the availability of further high-sensitivity observations with XMM-Newton along the orbit of IGR J11215{{formula:6ac21b91-dea8-4564-99b3-bf51dbfb9fd6}} 5952 might help us making a full comparison with the case of neutron starBe X-ray binaries which are endowed with similar high eccentricity and elongated orbits but which are often detected in X-rays (including pulsations)",
        " are endowed with similar high eccentricity and elongated orbits but which are often detected in X-rays (including pulsations) also during time intervals away from periastron . Due to its peculiar orbital configuration and the uniqueness of its periodic outbursts among the SFXT sources, it was already claimed in previous papers that IGR J11215{{formula:edb3e4ef-4f7e-485b-93ec-d104c42c4b05}} 5952 could be the missing link between SFXTs and the longer known class of Be X-ray binaries .The estimated upper limit on the X-ray emission from IGR J18410{{formula:2ae43f64-2d1f-453e-98d5-7032418b9b7c}} 0535 measured during the XMM-Newton observation 0844100101 provided, at the best of our knowledge, again the lowest luminosity value for this object , . As our upper limit is a factor of {{formula:2f54df72-0e3f-408a-989f-627de962b921}} 10 deeper than previously reported values, this result increases the dynamic range",
        " up wind clumps around the neutron star. These trends in the spectral variability support the idea that clumps play a major role in triggering the flares/outbursts from SFXTs but, as commented in previous literature papers, it cannot be excluded that additional mechanisms are at work to inhibit accretion in SFXTs for most of the time and permit accretion only when the increase in the local mass accretion rate caused by the clump is hampering their effectiveness in controlling the accretion flow .In the past, IGR J16418{{formula:b263d050-e134-40e1-abd7-47f7b9dc8b05}} 4532 has also shown other episodes of intriguing X-ray activity. During another XMM-Newton observation, reported about a large temporary increase in the local absorption column density that lasted about 1 ks and occurred slightly before the rise of a flare. This occurrence was also interpreted in terms of a clump approaching the neutron star and then been accreted onto the compact object (giving rise to the subsequent X-ray flares). In 2011, a peculiar episode of unusual low variability emission was observed with XMM-Newton and ascribed to the possible switch from the usual"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the scGW conduction bands and the BIS spectrum in this research paper?",
      "expected_output": "The scGW conduction bands are in good agreement with the BIS spectrum, with peaks labeled '1', '2', and '3' closely coinciding, apart from a constant shift of 0.8eV. This suggests that the scGW results correctly predict the relative positions of the sp and Ni bands.",
      "context": [
        " scGW conduction bands are a mixture of a dispersive band composed of {{formula:4e324d95-cc69-4ed9-90b7-c8fffb2a09e6}} approximately equally weighted on the Ni and O sites, and a nearly dispersionless {{formula:23a19a7f-56fd-4c57-aa3d-c0d452c6eb17}} state (see discussion of EELS, below). Peaks in the BIS spectrum labeled `1', `2' and `3' closely coincide to those in the scGW total DOS, apart from a constant shift of 0.8eV.3. The scGW valence bands are in very good agreement with experiment: indeed they agree as well with the Shen data as the latter agrees with an independent experiment by Kuhlenbeck et. al. (not shown).4. There is an increased dispersion in the valence bands relative to the LDA at the VBM because the nearly dispersionless Ni {{formula:c795d331-57fa-4142-88c4-a5fa04e61b71}} levels are pushed down. Thus the VBM acquires",
        "O along the [110] and [100] lines to ARPES data of Shen for the valence bands, and to the LDA conduction bands. The right panel shows the density-of-states (DOS) for both LDA and scGW , and Fig. REF shows the total DOS resolved into components. Also shown in the top panel are BIS data taken from Ref. sawatzky84. {{figure:8c12f148-b542-4b1a-a2b6-ab0a57294555}}{{figure:d82daae5-4fa7-499d-bfb2-a4df058b7af0}}Several features are of interest:1. The conduction-band minimum falls at the {{formula:e139d147-b907-444a-82b4-6bf017319215}} point; the VBM falls at the point (1/2,1/2,1/2) (not shown). The calculated minimum gap is 4.8 eV.2. The scGW conduction bands are a mixture of a dispersive band composed of {{formula:4e324d95-",
        "' and `3' should correspond to the peaks with the same labels in the BIS spectrum; indeed the EELS peaks and BIS peaks almost perfectly align if the EELS data is further shifted by 0.8 eV.)Apart from the 0.8 eV shift, the EELS data is in excellent agreement with the scGW results. Spacings between the three peaks agree to within {{formula:8bf0314b-e440-4fc9-9287-05f559d964ab}} 0.1 eV, and the spectral weight under each peak (estimated by numerical integration) also agree closely. This establishes that the scGW relative positions of the sp and Ni {{formula:0c5f7526-c863-47f9-9682-e74a5b7a7d1c}} bands are correctly predicted. This is a significant result, because the relative positions of the sp and {{formula:17ddd16d-65ca-4cdb-b589-5164f700cc20}} bands is a rather a delicate quantity . In contrast, the LDA underestimates the spacings between peaks 1 and 2 by {{formula:b8"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the role of topological pressure in the research paper and how does it relate to the main findings?",
      "expected_output": "The topological pressure in the research paper is a measure used to describe the complexity of a system. It is denoted by a specific formula and is related to the topological boundary of a set. According to Lemma 5.7, there exists a certain value such that for any given set, a specific condition is met. The paper also discusses the concept of accumulation points and their relation to topological pressure. For example, the notation of the preceding lemma is used to characterize certain properties of the system. Overall, the topological pressure plays a crucial role in understanding the main findings of the paper, which include the characterization of specific systems and their properties.",
      "context": [
        "d}} -pressure, will be denoted by {{formula:fa5fbb98-cbdc-44ec-8acc-197529761458}} . Its pressure is the topological {{formula:0e7b57f0-6f5c-425c-ab69-c273a7aaa6ef}} -pressure.For a set {{formula:ad9a40b1-1058-4404-b1d7-a86354c1ee23}} and an integer {{formula:836574b2-7c9a-492d-b8cd-b766aba684d4}} , we denote by {{formula:8d8e5ed4-a826-4c7c-b68f-440f1c1c22f4}} its topological boundary {{formula:050e2ee9-2ea2-46b0-91f7-37ad55f9e639}} .Note that {{formula:b6c370c4-e01f-45c6-b18c-22b8b94520de}} can be empty;",
        ":92cb75ee-18a0-4add-9618-0263c7bb8716}} otherwise the right hand side would be larger than the topological pressure of {{formula:cd70534b-45bc-4782-994d-3aa1520321dc}} . Lemma 5.7 There exists {{formula:d0a37fed-3c11-493c-ad93-8a52fdc27d8a}} such that for any {{formula:75a00da8-e889-4e26-b74c-f8f371afd2c1}} we have {{formula:2fffafe3-3b01-47ad-b38d-565d3f60c05f}} . We keep the notation of the preceding lemma. Let {{formula:4cb6eb22-cd74-4003-8172-6ec2aae4ebec}} be an accumulation point of {{formula:0cb45799-a6a9-488b-85fc-fbea3be029df}} . We first show that {{formula:0f78aea5-a",
        "da-49a4-864f-e8046b6322d5}}: Lemma REF characterizes {{formula:f43c8f2d-6a2b-4705-af32-bfa17b1dee7c}} and {{formula:8de1e45e-1590-4b90-bcc0-ae6763c71715}} : as soon as {{formula:cad14d58-8318-455c-99f6-f4adca21980b}} is defined, {{formula:562d36b4-31bb-45cb-b5f7-b5d4f68eccb1}} is the unique real number {{formula:c4f5810b-5a76-4862-b15b-c1d8a47d9f46}} such that the topological pressure associated to the potential {{formula:96276fd2-5e06-4bcf-b7ee-7e238b19a2aa}} Bm{{formula:f321085f-5cd2-47c3-a7"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Liouville's theorem in relation to universal quarternary diagonal forms, and how does it classify them?",
      "expected_output": "Liouville's theorem provides a generalization of Gauss's theorem, stating that every positive integer is represented by a universal quarternary diagonal form if and only if the form is one of the specific forms listed. The theorem classifies universal quarternary diagonal forms, providing a complete list of 54 forms that represent every positive integer. For example, the theorem states that a positive integer is represented by a form if it represents the integers 1, 2, 3, 5, 6, 7, 10, 11, 13, 14, and 15. This classification is significant, as it answers the question of representability by positive definite homogeneous quadratic polynomials with even off-diagonal coefficients.",
      "context": [
        "ke have shown the 290-Theorem, providing the necessary set (the largest element of which is 290) for universal forms when the corresponding matrix is half integral, that is, for totally positive integer quadratic forms.In 1863, Liouville proved the following generalization of Gauss's theorem, similar to Ramanujan's generalization of Lagrange's Four Squares Theorem.Theorem (Liouville) Let {{formula:82cf203a-977b-4944-896e-d5c79668e31b}} be positive integers with {{formula:74f0e04f-7bfa-41b4-b563-fae1138e7539}} . Then every positive integer is represented by {{formula:9308e8ef-1a75-4cdd-8c04-aacb03d6638c}} if and only if {{formula:7bdfff4e-8b4a-4371-9150-a4b813dee8b7}} is one of the following: {{formula:f4147d20-c879-40c2-a848-a9a",
        "-aa31-4c03-a8e2-52a3085bc1f2}} which he claimed to be the complete list of universal quarternary diagonal forms; 54 forms actually turned out to be universal and this list is complete.Recently, Conway and Schneeberger proved in unpublished work a nice classification for universal positive definite quadratic forms whose corresponding matrices have integer entries. This answers the question of representability by positive definite homogeneous quadratic polynomials with even off-diagonal coefficients.Theorem (Conway-Schneeberger) A positive definite quadratic form {{formula:f4ec29a4-707d-477c-a5e9-2ca537b85c73}} , where {{formula:74d9fb06-d9c9-4ece-ab32-e81ce61aaad5}} is a positive symmetric matrix with integer coefficients, represents every positive integer if and only if it represents the integers {{formula:7078d5c4-677a-4fcd-b085-e0f985707748}} and 15.Bhargava gave a simpler proof of the Conway-Sch",
        "ad72d65}} denote the triangular polynomial {{formula:dea9a6b8-483e-411a-81bc-b57bbf8021bc}} , the theorems of Lagrange and Gauss state that the positive integers are represented by {{formula:8d40d6ed-0335-4089-a9a3-411308e83260}} , and by {{formula:ecd454cf-b670-4207-845b-e8ae01ce8b05}} .In 1917, Ramanujan extended the question about four squares to ask for which choices of quadruples {{formula:c99cf825-562d-4e80-bcbd-c998f8aa624b}} of integers {{formula:36213483-8c77-4d8f-8973-94660ac0dd77}} represents every positive integer; we shall refer to such forms as universal diagonal forms. He gave a list of 55 possible choices of {{formula:61d64a24-aa31-4c03-a8e2-52a3085bc1f2}} which he claimed"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the Einstein coefficient play in determining the line integral intensity, and how does it impact the overall results of the research?",
      "expected_output": "The Einstein coefficient plays a crucial role in determining the line integral intensity, as it represents the probability of spontaneous emission. In the context of the research, the Einstein coefficient is used in the formula to estimate the integrated intensity of the line integral. A higher Einstein coefficient would result in a higher integrated intensity, indicating a stronger emission line. For example, in the research, the Einstein coefficient is used to analyze the C18O 2-1 emission region, which helps to estimate the mean H2 column densities of G108-N and G108-S. The results show that the mean H2 column densities are 6.7 and 1.0, respectively, which are critical in understanding the physical properties of the clumps.",
      "context": [
        "ically thin emission ({{formula:b48484ac-0164-4f26-b489-c83d47ed9df3}} 1) can be estimated with the integrated intensity of the line integral {{formula:31a6f009-6fe6-488a-9e25-43947e4e2f3a}} (K km s{{formula:072e2323-7805-4fe1-8970-ee5cc6d7a56a}} ) as the following, {{formula:b994ca58-ea57-4a2d-a60f-e53682262c44}}where {{formula:048e7cbb-1008-4e82-bb32-5818c3f6e6eb}} and {{formula:ca203222-155c-4aaa-8762-3cd4b4e75f0d}} are the frequency and wavelength of the transition, {{formula:a390cbda-850e-44e8-be1e-791bb4866865}} is the Einstein coefficient, {{formula:82d6aaa5-fd",
        "8b72b-8768-40d1-b23e-d973639602a2}} are the major and minor axes of deconvolved FWHM size measured at the 50 {{formula:36c8ff6f-1223-44f2-b392-b342a146527d}} contours of the C{{formula:147b1a39-c985-45d8-9a26-4f341fb0490a}} O 2–1 emission region. The mean H{{formula:2376847b-4688-4045-81f2-83fda98bad71}} column densities of G108–N and G108–S within the clump size are 6.7 {{formula:189f8670-4dd4-4963-bf29-a4fc257c6bca}} and 1.0 {{formula:196f6eb2-2ab5-4119-9f56-6622d89bce24}} cm{{formula:672984bc-9c9d-47c5-a1ff-8f9e53a30e86",
        "1.5) 196 (4.0)cccc Bolometric Luminosities, Bolometric Temperatures, and IR Spectral Indices of IRS 1 and IRS 2 0pt Sources {{formula:b0fbe957-ab89-4ca6-85ab-ed93419de481}} {{formula:ad5d55e1-e045-4ec7-8fac-a766421238e2}} (Class) Spectral index {{formula:2349e730-3ab2-4af7-92c8-ef337debf1e0}} (Class)[{{formula:36c2d501-c66e-4862-9032-fc5d54c647d8}} ] [K] IRS 1 27 341 (I) 0.61 (I)IRS 2 30 614 (I) 0.48 (I)cccccccc 0pt Derived parameters of C{{formula:06bd6a4c-fb19-4d57-9678-dd2c88a652a4}} O 1–0 of G108–N and G108–S from LTE analysis peak mean peak mean"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main factors that contribute to ambiguities and uncertainties in land cover classification, and how do these factors impact the overall accuracy of the classification results?",
      "expected_output": "The main factors that contribute to ambiguities and uncertainties in land cover classification include the lack of height information in aerial imagery, inherent inter-class ambiguities, and conflicting information from different datasets. These factors impact the overall accuracy of the classification results by introducing errors and uncertainties, which can be quantitatively measured using metrics such as probability, error percentage, distance, standard deviation, and the Shannon diversity index. For example, regions with conflicting information and missing data tend to have higher uncertainty, while areas with agreement among datasets have lower uncertainty. The use of expert knowledge and probabilistic graphical models, such as cluster graphs, can help resolve these ambiguities and improve the accuracy of land cover classification results.",
      "context": [
        " land cover classifications, showing a small overall accuracy improvement but consistent class-wise classification accuracy improvement over the initial classifications. However, neither approach inherently exploits expert knowledge or VGI data to support the boosting process. These two sources can provide critical prior information to resolve ambiguities and sensitivities when combining existing land cover classification datasets. Furthermore, the analysis of the uncertainties present in the final land cover classifications has largely been overlooked, despite its importance in understanding and utilising land cover classifications at a large scale.Various measures have been proposed to describe data uncertainty quantitatively. These include scalar values like probability, error percentage, distance (e.g. from ground truth), standard deviation , and the Shannon diversity index, a quantitative estimator of complexity . These measures have been used within the framework of various probabilistic approaches, such as Bayes networks, belief functions, interval sets, and fuzzy set theory . However, few of these approaches have been applied to land cover classification boosting and, thus, this field remains predominantly unexplored. For this reason, we take inspiration from literature based within the realm of probabilistic graphical models (namely cluster graphs), graph colouring, and image processing. Our proposed approach follows the same general assumptions as problems where neighbouring relationships",
        ", trade-offs are required to produce good enough land cover classifications. Thus, it becomes clear that no single land cover classification product can be produced to cover all use cases with a sufficient spatial resolution, coverage extent, accuracy, and granularity. Due to these reasons, there has been a growing interest in fusing different land cover classifications, including crowd-sourcing information, into a single more complete data product , , , . These methods are based on various data boosting approaches that play off each dataset's strengths to gain a more complete classification and minimise errors. While these approaches have seen various successes, spatially-weighted land cover prediction and the assessment of its related uncertainties have received limited attention.In this study, we address these challenges with four objectives:to explore the potential of a probabilistic graphical model approach, using cluster graphs, towards producing a more accurate land cover classification, to exploit the potential of expert knowledge for probabilistic reasoning in land cover classification, to perform uncertainty analysis on the outcome using the Shannon diversity index as a measure of uncertainty, and to potentially contribute to OpenStreetMap data with missing land cover information.To this end, we propose an efficient approach using cluster graphs for boosting spatial heterogeneous data. We then investigate the effect of",
        " datasets. Due to the nature of the aerial imagery, which is often used to generate land cover classifications, inherent inter-class ambiguities exist due to the lack of height information. The generated uncertainty maps can help develop better algorithms for land cover classification by either forming part of the optimisation function or providing experts with clues as to which areas are often misclassified and why.By comparing the uncertainty map, Figure REF , to the corresponding classification maps, Figure REF , it is clear that the uncertainty for each region tends to agree with intuition about the nature of certainty across the datasets. For example, regions that present conflicting information in two inputs and are missing information in the third are deemed to be more uncertain in the boosted map, while areas with agreement present a very low uncertainty. Perhaps one interesting observation is the low uncertainty in some regions where information is conflicting; this is likely due to the expert priors that enforce self-similarity based on Tobler's law of Geography.Uncertainty maps can provide useful inputs into ecological and climactic research where uncertainty about land cover classifications can help improve models of land use dynamics and ecosystem stability. Furthermore, the class-wise probabilities could open the door for manual intervention where the top {{formula:82cb4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of HAR in healthcare and how are its methods applied to achieve key results?",
      "expected_output": "The main findings of Human Activity Recognition (HAR) in healthcare include monitoring activities of elderly people, encouraging physical exercises for assistive correction of children's sitting posture, and detecting early signs of illness. The methods applied to achieve these results include using wearable sensor data and machine learning algorithms such as Decision Trees, Support Vector Machine, Naive Bayes, K-Nearest Neighbor, and Multilayer Perceptron. According to the paper, ensemble learning methods can boost the performance of these machine learning methods, leading to better results.",
      "context": [
        " the HAR. In fact, this algorithm has been suggested in , leading to impressive results compared to other traditional machine learning methods. In this study, we improve the study in by using more efficient classifiers as base models. The obtained results show that our proposed method give better performances.The rest of the paper is organized as follows. In Section 2, we present a brief of related works on HAR. Section 3 describes the methodology using in the study, including the sample generation process, the feature representation, the basic machine learning algorithms and the proposed voting classifier. The experimental results are shown in Section 4 and some concluding remarks are given in Section 5.2. A brief review of human activity recognition based on wearable sensor dataDue to its widely practical applications, the werable sensors based HAR has attracted a large number of studies. A number of machine learning algorithms have been applied to deal with sensors data in the HAR such as Hidden Markov Models ( ), Support Vector Machines ( ), and K-Nearest Neighbor ( ). Other machine learning algorithm like J48 Decision Trees and Logisitic Regression are also utilized for the HAR based on the accelerometer of smartphone. An extensive survey on werable sensor-based HAR was carried out by .In general, when few labeled data",
        "# arXiv Paper: 1905.03809\n\nWearable Sensor Data Based Human Activity Recognition using Machine Learning: A new approach\nabstract: Recent years have witnessed the rapid development of human activity recognition (HAR) based on wearable sensor data. One can find many practical applications in this area, especially in the field of health care. Many machine learning algorithms such as Decision Trees, Support Vector Machine, Naive Bayes, K-Nearest Neighbor, and Multilayer Perceptron are successfully used in HAR. Although these methods are fast and easy for implementation, they still have some limitations due to poor performance in a number of situations. In this paper, we propose a novel method based on the ensemble learning to boost the performance of these machine learning methods for HAR.\n: Wearable Sensor Data Based Human Activity Recognition using Machine Learning: A new approach[1]Nguyen, H.D. [2]Tran, K. P. [3]Zeng, X. [4]Koehl, L. [5]Tartare, G.[1]Faculty of Information Technology, Vietnam National University of Agriculture, Vietnam (e-mail: nhdu@vnua.edu.vn).",
        " sensor, Ensemble learning method, Machine learning.Recent years have witnessed the rapid development of human activity recognition (HAR) based on werable sensor data. One can find many practical applications in this area, especially in the field of health care. Many machine learning algorithms such as Decision Trees, Support Vector Machine, Naive Bayes, K-Nearest Neighbor and Multilayer Perceptron are successfully used in HAR. Although these methods are fast and easy for implementation, they still have some limitations due to poor performance in a number of situations. In this paper, we propose a novel method based on the ensemble learning to boost the performance of these machine learning methods for HAR.1. IntroductionThe rapid development of advanced technologies nowadays makes the study of recognizing human activity easier and more effective. Human activity recognition (HAR) is widely used in several fields such as security surveillance, human computer interaction, military, and especially health care. For instance, the HAR is used for monitoring the activities of elderly people staying in rehabilitation centers for chronic disease management and disease prevention. It is used to encourage physical exercises for assistive correction of children's sitting posture. It is also applied in monitoring other behaviours such as abnormal conditions for cardiac patients and detection for early signs of illness. More"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in the Fermi surface maps of BaK0.76 and BaK0.88, and how do these differences relate to their electronic properties?",
      "expected_output": "The main differences in the Fermi surface maps of BaK0.76 and BaK0.88 are related to their polarization-dependent FS maps and the determination of Fermi surface positions. According to Figures S2-S4, the raw FS images and momentum-mapped FS images show distinct features for each material. The integration energy window of 5 meV from the Fermi level reveals differences in the Fermi surface topology. The polarization dependence of the MDCs is similar to that of K122, indicating similar dominant orbital character of each FS sheet. These differences in Fermi surface maps relate to their electronic properties, such as the superconducting gap anisotropies, as shown in Figure S6 for BaK0.76. The EDCs and symmetrized EDCs measured below and above the superconducting transition temperature reveal distinct features, indicating differences in the electronic properties of BaK0.76 and BaK0.88.",
      "context": [
        "K0.88 and BaK0.76. These results are consistent with {{formula:3b228596-206c-4a92-bd69-c6498f45bbb9}} determined by the magnetization measurements. {{figure:0bb93f17-f898-4487-8a3f-b63fd767d2f1}}\nII. Polarization-dependent FS maps and determination of {{formula:ea4edf9a-8cc9-48fb-b87e-6fd8895dfa2e}} positions : Figures S2-S4 show the raw FS images plotted with respect to detector and rotation angles (a)-(c) and momentum-mapped FS images (d)-(e) for BaK0.93, BaK0.88 and BaK0.76, respectively. The integration energy window is {{formula:ebce1e30-3e94-4f97-b04f-42fc5d509afd}} 5 meV from {{formula:72ec1200-9555-4b8f-bc09-82895d00afbc}} , the same as that",
        ", as in the cases of K122 and BaK0.93, and are indicated by the symbols on the FS maps in Figs. S3 and S4. The polarization dependence of these MDCs is similar to that of K122, and thus dominant orbital character of each FS sheet is also similar to that of K122. {{figure:bfcd690f-ab61-42b0-9f38-04690bde8d43}}\nIV. SC-gap anisotropies of BaK0.76 : Figure S6 show the EDCs (a-c) and symmetrized EDCs (d-f) at {{formula:9c1df34d-8592-4e40-8053-d580782bed93}} of the inner, middle and outer FSs measured below {{formula:d4727083-3dbe-49df-832f-bae2ec08cb25}} (1.5 K) and above {{formula:938d8340-08a5-45e4-8f95-3aff41e55257}} (18 K) for BaK0.76 ({{formula:",
        "287c731}} -{{formula:ff2b6709-87e7-457f-a491-8500b7eb8ac5}} maps measured with {{formula:48acf0fd-1723-4717-9884-2e45104e2df2}} -, {{formula:5a5a1a5d-20b2-4a66-b377-f7dba2208aba}} -, and {{formula:8dd7d793-d114-4a2d-86be-1e294794398e}} -polarized light for BaK0.88 and BaK0.76, respectively. Momentum cuts are indicated by the red lines in the panel e of Figs. S3 and S4, respectively. {{formula:c93e7510-e5c0-442f-87dd-90c23b606932}} positions of three hole FSs (inner, middle, and outer) were determined by fitting to the three Lorentzians, as in the cases of K122 and BaK0.93, and are indicated by the symbols on the FS maps"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do FPGA-accelerated BNNs impact power consumption and inference time, and what are the main findings of this research in terms of performance improvements?",
      "expected_output": "FPGA-accelerated BNNs reduce power consumption by 16 times compared to GPU-accelerated networks and decrease inference times on MNIST and CIFAR-10 by over 9.89x and 9.91x, respectively. The main findings of this research show that FPGA-accelerated stochastic and deterministic BNNs achieve near state-of-the-art performance with significant improvements in power consumption and inference time.",
      "context": [
        "NN architectures, the MNIST and CIFAR-10 datasets are used. To ensure a fair comparison, on account of the limited resource availability of the Intel DE1-SoC development board used, the batch size, {{formula:35db2c5b-103a-4a7b-857a-3f3216e889d4}} , was fixed to 4 for all networks. The validation accuracy for all developed networks over 200 training epochs is presented in Figures (REF ) and (REF ).From Figures (REF ) and (REF ), it can be observed that both GPU- and FPGA-accelerated networks achieve very similar validation accuracy rates during learning. For all implementations, regularized networks require more training epochs to converge.The variations in validation accuracy trends reported between platforms can be associated to the different initial weights generated using the He weight initialization technique. Figures (REF ) and (REF ) also demonstrate that, networks employing stochastic and deterministic binarization techniques perform very similarly, compared to their base-line architectures employing no binary regularization techniques. For our FPGA-accelerated networks learning MNIST, the validation accuracy degrades by only 0.37% (for stochastic",
        "-accelerated networks learning MNIST, the validation accuracy degrades by only 0.37% (for stochastic) and 0.94% (for deterministic), compared to no regularization. For our FPGA-accelerated networks learning CIFAR-10, a validation accuracy decrease of 0.24% was observed for our network employing deterministic binarization, while our networks with stochastic binarization regularization showed a validation accuracy increase of 0.03%. These findings are in good agreement with the software implementations of the binarized networks reported in .To comprehensively compare the implemented FPGA- and GPU-accelerated networks, the total kernel power usages, learning time per epoch, inference time per image, and learning performances were determined and presented in Table REF . The total kernel power usages were determined using the Post Place & Route Estimator for FPGA post-synthesis, and NVIDIA-SMI for GPU. It was found that the power consumption of all FPGA-accelerated networks reduce by {{formula:29c3ec46-6a55-4989-bdc3-bf20bca0be55}} 16 times, compared to",
        " regularization techniques that binarize network weights. In this paper, we introduce, to the best of our knowledge, the first FPGA-accelerated stochastically binarized DNN implementations, and compare them to implementations accelerated using both GPUs and FPGAs. Our developed networks are trained and benchmarked using the popular MNIST and CIFAR-10 datasets, and achieve near state-of-the-art performance, while offering a >16-fold improvement in power consumption, compared to conventional GPU-accelerated networks. Both our FPGA-accelerated determinsitic and stochastic BNNs reduce inference times on MNIST and CIFAR-10 by >9.89x and >9.91x, respectively.\nIntroduction: Deep Neural Network (DNN) architectures have become integral to a variety of applications in Artificial Intelligence (AI) and Machine Learning (ML). While these learning networks and their underpinned elements have been actively researched since 1974 , the inception of recent modern GPUs and faster CPU architectures have greatly facilitated Neural Network (NN) research and enabled the development of highly accurate and complex DNNs.However, high-performance CPU- and GPU-accelerated DNNs are putative"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between classical machine learning and modern machine learning approaches when applied to tabular data, and how do these differences impact the learning process?",
      "expected_output": "The main differences between classical machine learning and modern machine learning approaches when applied to tabular data are that classical machine learning models, such as SVMs, linear and logistic regression, and tree-based methods, can only capture complex patterns in the table, making them explainable, whereas modern machine learning models, including deep learning algorithms like GNNs and attention-based methods, project tabular data into latent space to capture hidden properties and relations, making them more difficult to explain. These differences impact the learning process as classical models are best suited for small-size tables and limited to classification and regression tasks, while modern models can handle large datasets and provide more flexibility, but face challenges like transparency and interpretability.",
      "context": [
        " section will provide an overview of the evolution of learning on tabular data over time. This transformation occurred in two phases; The Classical Machine Learning phase and The Modern Machine Learning phase. SVMs, Kernel Methods, Regression Techniques like Logistic and Linear Regression, Classification Techniques, and Tree-Based Methods all come under the traditional ML phase. In contrast, the Modern ML phase includes all the deep learning algorithms, such as GNNs, attention-based methods, and many more. The models that come under the classical learning phase can only capture the complex patterns in the table, thus making the model explainable. In contrast, the models used in the modern learning phase project tabular data into latent space in order to capture hidden properties and relations of the table. These models are more difficult to explain than traditional learning models.Researchers have examined the tabular data from a variety of lenses, as illustrated in Figure REF . Tables , , graphs , , sentences , , images , , and trees , , , , , have all been used to represent tabular data. Each modality comes to have its own advantages and limitations. Below we tried to address all the state-of-the-art models from each modality with their limitations.\nThe Classical Machine Learning phase",
        " Modern Machine Learning Phase (Section REF ). The classical learning phase consists of the models such as SVMs , linear and logistic regression , and tree-based methods , , , , , . These models are best suited for small-size tables. However, the number of tasks these models can address is limited to classification and regression . In contrast, the Modern Machine Learning Phase contains models , , , , , , that use deep learning for learning latent space representation of table entities. Deep Neural Networks come with their own set of advantages and disadvantages. Some major pros of using deep learning for tabular data include flexibility, end-to-end training, building more extensive pipelines, and the ability to handle large datasets. In contrast, the significant challenges faced by deep learning models include transparency, interpretability, and heterogeneous features. We split the models into four broad categories according to how a table is visualized. Some models treat the table as an images , and graphs whereas others treat the table as simple text , , or as table only , .Accordingly, this survey is structured as follows:We initiate the discussion with definitions and preliminaries of tabular data in section . In this section, we talked about what is tabular data and provided definitions of the key terms used",
        "-based methods. These models are best suited for small-size tables. However, the number of tasks these models can address is limited to classification and regression. In contrast, the Modern Machine Learning Phase contains models that use deep learning for learning latent space representation of table entities. The objective of this survey is to scrutinize the varied approaches used by practitioners to learn representation for the structured data, and to compare their efficacy.\nIntroduction: Tabular dataTabular Data is also known as \"Structured Data\" in the literature. comprising rows (samples) with the same set of columns (attributes), is one of the most widely used data-type among various industries, including financial services, health care, research, retail, logistics, and climate science, to name a few. Tabular data is unique in several ways and comes with its own properties and challenges, as discussed in Section and , respectively, making it difficult to work with. Tabular data contain heterogeneous features, i.e., a table can be a mixture of different types of data: text, numerical, and categorical, to name a few. In addition, tables have intricate inter-dependencies between columns and intra-dependencies within the column.According to the survey , SQLSQL"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main requirements for using a Language Model as a Knowledge Base, and how do these requirements impact the model's performance and accuracy?",
      "expected_output": "The two main requirements for using a Language Model as a Knowledge Base are: (i) the ability to store a large number of facts involving a large number of entities, and (ii) the ability to query stored facts. These requirements impact the model's performance and accuracy as they enable the model to represent millions of entities and retrieve stored facts using natural language queries, such as paraphrased queries. For example, the paper explores entity representations that allow Language Models to represent millions of entities and presents a case study on paraphrased querying of world knowledge in Language Models.",
      "context": [
        "# arXiv Paper: 2008.09036\n\nLanguage Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries\nabstract: Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose single-token name is found in common LM vocabularies. Furthermore, the main benefit of this paradigm, namely querying the KB using a variety of natural language paraphrases, is underexplored so far. Here, we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to represent millions of entities and present a detailed case study on paraphrased querying of world knowledge in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.\nIntroduction: Language models (LMs) appear to memorize world knowledge facts during training. For example, BERT correctly answers the query �",
        " models (LMs) appear to memorize world knowledge facts during training. For example, BERT correctly answers the query “Paris is the capital of [MASK]” with “France”. This observation prompted to ask if LMs can serve as an alternative or complement to structured knowledge bases (KBs), thereby introducing the idea of treating LMs as KBs: During training, the LM encounters world knowledge facts expressed in its training data, some of which are stored in some form in the LM's parameters. After training, some of the stored facts can be recovered from the LM's parameters by means of a suitable natural language query (Fig. REF ). A LM with such a “built-in” KB is useful for knowledge-intensive tasks and question answering , and could improve natural language interfaces to structured data , . However, this emerging LM-as-KB paradigm is faced with several foundational questions. {{figure:4308903f-c3d5-425a-89b7-0f98dbb5f03b}}First question: KBs contain millions of entities, while LM vocabulary size usually does not exceed 100k entries. How can millions of entities be represented in LMs? circumvent this problem",
        " language queries, as opposed to hard, symbolic queries in a language like SQL or SPARQL, is one of the key motivations for using LMs as KBs.Contributions. We formulate two requirements for treating LMs as KBs: (i) the ability to store a large number of facts involving a large number of entities and (ii) the ability to query stored facts. After providing background on world knowledge in LMs (§), we make the following contributions:Code available at: https://github.com/bheinzerling/lm-as-kbA comparison of entity representations for scaling LM-as-KB to millions of entities (§); Empirical lower bounds on LM capacity for storing world knowledge facts (§); and A controlled study of knowledge transfer from stored facts to paraphrased queries (§).Terminology. In this work we are interested in storing and retrieving world knowledge facts in and from a LM. World knowledge is knowledge pertaining to entities, such as Barack_Obama. A fact is a piece of world knowledge that can be expressed with a concise natural language statement, such as the English sentence Barack Obama was Born in Hawaii, or with a relation triple, such as {{formula:84b93c93-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between beam intensity and the degree of damage, and how do the research findings support this correlation?",
      "expected_output": "The research paper suggests that the degree of damage is proportional to the beam intensity, which has a Gaussian profile. This correlation is supported by the findings that the change in transmittance, related to the change in absorbance, follows a specific function that depends on the beam intensity. For example, the time constant, which determines the recovery time, is largest at the center of the burn mark profile where the change in transmittance is the largest. This indicates that the recovery rate is independent of dose, as the time constants are independent of the distance from the peak, which is proportional to the dose.",
      "context": [
        "-28abc884dab2}} pixels or {{formula:db625833-f24a-4f9b-964b-23687034c240}} m, which is within one standard deviation of a direct measurement of beam at the line focus using a beam profiler. Using the fit parameters, we determine the degree of recovery as a function of the distance from the center. We can thus determine the degree of population recovery at each point on the burn profile. {{figure:dd6d08f4-1d73-4d5f-8809-42e41a2c0a0b}}Figure REF shows the change in transmittance as a function of time for several distances from the center of the burn line as quantified by the pixel number. The intensity at each pixel is determined using the fit parameters, as described just after Equation REF . The solid curves are the associated fits to the data of the master function {{formula:14969db0-1d42-44b9-8daa-a3e6d930df04}} . Since that data is noisy, only three sets are displayed. Where data is not shown, the fit functions are shown",
        "-4d21-87f5-78f9f219ee66}} , which is related to the change in absorbance, and {{formula:47d1021f-8923-4522-baf0-05af8400a811}} is the recovery rate.Assuming that the degree of damage is proportional to the beam intensity, which has a Gaussian profile, {{formula:d95ba0c6-ad46-48e9-af9f-eedce8d6fde6}} will be of the form {{formula:d368b46c-68b2-412a-86d9-4ce2d3aeaeac}}where {{formula:050f2c5d-3ab1-4bbd-bd34-c2d4c88e5df1}} is position of the center of the burn, {{formula:47856740-c008-4753-bff4-0c318f4bd24a}} the change in transmittance at the burn center, and {{formula:e484459a-f8d2-4cf9-8df9-9400ef",
        " is in agreement with the ASE measurements.The time constant, {{formula:3476a78c-c10a-4f1a-90b4-b5d3fc818055}} , which is determined from the time-dependence of the increased transmittance at the center of the burn mark profile, {{formula:4328484a-30a7-4300-9be7-fac642b7e8ad}} , gives the most precise measurement of the recovery time due to the fact that the change in transmittance there is the largest. {{figure:b2066534-9efc-425d-8695-9ce91a8c93a5}}Next we determine the recovery rates as a function of distance from the peak to determine the recovery rates as a function of dose. Since we postulate that the recovery rate is independent of dose, the time constants should be independent of {{formula:e7a52e1f-5b62-4c2e-9858-2d37b143b0e4}} since it is proportional to the dose. {{figure:bb88cfee-fe5c-4c12-91"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the Emoji Attack research and how does it exploit Judge LLMs' token segmentation bias?",
      "expected_output": "The main findings of the Emoji Attack research are that it can exploit Judge LLMs' token segmentation bias by inserting emojis within tokens, disrupting the token structure and embedding space. This results in a significant reduction in the 'unsafe' prediction rate, with an additional 25% reduction on average compared to token segmentation bias alone. The research also found that combining different delimiters, such as characters and emojis, can make simple filter defense strategies ineffective. For example, the 'unsafe' prediction rate of WildGuard decreased from 93.2% to 61.2% in the presence of token segmentation bias, and Llama Guard 2 showed a reduction of approximately 7%.",
      "context": [
        " vulnerability in Judge LLMs. Unlike traditional token splitting by spaces, the Emoji Attack involves inserting emojis within tokens of harmful responses. These inserted emojis disrupt the token structure, resulting in new tokens that cause more significant changes in the embedding space. We also developed a position selection strategy to identify optimal emoji insertion points, maximizing the difference between the sub-tokens and the original token in the embedding space. As illustrated in Figure REF (b), we use a lightweight surrogate model to find the optimal insertion points by comparing the cosine similarity between the sub-token embeddings and the original token. Compared to token segmentation bias alone, the Emoji Attack further reduces the \"unsafe\" prediction rate for Llama Guard and Llama Guard 2 by an additional 25% on average. Additionally, we demonstrate that combining different delimiters, like characters and emojis, can make simple filter defense strategies ineffective, highlighting the potential to advance jailbreaking techniques based on token segmentation bias.We also explore the practical application of the Emoji Attack in scenarios where Judge LLMs directly assess the target LLM responses to determine their suitability for sharing with users. In this scenario, although we cannot manipulate the responses, we can access the input",
        " find that all of them demonstrate token segmentation bias. For example, WildGuard exhibits the highest “unsafe” prediction rate of 93.2%, but this decreases to 61.2% in the presence of token segmentation bias. Even Llama Guard 2, based on the powerful Llama3 , shows a reduction of approximately 7%. These results suggest that current Judge LLMs are affected by token segmentation bias.\nEmoji Attack: As demonstrated in the above experiments, space insertion in token segmentation bias could lead to misclassification of harmful responses by altering token boundaries. However, this method exhibits a limited impact on the embedding space due to its uniform nature. To further reveal the vulnerability of Judge LLMs, we introduce the Emoji Attack. Unlike spaces, emojis introduce unexpected and diverse characters that significantly disrupt tokenization and representation. Consequently, the Emoji Attack leverages the rich semantic and visual complexity of emojis, resulting in a more pronounced and effective manipulation of Judge LLMs. In addition to the content inserted, our Emoji Attack also considers the position of emojis within each token to mislead Judge LLMs. Specifically, the position of emojis — whether at the beginning, middle, or end of a token",
        " notable susceptibility to this type of bias. Moreover, compared to token segmentation bias, the Emoji Attack further decreases the prediction ratio from 59.6% to 41.3%. This suggests that emojis have a more pronounced effect on reducing the detection capabilities of the Judge LLMs by introducing new emoji tokens. In addition, the proposed position selection strategy enhances the effectiveness of the Emoji Attack by identifying insertion positions with the lowest {{formula:8aebfe99-b973-4485-8bc2-96fa12612291}} , thereby maximizing the embedding distance between sub-tokens and the original token. Finally, our Emoji Attack can also serve as an evaluation metric for assessing the robustness of Judge LLMs. The results highlight that WildGuard consistently achieves the highest “unsafe” prediction ratio, demonstrating its effectiveness in detecting unsafe content. However, after adding bias, ShieldLM's prediction rate significantly drops to 3.0%, indicating its vulnerability to such attacks. Overall, our token segmentation bias and Emoji Attack reveal the vulnerability of current Judge LLMs. {{table:f18313fa-9b67-405d-8a0d-8c7457a0b4b7"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the main role of the IDA solver in the research paper and how does it contribute to the overall findings?",
      "expected_output": "The main role of the IDA solver in the research paper is to solve the Partial Differential Equation (PDE) using backward differentiation of order 1 to 5 with variable step widths. It contributes to the overall findings by computing the new value of the solution at each time step, allowing the researchers to analyze the behavior of the system over time. Specifically, the IDA solver is used to solve the equation that depends highly nonlinearly on one of the variables, and it does so by using a Newton method, which is a costly operation that needs to be done at every step.",
      "context": [
        "-9baa-d48ba9ae43fa}} , it takes the form given in section REF . This equation can be conveniently set up using the DifferentialGeometry package incorporated in the computer algebra system Maple as demonstrated in figure REF . (The equations that have actually been solved were simplified by multiplying intermediate terms with suitable powers of {{formula:32cddbc3-3ede-43d0-93b8-b5a1f0cb8ffd}} , but for the sake of clearness this has not been included in the Maple code in figure REF .) {{figure:473d556a-3001-4be3-9033-aeff86fa1425}}Unfortunately, Maple (as of version 14) is not able to solve PDEs with more than one space variable numerically. Thus the equations were translated into Mathematica. The PDE was solved using the method of lines. Spatial discretization was done using an equidistant grid, and spatial derivatives were replaced by finite differences. The time-derivative {{formula:0827b992-bba2-4cd7-ba2c-6ca8f6ead2bf}} appears",
        "-4d6e-afe0-238b58f253f8}} , and then the new value of {{formula:d2060105-7508-41e9-a75c-d4cd8a0dd3f4}} is calculated from the equation {{formula:1821cc92-f06c-4949-9d81-a0ca7268317d}} . The dependence on {{formula:dad8e0cd-643e-4a9a-9c9d-f0bfb645068f}} in this equation is of course highly nonlinear. A Newton method is used to solve it. This operation is quite costly and has to be done at every step, which is a main disadvantage of backward differentiation algorithms. Explicit methods are probably much better adapted to the problem. The implementation of an explicit solver is ongoing common work of the authors with Martins Bruveris and Colin Cotter.In the examples that follow, {{formula:ed4beaf9-2ed2-4f4c-aab3-52bb06ce7f17}} at time zero is a square {{formula:11f14ba0",
        "b992-bba2-4cd7-ba2c-6ca8f6ead2bf}} appears implicitly in the equation {{formula:0f344515-d27f-4146-b809-7c3b13333962}} , and this remains so when the operator {{formula:5d514a1e-6d76-4a20-bce4-ace13717c571}} is replaced by finite differences.The solver that has been used is the Implicit Differential-Algebraic (IDA) solver that is part of the SUNDIALS suite and is integrated into Mathematica. IDA uses backward differentiation of order 1 to 5 with variable step widths. Order 5 is standard and has also been used here. At each time step, the new value of {{formula:12a4a52a-6a7d-4a79-8ca0-7b3067f87c59}} is computed using some previous values of {{formula:5d12e721-7bc9-4d6e-afe0-238b58f253f8}} , and then the new value of {{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does thermal noise play in the formation of vesicles, and how do the researchers measure its impact in their study?",
      "expected_output": "Thermal noise plays a crucial role in the formation of vesicles, as it affects the dynamics of the system. The researchers measure its impact by performing simulations with different values of thermal noise, ranging from small to large, and observing the resulting vesicle formation processes. For example, in small thermal noise cases, many micelles with sharp interfaces are formed, whereas in large thermal noise cases, a collision-coalescence type coarsening process of micelles is observed. The study highlights the importance of considering thermal noise in simulations, as it can significantly influence the outcome of the vesicle formation process.",
      "context": [
        "97acb61f-81a9-4364-a4a9-56a66f68ebb1}} and the magnitude of noise is {{formula:435d0d43-edd4-4329-8f6a-8ac08ddf980f}} . The simulation is carried out up to 2500000 time steps (from {{formula:ce34c91a-f035-434a-982f-aa39bcfd452d}} to {{formula:84902b48-6947-4a84-8b2a-a4534400b90a}} ) and it requires about 20 days on a {{formula:b86fafcc-2e05-465b-a09e-9c18aef5acdb}} Xeon work station.The snapshots of dynamics simulations are shown in Figure REF . The observed vesicle formation process is different from the mechanism II, which is observed in the previous thermal equilibrium simulations , and EPD simulations . First, spherical micelles are formed (Figure REF (a)). Then the micelles aggregate and become large (Figure REF (b)), and a disk-like mic",
        " since the effect is large even for early stage of the dynamics. Thus we perform simulations for small systems (system size {{formula:3d5acb8d-4dcb-4f29-afac-a4f2733dc89d}} , lattice points {{formula:559691b2-897c-4184-b1a7-4b46f150cb72}} ) with different values of {{formula:50a865e4-81a1-4e44-b028-4fbf57df261c}} , {{formula:17d29c70-8c89-48fa-b1aa-8b4795b5ef15}} . All the other parameters are set to the previous vesicle formation simulations in Section REF (Figure REF ). Figure REF shows the snapshots of the simulations. For small thermal noise case (Figure REF (a),(b)), we can observe many micelles with sharp interfaces. For large thermal noise case used in Section REF (Figure REF (c),(d)) we can observe the collision-coalescence type coarsening process of micelles, driven by the",
        "9661f0-f9f0-4a07-8e4b-e4024d701013}} ) by Fraaije et al , , . However, since the thermal noise play an important role in the vesicle formation process, we cannot underestimate {{formula:ef23ca53-f5ba-474a-b586-0c08e884fca1}} . One may consider that the mean field approximation and the free energy functional (REF ) does not hold for low noise scaling parameter systems. It may be true, but as shown by Dean , the dynamic density functional equation holds even if there are not large number of particles. Thus here we believe that the mean field approximation still holds for our systems.To show the necessity of the large thermal noise in the simulations, we perform several simulations with various values of {{formula:7bffc9d8-55d2-41f2-8f2b-0e050efd50ec}} . We expect that the simulations for small boxes are sufficient to see the effect of the magnitude of the thermal noise, since the effect is large even for early stage of the dynamics. Thus we perform simulations for small systems (system size {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Cartan involution in Lie algebra decomposition, and how does it affect the overall structure of the algebra?",
      "expected_output": "The main findings of Cartan involution in Lie algebra decomposition show that it induces an involution on a specific subspace, which can be used to decompose the algebra into smaller components. For example, in the given context, the Cartan involution induces an involution on the subspace defined by the formula 5e8f4e17-22f7-4b80-9ed4-103e27f3f960, allowing for a decomposition of the algebra into a direct sum of subspaces. This decomposition is crucial in understanding the overall structure of the algebra, as it provides a way to analyze and study the properties of the algebra in a more manageable and structured way.",
      "context": [
        "d0a-3eb9-408a-b20a-6e747c320e8b}} , where {{formula:495a5a1d-77ff-46d2-bf06-86cc988c9c61}} is the complement of {{formula:4f0dd696-20c7-40a2-a097-6e970deb66de}} with respect to a non-degenerate {{formula:ba08c50a-7925-4b30-8a76-1c26136e08c2}} -invariant bilinear form on {{formula:913287a4-9590-4b18-8a81-92b25757dc01}} . If {{formula:55138c91-c60c-4652-8f84-80eaf0fe2bdd}} is the corresponding Cartan involution then {{formula:ed643981-ded1-4ec1-aa59-3619a1d9d36c}} and {{formula:0e378150-a6ec-445d-accd-0365",
        "-7c2c-4945-af24-bc6607dfd590}} in (REF ) is induced by the Lie algebra Cartan involution (REF ) and because {{formula:de8ed0e0-b430-4235-9aa5-4d97892e6d49}} takes values in {{formula:c1e75e3c-6f2f-45a6-b183-3f9f189fe1b7}} . So, we conclude that {{formula:18f8b6ff-8959-4dc0-97eb-966c66eea57c}} in (REF ) restricts to an involution, {{formula:adb2011e-a116-4aa1-afc7-7ded1fde6138}} , on {{formula:5e8f4e17-22f7-4b80-9ed4-103e27f3f960}} . Hence we can write {{formula:e9e387d4-8f6c-4d4f-8a18-5149122bb5a8}}where {{formula:",
        "-4b11-93db-21eba4c93a4c}}and, from (REF ), that {{formula:c47aff6f-004d-4723-bae2-6ca33081007d}}Let {{formula:8a546856-a0d9-4496-b5e3-053fffa7871c}}be the involution in {{formula:be77b9b2-5ffe-4284-a9d6-e1df015a259e}} defining the above decomposition. It is, of course, induced by the Cartan involution of the Lie algebra, {{formula:482f749a-e84f-4233-bbbf-282467954799}} defined in (REF ). Its {{formula:35d47b89-fa7d-41e3-9b83-ac3da8c529e1}} -eigenbundle is {{formula:d2ce3307-10f9-4d80-8117-67900abaa5f2}} and its {{formula:aea99b27-a5c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the relationship between metric duality and Taub-NUT symmetry in this research paper?",
      "expected_output": "The research paper demonstrates the duality between two configurations of the metric, which can be clearly shown in the context of Taub-NUT space duality. By confining motion to a cone and using conserved angular momentum, the problem can be reduced to 2-dimensions, allowing for the representation of the metric and coordinates. The paper also explores the relationship between Taub-NUT symmetry and Bianchi-IX metrics or solutions to Darboux-Halphen systems, highlighting another possible symmetry of the Taub-NUT. Additionally, the paper discusses the computation of spin connections and the analysis of conserved quantities in a general gauge-invariant form using the Holten Algorithm.",
      "context": [
        "d8bc-4305-9d31-e14d5add9383}}Evidence for the duality between these two configurations of the metric can be clearly demonstrated. To study Taub-NUT space duality, we confine motion to a cone ({{formula:2cd5d2de-d088-49ae-af3f-fc876e6f3620}} ). This is permissible because of the conserved angular momentum (REF ), for which , , {{formula:aa4aa238-535e-469e-a5d3-0742a5b90042}}This allows us to reduce the problem to 2-dimensions by rendering {{formula:a6907842-5bde-49ab-98fd-9026bd8518d4}} a constant co-ordinate, allowing us to write the metric as:{{formula:17a16480-a3b5-4bd4-ada8-7c37e0f02e8f}}We shall represent the co-ordinates as {{formula:27a019d8-1411-41c3-b70b-35f6dfe6a",
        "9d3}}Rescaling the radius and solving (REF ) with suitable constants of integration gives us: {{formula:e72bb6ec-f512-4782-a00c-d619a91f2995}} {{formula:2d03ae43-eb6d-4443-9795-00771bffb269}}and rescaling the metric as {{formula:7e48c4e2-c974-4654-80b8-d6d461c24588}} we get the Taub-NUT (REF ) and conserved quantity: {{formula:7777de31-f0fa-4693-bb2d-0b049d77b072}} {{formula:927af3da-02a3-4af8-85d4-aed3dd63d166}}This concludes another possible symmetry of the Taub-NUT as a member of Bianchi-IX metrics or solutions to Darboux-Halphen systems.\nCurvature and anti-self duality: Now that we have identified the individual vierbeins, we shall proceed to compute the spin connections. We can",
        "4-ab60-cc141c592d5c}}Now that we have redefined the Poisson Brackets to make Hamiltonian dynamics manifestly gauge invariant in the modified bracket, we can proceed to analyze the conserved quantities in a general gauge invariant form. This is done by the Holten Algorithm as shown in and discussed later as we shall see.\nA dynamical-systems description of Taub-NUT: The Euclidean Taub-NUT metric as shown in is given by:{{formula:e46be5e2-6090-49e8-b282-c5dbc23c7ea3}}For later reference, taking {{formula:d8e0a005-7419-4558-8b1c-a8ae80e95c98}} we shall re-write the above metric into this form :{{formula:0feae758-3e65-41e0-865a-1064616e873d}}We now consider the geodesic flows of the generalized Taub-NUT metric given by (REF ), for which we can compose the Lagrangian: {{formula:5ac4a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the impact of geon quotients on Einstein's theory, and how do the methods used support these findings?",
      "expected_output": "The research paper presents new families of geon-type black holes in various dimensions for Einstein's theory coupled to gauge fields. The main findings include the construction of rotating geons as quotients of asymptotically flat Einstein-Yang-Mills black holes and the existence of geon variants with trivial gauge bundles. The methods used, such as taking quotients of eternal black hole solutions and including antisymmetric tensor fields, support these findings. For example, the paper shows that the four-dimensional spherically symmetric Einstein-Yang-Mills hole has a geon variant with a trivial gauge bundle, demonstrating the impact of geon quotients on Einstein's theory.",
      "context": [
        "hog-like {{formula:0dee39ec-5c0a-4937-b1e4-a60514528332}} configuration over the two-sphere.\nConclusions and discussion: In this paper we have constructed new families of geon-type black holes in {{formula:3a212e82-a548-4f61-97c3-765ee22516cd}} dimensions for Einstein's theory coupled to gauge fields, by taking {{formula:048070c1-6e83-4365-8a7e-1623e0a027cd}} quotients of eternal black hole solutions that have a nondegenerate Killing horizon. In the static vacuum case, the existence of the quotient was equivalent to the existence of suitable isometries on the spatial sections, and antisymmetric tensor fields with a pure {{formula:d56c8519-789c-4fd3-a159-3d4e5177cd68}} action could then always be included by taking the field strength to be a section of a suitable, and in most cases nontrivial, bundle. With angular momentum, we constructed rotating geons as quotients",
        "ptotically flat Einstein-{{formula:51c78bfd-50cc-4195-9146-9bdf64de183e}} black holes , , a large variety of Einstein-Yang-Mills black holes have been found. A fairly recent survey is given in . The nonabelian gauge freedom makes investigation of geon quotients substantially more complicated than in the abelian case, especally as the known solutions tend to be given in gauges in which the spacetime symmetries are not manifest. We address here only one example, the four-dimensional spherically symmetric Einstein-{{formula:499dd3a9-5366-48d6-9472-f682c60b1673}} hole in which the cosmological constant may be negative or vanishing , , , , , , . We show that this hole has a geon variant with a trivial gauge bundle.We follow the notation of . The exterior metric is written in Schwarzschild-like coordinates as {{formula:aed3bda6-548b-4245-95e0-1d710e7121f8}}where {{formula:11b262d8-cd88-476f-",
        " asymptotic region. In subsection REF we recall relevant properties of the nondegenerate Gibbons-Wiltshire black holes with static asymptotic regions , and in subsection REF we construct geon versions of these holes. Subsection REF presents examples in the special case of constant curvature transversal space. Generalisations beyond the Gibbons-Wiltshire spacetimes, including generalisations to multiple (i.e., {{formula:9fb97bf6-5fb2-4ad1-8fbf-62f991624ad5}} ) gauge fields, are discussed in subsection in REF .\nGibbons-Wiltshire metric: Let {{formula:eb4d320f-083c-4f48-9a52-66e0327b972c}} , and let {{formula:b5f09b8f-c352-405f-ac72-e9d16ff3aa07}} be a {{formula:b1f53b40-d76e-4d3c-803d-bdc07211a19b}} -dimensional positive definite Einstein manifold: Writing {{formula:7de"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in Einstein's equations that suggest local flatness, and how do the methods used in the research lead to these key results?",
      "expected_output": "The main findings in Einstein's equations that suggest local flatness are based on the equivalence of the Einstein and Riemann tensors, which obey the double-duality identity. This equivalence implies that spacetime is flat in the absence of sources, as the Einstein equations reduce to a zero 'field strength' field equation in source-free regions. The research methods used to arrive at these findings involve the classification of Minkowski signature locally flat spacetimes and the global matching of flat patches at particle trajectories where sources and curvature do not vanish. For example, in D=3, general relativity has a similar property where spacetime is flat in the absence of sources, and the Einstein and Riemann tensors are equivalent.",
      "context": [
        "4561-b50a-657d6651f5aa}} ) tensors are equivalent, obeying the double-duality identity {{formula:33579e26-5a8d-4e74-8d3e-a329b65dc92a}} . Hence the Einstein equations {{formula:dcb49c6f-c8e3-43a8-ad58-371dc0e28bd3}} imply local flatness. Classification of such Minkowski signature locally flat, (or more generally locally constant curvature if there is also a cosmological constant so that {{formula:8a5d8231-f45b-4fb3-9a31-c04bde5c82dd}} ), spacetimes has also become a physical industry of its own. Here the physics involves global matching of flat patches at particle trajectories where the sources {{formula:fdf6809e-4ac7-4a17-b3e3-ce858e2c0148}} (and therefore curvature) do not vanish. This zero “field strength\" field equation in source-free regions in gravity is of course very reminiscent of",
        "156}} is the dual field strength, a 1–form). Thus the field is locally pure gauge wherever there are no sources; to find the general global solution with the properties that the field strength is equal to the current and vanishes elsewhere is then an interesting exercise. This is even more so in the nonabelian case where the abelian part is supplemented by the famous {{formula:65d9819d-6faf-4a74-8530-a7769a6deba9}} addition to yield the same (but now nonabelian) Euler–Lagrange equation {{formula:2928a072-f11d-4d4e-a992-6fee9709e4db}} . Now in D=3, general relativity has a very similar property: spacetime is flat in the absence of source, since Einstein ({{formula:2a499344-908e-45bd-80f4-5cbd2e6670fb}} ) and Riemann ({{formula:8699ba3f-bc55-4561-b50a-657d6651f5aa}} ) tensors are equivalent, obeying the double",
        " theory necessarily contains a 3-form potential {{formula:adba385f-39fa-4ea0-b4d0-cb23bbc0940d}} , and it was found that there has to be an addition {{formula:8106d327-c1d8-4ea5-826b-8618bc13a977}} , {{formula:6f1068fe-683a-48af-a796-5d5588a05ad3}} to the usual {{formula:b260a420-280c-4d20-89c0-f0f200df24cf}} kinetic term in the action. The Einstein gravitational constant {{formula:0c469781-e0c5-4334-826b-5789329438ab}} appears here, but not (of course) the metric. A smaller paradox is that despite appearances, {{formula:d6f244fc-4041-4950-a173-79f96dfdd6e8}} is both parity and {{formula:15bd4411-29b5-40e4-bf23-4f2a5d06d"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does CBOWRA use to detect medication anomalies and what are its key findings in this area?",
      "expected_output": "The CBOWRA method uses a Continuous Bag Of Words (CBOW) model to learn the representation of context and an accumulated ranking strategy to detect possible medication anomalies. The key findings of CBOWRA include its ability to detect medication anomalies with increased accuracy, as shown in experiments conducted on real hospital electronic medical records, where the topN accuracy of the proposed method increased by 3. The method also provides a simple, yet effective representation learning approach that is expert-free, saving considerable resources for practical adoption.",
      "context": [
        " in prescriptions. The method uses an Continuous Bag Of Words (CBOW) model to learn the representation of context, and an accumulated ranking strategy is included to help detect possible medication anomaly. The contributions of the manuscript are three folds.We proposed a simple, yet effective representation learning method to detect medication anomaly based on patients' diagnoses and prescriptions. The training process of the model is expert-free, which saves considerable resources for practical adoption. We modified conventional CBOW model in both structure and training procedure to predict the central word based on diagnoses. We also incorporate a ranking mechanism to help exaggerate the differences between normal drugs and the anomaly. To illustrate the effectiveness of the proposed method, we designed detection process for non-representation and representation learning methods. The experimental results confirmed the superiority of the proposed method, which further provides insights for adopting representation learning models in similar scenarios.The rest of the paper is organized as follows. The detection scheme, along with the construction of the detection model, are detailed in Section 2. The evaluations of the proposed method are conducted on real EHR data in Section 3, in which the details of data preparing and comparative methods are included as well. Section 4 presents the discussions regarding to the proposed method and the conclusions are drawn as Section 5.\nMethod",
        "# arXiv Paper: 1908.07147\n\nCBOWRA: A Representation Learning Approach for Medication Anomaly Detection\nabstract: Electronic health record is an important source for clinical researches and applications, and errors inevitably occur in the data, which could lead to severe damages to both patients and hospital services. One of such error is the mismatches between diagnoses and prescriptions, which we address as 'medication anomaly' in the paper, and clinicians used to manually identify and correct them. With the development of machine learning techniques, researchers are able to train specific model for the task, but the process still requires expert knowledge to construct proper features, and few semantic relations are considered. In this paper, we propose a simple, yet effective detection method that tackles the problem by detecting the semantic inconsistency between diagnoses and prescriptions. Unlike traditional outlier or anomaly detection, the scheme uses continuous bag of words to construct the semantic connection between specific central words and their surrounding context. The detection of medication anomaly is transformed into identifying the least possible central word based on given context. To help distinguish the anomaly from normal context, we also incorporate a ranking accumulation strategy. The experiments were conducted on two real hospital electronic medical records, and the topN accuracy of the proposed method increased by 3",
        ". In the same time, mining the context of drug corpus has drawn attentions in recent years , which motivates us to detect the errors through mining the semantic relationship between diagnoses and prescriptions.As discussed in previous content, possible causes of the errors include wrong typing of prescriptions, misuse of the system, wrong diagnosis, etc. In the following part of the manuscript, we address the issue as medication anomaly. Clinical experts identify them based on domain knowledge. It means there exit certain semantic links among patients, diseases and drugs. Detecting the anomaly requires the machine to understand the meaning of the text, but current approaches regarding to natural language processing are mostly used for correcting syntactic errors , . There is clearly a missing part of adopting NLP techniques to tackle the problem from the perspective of semantic connections between patients and drugs. To fill the gap, we were motivated to have the model learn the semantic inconsistency between the diagnoses and the prescriptions, and use the model to distinguish possible medication anomalies.Following the intuition, we proposed a representation learning method called Continuous Bag Of Words (CBOW) based on Ranking Accumulation (CBOWRA) to detect any possible anomaly in prescriptions. The method uses an Continuous Bag Of Words (CBOW) model to learn the representation of context, and an"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do fixed-point subalgebras play in relation to observables in the context of this research paper?",
      "expected_output": "Fixed-point subalgebras play a crucial role in relation to observables in this research paper. They are defined as the subalgebras that remain unchanged under a specific action, and they constitute the algebra of observables. For example, the fixed-point subalgebra of the Cuntz algebra is embedded into another algebra, satisfying a specific relation with respect to the canonical endomorphism. This unique existence of a field algebra with a fixed-point algebra is essential in the context of this paper, as it provides a way to directly check experimentally certain properties of observables.",
      "context": [
        "formula:3e24422d-dcb3-4165-b7f8-a7e35b77fdfb}} invariant under {{formula:b227fd84-3488-4d60-89db-d77e35a3fac1}} and are usually called observables which constitute the algebra {{formula:f598ca68-0f16-4825-a72b-86fdc2bc1c78}} of observables: {{formula:68cd5e5b-dc0b-41f9-8b77-551a7d1f3f74}}the fixed-point subalgebra of {{formula:f5f0fd34-0e12-4a8b-9aa4-0dd89976a21f}} under the action {{formula:43c25a0e-2d3c-4c02-b9d7-45beb86e45ba}} of {{formula:6a9e51c0-d966-46eb-911c-35ce2a959eb3}} . Thus, what we can directly check experimentally",
        "formula:feb8c73d-7fe3-4648-9e9b-4caaf9f5fd5c}} , {{formula:1c87835a-8edd-43f0-8141-39ec1283fc99}} , {{formula:a75406ee-b6c0-4721-a812-bc6c1f6b0eff}} whose fixed-point subalgebra {{formula:166f1876-c960-48ea-9530-4ed3f0206609}} is embedded into {{formula:47a9b021-59c8-4d5e-90c0-efe309d11975}} , {{formula:c6950832-2a2f-4dca-8bc4-6b2757428d9d}} , satisfying the relation {{formula:4a9d106b-bc0d-48f8-856d-476bc60873c1}} with respect to the canonical endomorphism {{formula:2c193e98-0529-40d3-9a06-1c74",
        "e8-9bf2-66a52272d262}} ), and the identification {{formula:a20b7f49-09a2-4227-bbd3-77425a156f2c}} for {{formula:593bb4ea-81cc-4710-8533-20e06136383e}} . Unique existence of a field algebra {{formula:4e4f9402-975b-4ee1-bdb9-1da377012da3}} with {{formula:94962415-4937-478c-b875-00bcb9f41a96}} (fixed-point algebra), where {{formula:2d0ea848-b49b-4c3f-8ceb-da717ff3944f}} is the Cuntz algebra defined as the unique simple C*-algebra consisting of {{formula:d76e0c12-a1b5-4e55-9d15-276fb36d1d04}} isometries {{formula:feb8c73d-7fe3-4648-9e9b-4caaf"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods do simulations use to model gravitational wave strain for nonspinning binaries, and what are the key findings of these simulations?",
      "expected_output": "Simulations use numerical techniques to model gravitational wave strain for nonspinning binaries. The peak frequency of the mode of the gravitational wave strain is modeled using a formula that takes into account the particle limit, which is set to a specific value in the nonspinning limit. The key findings of these simulations include an 8th order convergence rate for the waveform phase and amplitude, allowing for the use of a medium resolution as a reference grid for further simulations. These simulations also enable the study of the convergence of radiative quantities derived from the gravitational waveform, as shown in the provided tables and figures.",
      "context": [
        ", practical application of these gravitational waveforms requires longer evolutions. Other approaches to the small mass ratio regime have recently been followed , . Here we report on a new set of evolutions that are based on the numerical techniques refined for the longterm evolution of a spinning precessing binary with mass ratio {{formula:e44079f5-39d4-42a0-8db6-d922f2c3d98a}} . We study here the case of a nonspinning {{formula:b9093e45-4bc6-418d-8264-283afbfbf460}} binary in a convergence sequence to assess numerical and systematical errors. We then add a sequence of {{formula:6b651e3d-9f55-41df-ae16-05fd151e00d5}} , {{formula:9aeafa8e-de54-4aa8-82e1-b1437d21c271}} , and {{formula:2586feb6-0998-4935-9213-bf3cd6960b28}} , nonspinning binaries evolutions for about a dozen orbits before merger.\nSimulations'",
        "10d-40a8-9e3a-0e9a9e241516}}Analogously, we model the peak frequency of the {{formula:71562185-4a1d-4b7a-a6b5-f388aa887826}} mode of the gravitational wave strain for nonspinning binaries as, {{formula:7226560d-809a-4829-b578-3f5086cacea4}}where {{formula:7004b83c-23fa-4e77-bc6d-32f621b76397}} is the particle limit, taking the value {{formula:e6d0dee7-dd51-4859-a848-486e6d2b1b4b}} in the nonspinning limit . {{table:98d0252a-5301-4185-a2ae-83ac61098800}}Table REF summarizes the new values of all those coefficients with their estimated errors.\nConclusions: This study represents a new Numerical Relativity milestone for comparative studies to assess improvements in code efficiency, on gauge choices, and on improved",
        " REF displays the convergence rates for all the radiative quantities derived from the gravitational waveform. The detail of the convergence of the waveform phase and amplitude with global numerical resolution is shown in Fig. REF displaying an 8th order convergence rate (alignment not enforced). {{figure:52b9aca5-817e-4488-ae22-1d04fa1389fa}}{{table:3eb13bcf-4f73-4abd-a43d-dc7384b93f5b}}The high convergence of these results allow us to use the medium of the resolutions, n100, as the reference grid to perform a set of smaller {{formula:5b89675b-c678-4abf-b9cd-7e15961ab165}} simulations, each time halving the mass ratio and adding a new refinement level around the smaller hole (12, 13, 14, 15 levels for {{formula:af866feb-951f-495b-bf55-275d94c706b2}} , respectively). This basic grid configuration ensures exactly the same accuracy around the larger hole, the radiation and boundary zones, and in between the holes, while doubling"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do different particle types play in the formation and stability of hexatic order, and how do the research findings relate to the methods used to study this phenomenon?",
      "expected_output": "The research paper shows that different particle types play a significant role in the formation and stability of hexatic order. For example, the paper finds that the passive system shows more local hexatic orientational ordering for certain particles compared to others. The study uses methods such as structural analysis and examination of positional order to investigate this phenomenon. The findings suggest that the hexatic orientational order can occur in the absence of positional order, and that the system can exhibit glassy structural order. The use of random initial configurations and equilibrated final configurations allows the researchers to examine the local hexatic ordering of different particle types and how it increases in the passive system.",
      "context": [
        "-4246-9418-d7d7f104c615}} for {{formula:269d64a3-87bb-4c8d-bc9a-eb9dac8bcd95}} particles compared to the {{formula:2f6467cd-3a2c-4a63-905f-8f8b35951482}} particles means a larger correlation length of hexatic order for {{formula:9798a651-5003-4069-b32f-f46b3e107040}} particles. Thus, the passive system shows more (local) hexatic orientational ordering for the {{formula:17126f24-ec88-40f5-a8bd-b42294b2e2a6}} particles compared to the {{formula:67349a21-24b2-4941-b228-f3c69d817fe9}} particles in this system.The hexatic orientational order found in the absence of positional order at the size ratio 1:1.4 , is termed as the glassy structural order by Kawasaki and Tanaka . Therefore, we examine the positional order in the system",
        " REF (d). The absolute values of {{formula:216410bb-f769-43cb-b8c4-4ab431718bf0}} of majority of particles for the random initial configuration (see Fig. REF (a)), are far less than 1.0, showing a random orientational order, except for few particles, for them it shows local hexatic ordering ({{formula:037a940a-735c-48f1-9ef3-82f276eb4830}} 1) because the natural number of nearest neighbors of a particle are six in a dense 2D system. The equilibrated final configuration of the passive system at {{formula:192f3f03-dd7c-4463-b791-ea6a19329466}} 0.01 shows the local hexatic ordering of {{formula:96b22bf8-51f3-4582-b311-546ab965f9d8}} and {{formula:290bf11f-79a0-4b28-956a-b9c9bd4991e6}} particles that can be seen in Fig. REF (b",
        "956a-b9c9bd4991e6}} particles that can be seen in Fig. REF (b). Fig. REF (b) shows that the local hexatic ordering increases in the passive system at {{formula:3c692117-9731-4234-9b6d-bfbefd6d49e6}} 0.01 and the system shows mixing. Structural analysis of the passive system is given in Appendix , which shows that it is amorphous and does not posses long-range (or quasi-long-range) positional or orientational order. {{figure:352c8bdf-6128-4959-abc6-281a28b7aa3b}}\nConfigurations of active-passive binary mixture: The final configuration obtained at {{formula:f6a0bfe6-e659-49e3-a90f-4b4ee8f253d6}} 0.01, shown in Fig. REF (b), is used as an initial configuration for the active system ({{formula:9b0a685d-3f77-4cce-bb8c-d3367fbbd"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between BCs and KK masses, and how do the researchers' findings contribute to our understanding of this interaction?",
      "expected_output": "According to the research paper, the relationship between BCs (boundary conditions) and KK masses (Kaluza-Klein masses) is that the masses of the lowest KK gauge excitations are universal for all gauge bosons with the same BCs. The researchers found that before EWSB (electroweak symmetry breaking), gauge bosons with certain BCs have massless zero modes. After EWSB, the zero mode gauge bosons with specific BCs acquire masses. The paper provides specific examples of KK masses, such as 05e71db6-4fb7-4ea7-8e12-9b2003fe1e8d and 6b74271b-9b14-4104-b256-ddc064edd62a, which are numerically given. These findings contribute to our understanding of the interaction between BCs and KK masses, highlighting the importance of BCs in determining the masses of gauge bosons.",
      "context": [
        "c22cfa61805}} , after the mixing pattern, we have four charged bosons which are specified as {{formula:50e5e6e8-f46d-446c-a974-c5800c31be4c}} and {{formula:a7683295-c37e-465e-b628-16075d0e7c31}} while three neutral gauge bosons are given as {{formula:643571ea-d18d-4f77-a6e3-1c869b941251}} , {{formula:672bdffc-a6ee-43ae-843d-4269472dc243}} and {{formula:013f30cb-877e-465e-ab27-527f58655eec}} . Moreover, we mention the following remarks about the masses and profiles of various gauge boson fields that are obtained after solving the corresponding EOMs. Before EWSB, gauge bosons with {{formula:0f837893-4fd1-4310-8135-5f30d6f13ae6}} BCs have massless zero modes, which correspond",
        "4aa-cefe0d63b649}} and {{formula:ed988ab6-9976-47c3-8ac5-46f0eee3bf1b}} are {{formula:6021c521-60d4-46ab-b6c9-bbddf3b821ed}} {{formula:620d0a31-1d24-4aef-8bee-6f06ad03bc18}} {{formula:e09f74c5-1b73-489f-8ef6-88aaadb4a65e}}The masses of the lowest KK gauge excitations are numerically given to be {{formula:05e71db6-4fb7-4ea7-8e12-9b2003fe1e8d}} and {{formula:6b74271b-9b14-4104-b256-ddc064edd62a}} . Notice that the presented KK masses for the gauge bosons are universal for all gauge bosons with the same BCs. After EWSB, the zero mode gauge bosons with {{formula:c5de8",
        "9765-2ee01969b70c}} . Charged gauge bosons are defined as {{formula:6057324c-3278-4c64-a06b-1f7bee39e8fb}}Mixing between the bosons {{formula:54061f52-8e44-4a00-99b5-2f76ee672c8e}} and {{formula:2d50f286-52fa-49fd-9e30-3f02d011ebcb}} results in fields {{formula:d863aaa2-2e7c-4089-b6de-e5ed5675173a}} and {{formula:337c8931-7b30-493c-bd6c-4e554b4a500c}} , {{formula:c0cd8f29-9217-4219-84af-bb7324e9001a}}where {{formula:e76338df-69f4-419f-96c2-fe1f481d9525}}Further, mixing between {{formula:c3584819-9885-40df-9d"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does MambaMorph's module play in facilitating deformable registration, and how does it contribute to the research paper's main findings?",
      "expected_output": "MambaMorph's registration module plays a crucial role in facilitating deformable registration by introducing a Mamba-based approach that captures long-range correspondence and handles large volumes efficiently. It contributes to the research paper's main findings by enabling effective multi-modality registration, such as MR-CT registration, and alleviating the issue of well-aligned data deficiency through the proposed SynthRAD Registration dataset.",
      "context": [
        "ive loss.In this section, we are about to introduce our mamba-based registration module \\mathcal {R}_{\\psi }(\\cdot ,\\cdot ) in subsection 3.1, contrastive feature learning \\mathcal {F}_{\\theta }(\\cdot ) and \\mathcal {L}_{CL}(\\cdot ,\\cdot ) in subsection 3.2. Finally, we illustrate our SynthRAD Registration (SR-Reg) dataset processing approach in subsection 3.3. {{figure:d5598b08-f216-43a1-8638-35805114af87}}\nMamba-based registration module: Following the framework of TransMorph[C], our MambaMorph's registration module makes some modification which is shown in Fig. REF . After splitting a full size volume (e.g. 192\\times 208\\times 176 voxels in this paper), one may obtain a sequence with about 0.1 million tokens. Due to the poor long-range sequence modeling capacity and quadratic complexity w.r.t. sequence length, we assume that Transformer is not of high efficiency at this case. As a potential replacer of",
        " of Mamba is not time-invariant anymore and needed to solve via recurrence instead of convolution. Another branch of Mamba introduces a gated mechanism to be selective w.r.t. data. Notably, we add sinusoidal position embedding onto the input tensor to make Mamba position-aware.MambaMorph' registration module takes a concatenated volume as input, which is from moving volume feature and fixed volume feature. The input flows into two branches - a horizontal one with full volume and a UNet-like one where is split. At the latter branch, the volume is split into amounts of patches and seemed as a sequence of token (patch). After linear projection, consecutive stages of Mamba block and patch merging are applied on the sequence. Patch merging is a variant of down sampling, involving decreasing the amount of token from B \\times (H\\times W \\times D) \\times C to B \\times (H/2\\times W/2 \\times D/2) \\times 8C and projecting the number of channel from 8C to 2C. In the UNet-like module, Mamba-based encoder aims to capture long-range correspondence while the CNN-based decoder is",
        " and suffer from heavy memory burden due to attention mechanism. This work reproposes a well annotated MR-CT registration dataset and presents a registration framework with a light-weighted feature extractor which is guided by anatomical semantic information.\nMethod: To perform multi-modality (MR-CT in this paper) deformable registration, our MambaMorph adopts two proposed technique, i.e. Mamba-based registration module and contrastive feature learning. The framework of MambaMorph is illustrated in Fig. REF . In addition, we repropose an MR-CT deformable registration dataset called SynthRAD Registration (SR-Reg) to alleviate the issue of well-aligned MR-CT data deficiency.Given a pair of moving volume x_{m} and fixed volume x_{f}, their corresponding segmentation s_{m} and s_{f}, registration module \\mathcal {R}_{\\psi }(\\cdot ,\\cdot ) and feature extractor \\mathcal {F}_{\\theta }(\\cdot ), the registration in this paper can be formulated as follows:\\min \\limits _{\\psi ,\\theta } \\mathcal {L}_{dice}(s_{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of OS, CPU, and Python versions on AI system variability, and how do these factors affect the methods used and key results in this research?",
      "expected_output": "The main findings of the research paper indicate that environment configurations, including Operating System (OS), CPU Architecture, and Python Version, significantly impact the variability of AI-based systems. Specifically, the study found that variability exists in all three metrics evaluated: performance, processing time, and expense. Regarding OS, the research discovered that 23% of projects showed variability in performance between Linux and MacOS, and 20% between Linux and Windows. Almost all projects exhibited significant variability in processing time and expense between different OS. For Python Version, the study investigated its effect on AI-based systems' behavior and found that it contributes to variability. The research used Travis CI, a Continuous Integration platform, to conduct experiments with eight different combinations of the three key environment variables on 30 open-source AI-based systems. The methods used included evaluating variability using three metrics and analyzing the results to answer three research questions. The key results showed that variability is more frequently observed with respect to processing time and expense than performance, highlighting the importance of considering environment configurations when developing AI-based systems.",
      "context": [
        " the following contributions:To the best of our knowledge, this is the first empirical study on the variability of AI-based systems from the environment configuration point of view. We provide empirical evidence on the effect of different environment configurations on the variability of AI-based systems. We make our data and scripts available for reproducibility and future research .The rest of the paper is structured as follows. Background and research methodology are described in Section . In Sections , , and , we present the results and findings of our analysis for each research question. Sections , present discussion and threats to validity respectively followed by Section in which we talk about existing works. Finally, in Section , we summarize our findings and conclude the paper by describing potential future directions of research.\nMethodology and Background: To be able to conduct our experiments in different development environment configurations, we use Travis CI — a widely used Continuous Integration (CI) platform . The reason for choosing Travis CI is that a recent study on the usage of CI tools in ML projects reports that Travis CI is the most popular among open-source software (OSS) developers for building AI-based systems .\nEnvironment Configurations in Travis CI: In this study, the three configuration variables we experiment with are Operating System, CPU Architecture, and Python Version.",
        "# arXiv Paper: 2408.02825\n\nOn the Variability of AI-based Software Systems Due to Environment Configurations\nabstract: [Context] Nowadays, many software systems include Artificial Intelligence (AI) components and changes in the development environment have been known to induce variability in an AI-based system. [Objective] However, how an environment configuration impacts the variability of these systems is yet to be explored. Understanding and quantifying the degree of variability due to such configurations can help practitioners decide the best environment configuration for the most stable AI products. [Method] To achieve this goal, we performed experiments with eight different combinations of three key environment variables (operating system, Python version, and CPU architecture) on 30 open-source AI-based systems using the Travis CI platform. We evaluate variability using three metrics: the output of an AI component like an ML model (performance), the time required to build and run a system (processing time), and the cost associated with building and running a system (expense). [Results] Our results indicate that variability exists in all three metrics; however, it is observed more frequently with respect to processing time and expense than performance. For example, between Linux and MacOS, variabilities are observed in 23",
        " systems, versions of a programming language and associated libraries, and hardware configurations, can depend on many factors including developers' experience, business needs, and existing environment configurations of legacy systems. However, these choices may potentially induce variability in the performance of AI/ML models as “practitioners' degrees of freedom” , , which is a known issue in the field of applied statistics. However, in the domain of software engineering, no existing work studies the potential sources of variability in AI-based software from an environment configuration perspective. In this paper, we aim to address this issue by answering the following three research questions:RQ1: (Operating System) To what extent does the operating system induce variability in AI-based systems? We analyze whether variations in operating systems make AI-based systems behave differently. We find that variability in performance is observed in 23% of the projects between Linux and MacOS whereas 20% of the projects show such variability between Linux and Windows. Almost all projects show significant variability in processing time and expense between different operating systems. RQ2: (Python Version) How does the Python version contribute to the variability in AI-based systems? We investigate the effect of Python versions on the behavior of AI-based systems. We found that Python"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of QCD's role in dense matter via effective models and how do the methods used impact the key results?",
      "expected_output": "The main findings of QCD's role in dense matter via effective models show that QCD-like effective models are used to study phase transitions in dense quark matter and light meson physics. These models, such as chiral effective Lagrangians and Nambu – Jona-Lasinio type models, are employed to consider the charged pion condensation phenomenon at nonzero isospin chemical potential. The methods used, including the Gross-Neveu type models, impact the key results by providing a qualitative consideration of QCD at arbitrary energies and allowing for the study of color superconductivity and nonperturbative methods of quantum field theory. For example, the GN phase portrait in terms of baryon chemical potential vs temperature resembles the QCD phase diagram, making it a useful tool for understanding dense QCD phenomena.",
      "context": [
        "\\nu)$ there is a strip $0\\le\\lambda<\\lambda_c$ which lies as a whole inside the pion condensed phase. In this phase the pion condensation gap is an oscillating function vs both $\\lambda$ (at fixed $\\nu$) and $\\nu$ (at fixed $\\lambda$).\nIntroduction: It is well-known that QCD is a fundamental theory of strong interactions both in the vacuum and in hot and/or dense baryonic matter. However, it can be successfully used only in the region of high energies, temperatures and densities (or chemical potentials), where a weak-coupling expansion is applicable. Away from this region, different nonperturbative methods or effective theories such as chiral effective Lagrangians as well as Nambu – Jona-Lasinio type models (see, e.g., the papers , , , and references therein), are usually employed for the consideration of light meson physics, phase transitions in dense quark matter, etc. In particular, motivated by the fact that in heavy-ion collisions and compact stars the hadronic matter is isotopically asymmetric, different QCD-like effective models were studied at nonzero isospin",
        " hadronic matter is isotopically asymmetric, different QCD-like effective models were studied at nonzero isospin chemical potential {{formula:38dee7a2-a1cd-4ffa-a0dd-7f1ee134b3d4}} , , , , . There the charged pion condensation phenomenon, which is generated if {{formula:167faa83-bf8f-4e93-8518-50236c16ec57}} is greater than the pion mass {{formula:a2e12fac-ff42-460f-bd5e-bf300aef4f4d}} , was also considered.In all above mentioned papers the effective models are i) field theories in usual (3+1)-dimensional spacetime, and ii) they are employed for the description of QCD at rather low energies and densities. At the same time there is another class of theories that can be used as a laboratory for a qualitative consideration of QCD at arbitrary energies. These are the so-called Gross-Neveu (GN) type models, i.e. two-dimensional quantum field theories with four-fermion interactions , , . Ren",
        ") type models, i.e. two-dimensional quantum field theories with four-fermion interactions , , . Renormalizability, asymptotic freedom as well as the spontaneous breaking of chiral symmetry (in the vacuum) are the most fundamental features that are inherent both for QCD and all GN type models. In addition, the GN phase portrait in terms of baryon chemical potential {{formula:6caa8a4d-0847-4cb9-b34f-ca540776a1da}} vs temperature resembles qualitatively to a great extent the QCD phase diagram , , , , . Due to their relative simplicity in the leading order of a large {{formula:cf70c952-2b24-4ae4-bf3b-b9a438854561}} -expansion ({{formula:fb5904fc-8c21-4bd8-84fa-b52ec3c53943}} is a number of colored quarks), it is very convenient to use GN models for considering such a phenomenon of dense QCD as color superconductivity , and to elaborate new nonperturbative methods of quantum field theory , , . Moreover, the influence"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding position bias and its impact on click rates in this research paper?",
      "expected_output": "The research paper finds that position bias has important consequences on user behavior, with a strong correlation between post positions and click rates. Facebook's News Feed algorithm creates a position bias by showing repeated posts, which can lead to a lower Value of Access (VoA) for users. To reach a VoA of 10, a Facebook user must scroll 20 posts on average, indicating that the algorithm prioritizes showing the same post multiple times over showing new content.",
      "context": [
        " consistent with the conclusions of . Note, for instance, that when {{formula:36d75345-8cf1-463a-a681-71fe0f8476dd}} it is expected according to the proposed model that the timeline can be fulfilled with new posts, i.e. VoA{{formula:af70e021-f67e-41b5-ae1f-f89521fc23eb}} , but the Facebook News Feed recommends only 5 new posts on average. To reach a VoA of 10, a Facebook user must scroll 20 posts on average.Due to reordering, Facebook News Feed algorithm shows repeated posts, in detriment of newer ones, at the top of the timeline. One possible reason for this behavior is that Facebook shows the same post multiple times to assure that users will more likely see a post. To overcome this effect, users must scroll their timelines beyond the repeated posts to see new contents and obtain all the value of the access. Another possible reason is that Facebook strategically reduces the VoA as a way to increase the users' access rate. It worth noting that the position bias has important consequences on the behaviour of users as there is a strong correlation between post positions and click rates , .When",
        "-4927-afe5-fc3597af3afb}} , i.e. {{formula:9534235d-9921-4b80-9ca1-6870ddcc1a1a}} , the FIFO News Feed and model are very close, while for larger values of {{formula:38de04c6-b65d-49b1-b930-7ae8e1d3d831}} , i.e. {{formula:edd55366-63d1-45b0-ab37-b1810bcb2bcd}} , the FIFO News Feed approaches the News Feed measurements.At the regular sampling bots the model again typically predicts a VoA higher than the measurements of Facebook News Feed. After reordering the posts, the VoA of the FIFO News Feed is accurately captured by the model. Therefore, the difference in the VoA as predicted by the proposed model and the measurements is attributed to News Feed reordering. This finding reveals that Facebook Facebook creates a position bias by showing repeated posts and is consistent with the conclusions of . Note, for instance, that when {{formula:36d75345-8cf1",
        " position bias has important consequences on the behaviour of users as there is a strong correlation between post positions and click rates , .When {{formula:a1eea522-fd2d-4149-a247-816a1d0d702d}} , the model underestimates the FIFO News Feed VoA. According to equation (REF ), if {{formula:311037ce-3c7b-4de6-96d3-acd2e1036d63}} only the {{formula:c07c4769-e9d6-49e7-a685-eefd1f5e7705}} most recent posts enter the timeline, while the older ones are dropped by the social media. In reality, however, those posts can appear in future accesses. This effect becomes more noticeable in the region wherein VoA increases non-linearly with respect to {{formula:d18d2e9a-a376-466b-9c24-ae7949922803}} . In the linear region, i.e. {{formula:9478eee0-03ad-43a9-9dc6-48c8a7"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does Dynamic Time Warping play in identifying similar Bioactive Lipids, and how effective is it in detecting patterns?",
      "expected_output": "Dynamic Time Warping (DTW) plays a crucial role in identifying similar Broad Absorption Lines (BALs), not Bioactive Lipids, by finding an optimal alignment between two objects before calculating the distance. According to the research paper, DTW is effective in detecting patterns as it can cope with bad redshift determination and is not sensitive to local offsets of different spectral features. However, it is sensitive to the order of these features.",
      "context": [
        "0095-4a4b-8a89-01331621a0a2}} .Correlation coefficient {{formula:00be3459-7a54-48dd-b9d9-66ea0cf9debe}}Canberra Distance {{formula:6a0747ed-4031-4968-9088-d625f89dd2df}}Another distance measure we try is Dynamic Time Warp (DTW), a popular measure for time series , . This algorithm finds an optimal, under certain constraints, alignment between the two objects before calculating the (usually euclidian) distance. The possibility to align the spectra before calculating the distance is expected to help cope with bad redshift determination. In addition, BALs can have different offsets and widths, and thus appear at different wavelengths. In this work we want to detect objects that have BALs as similar, even if the BALs have different properties for the two objects. Note that with DTW, a redshifted BAL is not expected to be considered similar to a blueshifted BAL. This is due to the different relative position with respect to the emission line. DTW is not sensitive to local offsets of different spectral features (i.",
        " different relative position with respect to the emission line. DTW is not sensitive to local offsets of different spectral features (i.e. emission and absorption lines), but it is sensitive to their order.Distance Correlation is a measure of dependance that returns zero if and only if the two input vectors are independent. This is in contrast to the classical measure of dependance, the Pearson correlation coefficient, which is mainly sensitive to a linear relationship. The method has a maximum value of unity. In the experiments we performed this measure produces similar but slightly better results than the correlation coefficient; therefore, below we include results using the Distance Correlation measure but not the correlation coefficient.Cross Correlation is the sliding inner product of the two objects. It measures similarity as a function of displacement between them. We note that for applications of cross correlation on spectra, the correct thing to do is logarithmically shift the objects (i.e, shift the redshift), instead of linearly displace them. In order to use the fast Fourier transform method, we use regular (linear) cross-correlation. This is a valid approach for small shifts only. We take the maximum value of the cross correlation as the similarity measure. It is also possible to take only the",
        "-defined wavelength grid. This allows us to use objects of only a limited redshift range. In addition, this alignment requirement compels us to rely on the SDSS pipeline redshift in our pre-processing stage. This will lead to meaningless similarity values for objects with wrong pipeline redshift. The objects with wrong redshift are the ones that are not well described by the pipeline, and as such are of high interest for investigations such as the one performed in this work. For this reason calculating similarities that are invariant to shifts in features is an important goal for future work.\nAcknowledgements: IR would like to thank Natalie Lubelchick, Dalya Baron, and Sahar Shahaf for their advice and help. PBH acknowledges support from the Natural Sciences and Engineering Research Council of Canada (NSERC), funding reference number 2017-05983This research made use of: the NASA Astrophysics Data System Bibliographic Services, scikit-learn , SciPy , IPython , matplotlib , astropy , numba , and the SIMBAD database .This work made extensive use of SDSS data. Funding for the Sloan Digital Sky Survey IV has been provided by the Alfred P. Sloan Foundation, the U.S. Department"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the key equation relationships presented in Lemma 5 and how do they relate to the main findings of the research paper?",
      "expected_output": "According to Lemma 5, the key equation relationships are given by the formulas {{formula:99df3112-d73e-4c75-a4b0-959665c6ad97}} and {{formula:31ced5c7-30f3-4a92-8707-2f79973ab94b}}, where {{formula:de21acfa-3287-4f09-9095-36bbd2c1e809}} and {{formula:a9d197a4-70da-4c0f-a356-65761e61288b}} with {{formula:6aaa340f-3a3b-4352-8e1a-d952af7b6740327a9}}, corresponding to {{formula:174d21ee-56a8-4a04-b4bf-b34d3158549f}}. These relationships are crucial to the main findings of the research paper, as they provide a basis for calculating {{formula:d0e12881-febf-46b2-83cd-68465bbbac83}} and {{formula:fcd7c25c-308d-496e-9616-d39cdb969c75}}, and ultimately lead to the conclusion that there is a constant {{formula:761d7a59-f18f-4ab1-ab61-788d5eb24a3c}} such that {{formula:03e03152-43b9-4964-b1a7-29ca45001744}}.",
      "context": [
        " non-singular and we can substitute (REF ) into the solution formulas (REF )-(REF ) to calculate {{formula:d0e12881-febf-46b2-83cd-68465bbbac83}} and {{formula:fcd7c25c-308d-496e-9616-d39cdb969c75}} . Inserting these values in (REF ) and applying the triangle inequality proves the following lemma.Lemma 5 Let {{formula:99df3112-d73e-4c75-a4b0-959665c6ad97}} , {{formula:31ced5c7-30f3-4a92-8707-2f79973ab94b}} , where {{formula:de21acfa-3287-4f09-9095-36bbd2c1e809}} and {{formula:a9d197a4-70da-4c0f-a356-65761e61288b}} with {{formula:6aaa340f-3a3b-4352-8e1a-d952af7b674",
        "0327a9}} , corresponding to {{formula:174d21ee-56a8-4a04-b4bf-b34d3158549f}}In appendix we shall prove that there is a constant {{formula:761d7a59-f18f-4ab1-ab61-788d5eb24a3c}} such that {{formula:03e03152-43b9-4964-b1a7-29ca45001744}}Thus, for {{formula:b05853b9-2a24-4b34-9237-c89fad46af9d}} , there are two solutions that have bounded {{formula:92b57d93-e88b-445d-8875-9890b27174c8}} -norm: {{formula:b71e4dbb-e8b2-4220-8500-abe066dfca76}}with {{formula:62ae5e51-04f3-47b8-b211-d7f52f95cf21}}It is convenient to calculate the eigenvectors by inserting ({{",
        " hand side of (REF ) is a monotonically increasing function of {{formula:4db5eec8-1340-41cd-afea-d41b77a6cd68}} . Therefore {{formula:509c85db-e64f-46cd-a390-2caca9ec9314}} for {{formula:1512d34c-d790-4fcf-a6b0-b0c3001fa665}} . We conclude that the unique solution of (REF ) satisfies {{formula:4cba7ec8-63d8-46c3-aa1b-64242d251090}}Because {{formula:038f7eeb-bacb-477d-bbdc-5e2229052472}} must be non-negative, we have {{formula:9df55807-37f1-4c84-81d7-50c17b615f31}} . Relations (REF ) and (REF ) give {{formula:334d4049-a728-484a-83a6-799db71f8063}}Corollary 1 Let {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the dependence of electron mediated spin-spin interaction on spin-polarization, and how do the methods used in this research affect the key results?",
      "expected_output": "The main findings of this research indicate that the electron mediated spin-spin interaction depends on the spin-polarization of the electron medium and the excitation spectrum of the localized spins. For spatially anisotropic spin-polarized surface electrons, the interaction comprises the Ising, Heisenberg, and Dzyaloshinski-Moriya interactions. The methods used, including the consideration of non-equilibrium conditions and the proper time-dependence of local spins, affect the key results by demonstrating that the range function depends dynamically on time, spin excitation spectrum, and spin-bias between spin channels in the electron medium.",
      "context": [
        "ization of the substrate and the excitation spectrum of the local spins. For spatially anisotropic spin-polarization of the substrate, the spatial dependence of the interaction comprise components decaying as {{formula:cc54f398-173e-47fc-9969-4f461d9417e5}} and {{formula:e435a8bf-19e3-43c0-9f84-6fd56cf8b3c4}} . 75.30.Et, 71.70.Gm, 75.30.HxThe excitation spectra of spin systems strongly depends of the type of interactions that are involved. The magnetic moment of e.g. single Co , , , , , , , , Fe , , Cr , and Mn atoms become strongly anisotropic due to symmetry reduction in the interaction with electron medium. Studies of inelastic scattering processes of layered materials and single atoms , , , , , , , , , have given deepened insight to the excitation spectra of various elements, which then provide further detail to the understanding of the involved interactions. phys For magnetic systems, the interactions between the local spins can be of different character, which is often modeled using e.g",
        "bf0df62}} .In conclusion, we have studied the electron mediated spin-spin exchange interaction under non-equilibrium conditions for localized spins embedded in a two-dimensional system. It was demonstrated that the range function depends dynamically on time, the spin excitation spectrum, and the spin-bias between the spin channels in the electron medium. This leads to that the electron mediated exchange interaction between the localized spins is determined by the spin-polarization of the electron medium as well as of the excitation spectra of the local spins. In the case of spatially anisotropic spin-polarized surface electrons, the electron mediated spin-spin exchange interaction comprise the Ising, Heisenberg, and Dzyaloshinski-Moriya type of interactions, capturing the static case previously reported . It was, moreover, shown that earlier results for the range function, which were derived for systems in the static regime , , , , can be straightforwardly extended to slowly fluctuating spins. Particularly, the Dzyaloshinski-Moriya interaction was shown to decay as {{formula:f0a4afb0-f967-4ac1-8e0f-4451ca94b966}} for weak",
        " the spin-spin exchange interaction is mediated by the electronic environment.In this paper, we address the electron mediated exchange interaction between localized spin under non-equilibrium conditions in two-dimensional systems. The question is pertinent to recent measurements using e.g. scanning tunneling microscopy (STM) where local non-equilibrium conditions are created by the tunneling current. It is demonstrated that the resulting spin-spin exchange interaction depends on the spin-polarization of the electron medium and on the excitation spectrum of the localized spins. For spatially anisotropic spin-polarized surface electrons the spin-spin interaction comprise the Ising, Heisenberg, and Dzyaloshinski-Moriya interactions, where the latter is shown to asymptotically decay as {{formula:f4fb20e8-6ad9-4210-a84b-6cb058c7228f}} . In contrast to previous studies of the non-equilibrium RKKY interaction , , we here also include the proper time-dependence of the local spins.The presence of the local spin (or magnetic) moments, results in a spatially inhomogenous surface electron spin-polarization which can be transformed"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how frustration and ferromagnetic correlation affect quantum magnetism phases in this research paper?",
      "expected_output": "The research paper suggests that the interplay of quantum fluctuation, frustration, and ferromagnetic correlation leads to successive transitions between trivial and SPT spin-gap phases in one-dimensional frustrated quantum ferromagnets. The study of the frustrated ladder with ferromagnetic legs and unequal antiferromagnetic diagonal bonds shows that these transitions are a generic feature of such systems, with the strength of frustration controlling the alternation between different phases. For example, the model reduces to a specific case when one of the diagonal bonds vanishes, and similar successive transitions have been suggested in frustrated mixed spin chains with side chains.",
      "context": [
        " the frustrated ladder with ferromagnetic legs and unequal antiferromagnetic diagonal bonds. , This model reduces to the {{formula:9984e0f9-cf1a-4439-991e-71d139f803a8}} , if one of the diagonal bonds vanishes. Studies of these models that have no exact solutions even on the ferromagnetic-nonmagnetic phase boundary are highly desirable in the future.Further theoretical and experimental searches for these exotic phases would be a promising subject in the field of frustrated quantum magnetism. Thus far, these transitions have not yet been observed experimentally. For experimental detection, it would be important to clarify how the difference in ground-state phase is reflected in the finite temperature behaviors of experimentally observable quantities such as magnetic susceptibility and specific heat. These are left for future studies.The author thanks S. C. Furuya for showing him the results of Ref. furuya prior to publication and for fruitful discussion. Part of the numerical computation in this work has been carried out using the facilities of the Supercomputer Center, Institute for Solid State Physics, University of Tokyo, and Yukawa Institute Computer Facility in Kyoto University. This work is supported by JSPS KAKENHI Grant Number JP",
        "ula:1cd693b9-396d-4e04-8f7c-89b2a4693be7}} . Although this picture is consistent with the ES and the magnitudes of the edge spins, further investigation is desirable to confirm this picture more directly.The investigations in the present and preceding works , are limited to the models with exact solutions on the ferromagnetic-nonmagnetic phase boundary. In these works, the numerical results suggesting the presence of successive transitions are reinforced by the exact solutions. Hence, to conclude that the presence of the successive transitions is one of the generic features of one-dimensional frustrated quantum ferromagnets, it is essentially important to show that it persists even in the absence of exact solutions. Actually, similar successive transitions have been suggested in the frustrated mixed spin chains with side chains, although the results were not conclusive owing to the limited system size. This model contains the {{formula:ec9644a7-e4f3-4367-8fd5-2429f53c43d4}} as a limiting case. Another possible extension of the present model is the frustrated ladder with ferromagnetic legs and unequal antiferromagnetic diagonal bonds. , This model reduces to the {{form",
        " of trivial and SPT spin-gap phases alternate with the strength of frustration. , , We speculate that the presence of these successive transitions is one of the generic features of the one-dimensional frustrated quantum ferromagnets resulting from the interplay of quantum fluctuation, frustration, and ferromagnetic correlation. To confirm this speculation, however, it is desirable to investigate the topological properties of similar series of phase transitions in other models. Actually, in the {{formula:9082b273-45d1-42b9-a505-06931d46c63d}} -chain with a ferromagnetic main chain and alternating ferromagnetic and antiferromagnetic interactions between the main chain and apical spins ({{formula:3edb464f-7078-427f-9fe4-4a80deec842a}} ), a similar series of spin-gap phases have been found. However, the topological nature of the latter phases in the {{formula:c9b38bb7-76e9-47fc-bbc8-4f2612bf8bc7}} has not yet been investigated. In the present work, we regard the {{formula:e514"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the role of the dilatation operator in the context of Brauer basis, and how does it affect the main findings of this research paper?",
      "expected_output": "The dilatation operator plays a crucial role in the context of Brauer basis, as it is used to study the operator mixing problem for large operators. The one-loop dilatation operator is given by a specific formula and is used to evaluate the action of the dilatation operator on the Brauer basis. The main findings of this research paper include the construction of a mixing matrix, which is expressed in terms of representation date and is very constrained, and the study of the action of more general differential operators. The dilatation operator affects the main findings by providing a way to diagonalize the mixing matrix, which is a key step in understanding the operator mixing problem. For example, the paper shows that the action of the dilatation operator reduces to systems of harmonic oscillators when the corners of the Young diagram are well-separated, and that finding good conserved charges is a nice pathway towards diagonalising the mixing matrix.",
      "context": [
        " mixing condition for {{formula:6dfdc257-b1b3-43d1-8e36-650dbc561a9d}} . The mixing matrix we will obtain is expressed in terms of representation date and is very constrained. In other words, the mixing is very restricted on the representations. It would be a universal property for the diagonal bases labelled by a set of Young diagrams.The paper is organised as follows. In section , we will study the one-loop dilatation operator on the Brauer basis. We follow the idea in used to evaluate the dilatation operator on the restricted Schur basis. In section , we will write down the mixing condition in terms of the Brauer representations. In section , , we provide with the same study for two-loop. In section , the action of more general differential operators are studied. Section is given for summary and discussions. Appendix is for basic things of Brauer bases.\nOne-loop dilatation operator on Brauer basis: The operator is {{formula:9033d0af-fd38-4051-97d0-534b48803e42}}See appendix for a detailed explanation.The one-loop dilatation operator is given by",
        "ba1c578}} . Such diagonal bases can be a good starting point to study the operator mixing problem for large operators. There have been some studies along this line to find that the operator mixing is neatly handled in terms of Young diagrams , , , , , , , , , , , , , , . See , , , for former approaches to non-planar corrections.In this paper, we study the action of the dilatation operator on the Brauer basis introduced in . The basis has some labels, and one of the labels is an irreducible representation of the walled Brauer algebra {{formula:9e1c1962-fa65-4162-96e3-59fb3a00effa}} , , , which is given by a bi-partition, {{formula:3f4aa34e-5d59-4e25-8156-cc1d9d8bdad3}}Here{{formula:6c418642-9dd6-4f61-8dc3-f09cdef352a4}} is a shorthand notation to express that the Young diagram {{formula:7baea720-b13a-49bc-8f72",
        " of the integer {{formula:9120683d-9c2d-4c8c-8771-1946d886b265}} , it would also be nice to study the dilatation operator on the Brauer bases beyond su(2) sector , .We have explicitly confirmed that the construction of some 1/4 BPS operators in is valid at two-loop. We also saw that they do not appear in the image of the dilatation operator.The next direction we should proceed to is to diagonalise the mixing matrix. Recently there have been an interesting progress in diagonalising the mixing matrix on the restricted Schur operators , , , , . They have shown that the action of the dilatation operator reduces to systems of harmonic oscillators when the corners of the Young diagram are well-separated (displaced corner approximation). The idea underlying the approximation is to exploit a good set of conserved charges preserved by the dilatation operator. In the series of works, the charges encode open string configurations on giant gravitons , . (Another approach can be seen in .) Finding good conserved charges is a nice pathway towards diagonalising the mixing matrix. Because such conserved charges correspond to parameters of the dual physics, it would"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the Balmer decrement and dust attenuation in the context of this research paper, and how do the authors use this relationship to draw conclusions about their findings?",
      "expected_output": "The Balmer decrement is a unitless obscuration sensitive parameter, and its departure from the Case B recombination value of 2.86 indicates dust attenuation along the line of sight. The authors use this relationship to draw conclusions about dust attenuation in their sample of galaxies, finding that the mean Balmer decrements translate to a dust obscuration factor. They apply this correction to calculate the Hα luminosity and subsequently determine the star formation rates (SFRs) of the galaxies.",
      "context": [
        "cecf480822db}} is the luminosity distance in centimetres. The Balmer decrement is a unitless obscuration sensitive parameter and its departure from the Case B recombination value of 2.86 indicates dust attenuation along the line of sight. While dust geometries are complex, this approach implicitly models the dust as a foreground screen averaged over the galaxy . The exponent in the dust obscuration correction factor is equal to {{formula:38c96c2a-eeb3-4017-9356-b70c0512b3f4}} ], and {{formula:3c08cf2f-fde4-442c-9f06-fbcd64c1cf5c}} at a given {{formula:a8b60e3f-e600-4302-9820-d1efc3b12955}} is determined from the Galactic dust extinction curve (derived from observations of the UV extinction of stars). This is found to well describe the obscuration of the ionised gas in star-forming galaxies , . The SFRs are then calculated using the relationship given by assuming a initial mass function (IMF), i.e. SFR {{formula",
        " cataloguehttp://www.mpa-garching.mpg.de/SDSS/DR7/ to obtain absorption-corrected line fluxes and equivalent widths (EW) for these bright galaxies. These line measurements are made from stellar absorption-corrected SDSS spectra. The GAMA SFRs are calculated as described in and and are given in Table REF . In summary, the H{{formula:fb7f6d95-f9d9-4631-8ad2-eac68f589b20}} luminosity is calculated from the H{{formula:7cf60276-40d4-4b55-88fc-21f4bc39d8e8}} EW, which is aperture corrected and extinction corrected using the Balmer decrement as per the IFU measurements. The GAMA SFRs are then calculated using the relationship. We note that dust obscuration is not excessive in any of the galaxies in this sample: their mean Balmer decrements are {{formula:1307c9e5-0b8b-4c39-880a-de6877005545}} (Table REF ) which translates to a dust obscuration",
        "c39-880a-de6877005545}} (Table REF ) which translates to a dust obscuration factor of {{formula:d53ff88a-9515-446d-8d54-c2b957b9e243}} . do not correct H{{formula:13d2b938-6cfb-416b-a1a6-d1ff14fb4ffe}} equivalent widths for stellar absorption for the definition of their star-forming sample, however, they do correct H{{formula:7cdfbe33-717c-4b07-9966-5d4f0878b80e}} equivalent widths by adding a constant correction of {{formula:0cad89ce-5a0c-410f-a9bb-bbb83ba88639}} . We therefore use that addition here, i.e. stellar absorption corrected H{{formula:c1e9f925-981a-4bc1-a267-74fc49e33d94}} EW{{formula:6cf4b551-3133-4b18-830d-9ca5d0"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the role of polynomial splines in the convolution formula and how do they contribute to the overall method?",
      "expected_output": "The polynomial splines play a crucial role in the convolution formula as they are used to define the coefficients. According to the paper, the coefficients are given by the convolution formula, where the direct B-spline filter satisfies a specific condition. The polynomial B-spline basis is defined, and it is used to solve the problem. For example, in the case of the cubic spline, the impulse response is shifted and then decomposed into a cascade of causal and anti-causal filters. This decomposition is a practical way to implement the filter, and it contributes to the overall method by providing a stable and efficient solution.",
      "context": [
        "890-0808d3bfcc3e}}where `{{formula:70fa6232-7e88-4954-8a18-2aa036d4e48f}} ' denotes convolution. Figure REF shows the polynomial splines {{formula:5ac450ca-c795-44df-83cb-57fc888ad6f1}} of order {{formula:824bd915-2833-4f6d-9da5-3109b5389076}} . {{figure:1cc04692-d7a9-4dcb-bcfe-ade6de4de558}}In this formulation, the coefficients are given by the following convolution formula : {{formula:d6ac56ea-fd79-462f-bdb1-1d0d072d6485}}where {{formula:02149daa-1c72-4b12-8061-613a610fbf9e}} is the direct B-spline filter satisfying {{formula:2df7e8d8-23a0-43d2-b175-896371128967}} ,",
        "dc1286c-0b5c-4abc-995a-9121d1e37e78}} .On the other hand, assume that the space is {{formula:196959d2-020b-44c5-8316-23a5cef2798d}}where {{formula:a5b9eb8a-61ed-41e0-a495-86f022566343}} is the set of continuous functions with continuous derivatives up to order {{formula:316544cd-e522-4457-8a63-9831b28f3d0b}} . Then the solution is given by , {{formula:6004bb97-df7b-4e51-a73c-814dc34c3891}}where {{formula:da6dca01-f910-4361-b215-3306b900caf5}} is the polynomial B-spline basis defined by , , {{formula:dc60a42d-3361-4048-a890-0808d3bfcc3e}}where `{{formula:70fa6232-7e",
        "6ab22b4f1ad0}} lies out of the open unit disc {{formula:ddc3b329-09ff-4379-9f7f-70cfd643a673}} , and hence the filter {{formula:a0dfef0e-f782-49a3-9345-59796bd179d0}} becomes unstable. The same can be said of the other {{formula:ac1c6e2b-455d-4a56-992f-0f6b7748e371}} th-order splines . A practical way to implement this filter is to decompose {{formula:fc069f6b-cc84-4a8b-b704-d723f2f74a8a}} into a cascade of causal and anti-causal filters . In the case of the cubic spline, we first shift the impulse response of (REF ) as {{formula:2799ac35-00d5-4d8e-83f6-7d6c4b5d4cd1}}and then decompose {{formula:9d9d6cdb-7a18-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does SUPERBLINK use to identify high proper motion stars and what are its key findings in this area of research?",
      "expected_output": "SUPERBLINK uses an image-differencing algorithm to identify high proper motion stars, which works by identifying moving objects from their patterns of residuals after image subtraction. This method is successful in crowded fields of low Galactic latitudes. The survey also incorporates positions and proper motions from Gaia DR1 where available. Key findings include the identification of nearby K and M dwarfs, which dominate samples of stars with large proper motions, and the estimation of effective temperatures based on broadband, optical-to-infrared colors. For example, the SUPERBLINK catalog contains Tycho-2 V-band magnitudes and 2MASS magnitudes for most objects, allowing for the determination of cool dwarfs for TESS observations.",
      "context": [
        " V-band magnitudes for all the stars. The SUPERBLINK survey is based out of a project to identify all high proper motion stars from archival plates from the Digitized Sky Surveys (DSS). SUPERBLINK uses, for example, images from the National Geographic Palomar Observatory Sky Survey and the Second Palomar Observatory Sky Survey . Both surveys were originally performed using photograph plates, and both have since been digitized , , , enabling computation methods for identifying high proper motion objects.Instead of directly measuring stellar positions, SUPERBLINK use an image-differencing algorithm, which identifies moving objects from their patterns of residuals after image subtraction. As a result, the method works successfully in crowded fields of low Galactic latitudes . In addition, the SUPERBLINK survey includes stringent quality control tests, and all but the most obvious detections were individually examined by eye, on the computer screen, using a blink-comparator widget. Additional processing includes cross-correlation with several photometric catalogs, including GALEX, SDSS, USNO-B1.0, GAIA DR1, 2MASS, and WISE. The proper identification of the counterparts of these high proper motion stars at all the various epochs",
        "MASS, and WISE. The proper identification of the counterparts of these high proper motion stars at all the various epochs of those catalogs is made easy by the prior knowledge of their proper motion vectors.In addition to image-differenced proper motions, SUPERBLINK also incorporates positions and proper motions from Gaia DR1 where available , . The Gaia proper motions are labelled _new in the SUPERBLINK catalog and the CDC.As a base catalog for determining cool dwarfs for TESS observations, we therefore adopted the SUPERBLINK catalog as a primary source of targets. Although SUPERBLINK already lists 2MASS magnitudes for most objects, we independently cross-matched the catalog again with 2MASS in order to obtain the 2MASS photometric flags.SUPERBLINK contains Tycho-2 V-band magnitudes where available . More recent visible magnitudes are available from the American Association Variable Star Observers Photometric All-Sky Survey catalog. APASS contains Landolt B and V magnitudes, and SDSS-like {{formula:68b910c2-305b-4258-b011-d7a8633484ab}} , {{formula:675f9df8-fabc-4",
        ". On the other hand, stars with large proper motions are also relatively nearby ({{formula:15c95f9c-2922-41e1-8e41-95ed3706d6da}} 1 kpc) and thus unlikely to suffer significant amounts of reddening. As a result, samples of stars with large proper motions are generally dominated by nearby K and M dwarfs, and their effective temperatures can be estimated with some certainty based on broadband, optical-to-infrared colors.The SUPERBLINK proper motion catalog (Lepine, in prep), is an all-sky catalog of stars with large proper motions ({{formula:8f6e5efc-9efc-40c7-9d78-1bbff382b76f}} mas yr{{formula:d3c5ec35-f70d-45f2-af08-58fad8ee70ea}} ) which includes optical and infrared magnitudes for all its entries. These include optical G-band magnitudes from the first GAIA release, whenever available, and estimated optical V-band magnitudes for all the stars. The SUPERBLINK survey is based out of a project to identify all high"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of parity conditions on symplectic form finiteness in this research paper?",
      "expected_output": "The research paper finds that imposing parity conditions on the asymptotic part of fields and their conjugated momenta makes the symplectic form finite. Specifically, requiring the fields to have one definite parity and their momenta to have the opposite parity eliminates the logarithmically-divergent contributions to the symplectic form. The paper also shows that relaxing these parity conditions to allow for improper gauge transformations still yields a finite symplectic form, as long as certain conditions are met. For example, the asymptotic part of fields that transform non-trivially under gauge transformations must be such that it satisfies a specific equation, and other fields must satisfy the same parity conditions as before.",
      "context": [
        "402c09b-9a63-4ca7-9f95-6b4ebc251ab2}} . One possible solution to this issue is quite simple. One merely requires that the asymptotic part of the fields have one definite parity (either even or odd) as functions on the two-sphere at infinity and, then, imposes the opposite parity on their conjugated momenta. This way, the potentially logarithmically-divergent contributions to the symplectic form are actually zero. We will see that the presence of the massless scalar field will cause the parity conditions to be slightly more involved.To fully determine the exact form of the parity conditions, let us remind that they should be such that, not only do they make the symplectic form finite, but also the Poincaré transformations symplectic. Specifically, this happens when {{formula:abdbc684-db24-44fd-a78a-1ad961a67733}} , being {{formula:3b351d2c-b8be-4960-b8f7-edc1297ae6df}} the Lie derivative in phase space with respect to the vector field {{form",
        "strict) parity conditions, we simply need relax them so that they are satisfied up to an improper gauge transformations. Therefore, we require the asymptotic part of the fields that transform non-trivially under gauge transformations to be such that {{formula:dc3d7f2b-f01f-44e7-8161-f9aa3344db5d}}where {{formula:5812ae69-2873-4aa4-ae91-a8cd557afc50}} is an even function on the sphere. At the same time, the other fields are required to satisfy the same parity conditions as before, that is {{formula:6b1276c2-58fa-411b-8eb8-93349a14c1ca}}while {{formula:11543a68-5460-4706-a442-a0a9a4c2c19f}} and {{formula:554e5e0f-8ba2-41fa-939a-98aaa3930bcf}} are of definite, and opposite, parity.These relaxed parity conditions allow for certain the possibility of performing improper gauge transformations, thus extending",
        "-a549-6a3f9c36bc76}} by an even function. Finally, in order to make the symplectic form finite, we decompose also the momentum {{formula:87f55c49-2f44-4338-a6e4-66029f78c38d}} as {{formula:3fd35db6-f370-420f-838c-578edffd5026}}which needs to satisfy the fall-off conditions {{formula:40777f49-2f7c-4091-8309-af9f1a82c211}}In terms of the absolute values and the phases, the logarithmically-divergent contribution to the symplectic form is {{formula:3eb60e10-c7c7-4400-9fd6-de88d726da32}}which vanishes identically once we require that {{formula:911abfb6-23e2-4842-95ca-bfc2bac8316a}} has the opposite parity of {{formula:1478ee22-9e07-414c-b523-f110b2039"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do Garsia's techniques apply to martingale decomposition and what are the key findings of this application?",
      "expected_output": "Garsia's techniques apply to martingale decomposition through the martingale transform, which is used in conjunction with the Davis decomposition. This decomposition separates a martingale into a previsible part and another part. The key finding of this application is the Davis and Garsia Inequality, which provides an estimate for the martingale. Specifically, it gives the inequality that relates the martingale to its previsible and other components, leading to important conclusions about martingale behavior and estimates.",
      "context": [
        ":64e42ea8-191e-4012-ab0b-273e7c57a96c}}Use Theorem REF with {{formula:dc4eff35-31c8-4c53-b12b-c1afa2170fda}}\nDavis and Garsia Inequality: Let {{formula:1a4555c7-7ca5-4702-9c4f-01b10814d35c}} be a filtered probability space.The martingale transform techniques of Garsia , applied to the original Davis decomposition of a martingale {{formula:76312d23-5a3a-4ed0-979f-959ab1ecae5e}} into a previsible part {{formula:3c8978bf-b1ed-4d9c-9156-0a5987c51083}} and {{formula:3df3ea8a-da4a-4f5d-93e9-11771048c6a9}} gives the inequality {{formula:2d6727e5-d56f-4922-8aa7-",
        " gives the inequality {{formula:2d6727e5-d56f-4922-8aa7-7bec4fbf5e2b}}Thus the inequality (REF ) is a consequence of separate theorems due to Davis and Garsia respectively.We proceed by giving a new proof of (REF ) based on Theorem REF and a martingale thin-thick decomposition.Theorem 4.4 Every martingale {{formula:9ca7e0aa-8261-4556-97a4-81f31279b3c3}} in {{formula:1d7edb13-5489-4a7d-9fdd-b9e5162ab61c}} admits a decomposition as {{formula:cd577470-0906-4875-b4b3-fa7b966b890a}}where {{formula:15d38309-07db-4354-8951-fe7311a8df4f}} and {{formula:da8f61c1-c5a8-4bbe-b784-2a19dfabdaba}} are marting",
        "dc1-acfa-e9d62d603e0c}}and {{formula:69c03012-e343-46ec-92b9-874e79f6160c}}The estimate (REF ) implies of course the right hand side of the square function estimate (REF ) since the triangle inequality and the Burkholder-Gundy martingale inequality give {{formula:534fc1fc-acec-4d05-9c68-a8a126b64c6e}}The uniform previsible estimate (REF ) should be compared with uniform previsible estimates appearing in the classical Davis and Garsia inequality . (See also Section REF .) For general martingales the Davis decomposition guarantees only uniform estimates by previsible and increasing functionals such as {{formula:b4711966-2ae0-4acc-afd6-7bb1e79a049b}} Hence a routine application of the Davis decomposition could yield only {{formula:2959e61b-567f-4813-8eb2-4f36fa393199}}The present paper exploits a basic and general iteration principle extracted from the work of J. Bour"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between DreamMatcher's structure preservation and key-value replacement methods, and how do these differences impact the research findings?",
      "expected_output": "The main differences between DreamMatcher's structure preservation and key-value replacement methods are that DreamMatcher preserves the fixed target structure and accurately aligns the reference appearance by explicitly leveraging semantic matching, whereas key-value replacement disrupts the target structure and relies on sub-optimal matching. This difference impacts the research findings, as DreamMatcher significantly surpasses MasaCtrl for both subject and prompt fidelity, demonstrating the effectiveness of the proposed matching-aware value injection method. For example, in the user study, DreamMatcher more accurately aligns the reference appearance into the target context, even with large structural displacements, as shown in Figure REF.",
      "context": [
        " studies , , , , we adopted the CLIP and DINO image similarity metrics, denoted as I_\\mathrm {CLIP} and I_\\mathrm {DINO}, respectively. Note that I_\\mathrm {DINO} is our preferred metric for evaluating subject expressivity. As mentioned in , , DINO is trained in a self-supervised manner to distinguish objects within the same category, so that it is more suitable for evaluating different methods that aim to mimic the visual attributes of the same subject. For prompt fidelity, following , , , , we adopted the image-text similarity metric T_\\text{CLIP}, comparing CLIP visual features of the generated images to CLIP textual features of the corresponding text prompts, excluding placeholders. Following previous works , , , , we used ViT-B/32 and ViT-S/16 for CLIP and DINO, respectively.\nUser study: An example question of the user study is provided in Figure REF . We conducted a paired human preference study about subject and prompt fidelity, comparing DreamMatcher to previous works , , . The results are summarized in Figure in the main paper. For subject fidelity, participants were presented with a reference image and generated images from different",
        "} and I_\\mathrm {CLIP} do not simultaneously consider both the preservation of the target structure and the reflection of the reference appearance. They only calculate the similarities between the overall pixels of the reference and generated images. As a result, the key-value replacement, which generates subject-centric images and injects irrelevant elements from reference images into the target context, achieves better image similarities than DreamMatcher, as seen in Table in the main paper. However, as shown in Figure REF , DreamMatcher more accurately aligns the reference appearance into the target context, even with large structural displacements. More qualitative comparisons are provided in Figures REF and REF .This is further demonstrated in a user study comparing MasaCtrl and DreamMatcher, summarized in Figure REF . A total of 39 users responded to 32 comparative questions, resulting in 1248 responses. These responses were divided between two different questionnaires, with 20 users responding to one and 19 to the other. Samples were chosen randomly from a large, unbiased pool. An example of this user study is shown in Figure REF . DreamMatcher significantly surpasses MasaCtrl for both fidelity by a large margin, demonstrating the effectiveness of our proposed matching-aware value injection method. {{table",
        " DreamMatcher: MasaCtrl introduced a key-value replacement technique for local editing tasks. Several subsequent works , , , , , have adopted and further developed this framework. As shown in Figure REF , which provides a qualitative comparison of DreamMatcher with MasaCtrl, the key-value replacement is prone to producing subject-centric images, often having poses similar to those of the subject in the reference image. This tendency arises because key-value replacement disrupts the target structure from the pre-trained self-attention module and relies on sub-optimal matching between target keys and reference queries. Furthermore, this technique does not consider the uncertainty of predicted matches, which leads to the injection of irrelevant elements from the reference image into the changed background or into newly emergent objects that are produced by the target prompts.In contrast, DreamMatcher preserves the fixed target structure and accurately aligns the reference appearance by explicitly leveraging semantic matching. Our method also takes into account the uncertainty of predicted matches, thereby filtering out erroneous matches and maintaining newly introduced image elements by the target prompts. Note that the image similarity metrics I_\\mathrm {DINO} and I_\\mathrm {CLIP} do not simultaneously consider both the preservation of the target structure and the reflection"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do VESPA SFHs aid SN type analysis, and what are the main findings of this research in terms of supernova classification?",
      "expected_output": "VESPA SFHs aid SN type analysis by providing the star formation history of galaxies, which helps in understanding the environment where supernovae occur. The main findings of this research include the classification of 201 SNe into types Ia, II, and Ibc, with 82 SNe Ia, 93 SNe II, and 26 SNe Ibc. The study also finds that the time-integrated CC SN to SN Ia ratio is roughly 3:1, consistent with cluster measurements and LOSS data.",
      "context": [
        "S-VESPA sample: With the kind assistance of R. Tojeiro, we have derived VESPA SFHs for all of the LOSS galaxies that have SDSS spectroscopy. Our main sample of LOSS galaxies with VESPA SFHs consists of 3505 galaxies that hosted 201 SNe, among them 82 SNe Ia, 93 SNe II, and 26 SNe Ibc (see Filippenko 1997 for a review of SN types; here we classify SNe Ib and Ic as “Ibc”). An alternative sample, using a different VESPA dust model (see below), has slightly different numbers: 3508 galaxies hosting 202 SNe, among them 81 SNe Ia, 94 SNe II, and 27 SNe Ibc. These numbers of SNe are consistent with those expected based on the fraction of the total LOSS visibility time that is in the VESPA galaxies. Other than the increase in Poisson errors due to the smaller number of SNe, the limitation of the calculation to VESPA galaxies should have no effect on our DTD reconstruction.For the majority of these galaxies, it is practical to separate the SFHs into no more than four time bins:",
        " be sufficiently high for a reliable SFH reconstruction to be obtained. In particular, the presence of old stellar populations that are superimposed on younger and more luminous stars must be detectable. Naturally, spatially resolved, medium-spectral-resolution data of such a large sample of nearby galaxies would find many additional applications, and hence such data are worth the large effort required.Shortly before submission of this work, Brandt et al. (2010) presented a DTD reconstruction analysis of a different SN sample. Their methodology shares several elements with ours. Brandt et al. (2010) study 107 SNe Ia from SDSS-II. Like us, they use VESPA to derive SFHs for a sample of SDSS galaxies, binned into three time bins, identical to those we have chosen. Like us, they treat the DTD amplitudes in the three discrete bins as free parameters, which are determined by a maximum-likelihood procedure. However, rather than comparing directly the presence or absence of SNe in each galaxy to the predictions of the DTD model (as we have), they use the DTD to create mock SN-host samples, and compare the mean spectrum of the mock host samples to the mean spectrum of the real",
        " such that a time-integrated CC SN to SN Ia ratio of roughly 3:1, despite some tension, is consistent both with cluster measurements and with the LOSS data we have analysed here.\nTests on Simulated Samples: To test our DTD recovery procedure and examine its performance on different kinds of input datasets, we have repeated the Monte-Carlo mock survey generation described in §2, above, using the LOSS-SDSS galaxy sample and its VESPA-based SFHs (with one dust parameter). However, rather than inputing the best-fit DTD from the inversion of the real data, we can choose other model DTDs and convolve those DTDs with {{formula:c1864aea-37ff-48f8-a143-82fc7d456ae6}} according to Eq. REF –REF , or we can change the sample properties in additional ways, as described below. Using the actual LOSS galaxy SFHs and visibility times (as opposed to, for example, random SFHs and visibility times) makes for a more realistic simulation. As before, the number of SNe found in each galaxy in each mock survey is drawn from a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between mutual information and relative entropy in the context of this research paper, and how do the authors use these concepts to support their main findings?",
      "expected_output": "The research paper explores the relationship between mutual information and relative entropy in the context of 2d CFT and higher dimensions. The authors use the concept of mutual information to derive an expression for the relative entropy of disjoint subsystems in the small subsystem size limit. They calculate the relative entropy by finding the form of the modular Hamiltonian from the entanglement first law, which is similar to the trick used to derive the first asymmetric part of the relative entropy of a connected region. The authors' main findings are supported by their derivation of an expression for the relative entropy using the results of the mutual information calculations.",
      "context": [
        " (REF ) by a bulk calculation, we need to generalize their work to the asymptotically AdS space with the back reaction of the scalar condensate dual to {{formula:4f9831fe-c511-4854-8643-41a14964edb4}} .In the second part, we derived a formula for the relative entropy {{formula:cb7c0d41-7b1c-4f53-b135-4c5248e23161}} of disjoint subsystems in the small subsystem size limit. We did this by reading off the form of the modular Hamiltonian from the entanglement first law. It will be an interesting future work to study the relative entropy numerically.\nAcknowledgments: We thank Joan Simon and Gabor Sarosi for initial collaboration, David Williams and Hiroshi Ooguri for discussions. Part of this work was done during the YITP long term workshop \"Quantum Information in String Theory and Many-body Systems\". We would like to thank the institute for its hospitality. The work of T. U. was supported in part by the National Science Foundation under Grant No. NSF PHY-25915.\nSome details",
        "85-00f29c675298}} in the small subsystem size limit. We take {{formula:7bb97cb0-8fe5-4e74-ab8e-512db967554d}} to be the reduced density matrices of two excited states {{formula:75fe1454-04a7-4aa6-8b70-027b58935c8d}} on {{formula:4941184a-7bb4-4b4d-8f0e-9ffe7b9597e1}} . We calculate the relative entropy by finding the form of the modular Hamiltonian {{formula:7bb84c58-36dc-426b-9a47-874658d8c017}} from entanglement first law, {{formula:9728e292-0454-4a62-a84d-5b537f6d59d9}} . Similar trick was used in to derive the first asymmetric part of the relative entropy {{formula:d4a23709-03b6-4555-8ebe-863c73ccd305}} of a connected region A.\n Summary of ent",
        "31-b3e7-72e973b73453}} in the small interval limit {{formula:3a8222e4-5c0a-4691-8399-ddec6d03f26c}} . We do this by reading off the form of the modular Hamiltonian of the excited state {{formula:25654b6e-9c3f-459c-9881-6504dec64e96}} from the mutual information result of the first part.The organization of this paper is as follows: In section , and section , we discuss the mutual information of an excited state in the small subsystem size limit in 2d CFT. In section we generalize this result to higher dimensions. In section , we derive an expression of the relative entropy {{formula:2d182597-b4db-416c-bebd-f6acb8a95158}} of the disjoint subsystems in the same limit by using the results of the previous two sections.\n Mutual information in 2d CFT: In this section, we focus the on the mutual information, {{formula:01a3ea61-2edd-4788-8454-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does LO intensity manipulation enhance CVQKD security, and what are the main findings of this research in terms of improving security?",
      "expected_output": "LO intensity manipulation enhances CVQKD security by eliminating the impact of local oscillator fluctuations and optimizing the signal-to-noise ratio of the balanced homodyne detector. The main findings of this research include increasing the secret key rate over noisy channels, reducing the requirement for high detection efficiency and low electronic noise in the balanced homodyne detector, and improving resistance to channel excess noise. For example, the research shows that by tuning the intensity of the local oscillator, the secret key rate can be increased, especially in metropolitan QKD networks, as detailed in the paper.",
      "context": [
        "# arXiv Paper: 1403.2478\n\nEnhancement of the security of a practical continuous-variable quantum-key-distribution system by manipulating the intensity of the local oscillator\nabstract: In a practical continuous-variable quantum-key distribution (CVQKD), the fluctuations of the local oscillator (LO) not only make the normalization of Bob's measurement outcomes difficult, but also can change the signal-to-noise ratio (SNR) of an imperfect balanced homodyne detector (BHD), which may lead the security of a practical system of CVQKD to be compromised severely. In this paper, we propose that the LO intensity can be manipulated by the legitimate parties, i.e., being tuned and stabilized to a required constant value, to eliminate the impact of LO fluctuations and defeat Eve's potential attack on the LO. Moreover, we show that the secret key rate can be increased over a noisy channel, especially the channels of metropolitan QKD networks, by tuning the intensity of LO and thus the SNR of a practical BHD to an optimal value, and we find that, counterintuitively, the requirement on BHD (i.e., high detection efficiency and low electronic noise) can",
        " that, counterintuitively, the requirement on BHD (i.e., high detection efficiency and low electronic noise) can also be reduced in this case. To realize this manipulation, we give a schematic setup which thus can be used to enhance the security of a practical CVQKD system.\nNoisy coherent states protocol: In this Appendix, we detail the description about the entanglement-based (EB) scheme of a noisy coherent-state protocol of CVQKD for RR, which is analogous to the protocol based on squeezed states and noisy homodyne detection.The protocol with coherent states and homodyne detection is shown in Fig. REF . Generally, it is used to equivalently describe the practical prepare-and-measure (PM) implementation of CVQKD with a nonideal balanced homodyne detector (BHD) , . However, it also can be used to describe the EB scheme of a noisy detection protocol with the coherent states proposed in the main text. {{figure:1ade1c3d-0ada-466d-aa9f-0999ce8ed09b}}As shown in Ref. , the added Gaussian phase-insensitive noise can be modeled by the three physical cases",
        "b}}As shown in Ref. , the added Gaussian phase-insensitive noise can be modeled by the three physical cases: (i) nonideal homodyne detection with efficiency {{formula:3f03961f-6044-45cd-878f-c391cee9a740}} and electronic noise {{formula:888156ab-e0db-4e9a-ab14-46a9224c8621}} (see Refs. , ); (ii) perfect homodyne detection followed by adding some classical or quantum Gaussian noise of variance {{formula:77a1a130-024b-47f8-843f-ab8cd47ed26f}} ; and (iii) any combination of the previous cases giving the same {{formula:82215f87-2eb9-4d20-8f38-0959f11cc8b4}} . Over this added noise, the noisy homodyne protocol incredibly improves the resistance to channel excess noise as shown in Fig. REF of the main text.\nCalculation of the secret key rate: In this Appendix, we calculate the secret key rate of Gaussian modulation coherent states"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Context Diffusion regarding in-context learning, and how do the methods used enable this process?",
      "expected_output": "The main findings of Context Diffusion regarding in-context learning are its ability to effectively use visual context examples to control the appearance of the output image, alongside a query image and an optional text prompt. The methods used to enable this process include applying random replacement of 50% of the text prompts with empty strings, which empirically showed to be an important step, and using multiple context images to improve the fidelity of the generated images to the context. This is demonstrated through experiments on in-domain tasks, such as generating real images from HED, segmentation, and depth maps, and vice versa, as well as out-of-domain tasks like handling sketches as query images and image editing.",
      "context": [
        " pre-trained text-to-image model means the ability to generate images given a text prompt is already strong. Therefore, to enforce the model to pick up cues from an additional conditioning signal the visual context, we apply random replacement of 50% of the text prompts with empty strings, similar to , which empirically showed to be an important step.\nDatasets.: To train our model, we use a dataset that consists of 310k synthetic images and caption pairs, similar to Prompt Diffusion . Following their training setup, we extract three image maps: HED, segmentation, and depth maps from the training images. During training, for map-to-image tasks, the image maps serve as queries, and real images are used for visual context, while for image-to-map tasks, the real images serve as queries, and image maps are used for visual context. Note that the prompts and visual context are related and describe the same conditioning signal.At inference time, we use the test partition of the dataset to test the ability to learn from context. To demonstrate the generalization abilities of Context Diffusion to out-of-domain tasks, we extract other image maps, such as normal maps, canny edges, and scribble maps. Also, we",
        " Prompt Diffusion , by learning from six different tasks using generated images and their maps. At inference time, we use a query image to define the target structure and one or more context images to provide finer visual signals, alongside an optional text prompt. Our experiments study the generation ability of Context Diffusion for in-domain tasks, such as using HED, segmentation, and depth maps to generate real images and vice versa. We show the flexibility of our model to preserve structure from the query image and transfer other visual signals from the context even when the text prompt is missing. Moreover, to properly study in-context learning abilities, we experiment with unseen, out-of-domain tasks, such as handling sketches as query images, image editing, and more. This demonstrates the generalization abilities of Context Diffusion, unlike previous works. Furthermore, for such tasks, using multiple images as context helps improve the fidelity of the generated images to the context.\nContributions.: (i) We propose Context Diffusion, an in-context aware image generation framework. It enables pre-trained diffusion models to use visual context examples to control the appearance of the output image, alongside a query image that defines structure and an optional text prompt. (ii) We enable the use of multiple context",
        " advances, both in natural language , , , , , and media generation , , , , . Large language models in particular have shown impressive in-context learning capabilities , . This is the ability of a model to learn from a few samples on the fly, without any gradient-based updates, and extend it to new tasks. However, for generative models in computer vision, learning from context examples is still under-explored.Prompt Diffusion is perhaps the closest line of work that explicitly supports a single source-target image pair as a context example for image generation. It builds on the popular ControlNet model which introduced the idea of visually controllable diffusion models for image generation. Specifically, Prompt Diffusion attempts to learn the visual mapping from the source image to the target context image and applies it to a new query image, by also using a prompt for text-based guidance. However, we empirically observed that this model struggles to leverage the image pairs when the text prompt is absent. This results in low fidelity to the visual context examples, particularly when the examples are from a different domain than what is seen during training. For instance, if the source-target pairs show specific styles, they cannot be learned during inference just from the context examples. This is seen in"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on axiom schema in intuitionistic logic and how do the methods used impact the key results?",
      "expected_output": "The research paper presents key findings on axiom schema in intuitionistic logic, including the introduction and elimination rules shown in Figure REF, and the contradiction rule. The methods used, such as the definition of strong equivalence and HT-interpretations, impact the results by providing a framework for analyzing the relationships between infinitary formulas. For example, Lemma 2 states that two infinitary formulas are strongly equivalent if they are satisfied by the same HT-interpretations. The extension of the definition of strong equivalence to sequents, as shown in the formula, also contributes to the main findings. Overall, the research provides a detailed analysis of axiom schema in intuitionistic logic, highlighting the importance of strong equivalence and HT-interpretations in understanding the relationships between infinitary formulas.",
      "context": [
        "8-4441-a748-ec6c87bc4f93}} , and {{formula:07b420a4-7aeb-44bd-81eb-c362841a1c8c}} is shorthand for {{formula:5f306bad-6c5b-4dab-9ed2-6b509f3acada}} . A sequent of the form {{formula:7f1cc960-9aa8-475c-b0c8-dfc6db555d62}} will be identified with the formula {{formula:25f6ee11-2095-4818-8683-090889e0c973}} . {{figure:d300e71b-55b0-413b-851c-0f6791cb269e}}The postulates of propositional intuitionistic logic are the axiom schema {{formula:6e73f54f-a559-4368-93e3-1189444ea690}} , the introduction and elimination rules shown in Figure REF , the contradiction rule {{formula:83bb3839-fcb3-40c4-ab",
        "4d0-bdf1-46de-89d7-c311832c420d}} are strongly equivalent to each other iff the infinitary formula {{formula:99da9187-6dd8-4d0e-94cb-c7ed3454f7f4}} is strongly equivalent to {{formula:2545214e-f9fb-47bb-8d3c-99e3ea7b9681}} .Lemma 2 . Two infinitary formulas are strongly equivalent to each other iff they are satisfied by the same HT-interpretations.We extend the definition of {{formula:017ce76e-630d-407e-b155-2fef48b8659e}} to sequents as follows: {{formula:bac30d9d-5311-4797-b947-d9ff4ba19b4b}}(Here {{formula:948dbd5c-1fcf-4057-8d0b-f3da80753f26}} is the conjunction of all formulas in {{formula:8263bc64-17e7-4a40",
        "26}} is the conjunction of all formulas in {{formula:8263bc64-17e7-4a40-8ab4-cd98784754ab}} ; {{formula:da5ff671-83b5-4a90-a474-eceea6d2f779}} is the universal closure operation).Lemma 3. If a sequent {{formula:eac8b3df-7cca-4f67-a4e2-7ee91c092687}} can be derived from sequents {{formula:6a75bad7-3646-4c80-8f49-49df4130ef27}} by one application of an inference rule of Int then every HT-interpretation satisfying {{formula:0d8a2ade-ed03-4aa0-a645-8536d6529e2c}} satisfies {{formula:835d4390-564b-4a30-9066-49385ff4cd56}} .Lemma 4. For every axiom {{formula:d13f0547-d654-4def-b337-9d247d6"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the research paper use to demonstrate how optical isolators prevent Raman pump power, and what are the main findings regarding their effectiveness?",
      "expected_output": "The research paper uses an experimental setup to demonstrate how optical isolators prevent counter-propagating Raman pump power from entering the ASE source. The main finding is that the optical isolator is effective in preventing the counter-propagating Raman pump power, as it is used to prevent the pump power from entering the ASE source, with the total ASE power launched into the optical fiber being +2.6 dBm. The paper does not provide a detailed analysis of the methods used to demonstrate the effectiveness of optical isolators, but rather focuses on the overall performance of the Raman amplifier design using a machine learning-based framework.",
      "context": [
        " +2.6 dBm. An optical isolator is used to prevent the counter–propagating Raman pump power from entering the ASE source.Five fibers with different Raman gain coefficients are evaluated: 50 and 100 km of SSMF, 50 km of ULLF, 4.8 km of DCF, 7.5 km of HNLF, and 15 km of IDF. SSMF is widely deployed in terrestrial systems while ULLF is used for submarine and unrepeatered links due to its lower loss and wider effective area (although the reduced Raman gain). Both SSMF and ULLF are used as the gain medium for distributed Raman amplifiers.The DCF and HNLF are special highly nonlinear fibers used for discrete Raman amplifiers, the former presenting the advantage to also compensate for chromatic dispersion. The IDF is being used for discrete Raman amplifiers as an alternative to DCFs due to its lower attenuation . However, its original application is for distributed Raman amplifiers in submarine systems.The commercial Raman pump module consists of four pumps with adjustable powers and fixed wavelengths. The fixed wavelengths are 1454.4 nm, 1444.8 nm, 1434.4 nm, and 1423",
        "F), each with a different fiber length, covering distributed and discrete Raman amplifier applications. The accuracy of the ML–based framework in predicting the pump powers to provide a set of arbitrary gain profiles is investigated by comparing the target gain profiles with the profiles experimentally obtained when applying the pump powers predicted by the framework. A vast statistically analysis of several thousand cases is conducted and error distributions are estimated showing that the ML-based framework proposed in , can effectively design Raman amplifier with limited errors.Even though experimental results are shown only for the C–band in this paper, because of limitation in the available Raman pump wavelengths, we believe that main conclusions can be extended to larger bandwidth, i.e. C+L, as we have shown by simulation in .\nExperimental setup: Fig.REF shows the experimental setup used to generate the data–set for the ML–based Raman amplifier design. The input signal is emulated by an amplified spontaneous emission (ASE) source covering the entire C–band (192-196 THz). Its profile is also depicted in Fig.REF . The total ASE power launched into the optical fiber is +2.6 dBm. An optical isolator is used to prevent the counter–propagating Raman pump power",
        "oped fiber amplifiers (EDFA), BDFAs and SOAs to provide gain flattening and in general for gain profile shaping. Moreover, Raman amplifiers can be used in discrete or distributed stand–alone configurations to provide ultra-wideband gain , . The suitable selection of pump powers and wavelengths is the key in achieving a desired Raman gain profile. This is a challenging task as the relationship between gain profile versus pump powers and wavelengths is nonlinear and requires extensive numerical simulations to predict, especially for a large number of pumps .Recently, a machine learning (ML)–based framework for Raman amplifier design has been proposed , . The advantage of the proposed framework in , is that it can provide an ultra–fast and low–complexity pump powers and wavelengths allocation for the design of arbitrary Raman gain profiles.In this paper, extensive experimental investigations of the framework proposed by , are presented. We test different fiber types ranging from standard single mode fiber (SSMF), ultra–low loss fiber (ULLF), dispersion compensating fiber (DCF), highly nonlinear fiber (HNLF) and inverse dispersion fiber (IDF), each with a different fiber length, covering distributed and discrete Raman amplifier applications. The accuracy of the ML–based"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the effect of scalar fields on the motion of point-like sources in this research paper?",
      "expected_output": "The main findings of this research paper are that the self-force on a point-like source coupled with a massive scalar field produces a time-changing inertial mass. The paper derives the Harish-Chandra equation of motion of a radiating scalar charge under the influence of an external force, which includes the effect of the particle's own field. The energy-momentum and angular momentum balance equations yield the radiative parts of Noether quantities carried by the massive scalar field. The phenomenon of mass loss by scalar charge is also studied, where the charge loses its mass through the emission of monopole radiation.",
      "context": [
        "# arXiv Paper: 0907.3071\n\nSelf force on a point-like source coupled with massive scalar field\nabstract: The problem of determining the radiation reaction force experienced by a scalar charge moving in flat spacetime is investigated. A consistent renormalization procedure is used, which exploits the Poincar\\'e invariance of the theory. Radiative parts of Noether quantities carried by massive scalar field are extracted. Energy-momentum and angular momentum balance equations yield Harish-Chandra equation of motion of radiating charge under the influence of an external force. This equation includes effect of particle's own field. The self force produces a time-changing inertial mass.\nIntroduction: The problem of calculating the motion of an isolated point-like charge coupled to massive scalar field in flat spacetime is an old one which is currently receiving renewed interest. Classical equations of motion of a point particle interacting with a neutral massive vector field were first found by Bhabha following a method originally developed by Dirac for the case of electromagnetic field. In this method the finite force and self-force terms in the equations of motion are obtained from the conservation laws for the energy-momentum tensor of the field. It",
        " ) of particle's own field (REF ) supported on the world line {{formula:1353e0a5-d53d-4a8b-90cd-add6fa381f2f}} . The own field contributes also to particle's inertial mass {{formula:2579f407-0067-42fe-bda5-baf928305e6e}} defined by eq.(REF ).\nConclusions: In the present paper, we find the radiative parts of energy-momentum and angular momentum carried by massive scalar field coupled to a point-like source. Scrupulous analysis of energy-momentum and angular momentum balance equations yields the Harish-Chandra equation of motion of radiating scalar pole. This equation includes the effect of particle's own field as well as the influence of an external force.To remove divergences stemming from the pointness of the particle we apply the regularization scheme originally developed for the case of electrodynamics in flat spacetime of three dimensions , . It summarizes a scrupulous analysis of energy-momentum and angular momentum carried by non-local electromagnetic field of a point electric charge. The simple rule allows us to identify that portion of the",
        " external field.This is a special feature of the self force problem for a scalar charge. Indeed, the time-varying mass arises also in the radiation reaction for a pointlike particle coupled to a massless scalar field on a curved background . The phenomenon of mass loss by scalar charge is studied in , . Similar phenomenon occurs in the theory which describe a point-like charge coupled with massless scalar field in flat spacetime of three dimensions . The charge loses its mass through the emission of monopole radiation.\nEnergy-momentum of the scalar massive field of uniformly moving\nparticle: The simplest scalar field is generated by an unmoved source placed at the coordinate origin. Setting {{formula:6de73066-68ba-4b87-a37b-fac3e18d346e}} and {{formula:945fcfc6-6144-42b3-862f-db43150d6859}} in eq.(REF ), one can derive the static potential , : {{formula:c6e4e28b-7299-4dc6-8d0c-eeec865605c1}}where {{formula:d5767"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the polarizations of TT gauges relate to time-dependent tensors in the context of this research paper?",
      "expected_output": "In the TT gauge, the time-dependent TT tensor has two independent polarizations, which are related to the complex function decomposition. The expansion parameters are complex functions of the retarded time and are used to model the radiation field at large distances from the source. For example, the spatial components of the TT tensor are given by a specific formula, where the total mass of the system, distance from the source, and time-dependent TT tensor are key parameters.",
      "context": [
        " time dependent TT tensor. In the TT gauge, {{formula:5d06a14b-6685-44b4-9ccc-9a10b20e663c}} has two independent polarizations {{formula:4316d6f0-34d1-463c-826d-66999e0ff305}} and {{formula:79d72ece-2bde-426a-9d38-8da0708d9f8a}} , and the complex function {{formula:feebfa4d-55b9-4ed2-93bb-6689fb89e17c}} is decomposed as: {{formula:b4d49776-5d39-4b00-8744-fe0e5ec7fc18}}The expansion parameters {{formula:0f4cb3f4-0e17-46df-aeb9-81086cfdb609}} are complex functions of the retarded time {{formula:28fc1fec-debc-4aed-a1ff-d0670053a025}} and, if {{formula:c775a036-",
        "b-aabba0d1837b}} of spin-weight {{formula:6a474992-41f9-4f27-841e-51eb475fc700}} of the radiation field at large distance from the source, as specified in . In the Transverse-Traceless (TT) gauge, the spatial components {{formula:b00b75f8-df04-4028-80dc-e393040dbd67}} are: {{formula:91aec792-ab62-4a5c-9497-14776cf6b840}}where {{formula:1f7bd40c-82e0-4827-93fe-92a4da26d4e5}} is the total mass of the system, {{formula:930672da-effc-4d06-8fdf-47602ee52d75}} is the distance from the source, and {{formula:44819767-ff49-498b-9af3-54956df11d22}} is a time dependent TT tensor. In the TT gauge, {{formula:5d06a14b-6685-",
        " categories, different parameter choices were made to explore how the pipeline can detect numerical relativity simulations.\nInspiral only waveforms from the Post-Newtonian expansion.: The standard template for the LSC-Virgo search , , , , is based on the non-spinning post-Newtonian inspiral waveforms, calculated directly in the Fourier domain through the stationary phase approximation , . These waveforms, referred to as SPA or TaylorF2, are parameterized by the binary's component masses {{formula:d4efd283-3f9d-467c-853f-c8c021ef0269}} and {{formula:7e7d8833-8b19-4841-ba1f-31a714f0587f}} , , , or equivalently by total mass {{formula:dd9fc49a-f597-4b48-94ca-58b49111c5dc}} and symmetric mass ratio {{formula:22fb9cde-e725-4d40-8dcc-2b0e6931f871}} . The amplitude evolution is modelled to leading order and the phase evolution is modelled to"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between r, D_L, and D_T in dispersion, and how do these variables impact the main findings of the research paper?",
      "expected_output": "The relationship between r, D_L, and D_T in dispersion is given by the dispersivity ratio r = D_L / D_T. According to the research paper, dispersion in the cross-flow direction is typically 1-2 orders of magnitude smaller than in the stream flow direction. The magnitude of D_L and D_T is estimated with the aid of correlations based on experiments and simulations, and they depend on many parameters such as Schmidt number, Reynolds number, tortuosity of the medium, Péclet number, and fluid phases. The dispersivity ratio r may considerably vary as a function of the Péclet number of the flow, as shown in the example of a medium composed of monodispersed beads. This relationship is crucial in understanding the effects of dispersion on the flow in granular media, and it impacts the main findings of the research paper by influencing the characterization of the flow in the parameter's space.",
      "context": [
        " a general theoretical discussion on dispersion-induced mixing.A classical approach to account for the effects of dispersion consists of replacing the molecular diffusion coefficient, D in Eq. (REF ), with a dispersion tensor \\mathbf {D} which depends on the local flow conditions. Typically, the dispersion tensor is anisotropic and aligned with the flow, meaning that it can be decomposed into two components in the directions parallel (D_{L}, longitudinal dispersion) and perpendicular (D_{T}, transverse dispersion) to the local flow velocity \\mathbf {u}. This model is labelled as Fickian dispersion model . With these assumptions, the dispersion tensor takes the form:\\mathbf {D} = D\\mathbf {I}+(\\alpha _{L}-\\alpha _{T})\\frac{\\mathbf {u}\\mathbf {u}}{\\vert \\mathbf {u}\\vert }+\\alpha _{T}\\mathbf {u}\\mathbf {I},where \\mathbf {I} is the identity tensor, and the coefficients \\alpha _{L}=D_{L}/U and \\alpha _{T}=D_{T}/U",
        "{\\mathbf {D}}{D} = \\mathbf {I}+\\frac{1}{\\Delta U}\\left[(r-1)\\frac{\\mathbf {u}\\mathbf {u}}{\\vert \\mathbf {u}\\vert }+\\mathbf {u}\\mathbf {I}\\right].This expression suggests that the case of pure diffusion is recovered when D_{T}\\ll D, corresponding to \\Delta \\gg 1.With specific reference to granular media, additional simplifications allow a further characterisation of the flow in the parameter's space. Considering that the longitudinal dispersivity can be approximated , as \\alpha _{L}=D_{L}/U\\approx d, we can rewrite Eq. (REF ) as\\operatorname{\\!\\mathit {Ra}_\\mathit {d}}=\\frac{UH}{\\phi D_{T}}=\\frac{UH}{\\phi U\\alpha _{T}}=\\frac{rH}{d}.In bead packs, the permeability can be inferred from the Kozeny-Carman correlation , , i.e.k=\\frac{d^2}{36k_",
        " _{L}=D_{L}/U and \\alpha _{T}=D_{T}/U correspond to the dispersivities of the medium in longitudinal and transverse directions, respectively. Dispersion in the cross-flow direction is typically 1-2 orders of magnitude smaller than in the stream flow direction. The ratio of these two contributions is quantified by the dispersivity ratior=\\frac{D_L}{D_T}.The magnitude of D_L and D_T is estimated with the aid of correlations based on experiments and simulations . Longitudinal and transverse dispersion coefficients depend on many parameters, namely Schmidt number , Reynolds number , tortuosity of the medium , Péclet number , fluid phases . We refer to for a review of numerical, experimental and theoretical works in this field. {{figure:5194af04-6c65-41e2-981d-5e6946c36fb9}}We consider here an example of a medium composed of monodispersed beads, typical of numerical and experimental setups commonly employed in pore-scale investigations, and we show that the dispersivity ratio r may considerably vary as a function of the Péclet number of the flow (a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does differentiability optimize volume rendering parameters, and what are the main findings of this research in terms of improving rendering quality?",
      "expected_output": "The research paper introduces a differentiable volume rendering solution that optimizes volume rendering parameters by providing differentiability of all continuous parameters. This allows for automatic optimizations of external parameters and the volumetric density field. The main findings include the use of differentiable entropy for automatic viewpoint selection and optimizing transfer functions from rendered images. Additionally, the paper presents a novel approach for tomographic reconstruction from images using an emission-absorption model, enabling comparisons with algebraic reconstruction techniques and state-of-the-art differentiable path tracers. This leads to improved rendering quality by facilitating the consideration of small-scale changes and optimizing per-voxel densities.",
      "context": [
        "# arXiv Paper: 2107.12672\n\nDifferentiable Direct Volume Rendering\nabstract: We present a differentiable volume rendering solution that provides differentiability of all continuous parameters of the volume rendering process. This differentiable renderer is used to steer the parameters towards a setting with an optimal solution of a problem-specific objective function. We have tailored the approach to volume rendering by enforcing a constant memory footprint via analytic inversion of the blending functions. This makes it independent of the number of sampling steps through the volume and facilitates the consideration of small-scale changes. The approach forms the basis for automatic optimizations regarding external parameters of the rendering process and the volumetric density field itself. We demonstrate its use for automatic viewpoint selection using differentiable entropy as objective, and for optimizing a transfer function from rendered images of a given volume. Optimization of per-voxel densities is addressed in two different ways: First, we mimic inverse tomography and optimize a 3D density field from images using an absorption model. This simplification enables comparisons with algebraic reconstruction techniques and state-of-the-art differentiable path tracers. Second, we introduce a novel approach for tomographic reconstruction from images using an emission-absorption model with post-sh",
        ". Second, we introduce a novel approach for tomographic reconstruction from images using an emission-absorption model with post-shading via an arbitrary transfer function.\n: =1=9 =7.pdf,.png,.jpg,.jpeg.epsfigures/pictures/images/./Parameters:Input:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the evolution of hyperon gaps into quark gaps, and what methods were used to study this phenomenon?",
      "expected_output": "The main findings regarding the evolution of hyperon gaps into quark gaps are that the gaps at the hyperon Fermi surfaces due to hyperon-hyperon pairing evolve continuously to become the gaps at the quark Fermi surfaces, which characterize the color-flavor locked phase. As density increases, the gaps at the hyperon Fermi surfaces increase significantly, while the gaps at zero momentum decrease dramatically. The methods used to study this phenomenon involve analyzing the properties of the color-flavor locked phase and its excitations, and estimating the value of the gap, as described in Section 4 of the research paper.",
      "context": [
        " the CFL phase or a baryon in the hypernuclear phase — is characterized by a gap at the Fermi surface and by a gap at zero momentum, i.e. a mass. As a function of increasing density, gaps at the hyperon Fermi surfaces due to hyperon-hyperon pairing evolve continuously to become the gaps at the quark Fermi surfaces which characterize the color-flavor locked phase. During this evolution, the gaps are thought to increase significantly. In contrast, the gaps at zero momentum decrease dramatically with increasing density as they evolve from being of order the hyperon masses in hypernuclear matter to being of order the current quark masses at asymptotically high densities.Note that in order for quark-hadron continuity to be realized, {{formula:40d5de9d-29da-48c3-b271-265f9deed5a3}} must not be broken by hyperon-hyperon pairing. At every point during the evolution of the theory as a function of increasing density, there will be an unbroken {{formula:db097f77-792a-4206-bd70-29041994a888}} and a massless {{",
        " are neglectable, the nuclear matter phase, made of neutrons and protons only, is not continuously connectable with the color-flavor locked phase. If quark-hadron continuity is to be realized in the phase diagram of nature, what must happen is that, as a function of increasing density, one first goes from nuclear matter to hypernuclear matter, with sufficiently high density that all the hyperons have similar Fermi surfaces. This first stage must involve phase transitions, as the symmetries of hypernuclear matter differ from those of ordinary nuclear matter. Then, as the density is increased further, the hypernuclear matter may evolve continuosly to become CFL quark matter, with pairing among hyperons becoming CFL pairing among quarks.We now have a description of the properties of the CFL phase and its excitations, in which much can be described quantitatively if the value of the gap {{formula:db3808cc-2dd6-41d6-8ef9-f07d28cd894c}} is known. We describe estimates of {{formula:fc6f462d-15db-414b-bc66-c479d43b96b6}} in Section 4. First, however,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how scalable nodes enhance quantum network capabilities, and what methods were used to achieve these results?",
      "expected_output": "The main findings of the research paper are that scalable nodes can enhance quantum network capabilities by providing precise and robust control over multiple qubits and advanced functionality. The methods used to achieve these results include hybrid coherent control, entangling three types of qubits, and incorporating memory qubits into a logical state using a three-qubit repetition code. Specifically, the researchers demonstrated a Greenberger–Horne–Zeilinger-type state with fidelity exceeding 0.83, and used active feedback to correct errors and improve the final states. These results were achieved through precise control over an electron spin, a nuclear spin, and a photonic qubit, showcasing the potential of hybrid nodes in diamond for realizing metropolitan-scale quantum networks.",
      "context": [
        "# arXiv Paper: 2408.07752\n\nHybrid entanglement and error correction in a scalable quantum network node\nabstract: Recent breakthroughs have ushered the quantum network into a new era, where quantum information can be stored, transferred, and processed across multiple nodes on a metropolitan scale. A key challenge in this new era is enhancing the capabilities of individual nodes, providing precise and robust control over multiple qubits and advanced functionality for scalable quantum networks. Here, we report on precise and complex control in a hybrid quantum node based on a diamond color center. We demonstrate hybrid coherent control by entangling three types of qubits: an electron spin as an interface qubit, a nuclear spin with long memory time, and a flying photonic qubit, with their qubit frequencies spanning three distinct regimes from the optical domain to the rf domain. By incorporating two additional memory qubits, we encode three memory qubits into a logical state using the three-qubit repetition code and entangle this logical qubit with a photonic qubit. Leveraging hybrid qubits and precise control, we repeatedly read out the error syndromes of memory qubits through the electron spin, serving as an auxiliary qubit, then apply a real-time feedback operation",
        " Greenberger–Horne–Zeilinger-type (GHZ) state involving three different types of qubits—an electron spin serving as an interface qubit, a carbon-13 nuclear spin as a memory qubit, and a photon as a flying qubit—with fidelity exceeding 0.83(3). This entangled state connects qubits across very distinct frequencies in the rf, microwave, and optical domains, making it a crucial resource for network applications. Second, by harnessing hybrid control and incorporating additional memory qubits, we encode the memory qubits in a logical state and entangle it with a flying qubit. In this proof-of-principle experiment, we use a three-qubit repetition code to protect memory qubits from bit-flip errors. This is accomplished by repeatedly reading out error syndromes through the interface qubit and applying active feedback to correct errors. We verify error correction by comparing the final states with and without active feedback. Furthermore, we show that the final states can be improved by post-selecting results where the error syndrome outcomes indicate no errors, i.e. error detection. These results showcase the key advance of hybrid nodes in diamond and their potential for realizing full-fledged metropolitan-scale quantum networks"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about Hecke C*-algebras and their connection to number fields' rigidity, and how do the methods used contribute to these results?",
      "expected_output": "The research paper presents main findings on Hecke C*-algebras and their connection to number fields' rigidity. It shows that Hecke C*-algebras constructed from totally positive groups are less ad hoc and can be used to study rigidity for C*-algebras constructed from number fields. The methods used, including the Bost-Connes C*-algebra and the crossed product C*-algebra, contribute to these results by providing a framework for analyzing the KMS-structure and symmetry group of the associated C*-dynamical system. For example, the paper uses the reconstruction theorem and the dynamical characterization of number fields to obtain a complete solution to the rigidity problem for Bost-Connes C*-algebras, demonstrating that a *-isomorphism of Bost-Connes C*-algebras implies the underlying number fields are isomorphic.",
      "context": [
        "structure and symmetry group. The Hecke C*-algebras constructed from totally positive {{formula:0461b768-3c0b-45ac-8e8b-93e91c207e52}} -groups in are less ad hoc. These are full corners in the Bost–Connes C*-algebra by , so the results of the second-named author and Kubota in yield a rigidity theorem for such Hecke C*-algebras. However, from the perspective of rigidity for C*-algebras constructed from number fields, there are other C*-algebras that are more natural from a number-theoretic or dynamical view point.\nMain result: Let {{formula:633237df-cf8b-4e04-9f69-c05526ae63a1}} be a number field with ring of integers {{formula:1198ebf0-7734-4ed4-9c46-dec22821b6a8}} . We consider the crossed product C*-algebra {{formula:75dfe6a3-ca48-4579-9",
        ". Moreover, this conjugacy is constructed from K-theoretic invariants of the C*-algebras. The Bost–Connes semigroup dynamical system from is constructed from a number field using the action of the ideal semigroup on the balanced product of the integral adeles and the Galois group of the maximal abelian extension of the field, and combining the aforementioned reconstruction theorem with the dynamical characterisation of number fields obtained by combining and , one obtains a complete solution to the rigidity problem for Bost–Connes C*-algebras. However, this is an abstract rigidity result in the sense that a *-isomorphism of Bost–Connes C*-algebras implies the underlying number fields are isomorphic, but it is not clear how this isomorphism of fields is related to the initial *-isomorphism.The construction of Bost–Connes C*-algebras in is ad hoc: It is designed in such a way that the associated C*-dynamical system has the desired KMS-structure and symmetry group. The Hecke C*-algebras constructed from totally positive {{formula:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used to improve estimates and what are the key findings related to estimation improvement in this research paper?",
      "expected_output": "default",
      "context": [
        "6}|P_{=0}\\bar{\\partial }Z^au(t,x,y)| &\\lesssim \\sum _{|b|\\le N+7}|P_{=0}Z^bu|+\\langle {t-|x|} \\rangle \\sum _{|a|\\le N+6}|P_{=0}\\partial Z^au|\\\\ &\\lesssim \\varepsilon _1\\langle {t+|x|} \\rangle ^{-1/2}\\langle {t-|x|} \\rangle ^{-0.3}+\\varepsilon _1\\langle {x} \\rangle ^{-1/2}\\langle {t-|x|} \\rangle ^{-0.3}. \\end{split}Thus, the proof of () is completed. Analogously, () can be shown.\nPointwise estimates of the zero modes: This section aims to improve the bootstrap assumptions (REF ) and ().\nHigher order pointwise estimates of the zero modes: Lemma 3.1 Let u(t,x,y) be the smooth solution of (REF ),",
        " ).For |x|\\ge \\langle {t} \\rangle /2, it follows from (REF ) and (REF ) that\\begin{split} \\langle {x} \\rangle \\sum _{|a|\\le 2N-6}|P_{=0}\\bar{\\partial }Z^au(t,x,y)| &\\lesssim \\sum _{|b|\\le 2N-5}|P_{=0}Z^bu|+\\langle {t-|x|} \\rangle \\sum _{|a|\\le 2N-6}|P_{=0}\\partial Z^au|\\\\ &\\lesssim \\varepsilon _1\\langle {t} \\rangle ^{2\\varepsilon _2}. \\end{split}This completes the proof of (REF ).\nLower order pointwise estimates of the zero modes: Based on the estimates in the last subsection, we are able to improve the pointwise estimates of the zero modes in (REF ) and (). First, recall two lemmas from .Lemma 3.6 Let w(t,x) be a solution of \\Box"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on N-pure implication and how do the methods used contribute to the key results?",
      "expected_output": "The research paper presents a study on N-pure implication, where it is found that a given formula is N-pure. The main findings include the existence of certain elements that lead to a contradiction, thus proving the N-purity of the formula. The methods used, such as assuming the opposite and showing the contradiction, contribute to the key results. For example, the paper states that if a formula is N-pure, then there are certain elements that satisfy a specific condition, leading to a contradiction and thus proving the N-purity.",
      "context": [
        "-cdaf-4a49-87a5-352bb46d11c5}} . Since {{formula:b2524d0b-70ee-4359-833d-102f7385be1c}} is N-pure, then there are {{formula:9467a89e-c89f-40b5-abd8-b8f45ba87be3}} and {{formula:61189274-ef9c-474f-9220-eae0bc25f4c9}} such that {{formula:0a9d8251-d922-4859-8c76-c08501ce4ce2}} . Hence {{formula:9050b768-e5d8-494f-9e2a-2f7503694b33}} and so {{formula:7f68b39f-a59b-4df0-b19c-d214ce065f1f}} . This is a contradiction. Thus we have {{formula:ba8512db-4d3e-4190-bde8-a7778fddd50f}}",
        "4856-ae9e-7b854261d7f1}} is N-pure. So {{formula:07576e84-95ff-4d86-81f5-6aa1ae181784}} which is a contradiction. On the other hand, if {{formula:4736d2db-6998-476c-818f-f3875359937e}} , then it is obvious that {{formula:ed1929ba-693c-42cc-82c2-45cfe9c52e6f}} .{{formula:62f7f138-1d43-4748-81b4-6b652cb0688f}} It suffices to show that for each {{formula:920eaefe-5b10-482b-805e-5c77323be5cc}} there exists {{formula:2643beab-987f-49d5-b0fd-d0e1e69654fa}} such that {{formula:6935d8f0-0dc2-4acd-97d7-0137b33448a5}} , see ."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on the relationship between symmetric codes and alternant or Goppa codes, and how do their automorphisms compare?",
      "expected_output": "The research paper shows that symmetric codes are codes with a non-trivial automorphism group. Symmetric Goppa codes and alternant codes can be constructed by looking at the action of the projective semi-linear group on the support of these codes. The paper focuses on the subgroup of affine transformations of the projective semi-linear group, which allows it to cover all constructions of symmetric alternant or Goppa codes used in a cryptographic context. For example, Definition 4 in the paper defines an alternant or Goppa code with an automorphism group, and the paper explores how these automorphisms compare. Specifically, the paper notes that alternant codes may have several identical descriptions thanks to affine transformations, as shown in Proposition REF.",
      "context": [
        "3-ed3e628cce8e}} such that {{formula:aa4f8d55-e418-4e21-a2d4-cb87634cb5fd}} is in {{formula:ca4a87d5-8733-4df5-b543-b26a87a2d8cc}} whenever {{formula:dfe4fa3d-d859-493e-b7c7-3fa5f7c35bfb}} belongs to {{formula:726fd841-e019-42d0-9131-ca19007a9d81}} . Symmetric codes are then codes with a non-trivial automorphism group.We have seen in Proposition REF that alternant codes may have several identical descriptions thanks to affine transformations. Actually, symmetric Goppa codes and alternant codes can easily be constructed by looking at the action of the projective semi-linear goup on the support of these codes as shown in , . By projective semi-linear group, we mean here transformations of the form: {{formula:7dd737ab-fd5c-4f",
        " simplify some of the statements and propositions obtained here) we will only consider the subgroup of affine transformations of the projective semi-linear group. It should be noted however that this simplification permits to cover all the constructions of symmetric alternant or Goppa codes used in a cryptographic context , , , , and in some cases even to generalize them. Namely, we will deal with the following cases:Definition 4 Let {{formula:c3115c33-1cbf-4155-b8c3-f6b8183b3eb7}} be an alternant or Goppa code defined over a field {{formula:445fd80a-6749-4691-b64d-ad4746e0e64c}} of length {{formula:d741733f-128b-448b-90eb-476c77b2b881}} , with an automorphism group {{formula:c1c0e7ea-bd76-4e68-9fcc-5b3fc8190785}} . Given a nonnegative integer {{formula:7057db76-89a1-493d-832a-dd"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how on-site Coulomb energy affects the properties of graphene, and what methods were used to study this impact?",
      "expected_output": "The research paper discusses the impact of on-site Coulomb energy on graphene properties, with its magnitude being a topic of debate. At the tight-binding level, several authors have studied its effects using methods such as calculating the ground-state energy per unit cell, which involves summing occupied self-consistent single-particle state energies and accounting for double counting of the on-site Coulomb interaction energy. For example, the ground-state energy per unit cell is given by the formula {{formula:0460c539-7235-42a1-9101-3941b0c3ced0}}, which is a sum of the occupied self-consistent single-particle state energies minus a standard term accounting for double counting the on-site Coulomb interaction energy. The study of chiral nanoribbons also involves calculating the average edge magnetization, defined as the average of the th-site magnetization. These methods help understand how on-site Coulomb energy affects graphene's properties, such as its electronic structure and magnetization.",
      "context": [
        "ecd68df7}} at site {{formula:b57dfef9-de45-44a8-a738-1ed486594932}} , while {{formula:30ea0148-944b-4beb-8fb3-c7aeab91aea6}} is the number operator and {{formula:0b3cab4c-80ec-4566-b346-e4c9ed597131}} is its expectation value. The symbols {{formula:e304c227-69d1-4f69-ae1c-e474aecb97ad}} and {{formula:86f8c2db-09c4-4991-b2cf-5fc92c6b659f}} indicate that the sums run over nearest-neighbor and next-nearest-neighbor lattice sites, respectively.The magnitude of the on-site Coulomb energy {{formula:37612168-b627-4c10-8d02-c13f67da4b7a}} in graphene systems is under current debate in the literature . At the tight-binding level, several authors study paramet",
        "9512-4766-ba15-42449d24c057}} is the “volume\" of the Brillouin zone.The ground-state energy per unit cell {{formula:0460c539-7235-42a1-9101-3941b0c3ced0}} is a sum of the occupied self-consistent single-particle state energies minus a standard term accounting for double counting the on-site Coulomb interaction energy, namely, {{formula:522a43b2-1661-4096-871b-da42e0c803c9}}The {{formula:df85eb54-fbbe-4967-9b72-9550cef13a6a}} th-site magnetization (in units of {{formula:a09a9a3e-4f40-435b-8a8a-ecfc675b98b4}} ) is defined as {{formula:9a73cba8-2767-4916-97ee-6a6c233e5630}}For chiral nanoribbons, one is frequently interested in the average edge magnetization, conveniently defined as {{",
        " of {{formula:34135965-6151-4dd1-b99c-7a3c1b833c8b}} : For a given wave number {{formula:adadba9e-2248-4d0a-bce4-71b79bee8aae}} , the {{formula:3019aa7b-a8c1-4b12-a67a-495ee236b0b0}} th state occupation follows the Fermi distribution at zero temperature, namely, {{formula:d4259101-415d-4f2f-8d91-9c28d429e83e}} , where {{formula:1b1e0e3f-8cc8-4471-9f55-363d1eafe9a9}} is the Heaviside step function and {{formula:8541f4dc-0f84-42d0-87b0-02c1eaf290db}} is the chemical potential. The probability amplitudes {{formula:a1af205e-2031-4cbb-bea0-c89da8c70d"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods did the photometry codes and SFH reconstruction codes use to recover the star formation history of Cetus, and what were the main findings of this process?",
      "expected_output": "The photometry codes used to recover the star formation history of Cetus were DAOPHOT/ALLFRAME and DOLPHOT, while the SFH reconstruction codes used were IAC-pop/MinnIAC, MATCH, and COLE. These codes employed two stellar evolution libraries, BaSTI and Padova/Girardi. The main findings of this process were that Cetus continued forming stars through z ~ 1, long after the Universe was reionized, and that there is no clear signature of the epoch of reionization in Cetus' SFH. The study also found that the SFH of Cetus can be modeled using a constant star formation rate in the age range of 0-13.6 Gyr.",
      "context": [
        "-depth study of its stellar populations, and how they evolved with time, could provide important clues about the processes governing dwarf galaxy evolution.The paper is organized as follows. In § we present the data and the reduction strategy. § discusses the details of the derived color-magnitude diagram (CMD). In § we summarize the methods adopted to recover the SFH, and we discuss the effect of the photometry/library/code on the derived SFH. The Cetus SFH is presented in §, where we discuss the details of the results, as well as the tests we performed to estimate the duration of the main peak of star formation and the radial gradients. A discussion of these results in the general context of galaxy evolution are presented in §, and our conclusions are summarized in §.\nObservations and data reduction strategy: The data presented in this paper were collected with the ACS/WFC camera aboard the HST. The observations were designed to obtain a signal-to-noise ratio {{formula:7bac5ba1-374f-4d10-a697-9af8e9c7bd50}} at the magnitude level of the oldest MSTO, {{formula:d7f073",
        " that holds for the majority of Milky Way dwarf satellites. Our results also show that Cetus continued forming stars through z ~ 1, long after the Universe was reionized, and that there is no clear signature of the epoch of reionization in Cetus' SFH. We discuss briefly the implications of these results for dwarf galaxy evolution models. Finally, we present a comprehensive account of the data reduction and analysis strategy adopted for all galaxies targeted by the LCID (Local Cosmology from Isolated Dwarfs) project. We employ two different photometry codes (DAOPHOT/ALLFRAME and DOLPHOT), three different SFH reconstruction codes (IAC-pop/MinnIAC, MATCH, COLE), and two stellar evolution libraries (BaSTI and Padova/Girardi), allowing for a detailed assessment of the modeling and observational uncertainties.\nIntroduction: A powerful method to study the mechanisms that drive the evolution of stellar systems is the recovery of their full star formation history (SFH). This can be done by coupling deep and accurate photometry, reaching the oldest main sequence (MS) turn-off (TO), with the synthetic color-magnitude diagram (CMD) modeling technique , , , , , ,",
        " the observational errors have been simulated in the synthetic CMD.1) The synthetic CMD: it is created using IAC-star . We calculated a synthetic CMD with 8,000,000 stars from the inputs described in the following. We have verified that, with this large number of stars, no systematic effects in the derived SFH of mock galaxies—such as those discussed in —were observed, and so no corrections such as those described in that paper are necessary. The requested inputs are:a set of theoretical stellar evolution models. We adopted the BaSTI and Padova/Girardi libraries; a set of bolometric corrections to transform the theoretical stellar evolution tracks into the ACS camera photometric system. We applied the same set, taken from , to both libraries. the SFR as a function of time, {{formula:6782e29e-e4bb-437d-b106-43b5431c995d}} . We used a constant SFR in the age range {{formula:d89cac66-faa8-4006-9ce9-e2feba6099ba}} Gyr; No a priori age-metallicity relation Z(t) is adopted:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do initial conditions play in ensuring the well-definedness of the research findings, and how do the methods used in this paper account for these conditions?",
      "expected_output": "Initial conditions play a crucial role in ensuring the well-definedness of the research findings. The paper considers initial conditions that are continuous and continuously differentiable, denoted by formulas such as {{formula:f8ab965a-4f8c-4882-a00f-aa025ff8b327}} and {{formula:a68b3089-d469-4795-bf7e-e286b504c72a}}, to guarantee that the Lyapunov-Krasovskii functional is well-defined for all times. Specifically, the integral term of the Lyapunov-Krasovskii functional involves the time derivative of the initial condition, which must be well-defined. The methods used in this paper account for these conditions by adopting definitions such as the {{formula:eda9c37e-1ddb-4d38-9ee6-a8490fab72d4}}-norm and using theorems like the Lyapunov-Krasovskii theorem, as recalled in Theorem REF, to ensure the validity of the research findings.",
      "context": [
        "c-a3c6-04d4d2db24cb}} for all smooth function {{formula:efb2099c-f455-4d05-a7b9-b2430da66782}} with {{formula:bc3d9e3b-1dec-4791-81e0-06b1148df954}} . In this case, we note {{formula:a31b19b8-3109-4cd0-8cdb-61e1abf352b8}} . We endow {{formula:7a46f77f-d114-4384-9201-873c6c877382}} with the normThis definition of the {{formula:eda9c37e-1ddb-4d38-9ee6-a8490fab72d4}} -norm is extracted from in the context of the statement of the Lyapunov-Krasovskii theorem which is recalled in Theorem REF of this paper. Note that certain authors prefer to adopt the following definition : {{formula:6ade1227-35eb-47e5-9a80-ba0218",
        "d6631}} -norm relies on the fact that, following , Theorem REF is proved by using the following Lyapunov-Krasovskii functional that depends on the time-derivative of the system trajectory: {{formula:9e0d2cbe-e9a1-4c87-b146-505c505ae53c}}For times {{formula:66b5767c-be05-407f-aa36-687382863da0}} , the integral term of the above Lyapunov-Krasovskii functional involves the time derivative of the initial condition {{formula:02d31c2c-efc1-47eb-bd8f-1c1988e10bd4}} . This is the main motivation for considering initial conditions {{formula:96b01b91-f755-4db8-8ac6-f751762b5878}} , ensuring that {{formula:8440064e-849f-473d-968c-ebb93153c516}} is well defined for all {{formula:8a7481fe-aba0-43e7-a",
        "8325-a0a7-4b9b-8948-1a6bf110274d}} with the usual Euclidean norm {{formula:e464023d-f6fb-467f-aa40-3a5287638150}} . The set of continuous and continuously differentiable functions {{formula:f8ab965a-4f8c-4882-a00f-aa025ff8b327}} from an interval {{formula:612e5fd6-4282-4fbb-bb84-732c1512b36e}} to a normed space {{formula:19fbd408-b2bd-48e9-8a12-fbb90ba564fa}} is denoted by {{formula:be656dd0-c6b5-4cad-93b3-a25697b07bfc}} and {{formula:a68b3089-d469-4795-bf7e-e286b504c72a}} , respectively. For a given {{formula:1287410b-51ea-4632-8404-636dffba"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on vortex velocity correlations in phase-ordering kinetics and how were they obtained?",
      "expected_output": "The main findings of the research on vortex velocity correlations in phase-ordering kinetics are that the vortex velocity probability distribution for two distinct vortices can be determined, and at short distances, the most probable configuration is that a vortex-antivortex pair have only a nonzero relative velocity which is inversely proportional to the distance between them. These findings were obtained by using the time-dependent Ginzburg-Landau model and considering the effects of other vortices and order parameter fluctuations. The researchers also used an auxiliary field to map the positions of the vortices and calculated the correlation function, which led to the determination of the coefficient of proportionality explicitly.",
      "context": [
        " and each has the distribution of velocities found previously for a single vortex.The analysis here is built upon previous work on the ordering kinetic of {{formula:07d14306-cef9-4d0f-bd86-a1fdb302cf3b}} symmetric systems. The best available theories for the order parameter correlation function were built up in the early 1990's and have led to the belief that we have a fairly good understanding of how to calculate the associated scaling function. It also has become clear that the order parameter correlation function or structure factor is a rather structureless quantity which does not give a great deal of direct information about the underlying disordering agents. This led Liu and Mazenko to look directly at the correlations between defects in the scaling regime. The key new element in this work, as discussed in some detail below, was the realization that the positions of the vortices could be labelled by the zeros of the order parameter field which could, in turn, be mapped onto the zeros of an auxiliary field {{formula:a4a06941-262c-4c58-8f30-5e461c3134de}} . They were able to show, following work by Halperin",
        "cd6-90f0-b1207983174b}}which is clearly analytic in the small x regime. They argue in Ref.MWOJK that the use of the gaussian assumption in determining defect dyanamics, such as {{formula:99861e4a-51ca-4b36-841b-54ad17a2f3a0}} , has a stronger fundamental justification than in the case of the determination of the order parameter correlation function. In the calculation of the two-vortex velocity probability distribution presented here it is assumed that the order parameter field can be replaced by a gaussian field in those portions of space near a vortex core and the associted auxiliary field correlation function is of the OJK form.\nOrder Parameter Dynamics: The system studied here has a defect dynamics generated by the time-dependent Ginzburg-Landau (TDGL) model satisfied by a nonconserved {{formula:38cd5c71-c7ee-45ff-a7c3-682c5e8933de}} -component vector order parameter {{formula:cc4f9e3f-2350-4c38-8699-81c",
        "# arXiv Paper: cond-mat/9705052\n\nVortex Velocity Pair Correlations\nabstract: The vortex velocity probability distribution for two distinct vortices is determined for the case of phase-ordering kinetics in systems with point defects. The n-vector model driven by time-dependent Ginzburg-Landau dynamics for a nonconserved order parameter is considered. The description includes the effects of other vortices and order parameter fluctuations. At short distances the most probable configuration is that a vortex-antivortex pair have only a nonzero relative velocity which is inversely proportional to the distance between them. The coefficient of proportionality is determined explicitly.\nIntroduction: It seems plausible that much of the structure one sees in the phase ordering of many materials , can be associated with the evolution and correlation among defects like vortices, monopoles, disclinations, etc. These topologically robust objects grow out of the frustration suffered by a system with a continuous symmetry which is thermodynamically driven to align in a broken symmetry state. In the case of the n-vector model with the number of components ({{formula:05455ffb-8ce3-4dfd-b64f-747cd79311de}}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main methods used to identify k centers in the given research paper, and how do these methods impact the key results?",
      "expected_output": "The main methods used to identify k centers in the given research paper are k-means and k-median clustering algorithms. These methods impact the key results by providing a set of centers whose cost is no more than a certain threshold, with improved guarantees for stable instances. Specifically, the k-means algorithm identifies a set of centers with a cost no more than a specified value, while the k-median algorithm provides a set of centers with an error arbitrarily close to 1 for non-private instances.",
      "context": [
        " -means and 2.633 for {{formula:40a75c20-4b84-4d88-983d-fa5f384edd91}} -median . That is, the algorithm of for {{formula:08d3e810-f113-499f-92cb-540a2f304e77}} -means identifies a set of {{formula:ccf34077-27cd-40e2-b22b-1308175ded36}} centers whose cost is no more than {{formula:dfe6817e-05f2-44b1-aff3-5a8144b71d93}} . Furthermore, for stable instances, one can obtain significantly improved guarantees, with error arbitrarily close to 1 (non-privately) , , , , , . In this work we focus on the input stability notion of Ostrovsky et al. , defined as follows.Definition 1.1 (Ostrovsky et al. ) A clustering instance {{formula:24368307-229d-4f79-8da9-594e6324eadf}} is {{formula:edccf8f1-",
        " to several different notions of input stability, applicable for both the {{formula:b01783b2-ea48-46ed-ab20-23a09e65b85c}} -means and the {{formula:f71948f2-4538-4180-875e-a6c0b71e5b99}} -median objectives. Moreover, we give the first local-differentially private algorithm for clustering stable instances.\nOur Setting: Before formally presenting our results, we describe our setting more precisely. Consider an input database {{formula:b8d44e2c-17fc-401e-bcdd-37b59b5fa43c}} containing {{formula:fe4f8204-e7f4-4722-9347-60ec8f17f10f}} points in {{formula:117bb372-da73-4665-868b-89bf53dcb79b}} . In {{formula:cccfad2c-babb-41a5-8da4-d3a0f75978b4}} -means clustering, the goal is to identify a set {{",
        "2-a71d-4578-99c0-a520fece1d28}} -means clustering. The construction of differentially private {{formula:b256cbbb-eec1-4dc0-aa6e-14f047e459e6}} -means clustering algorithms has attracted a lot of attention over the last 14 years , , , , , , , , , , , , , , , . In particular, three of these works — the work of Nissim et al. and two followup papers , — have constructed private {{formula:3aee3e7e-0a4a-4c70-aa11-d88cfaa3bc8f}} -means algorithms for stable instances. While several interesting concepts arise from these three works, their algorithms — and more importantly, their analysis — can be tighten up, simplified, and at the same time be applied in a broader setting. Our work does precisely this: we simplify the existing constructions for private clustering on stable instances, while improving upon their analysis and relating it to several different notions of input stability, applicable for both the {{formula:b01783b2-ea48-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the effect of D0 charge on D4 brane behavior in this research paper?",
      "expected_output": "The research paper studies the effect of D0 charge on D4 brane behavior by introducing D0-brane charge through boosting and uplifting the D4-brane solution. The main findings include the generation of a non-confining background with D4+D0 brane solution, which exhibits chiral symmetry restoration with temperature and D0-brane charge. The boost parameter, related to the D0-brane charge, plays a crucial role in this behavior. The paper also explores the probe brane approximation with intersecting brane configurations in Type IIA theory, where coincident D4-branes are wrapped around a compact circle with radius and extended along certain directions.",
      "context": [
        " separation of flavor branes to the radius of the compact {{formula:2e4b4a5b-7f8c-4d50-bb68-b203d7ff730f}} circle.\nD4+D0 background: Recently, there has been various attempts to study the phase diagram of various geometric backgrounds by introducing charges through boosting the simple solutions and also been used to generate black hole solutions with different charges. Here we shall adopt the same technique to generate additional charges to the D4 branes. The prescription is to boost the solution of D4-branes along {{formula:800b5c25-82ec-435c-9eb9-6f63e285b8ab}} directions and then uplift the solution to 11-dimension and reducing the solution along {{formula:00140965-824a-4bf9-8f10-5d5aa23e1dc0}} direction to generate the new solution which is electrically charged. The result will be same if we interchange the first two steps. The extra direction that we add while uplifting the solution to 11-dimension is taken to satisfy anti-periodic boundary conditions for fermions after reducing it",
        " uplifting the solution to 11-dimension is taken to satisfy anti-periodic boundary conditions for fermions after reducing it to 10-dimension.The system that we are going to study is a non-confining background generated from the background studied in the previous sections by introducing another charge, which would be D0-brane charge, by first boosting the solution and then uplifting it to 11-dimension and reducing it to generate a D4+D0 brane solution. We are interested to study the chiral symmetry restoration in the above mentioned probe brane approximation with temperature and with the D0-brane charge. To simplify our life henceforth, we shall be saying the boost parameter not the D0-brane charge.The non-confining and near extremal solution is generated from the confining solution by a double Wick rotation, {{formula:09942c35-293a-4708-93b6-2b5c5eef5b62}} of (REF ). Now applying the above mentioned procedure yields {{formula:7c76a88d-1d71-4b31-a0f4-d0d5153d39c2}}The boost parameterSince {{form",
        " symmetry breaking, we are considering an intersecting brane configuration in Type IIA theory. The way we shall proceed is by considering a bunch of coincident {{formula:811855da-489b-4e94-9ae5-a4274405665e}} D4-branes wrapped around the 4th directions with radius {{formula:ae1fcaa8-a43b-412a-80c5-9479c03ec777}} and extended along (0123) directions. After taking the low energy limit we see the background is magnetically charged under a 4-form antisymmetric field. This background shows confining behavior. Let us add matter fields in the (bi)-fundamental representation to this via probe-brane approximation.The kind of branes that we shall add are of the coincident {{formula:2a1d76bd-4fd1-45a7-8e60-d43a9db85cff}} Dp-brane and {{formula:fcab1f4f-c19e-4e49-bdf9-169ea7f1c326}} anti-Dp branes stuck at two different"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how the Lorentz force affects Bohmian trajectories in this research paper, and what methods were used to obtain these results?",
      "expected_output": "The research paper finds that a non-zero curvature modifies the usual equation of Bohmian trajectories by the emergence of a Lorentz-force term. This term vanishes exactly when the velocity field is a pure gradient. The methods used to obtain these results include the Helmholtz decomposition of the connection and the introduction of Lagrangian fluid paths, also known as Bohmian trajectories, via the relation given in the paper. Specifically, the equation of Bohmian trajectories is given by the Lagrangian path equation, which includes the Lorentz-force term. The paper also discusses the relation between the new Quantum Hydrodynamics (QHD) framework and the original Schrödinger equation, and how the Lorentz force affects the Bohmian trajectories in this context.",
      "context": [
        "This equation again clearly demonstrates the importance of not utilizing the exponential form of the phase as we now have the additional Lorentz-force term {{formula:81fa0299-5a4a-40f4-aa0c-1dac973730c2}} which is absent in the equation () of standard quantum hydrodynamics. Indeed, one sees that this additional term vanishes exactly when {{formula:5bba6cc7-f603-453c-a8fa-3749f0f45ac1}} is a pure gradient. In the Bohmian interpretation, the Lagrangian fluid paths (aka Bohmian trajectories) are introduced via the relation {{formula:dd005f3a-6a49-4f7d-9d92-3ea51c149417}} , so that Bohmian trajectories obey the Lagrangian path equation {{formula:0c323247-71c3-4a30-9fed-27fbe1d6dcc7}}We observe that a non-zero curvature modifies the usual equation of Bohmian trajectories by the emergence of a Lorentz-force term. Notice",
        "4bf5-bf88-157e5af54e3e}}so that the hydrodynamic helicity is preserved in time, that is {{formula:3aa3dc63-eeda-4fb3-9a5f-2e3e2af99730}}Now that the hydrodynamic Bohmian interpretation has been discussed, it is not yet clear how this construction is actually related to the original Schrödinger equation of quantum mechanics. This is the topic of the next section.\nSchrödinger equation with holonomy: In this section, we discuss the relation between the new QHD framework given by equations (REF )-(REF ) and the original Schrödinger equation. As a preliminary step, we consider the Helmholtz decomposition of {{formula:4501d757-8e3f-4ffe-b452-1b1bfa2c9737}} , that is {{formula:26565d69-76ce-416f-a993-b903c24b4ec8}}where {{formula:597aaef5-4b7a-4ae1-9081-372f8851097d}}",
        "52470c0098d2}} as a single-valued function. The relation (REF ) is reminiscent of similar expressions for the Bohmian velocity {{formula:887f3a71-0301-4de1-92cf-4773919b373a}} already appearing in , although in the latter case these were motivated by stochastic augmentations of standard quantum theory. One can verify that upon substituting (REF ) into the equations of motion (REF ) and (REF ), the latter become {{formula:c56a8a26-f8c3-46a6-b33d-ebb76dcc66bc}}Here, we have discarded numerical integration factors in the first equation. We recognize that these correspond to the standard Madelung equations for a free elementary charge in a magnetic field {{formula:03b8c277-e20f-42bb-b242-033bab3d969e}} .At this point, we further characterize the Helmholtz decomposition (REF ) of the connection {{formula:7622f184-3ab4-49df-9a04-10e305447af8}} in terms of its defining relation (REF )."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does crossing symmetry play in the nonrenormalization of certain physical processes, and how is this relationship explored in the research paper?",
      "expected_output": "Crossing symmetry plays a crucial role in the nonrenormalization of certain physical processes, as it leads to relations between different amplitudes. In the research paper, this relationship is explored through the analysis of the triangle amplitude, where the crossing symmetry of the singular part leads to a relation in perturbation theory. For example, the nonrenormalization of one amplitude implies the same for another, as seen in the case of the amplitudes represented by the formulas 918397ea-d7cc-45f8-92bf-00bce99133f4 and a05c402e-0ff7-4aaf-a852-e9abc60078c0. The paper also discusses how the OPE methods can be used to study nonperturbative effects in the triangle amplitude, providing further insight into the role of crossing symmetry in these processes.",
      "context": [
        "46e2-9820-1a50976a0f48}} is moved to the vector vertex {{formula:12dc02b5-4320-4211-803f-4fc4a7a9183a}} .Thus, we see that the crossing symmetry of the singular part in the triangle amplitude {{formula:5e60695f-6659-4681-bc19-f6aeb85f27e0}} leads to the relation (REF ) in perturbation theory. Nonrenormalization of {{formula:918397ea-d7cc-45f8-92bf-00bce99133f4}} implies the same for {{formula:a05c402e-0ff7-4aaf-a852-e9abc60078c0}} .\nNonperturbative effects and OPE: To study a nonperturbative effect in the triangle amplitude {{formula:ff86e322-f027-497b-bb32-97152baa8c7b}} one can use the OPE methods. This section is a brief review of the OPE analysis made in Ref. . The analysis shows a non",
        "-5f6a-47d3-85df-95dd6e2314e8}} to the vector vertex {{formula:fcbe2850-081e-4132-bd14-cb88db57fe8d}} . In the chiral limit it moves via even number of gamma matrices in any order of perturbation theory. Together with the change of the momentum {{formula:a57846aa-6b6d-467c-9a3f-03e0b9db419b}} (which does not affect {{formula:3ceeb3ef-4ecb-458a-b804-1509148b93fa}} ) it shows the symmetry of the amplitude {{formula:3bfe40a3-9969-452b-868e-a1f2b76ef7e6}} .At first glance the symmetry under the {{formula:772accb1-f02b-4164-9557-13bad2856784}} permutation seems to be in contradiction with the general decomposition (REF ): the transversal part of {{formula:4d2e8d10-32e7",
        " with massless quarks similar triangles with the heavy regulator fermions propagating on the loops. The regulator triangles produce terms which are polynomial in momenta, in our case terms linear in {{formula:80d7ff9a-c1e1-4400-b743-40cf025bf2d7}} and independent on {{formula:ed707e30-58b3-4e9e-9f68-9049847210a6}} . Moreover, it is simple to see that these terms are antisymmetric under the {{formula:56d048ad-89f0-4ec3-a0e3-b848cad087f8}} permutation. Indeed, in the propagator of the heavy regulator the leading term contains no gamma-matrix that leads to the sign change when {{formula:8f9263f6-d4ef-4a93-a2ba-9ac202eda1cc}} from the axial vertex {{formula:2def9cad-f619-46e2-9820-1a50976a0f48}} is moved to the vector vertex {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between weak and strong continuity in the context of this research paper, and how do these concepts impact the findings and methods used?",
      "expected_output": "The main differences between weak and strong continuity in the context of this research paper lie in their definitions and implications. Weak continuity refers to the property where the family is weakly continuous, as implied by the formula. On the other hand, strong continuity, also known as a C0-semigroup, requires that the semigroup is strongly continuous, meaning it satisfies the condition for any. The distinction between these two concepts is crucial, as strong continuity is a stronger property that implies weak continuity. The paper highlights that not all bounded semigroups of composition operators are strongly continuous, especially in non-reflexive spaces. Understanding these differences is essential to the findings and methods used in the paper, as it affects the analysis of semigroups of composition operators on various spaces.",
      "context": [
        " {{formula:abd5d669-07e7-4ad7-b6c1-3cf004f1e568}} implies that the family {{formula:176e9002-747c-4cb3-bf13-ac8b0f4f8f51}} is weakly continuous. By , it is strongly continuous.In general, if {{formula:0cf057df-1011-4372-bad9-a8886584bc6c}} is not reflexive, not all bounded semigroups of composition operators on {{formula:a0d604f4-cf20-4477-8497-08b9ba918bc7}} are {{formula:4e92d92d-214b-4ced-8d26-e0c782a780b0}} semigroups. Moreover, for some spaces {{formula:07856fd2-4df9-4185-9d18-ea198a9c03be}} , there is no nontrivial {{formula:11b9b77c-feee-4625-9a0e-d2c2ab761",
        "a1-cd30b8b32e85}} of bounded linear operators acting on {{formula:bb1c9ecb-897d-4aa2-a00a-400fff678d5c}} is called a semigroup if {{formula:ef9144a3-aa5b-4cf1-9a60-199ecb36c7a6}} and {{formula:ccf6baca-eaa9-4ba3-8ed2-9d48a8a424e1}} for all {{formula:04211297-3389-4960-a352-8796057da779}} . It is called a {{formula:6db3fc3f-b0c3-4628-b9ee-837cb77fb773}} -semigroup if it is strongly continuous, that is, {{formula:0193e93d-877f-430e-a019-ee9a4236bf0b}} for any {{formula:945fbbcf-5896-4c41-bb61-55307b0692a6}} .We recall that, given",
        "11b9b77c-feee-4625-9a0e-d2c2ab761b75}} semigroups of composition operators on {{formula:012004f7-a25f-4ad7-80f5-8be1f6ef2931}} . See for spaces between {{formula:c3519b6a-30d4-4b6f-815d-4f5bba7ae1e2}} and the Bloch space, for certain mixed norm spaces; in these papers one can find references to earlier results. For non-reflexive spaces, it would be desirable to find an analogue of Theorem , where the words “{{formula:c57ee35a-d7eb-41eb-be7a-b151f73cf8c8}} semigroup” are substituted by a weaker property, valid for all bounded composition semigroups.On the other hand, as Avicou, Chalendar and Partington prove in , any semigroup of composition operators on {{formula:c13a8963-05ac-4334-a32d-70d21f59a5b7"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods did the researchers use to analyze river flows with the horizontal visibility graph, and what were the main findings of this study?",
      "expected_output": "The researchers used the network framework of the horizontal visibility graph to analyze river flows. They mapped daily time series from 141 different stations of 53 Brazilian rivers into complex networks and characterized these networks by investigating the degree distribution and the clustering coefficient. The main findings of this study were that the degree distributions of these networks were well described by exponential functions, indicating a long-range correlated nature of the river flow fluctuations. Additionally, the study found that the river discharges in several stations have evolved to become more or less correlated over the years, which could be related to changes in the climate system and other man-made phenomena.",
      "context": [
        " characterization of time series related to river flows is rarely reported. Furthermore, previous efforts have been mainly focused on well established/traditional methods of times series analysis (such as fractal/multifractal analysis) and, for instance, the interesting advances in mapping time series into networks have just recently attracted the attention of researchers working on this topic , , , , . Here, we further fill this gap by studying the flows in 141 different measuring stations that cover 53 Brazilian rivers via daily time series obtained from the period of 1931 to 2012. Specifically, we have employed the network framework of the horizontal visibility graph , , for mapping these river flow series into networks. By tracking the evolution of topological properties of these networks, the horizontal visibility approach reveals that the flow in several stations are becoming more or less correlated (and displaying more or less complex internal network structures) over the years, a behavior that could be related to changes in the climate system and other man-made phenomena.This work is organized as follows. We first describe our database and review some properties of the horizontal visibility approach. We next employ the horizontal visibility graph to our time series. Then, we characterize these networks by investigating the degree distribution and the clustering coefficient as well as evolutive features of these two measures.",
        "# arXiv Paper: 1510.07009\n\nCharacterization of river flow fluctuations via horizontal visibility graphs\nabstract: We report on a large-scale characterization of river discharges by employing the network framework of the horizontal visibility graph. By mapping daily time series from 141 different stations of 53 Brazilian rivers into complex networks, we present an useful approach for investigating the dynamics of river flows. We verified that the degree distributions of these networks were well described by exponential functions, where the characteristic exponents are almost always larger than the value obtained for random time series. The faster-than-random decay of the degree distributions is an another evidence that the fluctuation dynamics underlying the river discharges has a long-range correlated nature. We further investigated the evolution of the river discharges by tracking the values of the characteristic exponents (of the degree distribution) and the global clustering coefficients of the networks over the years. We show that the river discharges in several stations have evolved to become more or less correlated (and displaying more or less complex internal network structures) over the years, a behavior that could be related to changes in the climate system and other man-made phenomena.\nIntroduction: The study of earth-related systems has become even more important with the growing concerns",
        "}} and {{formula:988f4fd6-8251-4329-80ee-769d76c99592}} without intersecting any intermediate datapoint height, that is, {{formula:158a339a-53be-440c-bcb8-fd1b4b187659}}Notice that, in our case, the horizontal visibility algorithm will produce one network per year of the time series of a given station. The bottom panel of Fig. REF (a) shows an example of a network constructed from the previous procedure by considering data from the Paraná River collected at the Itaipu Station in the year {{formula:e04445bc-15c6-468b-83c9-b90387a3150a}} . Also, Fig. REF (b) illustrate the horizontal visibility approach applied to a simple time series. Thus, our approach enables the investigation of possible evolutive features of the river flow rates by tracking the evolution of network measures.We first evaluated the degree distribution of the networks {{formula:bf963c1b-9395-4bcb-a279-9eccbedc92ff}} , where {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of UV radiation and stellar winds on star formation, and what methods were used to obtain these results?",
      "expected_output": "The main findings regarding the impact of UV radiation and stellar winds on star formation in the Galactic Center are that they should be inhibiting the star-formation process. However, despite these inhibiting factors, young stars are still found in the region. The exact methods used to obtain these results are not specified in the given context, but it is mentioned that advances in star-formation and dynamical models in galactic nuclei have helped explain the occurrence of young stars in the Galactic Center.",
      "context": [
        " orbiting stars (CW) , . also propose the existence of a second disk of counter-clockwise orbiting stars (CCW), which, however, appears to be less populous. The existence of disk-like structures, which can be traced even within the innermost S cluster , suggests the association of star-formation in the Galactic Center with the accretion disks that formed during the previous enhanced accretion activity of Sgr A*.The mechanism which leads to the presence of young stars with the age of {{formula:96356fc7-d773-4b47-98d8-84df31bd04fc}} is still not completely understood because there are several factors in the Galactic Center environment that should be inhibiting the star-formation process, such as intense UV radiation and stellar winds, large turbulence, enhanced magnetic field, and the tidal forces from the SMBH . This problem is known as the “Paradox of youth\" , though with the advance of star-formation and dynamical models in galactic nuclei, it has been possible to explain the occurrence of young stars in the Galactic Center, see e.g. for a review.Three plausible scenarios are generally discussed:in-situ star-formation approach, in which stars form",
        ". for a review.Three plausible scenarios are generally discussed:in-situ star-formation approach, in which stars form in the gravitationally unstable accretion disk or within an infalling molecular cloud that gets shocked due to collisions with the surrounding gas or is tidally compressed , , , , , , , , ; in-spiral star formation approach which claims that the star cluster first starts forming outside the central parsec, e.g. within the circum-nuclear disk or even further away in the central molecular zone. The presence of an intermediate-mass black hole (IMBH) can shorten the dynamical friction timescale, which needs to be at most a few million years long in order for stars to be still young when they settle around the SMBH , , , , , , ; rejuvenation scenario, which proposes that late-type stars have undergone collisions with other stars, a dense accretion disc, or a jet, which caused their colder outer layers to be stripped off and made them appear hotter and younger than they actually would be , , , . In addition, late-type stars aligned with an accretion disk accrete material from it, which also leads to their rejuvenation by gaining fresh hydrogen .Apart from the cluster of young stars",
        " of {{formula:80492f9b-1de0-4d90-a72f-2ccdc3b267a7}} , , , , , , , , at the center of the Milky Way. The thermal Sgr A West region or the mini-spiral , is detected as a distinct filamentary structure in near- and mid-infrared bands , , , as well as in the mm- and radio domain , , . It is composed of three clumpy streamers orbiting around Sgr A* that consist of dust and ionized and atomic gas with the dust temperature of {{formula:416d9363-644f-475c-b308-2f0cde447606}} K and the gas temperature of {{formula:991a80a0-6694-4e56-ad61-bdc56afb0989}} K , .A surprisingly large number of massive young stars with the age of a few million years resides in the Milky Way's innermost half parsec (pc). They form at least one disk-like structure of clockwise orbiting stars (CW) , . also propose the existence of a second disk of counter-clockwise orbiting stars (CCW"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between II Z_E^10 surfaces and discriminant 14 cubic fourfolds, and how do the main findings of this research paper explain their connection?",
      "expected_output": "The research paper explains that a special cubic fourfold of discriminant 14 contains a surface S not homologous to a complete intersection. Specifically, it discusses the relationship between surfaces of type II Z_E^10 and discriminant 14 cubic fourfolds, stating that a smooth open subset U of Hilb_{P^5}^{P(t)} of dimension 58 parametrizes surfaces of type II Z_E^10. The paper also notes that if X is a special cubic fourfold of discriminant 14 containing a quartic scroll Σ and a quintic Del Pezzo surface D, then Σ = 3h^2 - D in H^4(X, Z), illustrating the connection between these surfaces and the cubic fourfolds.",
      "context": [
        "=\\pm 1 and a\\in \\mathbb {Z}. Then 10=Sh^2=3a+5b, and thus we see that b=-1 and a=5. Hence, S=5h^2-D in H^4(X,\\mathbb {Z}).Note that a threefold Y as in the statement must be non-degenerate, since S is. By , a non-degenerate smooth quintic threefold Y\\subseteq \\mathbb {P}^5 is a so-called Castelnuovo threefold. This is a quadric bundle over \\mathbb {P}^1 with 15 singular fibers. Moreover, its ideal sheaf fits in the short exact sequence0\\rightarrow \\mathcal {O}_{\\mathbb {P}^5}(-4)^{\\oplus 2}\\rightarrow \\mathcal {O}_{\\mathbb {P}^5}(-3)^{\\oplus 2}\\oplus \\mathcal {O}_{\\mathbb {P}^5}(-2)\\rightarrow \\mathcal {I}_{Y/\\mathbb {P}^5}\\rightarrow 0",
        "2)\\rightarrow \\mathcal {I}_{Y/\\mathbb {P}^5}\\rightarrow 0.From this resolution it is clear that Y is (2,3)-linked to a \\mathbb {P}^3, e.g., there exist quadric and cubic hypersurfaces Z and X^{\\prime } in \\mathbb {P}^5, respectively, such that Z\\cap X^{\\prime } =Y\\cup \\mathbb {P}^3. In particular, both Z and X^{\\prime } are necessarily singular. Since a surface S of type ^{II} Z_{E}^{10} is contained in a single quadric hypersurface of \\mathbb {P}^5, and this quadric is smooth, we see that S cannot be contained in Y. In particular, S and D are never linked in X via a smooth quintic Y.\nSpecial cubic fourfolds of discriminant 14: A special cubic fourfold is a smooth cubic hypersurface X\\subseteq \\mathbb {P}^5 containing a surface S not homologous to a complete intersection. The saturation of the rank 2 sublattice of H^",
        "b}_{\\mathbb {P}^5}^{P(t)} parametrizing surfaces of type Z_{B}^{10} and ^{II} Z_{E}^{10}, respectively. We thus have a smooth open subset U of Hilb_{\\mathbb {P}^5}^{P(t)} of dimension 58 parametrizing surfaces of type ^{II} Z_{E}^{10} by Lemma REF .If X is a special cubic fourfold of discriminant 14 containing a quartic scroll \\Sigma and a quintic Del Pezzo surface D, then it is easy to see that \\Sigma =3h^2 -D in H^4(X,\\mathbb {Z}). In fact, there exists a Segre threefold \\mathbb {P}^1 \\times \\mathbb {P}^2\\subseteq \\mathbb {P}^5, which has degree 3, that satisfies (\\mathbb {P}^1 \\times \\mathbb {P}^2)\\cap X= \\Sigma \\cup D by . The relation between surfaces of type ^{II} Z_{E}^{10} and"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do Wigner functions play in understanding the quantum-classical transition, and how do the research findings contribute to this field?",
      "expected_output": "Wigner functions play a crucial role in understanding the quantum-classical transition by providing a description of quantum systems in phase space, allowing for a reformulation of quantum mechanics with strong formal analogies to classical mechanics. The research findings contribute to this field by deriving master equations for Wigner functions with spontaneous collapse, which helps explain the emergence of classicality from quantum mechanics and provides a self-consistent dynamical theory in phase space. Specifically, the study tests David Albert's suggestion that stochasticity induced by spontaneous collapses is responsible for the emergence of thermodynamic irreversibility, although the simulations reveal that GRW-type perturbations do not lead to thermodynamic behavior if it is not already present.",
      "context": [
        "# arXiv Paper: 2106.00137\n\nMaster equations for Wigner functions with spontaneous collapse and their relation to thermodynamic irreversibility\nabstract: Wigner functions, allowing for a reformulation of quantum mechanics in phase space, are of central importance for the study of the quantum-classical transition. A full understanding of the quantum-classical transition, however, also requires an explanation for the absence of macroscopic superpositions to solve the quantum measurement problem. Stochastic reformulations of quantum mechanics based on spontaneous collapses of the wavefunction are a popular approach to this issue. In this article, we derive the dynamic equations for the four most important spontaneous collapse models - Ghirardi-Rimini-Weber (GRW) theory, continuous spontaneous localization (CSL) model, Di\\'osi-Penrose model, and dissipative GRW model - in the Wigner framework. The resulting master equations are approximated by Fokker-Planck equations. Moreover, we use the phase-space form of GRW theory to test, via molecular dynamics simulations, David Albert's suggestion that the stochasticity induced by spontaneous collapses is responsible for the emergence of thermodynamic irreversibility. The simulations",
        "6}} . Since the full width at half maximum of this Gaussian is {{formula:023a1c14-48b7-41b7-a638-044763a87cff}} kg m/s (which is extremely small), this assumption should also be unproblematic.\nConclusion: This article has two central results. First, we have derived master and Fokker-Planck equations for the Wigner function based on the GRW theory, the CSL model, the Diósi-Penrose model, and the dissipative GRW model. This provides a self-consistent dynamical theory in phase space that allows to explain the emergence of classicality from quantum mechanics regarding both the emergence of Liouville dynamics and the solution of the quantum measurement problem. Moreover, it makes the advantages of the Wigner framework available to researchers interested in spontaneous collapse models. Second, we have used Langevin equations derived from the GRW Fokker-Planck equation for a numerical test of Albert's suggestion that GRW collapses might be responsible for the approach to thermodynamic equilibrium. The simulations reveal that stochastic GRW-type perturbations do not lead to thermodynamic behavior if it is not already present",
        " suggestion that the stochasticity induced by spontaneous collapses is responsible for the emergence of thermodynamic irreversibility. The simulations show that, for initial conditions leading to anti-thermodynamic behavior in the classical case, GRW-type perturbations do not lead to thermodynamic behavior. Consequently, the GRW-based equilibration mechanism proposed by Albert is not observed.\nIntroduction: Wigner functions provide a description of quantum systems in phase space. Since their initial development , they have found a significant number of applications , , , in fields such as atomic physics , quantum optics , , , visualization of quantum effects , , computational electronics , , , and solid-state theory , , , , . Besides these practical aspects, they are also of interest for fundamental questions in quantum mechanics such as the theory of quantum chaos . Moreover, they have a natural connection to statistical mechanics , , including nonequilibrium descriptions such as the Boltzmann equation , , and the Fokker-Planck equation , , . In particular, it has been suggested that Wigner functions might be useful in understanding the origin of thermodynamic irreversibility .Since they allow to describe quantum mechanics in a way that has strong formal analogies to classical mechanics (Wigner functions are"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the Polyakov loop play in the Nambu-Jona-Lasinio model's predictions, and how does it impact the overall results?",
      "expected_output": "The Polyakov loop plays a significant role in the Nambu-Jona-Lasinio (NJL) model's predictions, particularly at finite temperatures and densities. However, in the vacuum, its effects vanish, making the predictions of the NJL model identical to those of the Polyakov-Nambu-Jona-Lasinio (PNJL) model. This means that for vacuum properties, such as the pion mass and decay constant, the Polyakov loop does not impact the results. For example, the pion mass and decay constant are reproduced in the model by tuning parameters with an ultraviolet cutoff, without the influence of the Polyakov loop.",
      "context": [
        "iral condensate in the vacuum {{formula:aa1b6db9-ddf4-4656-8729-14a72cc1fd56}} as a function of {{formula:6a7151fb-b662-4c35-88e0-43a6ddc0b1db}} in the chiral limit and at the physical point in the vacuum i.e. {{formula:d9617dd9-875b-455d-ad37-eee2b531e93a}} in the PNJL model. Note that here and in the following {{formula:e385eda2-9b3a-4436-87b4-2e843e375cf9}} is measured in units of the pion mass for {{formula:af257ff3-ab15-4d70-9656-3d26f72f8e18}} , i. e. {{formula:f1af7152-a426-459b-bce0-a32708540c5d}} MeV. Since the effects of the Polyakov vanish in the vacuum this is also the prediction of the NJL",
        "a-2a7704e99807}} separately. For {{formula:20d28952-aa77-4f1f-903f-db26f7cf3db7}} , one normally choses an ultraviolet cutoff {{formula:b27c7b52-d38a-40d1-8445-57fc360ccd90}} and tunes the parameters {{formula:5f7bf2c4-0c62-4fc9-aaf6-c010c20a8e1f}} and {{formula:d1d59c67-78cf-4092-8dfa-17663f9559c2}} such that one reproduces the pion mass {{formula:8f0401da-042e-4583-b177-86447eb9e961}} and the pion decay constant {{formula:45762f8d-9e92-45e9-9455-b1ae4107e940}} in the vacuum. For {{formula:046faf02-ad75-48ca-9f38-a9de397aca22}} , we have no experiments",
        " by {{formula:6efbef96-55f1-42a1-a736-98716bededf2}} , where {{formula:40d8ef57-62a0-4b50-8c95-9ba60361a292}} MeV, i. e. the {{formula:532105ff-afc8-4cbb-a21e-4f8bd5ddaa40}} values of the pion mass and the pion decay constant. With an ultraviolet cutoff of {{formula:ebcc7f39-e361-4a6b-b61d-8cb5597690d7}} MeV, this gives {{formula:b007a90b-4840-4787-9d6b-9d98c75c15cd}} (GeV){{formula:8e06a89e-5766-4553-bfac-5515deadb7a9}} and {{formula:6562ea21-9fbc-4e15-ba49-d9532728fccc}} MeV at the physical point and {{formula:0a45"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the device's role in measuring gaze and accommodation, and how does it contribute to the research findings?",
      "expected_output": "The device measures gaze and accommodation using 4 Purkinje reflections, with PR1&2 for gaze and PR3&4 for accommodation. It contributes to research findings by predicting accommodation from 4 diopters to 1 diopter with better than 0.25D accuracy, making it promising for ophthalmic applications and near-eye displays.",
      "context": [
        "# arXiv Paper: 2304.01296\n\nDynamic accommodation measurement using Purkinje reflections and ML algorithms\nabstract: We developed a prototype device for dynamic gaze and accommodation measurements based on 4 Purkinje reflections (PR) suitable for use in AR and ophthalmology applications. PR1&2 and PR3&4 are used for accurate gaze and accommodation measurements, respectively. Our eye model was developed in ZEMAX and matches the experiments well. Our model predicts the accommodation from 4 diopters to 1 diopter with better than 0.25D accuracy. We performed repeatability tests and obtained accurate gaze and accommodation estimations from subjects. We are generating a large synthetic data set using physically accurate models and machine learning.\nIntroduction: As eyes look at different points in space, some processes are going on in them. The effective focal length of the eyes is adjusted by a change in the shape of the human crystalline lens controlled by ciliary muscles. This process is called accommodation and the distance where the eyes are focused is called accommodation depth . {{figure:d291d4a9-c01c-4854-aa76-16d7fa14e35a}}In addition, eyes converge to fixate upon points in space",
        " one subject using our ML algorithm are shown in Figure REF . {{figure:a15edf26-d227-49f6-8c58-3afd59915c9c}}\nConclusion: In this study, we proposed a prototype device for dynamic gaze and accommodation measurements based on 4 Purkinje reflections (PR). Our model predicted the accommodation from 4 diopters to 1 diopter with RMSE of lower than 0.2D for subjects. The results show that the proposed device is promising especially in measuring accommodation depth and vergence in ophthalmic applications and near-eye displays.\nFunding: This project is sponsored by the European Innovation Council’s HORIZON-EIC-2021-TRANSITIONCHALLENGES Program, Grant Number 101057672 and Tübitak’s 2247-A National Lead Researchers Program, Project Number 120C145.",
        "-aa76-16d7fa14e35a}}In addition, eyes converge to fixate upon points in space. The distance between a specified point and the eyes is called vergence distance. Accommodation and vergence responses follow each other. Even if they follow each other, there are some mismatches in some cases. This is called vergence-accommodation conflict (VAC) . So, even if accommodation is somehow predicted by vergence information, there are mismatch cases like 3D films and monocular applications. In ophthalmology, eye accommodation depth is measured for various applications such as the diagnosis of presbyopia and anisometropic amblyopia.In this work, we developed a prototype device for dynamic gaze and accommodation measurements based on 4 Purkinje reflections (PR) suitable for use in AR and ophthalmology applications. PR1&2 and PR3&4 are used for accurate gaze and accommodation measurements, respectively.\nPurkinje Images: Their Formation, Relation to Accommodation and Vergence: Purkinje images (or reflections) are reflections from various layers of the human eye. Four of these Purkinje reflections can be captured and specified clearly. There are named P1, P2"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main conditions for effective distribution representation and how do they impact the research findings?",
      "expected_output": "The main conditions for effective distribution representation, as stated in Theorem 3.1, include necessary conditions on the vector parameter, such as conditions on formula:85883fd8-c5cc-4167-a344-3ff0f3b74419, which implies formula:1d4326c4-2cce-4a1d-a60c-fbe12c825822. These conditions impact the research findings by ensuring that the distribution admits a representation over a specific space, allowing for the writing of the distribution as in Eq.(REF). For example, given formula:7953c5da-5357-497c-a9fb-f6eaf50cc370, we can write it as in Eq.(REF), and vice versa, Theorem REF does not provide any condition on formula:fe2aa6ff-5fd0-4033-afdd-8c0e68fd0a31 for formula:f4c70ae5-3884-4921-a67a-9d13497bb08b such that formula:57708037-2de2-4c36-87bd-4e8200c88bf9 represents a density function over formula:81cf896e-7d10-4411-96e7-c51cf32e5e87.",
      "context": [
        "}} .Theorem 3.1 Any distribution {{formula:47ddb70b-c43a-416a-a77b-61e6f3a3874e}} admits the following representation over {{formula:77d2b101-9e38-4e65-98a8-1990e7fbd763}} {{formula:114f1765-ec0d-422d-ad44-9786fe7ebdf4}}where {{formula:e6631b4c-e53e-4a9d-8e59-660e4e35860b}} , {{formula:1a55d417-c916-451c-8d21-1810ca948a7f}} , {{formula:329c3dc0-c788-4ee3-a169-8dfcec423b09}} and {{formula:42d38464-0fde-42a6-b951-e0d3cf812a48}} .Necessary conditions for {{formula:13b41e92-d3c8-4206-a8a4-28",
        " admits the following representation over {{formula:f6b69dfa-de10-42de-8d6f-8149a035e5bc}} {{formula:3d147f9e-77ab-4117-ab0b-675f454e5ed4}}We observe that given {{formula:7953c5da-5357-497c-a9fb-f6eaf50cc370}} we can write it as in Eq.(REF ). Vice versa Theorem REF does not provide any condition on {{formula:fe2aa6ff-5fd0-4033-afdd-8c0e68fd0a31}} for {{formula:f4c70ae5-3884-4921-a67a-9d13497bb08b}} such that {{formula:57708037-2de2-4c36-87bd-4e8200c88bf9}} represents a density function {{formula:69eddc22-d53a-41f8-9ebe-07b581721d94}} over {{formula:81cf896e-7d10",
        "-4411-96e7-c51cf32e5e87}} can be written as {{formula:f2624b41-d0ad-47ac-bf9e-ff5a79d64d42}} .If {{formula:4d596810-9f50-43b6-ba11-7d57a58f286e}} is a distribution in {{formula:a17e93b2-3ad0-4059-b039-de9b6cbc0e78}} , the vector parameter {{formula:ccd4168c-b4af-4e7e-b9c4-b70a98be83f4}} must satisfy the following necessary conditions:{{formula:a5033e80-4b40-4219-9bd0-b62bb909aa23}} . The condition {{formula:85883fd8-c5cc-4167-a344-3ff0f3b74419}} implies {{formula:1d4326c4-2cce-4a1d-a60c-fbe12c825822"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the role of Chernoff-Hoeffding inequality in understanding expert set generalization errors, and how does it impact the main findings of this research paper?",
      "expected_output": "The Chernoff-Hoeffding inequality plays a crucial role in understanding expert set generalization errors by providing a bound on the probability of the error exceeding a certain threshold. In the context of this research paper, it is used to derive the expected number of experts of each expert set and to prove the non-existence of wrongmaxpi. The inequality helps to establish a relationship between the number of experts and the generalization error, allowing the researchers to analyze the impact of varying the number of experts on the overall generalization errors over time, as shown in Fig. REF. This has significant implications for the main findings of the paper, as it provides a theoretical foundation for understanding the behavior of expert sets and their generalization errors.",
      "context": [
        "-4a8c-a201-ba0cf0615203}} makes {{formula:4173ede1-4767-4afe-b132-f4ea6a4d1e16}} with probability at least {{formula:797da3ac-478f-41c2-bcbf-dc163fa011de}} .Given {{formula:be06ae41-df33-450a-a286-a32b573a475a}} experts in total, the expected number of experts of each expert set {{formula:d04f5fbc-8dc2-41ea-b20e-2963170a6ee6}} is {{formula:f344cd2e-f3d8-4ee3-a5c6-920354b5fd1c}} . Let {{formula:94d7c7a4-9385-4bf1-8e52-f26624cdef03}} . Then, by Chernoff-Hoeffding inequality, we obtain {{formula:85fabfe8-a31a-4db4-ac09-b46b09d563c9}}To",
        "oration by contradiction. Assume there exist two experts {{formula:ce544c82-45c2-4111-bee6-e11a0288d5e5}} and {{formula:d12cc5b2-260e-46ed-b6ee-3f05db4e9b3e}} such that {{formula:b254670a-b8a8-44e8-8b2c-d6baf9417b42}}which is equivalent to {{formula:c9773b9a-a5f0-42b5-8f22-9dd39438fe40}}because of {{formula:223e72db-ea16-4fb8-a0e2-825d11663463}} and {{formula:f15c52b2-deee-48eb-83f4-8b0c7538ce56}} based on the definition of expert set {{formula:232a1034-fbbe-4125-9e2a-4be199c59163}} in Mn. Then we prove wrongmaxpi does not exist at {{formula:a",
        "f-c71f-4e23-afe8-9fcc79e9e0ff}} , {{formula:75273016-0570-4652-b6c9-ca590fecc3fd}} task types, {{formula:14ccc677-9afc-4763-a840-046cc87e4b3b}} , {{formula:7149fd07-b41e-4302-b0ab-28c36e57ec1e}} , {{formula:f3f3f310-37f9-45cf-a3b4-1491cd084baf}} , {{formula:368521d1-a4ea-4034-8ed5-c19ac5d635c7}} , and {{formula:81a25c11-4db4-4e8a-9dc2-fc22819a0222}} . We vary the number of experts {{formula:ca43be34-518a-4a29-8b45-cf487836c556}} in Fig. REF to compare the overall generalization errors over time of"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about graphene's time-reversal symmetry and its potential applications in AI?",
      "expected_output": "The research paper discusses the time-reversal symmetry in graphene, which is expected to persist in the absence of a magnetic field. The paper finds that the ultimate low-energy physics of graphene is described by the orthogonal Wigner-Dyson class AI, and that Coulomb interactions play a strong role. The authors also analyze the disorder class in graphene and find that it possesses time-reversal invariance and spin rotational symmetry. The potential applications of these findings in AI are not explicitly stated in the paper, but the understanding of graphene's symmetry properties can contribute to the development of new materials and technologies, including those used in artificial intelligence.",
      "context": [
        ". (REF )–(). We emphasize, however, that in the current situation of zero-magnetic field graphene experiments, , , , , only the physical time-reversal symmetry [Eq. ()] presumably persists, and the ultimate low-energy, asymptotic physics must be described by the orthogonal Wigner-Dyson class AI. Moreover, the Coulomb interactions are expected to play a strong role, as we will demonstrate in this paper.We now complete the formal setup of our graphene field theory. We employ the zero-temperature, imaginary time path integral formalism, implementing replicas {{formula:bd333417-b48a-4298-b5e8-bc61614e014e}} in order to average over the disorder. (The index {{formula:73f065f2-fc89-4b14-9c0d-bf09e05d9091}} , with {{formula:ef278abf-b8aa-4340-96e2-ce85c321ab55}} at the end of the calculation). The post-disorder averaged, imaginary time, coherent state path integral is {{formula:e00839",
        "formula:7fb6a522-8339-4b34-92ef-ea052c9c804f}} nonzero) describes a critical, delocalized phase possessing extended wavefunctions near zero energy. , This disorder class (in the absence of interactions) was discussed specifically in the context of graphene in Ref. OstrovskyGornyiMirlin1; these authors argued that the conductance of the non-interacting CI class model should be of order the conductance quantum, and independent of the disorder strength.\nGraphene: generic disorder: At last, we turn to an analysis of our large-{{formula:57c8c460-1511-4219-ae2d-a99e911cbd61}} graphene field theory, characterized by the full one-loop RG flow equations (REF ). In the absence of any fine-tuning of the disorder potentials, our graphene model possesses time-reversal invariance [Eq. ()] and (physical) spin {{formula:0ae35a49-abfa-464d-ab31-245f69f91227}} rotational symmetry in every disorder realization. This system is in the standard",
        " experiments of Refs. Geim1,Geim2,Kim,MorozovNovoselov,Kim2. One expects the potential arising from these impurities to vary relatively slowly on the graphene lattice scale, and therefore to manifest itself primarily through the effective scalar disorder potential {{formula:e6e2c0ce-eddc-414a-8068-5d62b7df7998}} in the low-energy theory [Eqs. (REF )–(REF )]. We emphasize that the translation of any lattice-scale disorder potential into the single effective disorder field {{formula:0b685038-6e11-4c60-83e9-37c35695acb3}} is typically impossible, and all disorder types consistent with the underlying symmetry [i.e. time-reversal invariance, Eq. ()] will appear in the low-energy theory. Nevertheless, theoretically we are free to fine-tune the disorder distribution in Eqs. (REF ), (REF ), and (REF ) by setting to zero all disorder parameters except {{formula:bf5bb0f9-bd8b-4812-9442-9f32606"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the transformation e^Z affect the inclusion of c_0T^{EPS}_0 in Q, and what are the main findings related to this process in the research paper?",
      "expected_output": "default",
      "context": [
        " ). Since the subsequent transformation by e^Y does not produce any new term containing b_5, we find that the b_0-dependent part of B^{[2]}_0 is given by (1/2\\sqrt{2})b_{5,0} ( \\partial x^+)_0, which according to the identification (REF ) is exactly equal to b_0. Therefore, the appropriate transformation is of the form e^Z withZ &=& c_0 \\check{B}_0 \\, ,\\qquad \\check{B}_0 \\equiv B^{[2]}_0 -b_0 \\, .A simple calculation then yieldsQ^{[3]} &\\equiv & e^Z Q^{[2]} e^{-Z} = Q^{[2]} + c_0 T^{EPS}_0 -c_{5,0} ({\\cal P}_5 - X_{\\tilde{p}}{\\cal R}_{5p})_0 \\, .Note that apart from the desired term c_0T^{EPS}_0, the above transformation also generated the last term, which comes from -c_0 \\left",
        "^{EPS}_0, the above transformation also generated the last term, which comes from -c_0 \\left\\lbrace b_0,Q^{[2]}\\right\\rbrace = -c_{{\\tilde{5}},0} \\left\\lbrace b_{5,0},Q^{[2]}\\right\\rbrace . It subtracts off from the term c_5 ({\\cal P}_5 - X_{\\tilde{p}}{\\cal R}_{5p}) in Q^{[2]} the part proportional to c_{5,0}. This turns out to be quite significant. As already pointed out at the beginning of this subsection, the zero mode part of the term -4c_{\\tilde{5}} \\partial x_5^+=-ip^+c \\partial x^- in c_{\\tilde{5}}{\\cal P}_5 must be removed in order to obtain the correct structure we need in {\\tilde{Q}} of (REF ). What we have found is that the subtraction of b_0 from B^{[2]}_0 needed to correctly generate the c_0T^{EPS}_0 structure simultaneously",
        "c_{\\tilde{p}}d_p \\, ,which, apart from a factor of 2, is very similar to the transformation used in to connect \\hat{Q} with the Berkovits' BRST-like operator Q_B. Although rather tedious, the calculation is straightforward. It turns out that the net result is simply to remove c_{\\tilde{p}} from Q^{[1]}. Thus we obtainQ^{[2]} &\\equiv &e^Y Q^{[1]} e^{-Y} = Q + c_{\\tilde{5}}({\\cal P}_5 -X_{\\tilde{p}}{\\cal R}_{5p}) -iX_{\\tilde{P}}{\\cal P}_P + {i \\over 2} X_{\\tilde{P}}X_{\\tilde{Q}}{\\cal R}_{PQ} \\, .At this stage, let us try to generate the c_0T^{EPS}_0 structure appearing in (REF ). This step was not needed in the bosonic string case, since the BRST operator already contained such a term. On the other hand, in the EPS"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main improvements of NNPDF3.0 in PDF analyses and how do they impact the research findings?",
      "expected_output": "The main improvements of NNPDF3.0 in PDF analyses include a more efficient code rewritten in C++, over a thousand new data points from HERA II and LHC, and a refined fitting methodology validated by statistical closure tests. These improvements impact research findings by reducing the uncertainty of theoretical predictions at hadron colliders, particularly for key processes such as the characterization of the Higgs boson and BSM searches. For example, the inclusion of ATLAS high-mass Drell-Yan production data and transverse momentum distribution of W and Z bosons provides important information on quark-antiquark separation and gluon distribution, respectively.",
      "context": [
        "# arXiv Paper: 1407.3122\n\nTowards the NNPDF3.0 parton set for the second LHC run\nabstract: The full exploitation of the increasingly precise LHC measurements is essential in order to reduce the uncertainty of theoretical predictions at hadron colliders. The NNPDF2.3 fit was the first PDF determination including the effect of the early LHC data. Here the new NNPDF3.0 PDF set is announced and its main features are presented. The novel NNPDF analysis is based on an improved fitting methodology, statistically validated by closure tests. Over a thousand new data points are included, both the recent HERA II measurements and a wide set of new LHC data. In this contribution details on the experimental data are given and their impact on PDF uncertainties is displayed.\nIntroduction: Parton Distribution Functions (PDFs) play a crucial role in the LHC physics program. The PDF error is the dominant source of theoretical uncertainty for several key processes at the LHC, and therefore it limits the accuracy of both SM analyses, such as the characterisation of the Higgs boson, and BSM searches, particularly those involving heavy particles in the final state. The excellent performance of the L",
        "iggs boson, and BSM searches, particularly those involving heavy particles in the final state. The excellent performance of the LHC machine asks for a completely new level of precision in PDF analyses, especially in view of the high–energy run.The NNPDF collaboration has developed a methodology to obtain a reliable determination of PDFs and a robust estimate of their error, based on Monte Carlo techniques and on the use of Neural Networks. In the latest NNPDF analyses, the NNPDF2.3 and the NNPDF2.3QED , several LHC measurements have been included for the first time in a fit. Their impact was found to be significant, and particularly crucial to determine the photon PDF. The leading order NNPDF2.3QED fit is the new default PDF set in Pythia 8, used in the recent Monash tune .The NNPDF3.0 analysis represents a major upgrade. First of all, the fit is based on a more efficient code completely rewritten in c++. Most importantly, over a thousand new data points are analysed, both from HERA II and from the LHC. The significant increase in the number of experimental data is accompanied by a refined fitting methodology both based on and validated by a statistical closure",
        " the early measurements, the impact of the LHC data turned out to be moderate but non negligible. In the past few months, several new measurements have been made available by the LHC experiments, based on a larger statistic sample and with smaller systematic uncertainties. They are expected to provide a unique handle on PDFs. On top of the vector boson production data from ATLAS, LHCb and CMS that were already present in NNPDF2.3 we include: the ATLAS high-mass Drell-Yan production data, based on an integrated luminosity of 4.9 fb{{formula:28ad2cf7-319f-4bae-8f22-a681927bed43}} , which give important information on the large-{{formula:fcea78da-afc2-4b8a-ad8d-d0ff5196d898}} quark-antiquark separation; the transverse momentum distribution of {{formula:7268ea68-edc0-4fa0-ade6-8bc3feab1b24}} bosons measured by ATLAS, which we expect to provide a complementary constraint on the gluon in the medium-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the SSformer model use to reduce complexity, and how do these methods contribute to its main findings and key results?",
      "expected_output": "The SSformer model uses several methods to reduce complexity, including a decoder that aggregates information from different layers to obtain both local and global attentions, and a hierarchical design inspired by Swin Transformer. These methods contribute to its main findings, which show that SSformer yields comparable mIoU performance with state-of-the-art models, while maintaining a smaller model size and lower compute complexity. For example, the paper states that Swin Transformer's compute complexity is linearly growing with the size of the image, whereas the original Vision Transformer's complexity is of quadratic growth. By building upon this idea, SSformer achieves a balance between performance and efficiency, making it a lightweight yet effective transformer model for semantic segmentation.",
      "context": [
        " complexity, which limited its wider appliance .\nSwin Transformer : The emerging of Swin Transformer effectively fills the gap between Transformer and semantic segmentation, by the reason that Swin Transformer adds an inductive bias to make Transformer better adapt to dense prediction tasks of semantic segmentation. It disperses more attention to the connection of each patch by the application of shifted window and patch merging.\nSETR : SETR deploys an original transformer to encode images as a sequence of patches. ViT is the first work showing that a pure Transformer based image classification model can yield the state-of-the-art performance, while SETR is the first work to show that a pure Transformer can also be the-state-of the-art structure in image segmentation. SETR further proves that Transformer can make advancement in the field of semantic segmentation. However, due to the use of original Transformer and redundant decoders, SETR has huge compute complexity and vast model parameters, which are often unacceptable in practice. As a result, the main purpose of our work is to reduce compute complexity and model parameters.\nThe Proposed SSformer: In this section we will introduce SSformer, our simplified but still powerful semantic segmentation",
        "# arXiv Paper: 2208.02034\n\nSSformer: A Lightweight Transformer for Semantic Segmentation\nabstract: It is well believed that Transformer performs better in semantic segmentation compared to convolutional neural networks. Nevertheless, the original Vision Transformer may lack of inductive biases of local neighborhoods and possess a high time complexity. Recently, Swin Transformer sets a new record in various vision tasks by using hierarchical architecture and shifted windows while being more efficient. However, as Swin Transformer is specifically designed for image classification, it may achieve suboptimal performance on dense prediction-based segmentation task. Further, simply combing Swin Transformer with existing methods would lead to the boost of model size and parameters for the final segmentation model. In this paper, we rethink the Swin Transformer for semantic segmentation, and design a lightweight yet effective transformer model, called SSformer. In this model, considering the inherent hierarchical design of Swin Transformer, we propose a decoder to aggregate information from different layers, thus obtaining both local and global attentions. Experimental results show the proposed SSformer yields comparable mIoU performance with state-of-the-art models, while maintaining a smaller model size and",
        "former yields comparable mIoU performance with state-of-the-art models, while maintaining a smaller model size and lower compute.\nIntroduction: The field of computer vision and multimedia has been largely changed since Transformer was applied in image or video from a sequence-to-sequence perspective. It challenged the dominance of convolution neural network in this field . However, simply applying Transformer into semantic segmentation brings huge time and space complexity, which limits its wider appliance.To reduce computational complexity, in Swin Transformer, the number of tokens is reduced by patch merging layers as the network gets deeper, and the self-attention calculation is limited to the non-overlapping local window. Generally speaking, Swin Transformer is linearly growing with the size of the image while the compute complexity of the original Vision Transformer (ViT) is of quadratic growth .Swin Transformer is able to serve as a general-purpose backbone for computer vision. It has already been applied in detection, classification and segmentation and obtains strong performances. When Swin Transformer is applied to semantic segmentation, it is usually equipped with Upernet as its decoder, which can bring the mIoU value as high as 53.5"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the CATFISH simulation model replicate black hole events, and what are the key findings from this research?",
      "expected_output": "The CATFISH simulation model replicates black hole events by incorporating theoretical results from the literature, including inelasticity effects, exact field emissivities, and corrections to semiclassical black hole evaporation. The key findings from this research are that CATFISH allows for the most accurate description of black hole events at the TeV scale to date, and its flexibility enables the comparison of signatures from different theoretical models of black hole production. For example, the paper mentions that CATFISH can simulate black hole production in ultrahigh-energy cosmic ray air showers and in lepton colliders, and that precompiled executable versions are available for Linux and Mac OS platforms.",
      "context": [
        " BHs, thus allowing their detection. The spectrum of massive excitations in string theories suggests that branes may also decay thermally . Under the most favorable circumstances, the BH event rate at the LHC should be comparable to the {{formula:5dfb8163-19bb-43b9-bc6c-78dd4680cb5d}} event rate.Until now, numerical studies of observational signatures have used Monte Carlo (MC) generators implementing the semiclassical picture outlined above. Currently, there are two MC generators for BH production at particle colliders: TRUENOIR and CHARYBDIS . However, recent results have modified significantly our understanding of BH formation and evolution. It is thus timely and worthwile to examine the observational signatures of BH events beyond the simple semiclassical picture. To this purpose, we have developed a new Fortran MC generator for BH events at the LHC which includes many of the accepted theoretical results in the literature. The generator, called CATFISH (Collider grAviTational FIeld Simulator for black Holes), interfaces to the PYTHIA MC fragmentation code using the Les Houches interface . CATFISH allows the most accurate description of",
        " to the PYTHIA MC fragmentation code using the Les Houches interface . CATFISH allows the most accurate description of BH events at the TeV scale up-to-date. Its flexibility permits to compare the signatures of different theoretical models of BH production. MC generators with similar characteristics of CATFISH have already been successfully utilized to simulate BH production in ultrahigh-energy cosmic ray air showers and in lepton colliders . Precompiled executable versions of CATFISH (Linux and Mac OS platforms) are available at the CATFISH website http://www.phy.olemiss.edu/GR/catfish.\nBasics of BH formation and evolution: In this section we follow Ref. and briefly review the basics of BH formation and evolution.\nBH formation and cross section at parton level: Thorne's hoop conjecture states that an event horizon forms when a mass {{formula:7889341c-9699-4fc1-b454-ebe56d2dd4b7}} is compacted into a region with circumference smaller than twice the Schwarzschild radius {{formula:ee8589ff-5d8c-4a6f-9f5c-6d99",
        "# arXiv Paper: hep-ph/0609001\n\nCatfish: A Monte Carlo simulator for black holes at the LHC\nabstract: We present a new Fortran Monte Carlo generator to simulate black hole events at CERN's Large Hadron Collider. The generator interfaces to the PYTHIA Monte Carlo fragmentation code. The physics of the BH generator includes, but not limited to, inelasticity effects, exact field emissivities, corrections to semiclassical black hole evaporation and gravitational energy loss at formation. These features are essential to realistically reconstruct the detector response and test different models of black hole formation and decay at the LHC.\nIntroduction: The fundamental scale of gravity may be much lower than the measured gravitational scale . In scenarios with large or warped extra dimensions, the observed weakness of gravity is explained by assuming that Standard Model (SM) fields are constrained to propagate on a four-dimensional submanifold, whereas gravitons propagate in the higher-dimensional spacetime . If the gravitational coupling constant is of the order of few TeVs, super-Planckian events at CERN's LHC could lead to the formation of subnuclear Black Holes (BHs) and branes (For reviews"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding ASP corrections and their impact on program consistency in this research paper?",
      "expected_output": "The main findings of this research paper are that algorithms for calculating maximally satisfiable sets (MSSes) in propositional logic can be adapted to Answer Set Programming (ASP), despite ASP being non-monotone. The paper devises a technique for adapting these algorithms to maximal consistency in ASP and shows how maximal consistency can be used to calculate minimal corrections to ASP inputs. The experimental evaluation demonstrates that having a portfolio of such algorithms is beneficial, with the progression-based algorithm being the most effective overall in calculating corrections to inconsistent programs.",
      "context": [
        "# arXiv Paper: 1406.7838\n\nOn Minimal Corrections in ASP\nabstract: As a programming paradigm, answer set programming (ASP) brings about the usual issue of the human error. Hence, it is desirable to provide automated techniques that could help the programmer to find the error. This paper addresses the question of computing a subset-minimal correction of a contradictory ASP program. A contradictory ASP program is often undesirable and we wish to provide an automated way of fixing it. We consider a minimal correction set of a contradictory program to be an irreducible set of rules whose removal makes the program consistent. In contrast to propositional logic, corrections of ASP programs behave non-monotonically. Nevertheless, we show that a variety of algorithms for correction set computation in propositional logic can be ported to ASP. An experimental evaluation was carried showing that having a portfolio of such algorithms is indeed of benefit.\nIntroduction: Answer set programming (ASP) is a powerful paradigm for modeling and solving combinatorial and optimization problems in artificial intelligence. An inconsistent program is such a program that does not have a solution. This might be due to a bug in the program and for such cases it is desirable to provide the programmer with tools that would help",
        " form, an MSS is a subset of the formula's clauses that is satisfiable and adding any clause to it makes it unsatisfiable. There is also a dual, minimal correction subset (MCS), which is a complement of an MSS .There is an important difference between propositional logic and ASP and that is that ASP is not monotone. This means that an algorithm for calculating MSSes cannot be immediately used for ASP. We show, however, that it is possible to port existing MSS algorithms to ASP. This represents a great potential for calculating maximally consistent sets in ASP as a bevy of algorithms for MSS exist , , .The main contributions of this paper are the following. (1) It devises a technique for adapting algorithms from MSS computation to maximal consistency in ASP. (2) Using the technique a handful of MSS algorithms is adapted to ASP. (3) It is shown how maximal consistency can be used to calculate minimal corrections to ASP inputs. (4) The proposed algorithms were implemented and evaluated on a number of benchmarks.The paper is organized as follows. sec:mmodels introduces the concept of maximal consistent subsets and proposes a handful of algorithms for computing them. sec:mcs relates maximal consistent subsets",
        " calculated by computing a maximal consistency with respect to only slightly modified program.We show that algorithms for calculating maximally satisfiable sets (MSSes) in propositional logic can be adapted to ASP. This is an interesting result as unlike propositional logic, ASP is not monotone. For three MSS algorithms we show how they are ported to ASP (Algorithms –). A similar approach, however, could be used for other MSS algorithms.The algorithms for maximal consistency let us then calculate minimal corrections. For these corrections we assume a general scheme where rules may be added or removed in order to restore consistency. For evaluation we return to the initial motivation of the paper and that is the calculation of corrections to data that make a program inconsistent. A number of instances from various problem classes were considered. The progression-based algorithm (alg:p) turned out to be the most effective overall. Nevertheless, it was not a winner for each of the considered classes. In contrast, the maximum-cardinality approach clearly performed the worst. Here, however, we should point out that we used the default implementation of minimization in clingo and it is the subject of future work to evaluate other algorithms. (A number of MaxSAT algorithms are being adapted to Max"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the relationship between temperature and condensate volume in this research paper?",
      "expected_output": "The research paper finds that the volume fraction of the condensate decreases with decreasing temperature and vanishes on the left binodal boundary. Additionally, the BEC critical temperatures in the MP domain are shown to be dependent on the considered values of certain parameters, and the position of the triple point only slightly depends on these parameters.",
      "context": [
        " condensate decreases too and vanishes on the left binodal boundary. The horizontal lines in Fig. REF (b) show the BEC critical temperatures in the MP domain for two considered values of {{formula:6183aa71-7bda-4a1e-a07a-ba654f161374}} .\nThermodynamic relations for two–component system: Similarly to one-component matter, we take into account multiparticle interactions in the {{formula:7d00c97d-27a2-41c7-9307-d4c088d3bf08}} mixture by introducing a temperature-independent excess part of pressure {{formula:046386c3-73cf-4c8c-bfc4-3b462c195b8a}} {{formula:8c2322e0-0237-4331-9af7-8c33b6565db2}}A similar expression can be written for the free energy density, {{formula:a24c217a-bb73-45b2-af45-3ac3a5491a6c}} , after introducing the excess term {{formula:edcbcac",
        "}} increases (decreases) with {{formula:f77c235d-4a44-47e0-b472-04157017b671}} , but the position of the triple point only slightly depends on this parameter. Note that the BEC region in Fig. REF (a) extends to the right from the LGPT line and below the BEC line. According to Fig. REF (b), the BEC line in the {{formula:7d36b2be-2e57-4ac9-bda0-65883e7c5f3a}} plane is not sensitive to {{formula:0f799e98-85f5-4f8f-a7ba-a23053418a5b}} outside the MP region. It is clear that inside this region the BEC occurs only in the liquid phase, which volume fraction diminishes with decreasing {{formula:489ae4ce-7b4e-48e8-bd3b-960b9355ccb6}} . Therefore, the volume fraction of the condensate decreases too and vanishes on the left binodal boundary. The horizontal lines in Fig. REF (",
        "ula:0100f2af-8a6f-41b3-b7cb-6eb5fa01649a}} (set A) and {{formula:a8241a8e-30b7-4c73-b3c1-ed5335a7b538}} (set B) GeVfm{{formula:6dc6be44-e5ab-4f0e-bcd2-1426987a8f1b}} .\nPhase diagram of interacting {{formula:51367af0-710f-42e7-9f1a-12c99a9411d6}} matter: In this section we consider the EoS of the chemically equilibrated {{formula:5d70103e-f94b-4c00-9305-014f0b79ce48}} mixture at nonzero temperatures. We apply explicit relations for pressure and free energy derived in Sec. REF . By solving Eq. (REF ) one can find allowable states of matter in the {{formula:5ca9143b-0377-4ef7-886c-52b830b83569}} or {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of torque reversals on spin trends in this research paper?",
      "expected_output": "The main findings regarding the impact of torque reversals on spin trends in this research paper are that the torque reversal episodes in 4U 1907+09 and 4U 1626-67 are cyclic and do not seem to be associated with a drop in luminosity or a change in the shape of the pulse profile. The proposal to explain torque reversals through accretion onto an oblique rotator can account for the long-term constancy of spin trends, the large magnitude of torque reversals, and the constancy of X-ray flux over the torque reversal. However, further monitoring is required to determine whether the observed change is a sign of a long-term torque reversal or a short deviation from a long-term spin-down trend.",
      "context": [
        " system. Finally, we also emphasize that the torque reversal in 4U 1626{{formula:f32e123b-52e2-4104-b891-2561ae3f6f48}} 67 was from a spin up to a spin down, while for 4U 1907+09 it was from a spin down to a spin up. Since the torque reversal episodes are cyclic, however, we do not expect this latter difference to be of major importance.In conclusion, the torque reversal of 4U 1907+09 with no associated drop in luminosity and no change in the shape of the pulse profile seem to be difficult to reconcile with models explaining the large {{formula:2ddc094f-680f-4d32-b22f-551dbe77c150}} through the presence of a retrograde disk. On the other hand, the proposal of to explain torque reversals through accretion onto an oblique rotator seems to be able to explain the long term constancy of {{formula:71d614e0-60c9-4d29-bf31-30f55f70c186}} trends in 4U 1907+09 and 4U 1626{{formula:",
        "-30f55f70c186}} trends in 4U 1907+09 and 4U 1626{{formula:3f69ee91-5dfc-4b69-a90f-9c1dd1f37302}} 67, the large magnitude of {{formula:b1bf2cce-291c-41a0-b986-01a1142d6e06}} , and the constancy of X-ray flux over the torque reversal, although further and more detailed theoretical studies are required. As a prediction of the model for systems such as 4U 1907+09 and 4U 1626{{formula:9ba12678-8de3-4b12-a9ba-f91baae0c54f}} 67 is that torque reversal episodes are rare events, further monitoring of the pulse period of 4U 1907+09 is required to determine whether this change is a sign of a long term torque reversal or whether we are just observing a short deviation from a long term spin down trend.We acknowledge the support of the Deutsches Zentrum für Luft- und Raumfahrt under grant numbers 50OR0302, 50OG9601, and",
        "-45e8-be06-3e13da506f0d}} , all observed properties of the spin history of 4U 1626{{formula:e49b06dd-9941-4e7c-8ac7-861a928b039c}} 67 including the large and values of {{formula:22c920f7-23f6-477e-b387-457a4866c81e}} before and after the torque reversal and the virtually unchanged luminosity of the source can be explained (in the model, the required change in luminosity is only {{formula:52219043-bd9c-43be-b3c6-c3c76a7a4d47}} 5%). For these model parameters, the limit cycle of 4U 1626{{formula:d1978149-200a-4703-bd3f-6808b15178c8}} 67 is predicted to have a long period.The similarity between the pulse histories of 4U 1907+09 and 4U 1626{{formula:e75bf8b4-6263-4267-823d-29f7d6722012}} 67 suggests"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of anisotropic stress on universe evolution, and how do the research methods used support these conclusions?",
      "expected_output": "The research paper finds that anisotropic stress has a significant impact on the evolution of the universe, particularly in the context of dark energy. The authors use the Einstein and conservation equations to model the evolution of the universe, assuming a flat geometry and no primordial anisotropic stress. They find that the anisotropic evolution due to dark energy and the evolution of initial metric perturbations decouple and can be computed independently. The paper also discusses the possibility of anisotropic universes with metric perturbations, which could help explain the anomalies observed in the cosmic microwave background (CMB) anisotropies. The research methods used, including the use of time-dependent spatial metrics and the assumption of no primordial anisotropic stress, support the conclusions that anisotropic stress can play a crucial role in the evolution of the universe.",
      "context": [
        " (m), which will be isotropic and pressureless, and dark energy (de), where the pressure tensor of the dark energy is given by (REF ). The Einstein and conservation equations can be written as {{formula:a76510d9-e5f4-4a82-922a-52fdad825f5f}}where {{formula:e04c5d2f-7cd6-47d6-b393-bf13dce45afb}} , {{formula:f6c8c425-5b44-481a-8979-930a0c865a2a}} and we have used a flat geometry. One might think that there would be a velocity generated , but this would only be created by the gradient of the anisotropic stress which is absent in our model. Hence, we can consistently set this to zero and ignore it. Assuming there is no primordial anisotropic stress, that is, initially {{formula:dbe5e307-a5b7-4314-998f-22678cef99e9}} , these equations can be solved to give {{formula:f621f37e-4",
        " to any particular manifestation of anisotropic dark energy, for example, as discussed in refs. , The reason for this is that, to first order, the anisotropic evolution due to {{formula:22059117-c017-4295-b038-fbca2bd39eef}} and the evolution of initial metric perturbations decouple and can be computed independently, then added at the end. If we use the time dependent spatial metric {{formula:434727d8-fe0d-42bb-987d-d7f4d491baca}} to describe the anisotropic evolution and {{formula:fdedaf32-11a0-4623-bb14-55db43fdc0ba}} to represent effect the metric perturbations created during inflation, then our approach works when {{formula:80c4a437-d565-45ae-9051-4574f68078eb}} . It should also yield qualitative information when {{formula:5261d515-2ddd-430c-9cb8-4db2ebfbf6f7}} .Anisotropic universes: We will consider universes with metric {{formula",
        " {{formula:3d5ae373-b35e-40d2-a81a-fc7c44cb7db9}} in order to achieve acceleration.Measurements of CMB anisotropies appear to show that there a number of anomalies in their detailed statistical properties , , , . These suggest that the anisotropies may not be compatible with an isotropic, Gaussian random field on the very largest scales. Given that the Fourier modes corresponding to these anisotropies crossed the horizon in the last few Hubble times, that is, when the dark energy has come to dominate the expansion of the Universe, it is tempting to connect the two.The equation of state parameter {{formula:02a1930f-0f06-4885-adb9-4b32bb6de45b}} is often considered to be a function of time, but the overall pressure tensor, {{formula:bffea025-ea09-4b54-a3bf-fa19fe52c993}} , is usually assumed to be isotropic, that is {{formula:224b6479-a6a5-4a72-b411-33b8e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on truncated Euler equations in predicting large scale flow, and how do the methods used impact the key results?",
      "expected_output": "The research paper 1507.01874 presents a numerical study of the statistical properties of three-dimensional dissipative turbulent flows at scales larger than the forcing scale. The main findings indicate that the large scale flow can be described to a large degree by the truncated Euler equations with the predictions of the zero flux solutions given by absolute equilibrium theory. This means that the functional shape of the large scale spectra can be predicted, provided that scales sufficiently larger than the forcing length scale but also sufficiently smaller than the box size are examined. The methods used, such as numerical simulations, allow for the examination of scales larger than the forcing scale, which is important for understanding many geophysical and astrophysical flows. The key results show that the truncated Euler equations can accurately predict the large scale flow, and deviations from the predictions of absolute equilibrium are discussed.",
      "context": [
        "# arXiv Paper: 1507.01874\n\nStatistical equilibria of large scales in dissipative hydrodynamic turbulence\nabstract: We present a numerical study of the statistical properties of three-dimensional dissipative turbulent flows at scales larger than the forcing scale. Our results indicate that the large scale flow can be described to a large degree by the truncated Euler equations with the predictions of the zero flux solutions given by absolute equilibrium theory, both for helical and non-helical flows. Thus, the functional shape of the large scale spectra can be predicted provided that scales sufficiently larger than the forcing length scale but also sufficiently smaller than the box size are examined. Deviations from the predictions of absolute equilibrium are discussed.\n: Statistical equilibria of large scales in dissipative hydrodynamic turbulence V. Dallas S. Fauve A. Alexakis Laboratoire de Physique Statistique, École Normale Supérieure, CNRS, Université Pierre et Marié Curie, Université Paris Diderot, 24 rue Lhomond, 75005 Paris, FranceWe present a numerical study of the statistical properties of three-dimensional dissipative turbulent flows at scales larger than the forcing scale. Our",
        " present a numerical study of the statistical properties of three-dimensional dissipative turbulent flows at scales larger than the forcing scale. Our results indicate that the large scale flow can be described to a large degree by the truncated Euler equations with the predictions of the zero flux solutions given by absolute equilibrium theory, both for helical and non-helical flows. Thus, the functional shape of the large scale spectra can be predicted provided that scales sufficiently larger than the forcing length scale but also sufficiently smaller than the box size are examined. Deviations from the predictions of absolute equilibrium are discussed.Experimental and numerical studies of three-dimensional homogeneous hydrodynamic turbulent flows have been so far mostly focused on the finite energy flux solutions of the Navier-Stokes that manifest themselves on scales smaller that the forcing scale for which the Kolmogorov cascade and intermittency take place . This is because the flows of many experiments designed to study statistically stationary turbulent regimes are forced at scales not much smaller than the size of the container. This is also the case of most direct numerical simulations (DNS) for which the flow is often forced in the largest possible modes aiming for the largest scale separation between the forcing scale and the small scales in the dissipative range. A notable exception is",
        " modes aiming for the largest scale separation between the forcing scale and the small scales in the dissipative range. A notable exception is of course the limit of two-dimensional flows for which the inverse cascade of energy leads to a negative flux of energy that excites scales larger than the forcing scale.Many flows of geophysical or astrophysical interest far from the two-dimensional limit involve spatial structures at scales larger than the forcing scale. At these scales, no energy flux is expected and the usual Kolmogorov cascade picture does not hold. This is also true for some flows involved in industrial processes, such as large scale turbulent mixing. Dynamical and statistical properties of the zero flux solutions in scales larger than the forcing scale could thus be of interest for many applications in three-dimensional hydrodynamic turbulence.Despite the lack of quantitative studies of the large scales in three-dimensional statistically stationary turbulence, it has been believed since a long time that the scales larger than the forcing scale are in statistical equilibrium (see page 209 of reference ). The argument is that the energy driving the flow is transferred from the forcing scale {{formula:fa052206-d507-45da-8cf0-2c8f50ed9afb}} to the dissipation scale {{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between edge resistances and vertex degrees in the context of this research paper, and how do the authors' methods and findings shed light on this topic?",
      "expected_output": "The research paper explores the relationship between edge resistances and vertex degrees, indicating that edge resistances between 1 and a certain value, and vertex degrees within a specific range, impact the effective resistance. For instance, when all edge resistances are set to 1 and vertices have a certain degree, the effective resistance is determined. The authors' methods involve analyzing loop-erased random walks on graphs with specific properties, shedding light on how edge resistances and vertex degrees influence the behavior of these walks.",
      "context": [
        " {{formula:0881712f-aa4c-4597-8bfe-81a7fe0a7661}} , maximum degree {{formula:d11595de-feb8-47a5-9911-dbae2f7330c0}} , and edge resistances between 1 and {{formula:0d82dd0a-44f7-4c32-8eb7-a47c4255cd0a}} , then there exists a value {{formula:8a7f032d-aa59-418a-a9e5-f95808acdcb9}} (depending only on {{formula:d821c4da-bf2e-4132-b5be-9f0ba154efde}} and {{formula:344f2c61-bc70-4bab-b937-ba333cef1440}} ) such that for every {{formula:6700e83d-d73e-4b42-872e-09315be69020}} on level 2 of {{formula:d5cf8218-2cac-4fa6-b",
        "formula:8e57f883-2b35-469a-b465-052c21b0d158}} , maximum degree {{formula:650b5f18-1aa3-424a-9a6b-59707d915593}} , and all edge resistances in {{formula:704aecaf-5e03-408a-8ecb-d8c2802f463a}} , then for a loop-erased random walk {{formula:f7dd53e5-8e2e-4d46-8dc0-6bfbaeeae5d1}} on {{formula:c7867c8f-cf7b-4af6-9714-213c78967529}} , we have {{formula:404d1f50-7e1e-4dea-9312-3755528fb431}}First note that {{formula:0381b69f-3701-48a5-90f5-80fe2a928200}}where {{formula:7d18c5af-ef18-40f0-9d79-b534",
        "formula:eebf259e-8623-498c-8b97-c91ac6dac1cb}} have degree between {{formula:d8732640-f115-4c1e-b6b8-97ac03d01fb3}} and {{formula:f0a58763-f360-4e7e-aca1-a8960cf8969c}} and all edge resistances are between 1 and {{formula:7fcc22fb-3bba-4c65-801c-62a416056015}} ), we take all edge resistances equal to 1 and assume all vertices in {{formula:4b435026-0879-430f-981f-09a44e2b286b}} have degree {{formula:4457ed07-d56e-4f46-8d88-3c4efdc58e45}} , which gives us an effective resistance equal to {{formula:18ca2695-fa3e-429e-987a-caab4ccb5afa}} . Likewise, for an upper bound on {{formula:2bf26"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how functionalities map inputs to outputs in two-party problems and what methods were used to achieve these results?",
      "expected_output": "The research paper discusses how functionalities map inputs to outputs in two-party problems by defining a random process that takes pairs of inputs to pairs of outputs. This process is denoted by a map of the form F:({0,1}^n) x ({0,1}^m) -> ({0,1}^p) x ({0,1}^q), where x denotes the input of the first party and y denotes the input of the second party. The paper uses a two-party protocol for computing the functionality F and analyzes the view of each party during the execution of the protocol. The results are achieved through tedious but straightforward computations for each pair of inputs, allowing the authors to state the main findings on how functionalities map inputs to outputs in two-party problems.",
      "context": [
        "60c}}The following concepts are well known (Section 7.2 ):A two-party problem can be cast by defining a random process that maps pairs of inputs to pairs of outputs. Such a process is called a functionality and is denoted by a map of the form {{formula:9e7c959e-8443-497b-b87c-21521dd7c22d}} , with {{formula:b791319f-12e7-4b8d-bba6-2af801221ba5}} . Here {{formula:70763ee6-45ff-4449-8384-7510698115d3}} denotes the input of first party, and {{formula:3c6fde42-bb9b-4a4f-a771-764180246adf}} denotes the input of the second party. After evaluating {{formula:d3a0d40b-3720-4b7b-8b09-0790e97eba2c}} , the first party should receive {{formula:b7b32076-6d77-49e3-a60e-d6c",
        " {{formula:b7b32076-6d77-49e3-a60e-d6c4c8257650}} and the second party should receive {{formula:af83cf2a-7af2-41d9-8726-f49790539285}} .Let {{formula:4c803bc7-9e7f-40d7-a589-011ca84d1520}} be a functionality and {{formula:07015ef2-564f-47e7-ae32-f79be8530164}} a two-party protocol for computing {{formula:eca6a2b8-392e-49c3-a5ec-ad91987788c6}} . The view of the first (resp. second) party during an execution of {{formula:63c8bb0e-f1ce-4281-b992-1c7f36910cde}} on {{formula:0359feb3-8707-49ca-9c14-bceb2083e7de}} is denoted by {{formula:a3968c21-74",
        "-502f-4cf5-99b6-998daca93f1d}} , then {{formula:72db5369-27ac-4c6c-ae8d-0ef961a5d0ad}}For the sake of simplicity, we consider the case {{formula:38ae0b6d-0add-4414-a02c-a772110c5a43}} . Then condition (REF ) implies: {{formula:f8772460-c9a0-4d7a-8494-952bb4195d1f}}Therefore: {{formula:04e6f096-dca1-4144-9907-0fc4a6fc1874}}Observe that the first condition in (REF ) implies: {{formula:7abfd4b2-8cb2-4b5d-9319-7e7187a7aa84}}After performing the above tedious but straightforward computations for each pair {{formula:45a65244-d025-47a6-956f-2f9ec4934e6f}} we are able to state the following result"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the framework address author name ambiguity in its research methodology and what are the key findings related to this issue?",
      "expected_output": "The framework addresses author name ambiguity by proposing a novel generative adversarial framework that combines two categories of models: content information and relation information. The discriminative module distinguishes whether two papers are from the same author, while the generative module selects possibly homogeneous papers directly from the heterogeneous information network. This approach eliminates the need for complicated feature engineering and labeled samples. The key finding is that the framework can effectively disambiguate author names by integrating content and relation information in an adversarial way, as shown in Figure REF.",
      "context": [
        " .\nFramework: The proposed framework is shown in Figure REF . In order to represent the information of the heterogeneous information network, we first embed content information and relation information into low-dimension representation space, where two papers are close in the feature space if they are similar. Then to integrate the content information and the relation information, and to select homogeneous papers in an adversarial way, we employ a generative adversarial module. The generative module aims to explore possible homogeneous papers from the heterogeneous information network, while the discriminative module tries to distinguish the generated negative papers and pseudo-positive papers. In such a way, the reward from the discriminative module guides the exploration for the generative module to select homogeneous papers. Moreover, the high-quality papers generated with high-order connections can make the discriminative module aware of the topology of the network. {{figure:e5b21f2a-f8dd-42fe-9bb7-97bdd5d91c04}}\nContent representation: Papers written by different authors have various topics and literary styles. We extract those content features by integrating Doc2vec module into our framework. This module learns a low dimension vector {{formula:d",
        " the network are likely to be written by the same author. Thus constructing the network becomes the critical part of these methods, e.g., paper network , paper-author network . However, either complicated feature engineering or the supervision is required.The two categories of methods are like the two sides of the same coin. The first introduces supervision but cannot process high-order connections, while the second models the high-order connections but requires the supervision. An intuitive idea is to combine them together to build a unified model which can eliminate the requirement of labeled samples and complicated feature engineering to some extent. Inspired by generative adversarial networks , we may combine the two categories in an adversarial way. In this paper, we propose a unified framework with discriminative module and generative module. The discriminative module directly distinguishes whether two papers are written by the same author based on feature vectors. This module is learned in a self-training way, and it requires negative samples generated by the generative module. The generative module works on the heterogeneous information network and selects papers viewed as the homogeneous pairs.In this framework, the discriminative module can guide the exploration of the generative module to select homogeneous papers on the raw network. And the generative module can",
        "# arXiv Paper: 2002.09803\n\nAuthor Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning\nabstract: Author name ambiguity causes inadequacy and inconvenience in academic information retrieval, which raises the necessity of author name disambiguation (AND). Existing AND methods can be divided into two categories: the models focusing on content information to distinguish whether two papers are written by the same author, the models focusing on relation information to represent information as edges on the network and to quantify the similarity among papers. However, the former requires adequate labeled samples and informative negative samples, and are also ineffective in measuring the high-order connections among papers, while the latter needs complicated feature engineering or supervision to construct the network. We propose a novel generative adversarial framework to grow the two categories of models together: (i) the discriminative module distinguishes whether two papers are from the same author, and (ii) the generative module selects possibly homogeneous papers directly from the heterogeneous information network, which eliminates the complicated feature engineering. In such a way, the discriminative module guides the generative module to select homogeneous papers, and the generative module generates high-quality negative samples to train the discriminative"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main steps in the proof of Theorem REF and how do they contribute to the overall conclusion of the research paper?",
      "expected_output": "The main steps in the proof of Theorem REF involve introducing several notations and definitions, including the Helmholtz projection operators, and using these to rewrite the system in terms of specific formulas. The proof also relies on Lemma 2.2, which plays a crucial role in the subsequent analysis. The steps contribute to the overall conclusion by providing a closed system and equivalent relations that ultimately lead to the completion of the proof of Theorem REF, which is supported by the NSFC under grant number 11601533.",
      "context": [
        "-85b2ea38cb7c}}Before going into details of our theorem, we introduce several notations. Define {{formula:263201ea-8342-438e-8492-62842eae3089}}according to the Helmholtz projection operators, we have {{formula:3fcc9c72-e2a6-431b-af88-fd35bdfeafaa}}with {{formula:43e28a5a-fbb2-49e4-97ac-a80ea324f6af}}From the above definition, one can infer that {{formula:1e4a47d8-2cff-465d-adef-d8f9289be053}}hence, we can rewrite (REF ) in terms of {{formula:dc6922c7-b4f8-4cfa-a463-9075617fa804}} that {{formula:2f4fed77-e66c-4d6d-99cc-96a395f16b8d}}Obviously, the above system (REF ) is a closed system regarding of {{formula:621a7c",
        "3a01-4462-9f2c-a15b52543166}}which combines with the equivalent relation (REF ) give {{formula:075f4f53-fc3c-4b76-8c0c-3614a9184a06}}With (REF ) in hand, we can follow the same argument as the derivation of (REF ), (REF ), (REF ) to get {{formula:6f1e55b7-704e-48ed-81b3-83ecf7d25db0}} {{formula:a094df63-9d7f-4c86-a65d-12979f0d1db4}}and {{formula:e1520343-0ad6-47ae-9057-441524dd8e1c}}The proof of Theorem REF is completed.Acknowledgement. This work is supported by NSFC under grant number 11601533.",
        "9af6667}} satisfies is: {{formula:adae8867-29fd-4121-baf1-fbb24f69511d}}which implies that {{formula:733322f2-ae64-4580-8758-e85630d8b2ae}}The following lemma plays a crucial role in our subsequent analysis.Lemma 2.2 For any {{formula:d8e68798-4887-4e6a-b432-8557079d6c0d}} with {{formula:a1b4a140-1836-493c-837f-5d91717c17b1}} . Assume that {{formula:b9a4affd-a062-4a87-92f4-9094894d60b5}} , {{formula:7bf524c0-d307-4529-9ae5-2ec28229dfd4}} , and {{formula:f534ca42-1ee1-4f87-844e-01ac6eb6faeb}} . Then there exists a positive constant {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the proposed technique in this research paper balance error detection with energy efficiency, and what are the key findings related to this trade-off?",
      "expected_output": "The proposed technique balances error detection with energy efficiency by using a trade-off between the error detection capability and energy efficiency. According to the paper, the technique has a higher error detection capability than MSR, but yields an energy efficiency gain. The key finding is that the technique serves as a knob to trade True Negative Rate (TNR) for efficiency and can be easily adjusted during run-time to get the optimal results. For example, when the threshold is set to 0.9, the normalized FLOPs increase from 0.66 to 0.67, and going beyond this threshold degrades the TNR and increases the FLOPs significantly.",
      "context": [
        "fc}} higher) and yields energy efficiency gain of {{formula:33f013d7-29e9-4d7a-82f7-0ba41bc34599}} than MSR. Thus, the proposed technique has a trade off between the error detection capability and energy efficiency. {{table:07334c83-52d4-48d0-90f8-88756322193d}}The authors in , also focus on detecting natural errors and have reported better detection rate than MSR. But, both these techniques impose enormous energy requirements. The most recent work by Bahat et al. use KL-divergence between the outputs of the classifier under image transformations. For each image, they create `m'{{formula:81d74ee9-15b7-46d6-af95-1ac1e2819901}} transformed images, pass it through the network and the resulting top N logits from original and transformed images are used to detect natural errors. This requires to compute {{formula:9fdf7b01-0b16-45c3-897a-0c879ee454e1}} more FLOPs than MSR and nearly",
        " KL-divergence between the outputs of the classifier under image transformations can be used to distinguish correctly classified examples from adversarial and natural errors. To enhance natural error detection, they further incorporate Multi Layer Perceptron (MLP) at the final layer which is trained to detect misclassifications.Most prior works on the line of error detection do not consider the latency and energy overheads that incur because of the detection mechanism. It is known that deeper networks expend higher energy and latency during feed-forward inference. Adding a detector or detection mechanism on top of this will give rise to additional energy requirements. The increase in energy may make these networks less feasible to employ on edge devices. Many recent efforts toward energy efficient deep neural networks (DNNs) have explored early exit techniques. Here, the main idea is to bypass (or turn off) computations of latter layers if the network yields high confidence prediction at early layers. Some of these techniques include the adaptive neural networks , the edge-host partitioned neural network , the distributed neural network , the cascading neural network , the conditional deep learning classifier and the scalable-effort classifier . So far, there has been no unified technique that enables energy efficient inference in DNNs while improving their robustness towards",
        "-4dc4-b44e-44f66740d028}} =0.9) while the normalized #FLOPs increase from 0.66 to 0.67. Going beyond {{formula:6552bd46-3224-4d08-a08a-77ef0994a515}} =0.9 degrades the TNR and increases the #FLOPs by significant amount. Thus, {{formula:fcf312c1-e244-43be-b80c-51f9e200dee1}} serves as a knob to trade TNR for efficiency and can be easily adjusted during run-time to get the optimal results.\nResults: This section summarizes results on detection capability and energy efficiency obtained from DNN with RACs. We train VGG-16 and ResNet-18 for classifying CIFAR-10. For training CIFAR-100 dataset, we use VGG-16 and ResNet-34. In addition, we also evaluate our approach on ResNet-18 architecture with Tiny-ImageNet dataset. {{table:863c18a4-c273-4e31-a7c2-2d1c6d77"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main challenges of Federated Learning and Data Generation, and how do they affect the evaluation of Federated Data Generation methods?",
      "expected_output": "The main challenges of Federated Learning (FL) and Domain Generalization (DG) are the heterogeneity among clients and the lack of consideration of FL challenges in DG benchmarks, and vice versa. In FL, client heterogeneity, such as class imbalance, can significantly affect the performance of models. In DG, the inability to access test domain data makes it difficult to evaluate the model's performance outside the training domains. These challenges affect the evaluation of Federated DG methods, making it essential to develop a benchmark methodology that considers both FL and DG challenges, as proposed in the paper, which evaluates 14 representative methods on 7 diverse datasets.",
      "context": [
        " heterogeneity present in these datasets is insufficient for proper DG evaluation. In summary, DG benchmarks do not consider FL challenges, and FL benchmarks do not consider DG challenges.Major contributions: We develop a benchmark methodology for evaluating Federated DG with various client heterogeneity contexts and diverse datasets, and we evaluate representative Federated DG approaches with this methodology. 1) We develop a novel method to partition dataset across any number of clients that is able to control the heterogeneity among clients. (see sec:newpartition). 2) We propose the first Federated DG benchmark methodology including four important dimensions of the experimental setting (see sec:main). 3) We compare three broad approaches to Federated DG: centralized DG methods naïvely adapted to FL setting, FL methods developed for client heterogeneity (e.g., class imbalance), and recent methods specifically designed for Federated DG on 7 diverse datasets. Our results indicate that there still exist significant gaps and open research directions in Federated DG. 4) We release an extendable open-source library for evaluating Federated DG methods.Notation: Let [A] := \\lbrace 1,2,\\cdots , A\\rbrace denote the set of integers from 1 to A. Let d \\in [D] denote the d-th domain",
        "# arXiv Paper: 2307.04942\n\nBenchmarking Algorithms for Federated Domain Generalization\nabstract: While prior domain generalization (DG) benchmarks consider train-test dataset heterogeneity, we evaluate Federated DG which introduces federated learning (FL) specific challenges. Additionally, we explore domain-based heterogeneity in clients' local datasets - a realistic Federated DG scenario. Prior Federated DG evaluations are limited in terms of the number or heterogeneity of clients and dataset diversity. To address this gap, we propose an Federated DG benchmark methodology that enables control of the number and heterogeneity of clients and provides metrics for dataset difficulty. We then apply our methodology to evaluate 14 Federated DG methods, which include centralized DG methods adapted to the FL context, FL methods that handle client heterogeneity, and methods designed specifically for Federated DG. Our results suggest that despite some progress, there remain significant performance gaps in Federated DG particularly when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets. Please check our extendable benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark.\nIntroduction: Domain generalization (DG) formalizes a special case of train-",
        "MA , which are naturally designed to solve Federated DG. We also consider FedProx and Scaffold from the Federated methods tackling client heterogeneity approach. All those methods will be compared to two baselines. ERM objective with Adam optimizer and its FL counterpart FedAdam , which is a variant of FedAvg .\nMain Results: In this section, we present the performance results of 14 representative methods, derived from distinct research areas, on 7 diverse datasets. For each dataset, we fix the total computation and communication rounds for different methods for a fair comparison.Client number Equipped with our Heterogeneous Partitioning, we can explore various levels of client heterogeneity and relax the assumption that C=D so that we can leverage both pseudo-realistic and real-world datasets and evaluate methods on larger scale of FL clients. In particular, we set the number of clients to 100.Validation domain In DG task, we cannot access the test domain data. However, we are particularly concerned about the model performance outside the training domains, thus we preserve a small portion of the domains we can access as held-out validation domains, and the held-out validation domains are used for model selection and early stopping. Please see sec:expsetting for more detail."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on how noise force simplifies dissipative quantum systems dynamics, and what methods were used to achieve these results?",
      "expected_output": "The research found that interpreting the auxiliary quantity as a noise force simplifies the dynamics of dissipative quantum systems. The main findings include the development of a novel algorithm, called the SLED algorithm, which is highly efficient and accurate. This algorithm generates an ensemble of Gaussian noise trajectories, propagates the system for each noise trajectory, and averages over the ensemble. The results show that the statistical error is virtually independent of time and approaches a constant value for long times, making it more efficient than conventional quantum Monte Carlo methods. The SLED algorithm was tested on the damped harmonic oscillator and the dissipative double-well system, demonstrating its practical feasibility and power.",
      "context": [
        " interpreting it as a noise force. This yields a new and elegantly simple algorithm: Generate an ensemble of Gaussian noise trajectories with the appropriate spectrum, propagate the system for each noise trajectory, and average over the ensemble. In stark contrast to QMC methods, we find a statistical error that is virtually independent of time.At a microscopic level, dissipation is caused by the interaction of a non-thermal system with a vast number of environmental modes at or close to thermal equilibrium. Although cumulative effects of these modes interacting with the system under study can have a drastic effect on its dynamics, the coupling to each individual mode is usually weak. This justifies the paradigmatic model employed by Caldeira and Leggett to describe dissipative quantum systems – a particle in an arbitrary potential interacting linearly with a vast number of harmonic excitations, {{formula:9924b5ce-0bcc-4f41-b01c-2883a37787ce}}The effect of the environmental modes is fully characterized by their spectral density {{formula:b90b852f-ff14-4e56-b9cc-4cdb97d07d78}} , which takes on the form {{formula:f6abe",
        " states.In conclusion, we have developed a novel algorithm for the dynamics of extended dissipative quantum systems, which is highly efficient. It is also shown to be accurate on first principles and by explicit numerical tests. We have presented simulation data both for the damped harmonic oscillator and the dissipative double-well system which show the practical feasibility and power of this method. Empirically, we find that the statistical error of our results is extremely small, asymptotically approaching a constant value for long times. This implies a CPU requirement that scales linearly with the system time {{formula:fc119b1b-e9e1-4297-9f46-5258216b80f3}} , compared to the exponential rise for conventional quantum Monte Carlo methods. The SLED algorithm is also easily implemented on parallel computers. The system propagation for different noise realizations can be performed in parallel without any dependencies beyond the accumulation of final results.We are currently extending the SLED method to study dynamics on electronically coupled energy surfaces in condensed-matter problems. The problem of greatest interest is the dynamics of electron transfer reactions in the inverted region. To model a bath with an arbitrary cutoff frequency, the SLED method has to be extended to treat systems with",
        " that the integrand is not a local functional of the paths. Except for the case where the memory effects decay on a short time scale , deterministic integration methods fail, making it prohibitively expensive in both computation time and memory requirements. To date, the only exact numerical approach to dissipative quantum dynamics with arbitrary memory time has been the dynamical quantum Monte Carlo (QMC) method. This method, however, is severely limited by the sign problem, i.e. the signal-to-noise ratio of the numerical result tends to zero exponentially with the time scale of the problem.In this Letter, we deal with the problem with non-local influence functionals arising from arbitrary long bath memory. We recover fully time-local dynamics for the ubiquitous case of Ohmic dissipation by applying a simple Hubbard-Stratonovich transformation. The time-local path integral can then easily be solved by propagating an equation of motion for the reduced density matrix. The auxiliary quantity introduced by the transform appears in the dynamics as an additional classical time-varying force. The functional integration over the auxiliary quantity can be performed stochastically by interpreting it as a noise force. This yields a new and elegantly simple algorithm: Generate an ensemble of Gaussian noise"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the lifetime return objective enhance exploration across episodes in this research, and what are the key findings related to its implementation?",
      "expected_output": "The lifetime return objective enhances exploration across episodes by allowing the agent to maximize the return over a lifetime, which can span multiple episodes. This enables the agent to explore and collect information even if it doesn't yield an immediate positive reward. The key findings are that the lifetime return objective is beneficial in domains that require exploration across multiple episodes, such as Random ABC, Non-stationary ABC, and Key-Box. Additionally, the lifetime history is crucial to achieve good performance, as it allows the agent to capture past information and provide useful exploration strategies. For example, in Random ABC, collecting an uncertain object is necessary even if the episode terminates with a negative reward, and the lifetime return objective supports this exploration.",
      "context": [
        " To study relative benefits of the proposed technical ideas, we conducted an ablation study 1) by replacing the long-term lifetime return objective ({{formula:6c079c85-9bc8-49c7-a4a9-f0824ba7675b}} ) with the episodic return ({{formula:4472b0ca-f979-4743-8ea2-3f5751613e3f}} ) and 2) by restricting the input of the reward network to the current time-step instead of the entire lifetime history. Figure REF shows that the lifetime history was crucial to achieve good performance. This is reasonable because all domains require some past information (e.g., object rewards in Random ABC, visited locations in Empty Rooms) to provide useful exploration strategies. It is also shown that the lifetime return objective was beneficial on Random ABC, Non-stationary ABC, and Key-Box. These domains require exploration across multiple episodes in order to find the optimal policy. For example, collecting an uncertain object (e.g., object A in Random ABC) is necessary even if the episode terminates with a negative reward. The episodic value function would directly penalise such an under-performing",
        "4927-9472-66819e5d1bcd}} , which produces a scalar reward on arriving in state {{formula:dea377ce-3216-478f-9c7f-1eaf74674f68}} by taking into account the history of an agent's lifetime {{formula:6e81a872-bed8-4d88-a052-4f3938c04b6c}} . We claim that giving the lifetime history across episodes as input is crucial for balancing exploration and exploitation, for instance by capturing how frequently a certain state is visited to determine an exploration bonus reward. The lifetime value function is a separate recurrent neural network parameterised by {{formula:efb82f26-01fa-4586-8350-cdc8755f050a}} , which takes the same inputs as the intrinsic reward function and produces a scalar value estimation of the expected future return within the lifetime.\nPolicy Update ({{formula:6197fb97-57bc-4fbe-b682-b7d462f3ad37}} ): Each agent interacts with an environment and a task sampled from a distribution {{formula:28249522-",
        "4be6-9beb-7da149e2949a}} , which is policy gradient in this paper.We assume that the policy parameter is updated after each time-step throughout the paper for brevity. However, the parameter can be updated less frequently in practice. Note that the optimisation of {{formula:286f9ec0-3886-4b5e-8bad-7adf85792301}} spans multiple lifetimes, each of which can span multiple episodes.Using the lifetime return {{formula:56b40b30-f53c-4a59-a980-7968af196828}} as the objective instead of the conventional episodic return {{formula:ee44ec8a-4ac6-4116-a20f-5ee6c4c31575}} allows exploration across multiple episodes as long as the lifetime return is maximised in the long run. In particular, when the lifetime is defined as a fixed number of episodes, we find that the lifetime return objective is sometimes more beneficial than the episodic return objective, even for the episodic return performance measure. However, different objectives (e.g., final episode return) can be considered depending on the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do CNNs, BiLSTM layers, and highway connections contribute to the accuracy of supertagging in the research paper, and what are the main findings regarding their effectiveness?",
      "expected_output": "According to the research paper, CNNs contribute to the accuracy of supertagging by capturing morphological information through character-level representations. BiLSTM layers, specifically a deep bidirectional Long Short-Term Memory network, are used as the core of the supertagging model. The addition of highway connections, along with concatenation after each BiLSTM layer, yields an increase of 1.3% in accuracy. The main findings suggest that these techniques, including the use of CNNs for encoding morphological information and the exploration of additional BiLSTM layers and highway connections, substantially improve upon previously reported results in supertagging and parsing.",
      "context": [
        " LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.\nInput Representations: The input for each word is represented via concatenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30-dimensional character-level representation from CNNs that have been found to capture morphological information , , . The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word. We initialize the word embeddings to be the pre-trained GloVe vectors ; for words not in GloVe, we initialize their embedding to a zero vector. The other embeddings are randomly initialized. We obtain predicted POS tags from a BiLSTM POS tagger with the same configuration as in ma-hovy:2016:P16-1.The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representation obtained from CNNs in the same fashion as in the supertagger.We fix the embedding of the ROOT token to",
        " from CNNs in the same fashion as in the supertagger.We fix the embedding of the ROOT token to be a 0-vector. We also consider adding 100-dimensional embeddings for a predicted POS tag and a predicted supertag , . The ablation experiments in illustrated that adding predicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, empirically showed that their dependency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tagger. Indeed, and demonstrated that their unlexicalized neural network TAG parsers that only get as input predicted supertags can achieve state-of-the-art performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are randomly initialized.\nDeep Highway BiLSTM: The core of the supertagging model is a deep bidirectional Long Short-Term Memory network . We use the following formulas to compute the activation of a single LSTM cell at time step {{formula:8a9a2975-b373-457a-b044-43b71af4f1e4}}",
        ") while the CCG-annotated version only includes 1286 distinct supertags (439 occur once). This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags , , .In this work, we present a supertagger and a parser that substantially improve upon previously reported results. We propose crucial modifications to the bidirectional LSTM (BiLSTM) supertagger in . First, we use character-level Convolutional Neural Networks (CNNs) for encoding morphological information instead of suffix embeddings. Secondly, we perform concatenation after each BiLSTM layer. Lastly, we explore the impact of adding additional BiLSTM layers and highway connections. These techniques yield an increase of 1.3% in accuracy. For parsing, since the derivation tree in a lexicalized TAG is a type of dependency tree , we can directly apply dependency parsing models. In particular, we use the biaffine graph-based parser proposed by together with our novel techniques for supertagging.In addition to these architectural extensions for supertagging and parsing, we also explore multi-task learning approaches for TAG parsing. Specifically,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the encoding module in this research paper generate multi-query results, and what methods are used to achieve this?",
      "expected_output": "The encoding module in this research paper generates multi-query results by parallelizing K similar pipelines, each taking a resized input as a query. The module uses a downsample factor to resize the input to different resolutions, creating multiple encoded representations. Specifically, the i-th encoder converts a sample to K encoded representations, which are then used by the decoder to map anomaly-free latent representations to the desired domain. This process allows the encoding module to generate multi-query results, enabling the model to handle various tasks, as described in section 4 of the paper.",
      "context": [
        "bcb32bf98b34}} . In our model, the encoding module can be seen as a multi-query generator. The decoder is trained to reconstruct the samples by taking the fused features as input. Specifically speaking, we parallel K similar pipelines to form a encoding module, each pipeline takes {{formula:1b8caa69-de93-4ac5-9551-7d8b1b7e0468}} as inputs, which is obtained by resize {{formula:0e08e194-b448-4288-8313-a40d46ef94f5}} to resolution {{formula:8f889773-a3a7-441c-9618-c3b1f4699427}} . We select a downsample factor {{formula:674f104d-3c9b-4940-99bd-4c0c1bccfd1e}} , and denote the resolution of {{formula:0b9f19c9-5aa3-493b-9226-e788aa9e3a49}} to be {{formula:f390b4c1-04bd-4470-b8",
        "50789f2b2d50}} -th sub-domain, {{formula:6c76226f-fb11-40a0-b95c-d92d1176d800}} to represent the domain of anomaly-free feature space.Let {{formula:914606dc-654d-4674-abb7-1c302110d355}} denote the i-th encoder and {{formula:749a9c06-013b-45b6-bb5b-a6b3e0c928ac}} denote the decoder.Given a sample {{formula:e88bcce8-4d66-4927-85c0-da60e7aefb44}} , the encoder converts it to K encoded representations as {{formula:9796a09e-143d-463e-808b-1df6697679e5}} ; and the decoder is trained to mapping anomaly-free latent representations {{formula:fd635400-e0ed-422a-973a-c33f7a6e6daa}} to the domain {{formula:7ce34eeb-",
        "3a49}} to be {{formula:f390b4c1-04bd-4470-b811-06731d46a6ed}} , then {{formula:532c74a7-3721-4c3d-9056-c23b2e7f4d74}} .Then the features of the image {{formula:b346d938-fe9c-4002-8974-dc6d3c8cc98c}} at different resolutions are then outputed by the corresponding pipeline.We will choose the pipeline considering the tasks we are handling, which will be described in section4.We first define {{formula:c82951bd-2a72-4405-be29-2d838d88f141}} to represent the domain of the data samples, {{formula:fe172357-5466-4367-bd00-336663b909cf}} to represent the {{formula:2f2560f3-f4ba-44f8-bf72-50789f2b2d50}} -th sub-domain, {{formula:6c76226f-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does cFM ordering play in driving orbital ordering in BNOO, and how do the researchers support this claim in the paper?",
      "expected_output": "According to the paper, cFM ordering plays a crucial role in driving orbital ordering in BNOO. The researchers support this claim by demonstrating that the staggered orbital pattern only arises when cFM order is imposed. They also show that the two-sublattice orbital ordering is accompanied by a distinct orbital contribution along high symmetry lines in the partial density of states and band structure. Additionally, the spin density plots illustrate the two-sublattice orbital ordering within the alternating planes, further supporting the claim that cFM order drives orbital ordering in BNOO.",
      "context": [
        " at sufficiently low temperatures, coexists with the two-sublattice cFM order, with a net magnetic moment of {{formula:bb567a11-a31a-4a7b-aaf2-b490db8f0eb2}} per osmium atom along the [110] direction. One key question that remains is whether such cFM order implies the existence of complex orbital/quadrupolar order.In this paper, we report a two-sublattice orbital ordering pattern that coexists with cFM order in BNOO, as revealed by DFT+U calculations. Evidence for this order is apparent in BNOO's selective occupancy of the {{formula:8adeb250-fbfc-4abd-9243-e26e2692aeeb}} orbitals and spin density distribution. More specifically, the staggered orbital pattern is manifest in BNOO's partial density of states and band structure, which possesses a distinct {{formula:a82b92c0-7d60-4cde-b097-4f7f20056347}} orbital contribution along high symmetry lines. This staggered orbital pattern is not found in the FM[",
        " the cFM reported in Ref. 12, is accompanied/driven by an orbital order.Motivated by the cFM order detected in NMR experiments, here we investigated BNOO's orbital ordering pattern from first principles. We found two-sublattice orbital ordering, illustrated by the spin density plots, within the alternating planes in which the total magnetic moment resides. An auxiliary signature of the orbital ordering is revealed by the occupancies of the {{formula:759e5ad5-a87b-4d5e-a6f2-b25459872d1c}} orbitals in the density of states and band structures. Our first principles work demonstrates that this two-sublattice orbital ordering mainly arises from cFM order and strong SOC. Moving forward, it would be worthwhile to more thoroughly investigate the cFM order observed in this work using other functionals or methods more adept at handling strong correlation to eliminate any ambiguities that stem from our specific computational treatment.\nAcknowledgments: The authors thank Jeong-Pil Song and Yiou Zhang for enlightening discussions. We are especially grateful to Ian Fisher for his long term collaboration on the physics of Ba{{formula:2aa576f4-7e6",
        "-4511-9483-6979ef0a8b02}} -component of the spin density over two sublattices for both types of imposed magnetic order. It is evident that the staggered orbital pattern only arises when cFM order is imposed. Therefore, we demonstrate that the staggered orbital order can solely coexist with cFM order.We note also that in Figs. REF and REF there is non-negligible spin density on the O atoms of the OsO{{formula:63f2d94a-146c-4cbf-97f7-ffa166980194}} octahedra. It is usually thought that atoms with closed shells, like O in stoichiometric compounds, possess negligible spin densities. This is an unexpected feature in BNOO that has been previously noted in Ref. Pickett2015, and is due to the stronger {{formula:0d602b3c-2fb6-420e-8338-247c9f68acad}} -{{formula:68a789cc-d8e8-4d1e-b767-d2a5093d6d49}} hybridization, which results in OsO"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of modified gravity theories on conformal mode analysis, and how do the methods used in this research impact the key results?",
      "expected_output": "The main findings of modified gravity theories on conformal mode analysis show that these theories are currently the most actively explored options for dynamical analysis. However, introducing an extra degree of freedom in the conformal mode should be treated with care to avoid fine-tuning problems. The methods used in this research, such as Legendre transformation and change of frame, can trade off the function for a self-interacting scalar field and its potential. For example, the paper mentions that even if one were to explain the observed accelerated expansion, they would still need to explain why the observed quantum vacuum energy density is so small by particle physics standards. Overall, the research highlights the need for theory-independent cosmographic tests to develop a better understanding of the conformal mode and its implications for our understanding of the universe.",
      "context": [
        " wherein a Legendre transformation can trade off the function {{formula:d821f9ed-c32b-40dc-a395-6d0bbd405ec5}} for a self-interacting scalar field and its potential . Even so, this is not necessarily an exhaustive list of available options for a dynamical analysis of the conformal mode , . Unfortunately none of these approaches are quite as “technically clean” as one might wish for.From a phenomenological perspective the modified gravity {{formula:453d6fdb-e947-49b3-82b5-49715834005f}} theories , , , are currently the most actively explored of these options. More generally, in any of these dynamical models the introduction of the extra degree of freedom in the conformal mode (scalar field) should be treated with extreme care — there is always the potential of introducing a new fine-tuning problem. For instance, even if one were to kill off the need for dark energy (accelerated expansion) one would still need to explain why the observed/inferred quantum vacuum energy density is so small by particle physics standards: {{formula:00532130-a623",
        ", modulo potential problems due to data selection cuts.)\nDynamics: When attempting to move beyond cosmography, to develop an appropriate notion of CFLRW dynamics, there is from a theoretical perspective almost an embarrassment of riches to draw on. A conformal mode in the spacetime geometry (effectively an extra scalar field) can be accommodated in any number of ways — from various geometrically inspired “conformal gravity” models , , , through to various models that perform a “change of frame” to trade off the conformal mode for some sort of scalar field , — be it some form of inflaton, dilaton, axion, string modulus, or Higgs-like scalar field, through (more speculatively) to possibly invoking the difference between the “string frame” of particle physics from the “Einstein frame” of general relativity , through to various {{formula:0c5acc85-3ca2-4011-a579-1d8c79f1deec}} modified theories of gravity , , , , wherein a Legendre transformation can trade off the function {{formula:d821f9ed-c32b-",
        "inferred quantum vacuum energy density is so small by particle physics standards: {{formula:00532130-a623-4d93-9312-16cdaa0048eb}} .Given the quite extensive range of possibilities listed above, I feel that focussing on any particular dynamical option would be at this stage premature. Instead it would seem advisable to spend some extra effort to develop additional direct, (and as much as possible, theory-independent), cosmographic tests. Remember, (assuming the usual Einstein equations), that 97% of the mass-energy of the universe is “missing in action”. Most of the current dynamical alternatives to dark energy/dark matter are rather ugly, so theory-free cosmographic tests hold considerable interest.\nDiscussion: While traditionally the usual FLRW-based cosmological models have assumed that the cosmological spacetime geometry is smooth plus “small perturbations”, I feel it is worthwhile to consider the possibility of large non-perturbative deviations from FLRW cosmology. Among all possible large non-perturbative deviations from FLRW cosmology, one class stands out as meriting particular attention: Conformal deformations of FLRW geomet"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do twisting system pairs play in the construction of bialgebras' Hopf envelopes, and how do the main findings of this research relate to their methods and key results?",
      "expected_output": "Twisting system pairs play a crucial role in constructing the Hopf envelope of a bialgebra. According to the research, any system of twisting functionals on a bialgebra B can be extended uniquely to a system of twisting functionals on its Hopf envelope. The main findings show that twisting system pairs are used to introduce the notion of a twisting system pair of B, which is then lifted to that of its Hopf envelope, and this is used to construct a certain 2-cocycle explicitly. For example, Lemma 4.1 states that a collection of linear functionals is a system of twisting functionals on B if and only if it satisfies certain conditions, and this is used to extend the system of twisting functionals from B to its Hopf envelope.",
      "context": [
        "ell _{p+1}}(V)\\otimes \\cdots \\otimes S^{\\ell _{p+q}}(V).This concludes the inductive step. Finally, the uniqueness of the extension of \\alpha from B to \\mathcal {H}(B) follows from lem:inverseSLinear.\n2-cocycles via twisting system pairs : Throughout this section, let B be a bialgebra satisfying the twisting conditions given in defn:twisting conditions. In this section, we introduce the notion of a twisting system pair of B, which we lift to that of its Hopf envelope \\mathcal {H}(B) and we use it to construct a certain 2-cocycle explicitly.Lemma 4.1 Let B be a bialgebra satisfying the twisting conditions. Consider a collection of linear functionals \\alpha =\\lbrace \\alpha _i: i\\in \\mathbb {Z}\\rbrace with convolution inverse \\lbrace \\alpha _i^{-1}: i\\in \\mathbb {Z}\\rbrace on B. The following are equivalent:(1) The collection of maps \\alpha is a system of twisting functionals on B. (",
        " is straightforward to check that \\alpha is a system of twisting functionals on T, with convolution inverse \\alpha ^{-1}, extending that on \\langle V\\rangle . By twistL-deg-1, it remains to show that \\alpha (W)=0, which would then yield a system of twisting functionals on \\mathcal {H}(B)=T/W extending that on B via the natural bialgebra map B\\rightarrow \\mathcal {H}(B). We will show that \\alpha and \\alpha ^{-1} vanish onS^{\\ell _1}(V)\\otimes \\cdots \\otimes S^{\\ell _p}(V)\\otimes S^k(R)\\otimes S^{\\ell _{p+1}}(V)\\otimes \\cdots \\otimes S^{\\ell _{p+q}}(V),\\\\ S^{\\ell _1}(V)\\otimes \\cdots \\otimes S^{\\ell _p}(V)\\otimes (M \\circ (\\operatorname{id}\\otimes S) \\circ \\Delta -u \\circ \\varepsilon )",
        " functionals on B can be extended uniquely to a system of twisting functionals on its Hopf envelope \\mathcal {H}(B). Moreover, any system of twisting functionals on \\mathcal {H}(B) is obtained from some system of twisting functionals on B in such a way.Let \\alpha =\\lbrace \\alpha _i\\rbrace _{i\\in \\mathbb {Z}} be a system of twisting functionals on B. We use the presentation of \\mathcal {H}(B)=T/W based on B=\\langle V\\rangle /(R) as discussed above.First, we lift the system of twisting functionals \\alpha =\\lbrace \\alpha _i\\rbrace _{i\\in \\mathbb {Z}} to the free bialgebra \\langle V\\rangle in the following way. By formulas (REF ), we can extend the restrictions \\alpha _i|_V and \\alpha ^{-1}_i|_V on the subcoalgebra V to the free bialgebra \\langle V\\rangle . By abuse of notation, we still write them as \\alpha =\\lbrace \\alpha _i"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does anisotropy play in altering diffusion constants, and how do the researchers measure its impact in this study?",
      "expected_output": "Anisotropy plays a significant role in altering diffusion constants, as it decreases with both the anisotropy in the Frank elastic constants and the parameter represented by formula d5df3d8a-0303-417c-b2ba-bd1af9ed6318. The researchers measure its impact by calculating the scattering mean-free-path and analyzing the spatial and temporal autocorrelation function for the electric light field, which is influenced by the anisotropy of the magnetic susceptibility and the Frank elastic constants.",
      "context": [
        "a37430-ff3d-4d2d-87be-ce29326d61b3}} and {{formula:88a0dbc0-535e-4d23-98c9-977e412b9909}} are finite even though, as noted earlier, the scattering mean-free-path for the extraordinary light ray is infinite. The anisotropy in the diffusion constants decreases with both {{formula:d5df3d8a-0303-417c-b2ba-bd1af9ed6318}} and anisotropy in the Frank elastic constants. In the limit {{formula:d788d001-102b-4a4f-8129-c0761069d63b}} and {{formula:365413b6-ba3c-4b79-851a-2eb9ab331a9c}} , {{formula:81a670e1-0b73-401e-8e64-73076f6de29a}} is not unity because of the inherent anisotropy in the structure factor. The diffusion approximation is valid only for times {{formula:2e",
        "eb1adc35b5a9}} . The scattering mean-free-path {{formula:2ba914d2-53dc-4b8b-a524-510072c172c9}} in the nematic phase has been calculated , . For small {{formula:ce7e867a-431b-449d-b5bb-5b3aeed5fd5d}} for the extraordinary ray it has the form {{formula:5f466075-9b5d-4cc6-8ccb-e3409c487303}} , where {{formula:0be4852b-a5bc-416d-8b3b-efa2867860d2}} is an appropriately averaged elastic constant.Let us look at the spatial and temporal autocorrelation function for the electric light field: {{formula:5837eb9a-63f5-494e-ab2b-78a6f94fa95a}} , where we have already introduced center of “mass” ({{formula:2b0589bc-9568-4f2e-bbff-d2b7",
        "74-189c-499e-beb6-108e90c897a4}} , {{formula:3a3054e3-51d2-4864-a33b-9c94d17aa380}} , and {{formula:4a5a2186-8ff9-4022-b58f-dab2a71b5f77}} are the Frank elastic constants, {{formula:697d0e59-4cc0-42e7-8462-361e58c927c4}} is the external field parallel to {{formula:e8d557f4-2e12-45e7-a803-b530cb5df4a4}} , and {{formula:cd7cf2c0-6bb6-4b10-8329-e03d67fffe3b}} is the anisotropy of the magnetic susceptibility. The quantity {{formula:b19192e7-ba1a-4556-a5a5-d71c35d7cd52}} is a combination of viscosities which appear in the hydrodynamic equations of the director field,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the mean field gap and the insulator-metal transition, and how do the research findings support this connection?",
      "expected_output": "The research findings show that the mean field gap initially decreases but never reaches zero, and then increases robustly as the parameter is increased, indicating that the mean field ground state becomes more insulating. However, the exact solution reveals a transition to an un-gapped system, indicating an insulator-metal transition. The mean field gap reproduces the correlation within one manifold, and the difference between the second and first eigenvalues within the manifold shows a strong correlation. For example, in Fig. REF (b), the gap of the mean field solution is plotted as a function of the parameter, and the exact solution indicates a transition to an un-gapped system above a certain value. This supports the connection between the mean field gap and the insulator-metal transition.",
      "context": [
        " In Fig. REF (b) we plot the gap of {{formula:9e2340e8-bdbc-46b4-a00c-edcc6bcc88b6}} as function of {{formula:2ed53ceb-ae0a-4af2-a80d-94e795054755}} . We see the mean field gap first decreasing, but never reaching zero, and then increasing robustly. Therefore, the mean field ground state becomes more insulating as {{formula:1e18c4c9-42a2-4cdd-a8a9-c0649d0b95b0}} is increased. At first sight, this seems to be in total contradiction with the insulator-metal transition of the exact ground state. But if we look at the gap within one manifold, we actually see a very strong correlation. In Fig. REF (b) we also plot the difference between the second and first eigenvalues within the {{formula:5377536c-9fd1-4981-bc40-532c0b12f989}} manifold. As one can see, the mean field gap actually reproduces",
        "683-e03e4b410707}} and {{formula:d569c67d-14ca-4a16-bf51-2e17f54347cc}} . As one can see, the exact solution indicates a transition to an un-gapped system. In fact, above {{formula:9090b361-9573-4abf-80c2-26667b27b4f2}} , the ground states of the manifolds {{formula:7c7ab9da-ec90-40b4-97a8-20cca03bcffa}} form a continuous sequence of energy levels. Therefore, at {{formula:49dee099-3cbf-4e32-8c02-f68cee394a5b}} we have a true insulator-metal transition.As already mentioned in the Introduction, the non-trivial mean field solutions found so far predict metal-insulator transitions at the symmetry breaking. The situation is similar in the present case, even though we start from a band insulator. In Fig. REF (b) we plot the gap of {{formula:9e2340e8-b",
        " jump at {{formula:f09b5cf4-d46c-4907-b667-14a50bb3aa91}} .But the most interesting difference concerns the gap of the energy spectrum. In Fig. REF (a) we plot the gap of {{formula:21e0f997-2fdc-45a1-a7dc-67b1da49387a}} (no length normalization) as function of {{formula:cb9471d2-97bd-4e3e-832a-15321607dfd2}} , with {{formula:40d3d7a6-a4c0-4955-8afe-df04d5869d52}} fixed at 4000. The gap of {{formula:e174f9c0-0bff-4b34-9aaf-47d3cd030448}} is given by the difference between the ground state energies of the manifolds {{formula:3d4c9bd4-df49-4ad2-b683-e03e4b410707}} and {{formula:d569c67d-14ca-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the polarisation properties of Fast Radio Bursts reveal information about the magnetic fields in the Intergalactic Medium, and what are the key findings from this research?",
      "expected_output": "The polarisation properties of Fast Radio Bursts (FRBs) reveal information about the magnetic fields in the Intergalactic Medium (IGM) by showing evidence for strong magnetic fields and high polarisation fractions. According to the research, the data do show evidence for strong magnetic fields, but the picture is far from uniform across the sub-sample where this information exists. For example, eight FRBs had polarisation properties reported in the literature, indicating that the polarisation properties of FRBs can be used to study magnetic fields in the IGM and/or the host environments.",
      "context": [
        "5605180-27bf-4e4d-ade5-798f0c7e0bab}} clearly requires a larger sample, but in doing so one could determine where the baryonic content is located, e.g. whether it is within or well beyond galaxy halos . The high-z applications of DM-based tests are perhaps the most powerful, and are considered in more detail below. But beyond these one can use the polarisation properties of FRB signals to study magnetic fields in the IGM and/or the host environments. The data do show evidence for strong magnetic fields and high polarisation fractions , but the picture is far from uniform across the sub-sample where this information exists . The total intensity time-frequency profiles of FRBs can be used to study IGM turbulence . By quantifying the effects of multi-path propagation (usually termed `scattering') and how this broadens the FRB pulses in a frequency-dependent way, one can determine the length scales of IGM turbulence with very fine spatial resolution of perhaps pico-arcseconds . With a sufficiently large sample one could perform tomography of the cosmic web.At the time of writing there are six telescopes which have discovered an FRB and many teams using",
        " this effect. One final problem which may result in missed FRBs stems from how to define an FRB in a basic observational sense. Any definition must be in reference to a model for the Milky Way's electron density model. The two current foremost models , , NE2001 and YMW16, are both uncertain and disagree to different magnitudes for different lines of sight meaning there are many `grey areas' where one is uncertain whether a given source is Galactic or not . The upshot is that some sources are FRBs according to YMW16, but not according to NE2001, with the opposite possible for some lines of sight . The FRBs that would be missed due to this confusion are very nearby and so of no use for cosmological tests but for studies of FRB progenitors are of the highest interest. Questions as to the rates, source count distribution, pulse width distribution and completeness of surveys are all entwined, and must each be very carefully treated. Ignoring any of these aspects can result in wildly varying and incorrect predictions for the others.v) What are the polarisation properties of FRBs? At the time of writing eight FRBs had polarisation properties reported in the literature , , , , , , . Amongst this group there",
        " such events is severely compromised by the accessible field of view. Sensitivity has already been shown to be key for finding the most distant FRBs, needed for the high-{{formula:f6d30a34-754c-4550-a722-83cde694e844}} cosmology tests — the highest-DM FRB that has not been discovered by Parkes is FRB 160317 which has {{formula:574766f9-dabd-4a13-abd0-056dcd4f45f9}} — despite the rather low rate of FRBs at Parkes it has discovered the top seven highest-DM FRBs. One area where an abundance of hidden FRBs may come to light is at sub-millisecond time scales. The left-hand side of Figure 1, showing the fastest signals, is mostly empty due entirely to this region of parameter space not having been searched. The broadband nature of FRBs suggests this region may be populated and if it were possible in terms of data rates to get even slightly closer to raw (i.e. Nyquist) sampling of the data stream, such FRBs will be discovered. Parkes for example reduces its data rate"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the significance of substitution closure in CCS processes and how does it impact the overall outcome of these processes?",
      "expected_output": "Substitution closure in CCS processes refers to the ability to replace free names in a process with other names, which is crucial for ensuring the correctness and consistency of the processes. According to the research paper, substitution closure allows for the binding of names in the continuation process and restricted process, enabling the creation of more complex and dynamic processes. For example, the input prefix can bind a name in the continuation process, and name restriction can bind a name in the restricted process. This substitution closure property has a significant impact on the overall outcome of CCS processes, as it enables the creation of more flexible and adaptable processes that can respond to changing conditions and inputs.",
      "context": [
        "F , that discusses substitution closure and noninterleaving semantics, is new.\nMicroCCS Processes and Normal Forms: We consider an infinite set {{formula:71b70c83-3694-4a8f-b643-fca7139815d7}} of names, and let {{formula:d874a482-93db-4160-bb37-d46aab8cdbf6}} range over names. We define on top of {{formula:597b1123-18ee-4eac-89d8-0da1845d2f1b}} the set of processes of {{formula:d9fbb229-1c36-45f9-9805-0911f30353e6}} CCS, the finite, public (that is, without restriction), sum-free CCS calculus, as follows, where {{formula:3c4baae0-1ac7-4835-bd84-deafbae0df4e}} range over processes: {{formula:6c855ba1-bd6a-4887-8188-67d8e999239a}}{{form",
        " is also possible on the right hand side (in the first summand). We do not enter any further into the details of this proof.\nThe Finite, Sum-free {{formula:818a28d1-0f21-4ffb-9bc6-4b3e5200535a}} -calculus: Processes of {{formula:add184b6-36d6-4d5a-bbc8-622ae3127053}} are built from an infinite set {{formula:e30b41cd-d4e6-4493-adb6-7a1f566a6e0c}} of names (we let {{formula:425fd3d5-c973-49d9-b094-118f3d7f2b48}} range over names), according to the following grammar: {{formula:0b044622-256e-404a-8dd5-15efbb176f52}}The input prefix {{formula:d4c15f9e-7e40-413d-8443-fa386e7a8288}} binds name {{formula",
        "e-7e40-413d-8443-fa386e7a8288}} binds name {{formula:2a21cc35-1bf1-4b3a-b896-687a7dc867db}} in the continuation process, and so does name restriction {{formula:41879d3b-34e6-4ca9-b6cd-e5ccd6a8be35}} in the restricted process. A name that is not bound is said to be free, and we let {{formula:dc96e863-78a2-4602-9763-2f6bbb6441dc}} stand for the free names of {{formula:ead053fd-5e0e-4e78-8c8d-ba26bf4fd49f}} . We assume that any process that we manipulate satisfies a Barendregt convention: every bound name is distinct from the other bound and free names of the process. We shall use {{formula:9959d474-1d3d-4f12-9ecd-1689c5c4fe87}} to range over free names of processes, {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how nonstoichiometry affects the magnetic properties of jarosite, and what methods were used to obtain these results?",
      "expected_output": "The main findings on how nonstoichiometry affects the magnetic properties of jarosite are that deficiency of the A metal or ammonium cation is charge compensated by incorporation of H3O ions, and under-occupation of the B site results in protonation of the hydroxy groups, which could have important consequences for the magnetic properties. The methods used to obtain these results are not explicitly stated in the provided context, but it can be inferred that they involve analyzing the structural and magnetic properties of jarosite using techniques such as X-ray diffraction and magnetic measurements.",
      "context": [
        "stoichiometry.: It is well known that the jarosites are subject to nonstoichiometry on both the A and the B sites. Deficiency of the A metal or ammonium cation is charge compensated for by incorporation of H{{formula:a427ec5d-95e5-49dd-b6aa-811c65abe3ab}} O{{formula:f94aaa6a-d06c-4014-8eca-bbd44fb89cf8}} ions onto the A site, so that the site occupation remains essentially at unity. The extreme limit of this situation is of course the hydronium salt itself. Under-occupation of the B site results in protonation of the hydroxy groups in the structure. As these groups play the dominant rôle in the mediation of the superexchange between the metal centres, it is conceivable that such disorder and the associated randomness in the exchange interactions, could have important consequences for the magnetic properties. {{figure:52d44e9a-d5da-4b0c-b4ae-0a95f93b2ffb}}\nConventional Orderings: Long-range magnetic structures with the",
        "1206618dd1d2}} O ions are no longer orientationally mobile, as is the situation at low temperature, these hydrogen bonds will create interactions between neighbouring sulfate groups that are not present in the other jarosites. Due to the complexity this would generate, it is not yet clear if they will act to hinder or improve the interlayer exchange. However, the large difference in the magnetic properties of the hydronium jarosite from those of the monatomic ions, even for compositions with the same Fe atom occupation, suggests that there is indeed a disruption of simple interlayer exchange couplings, and concomitantly a destabilisation of any long-range ordered state that they would induce.That the ammonium salt behaves like the other spherical A-site metals, suggests that its hydrogen bonding is either less disturbing, or of a lesser consequence, than that of H{{formula:0c77622b-1b55-49a1-b2ed-bdabefda6a50}} O. Further studies are required before the effects of the A cation can be understood.\nInfluences of nonstoichiometry.: It is well known that the jarosites are subject to nonstoichiometry on both the",
        "iferromagnetic coupling. {{figure:253dc055-8e59-422b-be6f-afea2e406555}}{{figure:a6285686-da79-48b8-9888-37c337a34f3c}}\nEffects of A cation: The marked difference in the responses of hydronium jarosite from the other iron jarosites , , indicates that the A cation is capable of having a profound influence on the low-temperature magnetic properties of these systems. This is most likely a consequence of the non-spherical symmetry of the H{{formula:bf64d4b0-521b-434c-859d-b372ff0ca1f0}} O{{formula:4f478e14-d050-4d88-8970-905358fee910}} ion and its ability to hydrogen bond with some of the 12 sulfate group oxygens that coordinate the A site. When the H{{formula:59f3bf51-0e57-4e32-ab10-1206618dd1d2}} O ions are no longer orientationally mobile, as is the situation at low temperature,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between MSE and KL losses in the context of distillation, and how do they affect the overall performance of the model?",
      "expected_output": "The main differences between MSE and KL losses in the context of distillation are their objectives. MSE loss aims to minimize the absolute error between predicted and target values, while KL loss aims to reduce the distributional disparity between predicted and target values. By combining these two losses, the model can optimize both aspects concurrently, enhancing the performance of distillation fine-tuning. For example, in the paper, employing a combination of MSE and KL divergence loss terms yields superior performance, as shown in Table REF.",
      "context": [
        " or parameters of the model step-by-step to observe the effect on overall performance.We experimentally explore the effect of different distillation losses on distillation fine-tuning performance, and the results are shown in Table REF . Our ablation study reveals that employing a combination of MSE and KL divergence loss terms can yield superior performance. Specifically, the MSE loss aims to minimize the absolute error between predicted and target values, while the KL loss aims to reduce the distributional disparity between predicted and target values. By integrating these two losses, we can optimize these critical aspects concurrently, thereby enhancing the performance of distillation fine-tuning.We compare the effect of different model architectures on distillation, as shown in Table REF . Our results show that the hybrid student model combining CNN and Transformer improves knowledge distillation, but only improves ACC by 0.97%. Noteworthy is the observation that the FPD method with TransScan module achieves a significant increase in performance, marking up to 4.85% and 6.2% improvement in ACC and AUC, respectively. The introduction of the TransScan module provides a remarkable improvement in model performance. {{table:7156ea45-824b-4f0f-a1cd-b",
        "084f1c2}}In addition, combining these three strategies we compare VFD and FPD. The results reveal that the distilled student model of FPD method outperforms the VFD method across all fine-tuning strategies. Particularly, we observe a more substantial performance boost of up to 5.2% on the TCGA-BRCA dataset, surpassing the performance delivered by the BROW model. On the TCGA-BRCA dataset, Li et al. achieve an AUC of 93.5% by fine-tuning the ResNet50 model. We fine-tune on distilled ResNet50 model and attain an AUC of 94.36%, resulting in a performance improvement of 0.86%. Furthermore, our FPD_ETC method demonstrates a significant increase in AUC of 2.65% compared to the method in . These results validate the effectiveness of the distillation techniques and highlight the advantages of feature projection distillation fine-tuning.We visualize and analyze some cases, as shown in Fig. REF . The visualization results compare the attentional heatmaps corresponding to different model classifications, including the proposed FPD_ETC method and VFD_ETC, as well as the large model approach.",
        " unsupervised feature distillation technique to distill the large model, and adopting End2end Train CLAM (ETC), a fine-tuning strategy for the distilled student model, the model efficiency and performance are significantly improved in dealing with the slide-level pathology image classification problem. \\bullet We also propose an FPD method, which uses the selective convolution mechanism introduced in the TransScan module to achieve adaptive adjustment of the receptive field size, and adopts Mean Squared Error (MSE) and Kullback-Leibler (KL) divergence as the distillation loss to further enhance the model. \\bullet We analyze the full-parameter fine-tuning, parameter-efficient fine-tuning, and distillation fine-tuning methods in terms of inference metrics such as parameters (Params), Memory Access Cost (MAC), Giga Floating-point Operations Per Second (GFLOPS), and Frames Per Second (FPS), highlighting the high efficiency of the distillation fine-tuning methods.\nRelated Work: In this section, we present a concise review of the existing literature, focusing on three key areas: medical large models, knowledge distillation and fine-tuning.\nMedical Large Models: In recent years"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of hyperelasticity on graphene's behavior in this research paper?",
      "expected_output": "The main findings regarding the impact of hyperelasticity on graphene's behavior in this research paper are that a hyperelastic material model is necessary to accurately describe graphene's nonlinear and anisotropic behavior under large deformations. The research shows that linear elastic material behavior underestimates the frequencies of graphene-based structures, whereas the proposed nonlinear material model avoids this problem. The frequencies are strongly affected by nonlinearities, including nonlinear compressing, stretching, and bending, as well as substrate interaction. The results highlight the importance of including pre-stretch or substrate adhesion in modal analysis to control resonance and stability in graphene-based devices such as MEMS and NEMS.",
      "context": [
        " significant pre-tension , .Isogeometric analysis (IGA) is a new computational technique that connects CAD and FE analysis . Recently, an isogeometric FE formulation has been developed by for the analysis of liquid and solid membranes based on inherent curvilinear coordinates. It has been extended to anisotropic membranes by and rotation-free shells by and . This shell formulation has been applied to biomaterials and composites by , and to graphene by . The latter work uses the anisotropic membrane model of and extends it to a shell formulation by including the Canham model . This new model can simulate the anisotropic behavior of graphene-based structures under large deformation and it has been used to simulate indentation and peeling of graphene sheets and torsion and bending of carbon nanotubes (CNT). Thermal fluctuation are not considered in the current study. But the proposed model does allow for an extension to those. Thermal fluctuations can result in structural softening, but they can be suppressed with small pre-strains above 1% , . Therefore and obtain similar material properties as the experimental results of by using a hyperelastic constitutive law that disregards thermal fluctuations.This work reports new data of the effect of stretching and contact on",
        "otubes. obtain the natural freqencies of single walled carbon nanotubes by using Donnell thin shell theory. used a nonlinear finite element (FE) method to analyze large deformations and obtain the nonlinear frequencies of graphene membranes for nanomechanical applications. A linear material model works well for infinitesimal strains. But, the mechanical properties of graphene vary in large strains. Hence, nonlinear hyperelastic material models should be used to model the material behavior in large strains , . Thermal vibration of rectangular, circular and annual graphene sheets are studied by , , and . The vibrational properties of multi-layer circular and rectangular graphene sheets are obtained by and . has modeled the size-effects on vibrational properties of rectangular plates. The vibrational properties of a graphene sheet can be calculated by molecular mechanics and molecular dynamics . have calculated the natural frequencies and mode shapes of CNTs by analytical approaches and validated them by experimental, atomistic and FE results. have computed the natural frequencies, mode shapes and force vibration of CNTs.All mentioned continuum models are limited by linear elastic material behavior. However, graphene shows nonlinear and anisotropic behavior under large deformations . develop a hyperelastic material model for graphene that is based on three strain",
        " graphene-based structures are determined with a new rotation-free shell formulation based on isogeometric finite elements. A hyperelastic material model is used to describe graphene-based structures under large deformations, accounting for nonlinear compressing, stretching and bending. The frequencies are affected strongly by those nonlinearities. Additionally, the nonlinearities of substrate interaction have a strong effect on the frequencies. A calculation based on linear elasticity can underestimate the frequencies, whereas the proposed nonlinear material model avoids this problem. It is therefore important to include the effects of pre-stretch or substrate adhesion in the modal analysis. The results of the current research can be used in the detailed design of graphene-based devices such as MEMS and NEMS. These calculations are essential to control resonance and stability. The modal analysis for square and circular plates under pure dilatation, uniaxial stretch, and substrate adhesion is conducted. The natural frequencies are verified with the analytical solutions for zero pre-load. The natural radial breathing mode of a CNT is compared with results from the literature and there is good agreement. The variation of CNT frequencies with uniaxial stretch and compression is obtained and the influence of chirality is investigated. The strains where"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the interactions between flagellum, head, and forces in this research paper?",
      "expected_output": "The research paper discusses the interactions between the flagellum, head, and forces in the context of spermatozoon motion. The main findings include the use of a second-order-accurate finite difference scheme to approximate the algebraic system of equations for the flagellum's motion. The paper also highlights the importance of determining the linear and angular velocities at the base of the flagellum, which couple the motion of the flagellum to that of the head. Additionally, the paper analyzes the dynamics of the spermatozoon head and the non-local hydrodynamic interactions between the head and flagellum. The boundary conditions, including the force and moment balance equations, are also discussed. Overall, the paper presents a comprehensive model for understanding the complex interactions between the flagellum, head, and forces that govern spermatozoon motion.",
      "context": [
        " approximated with a second-order-accurate finite difference scheme. This results in an algebraic systems of equations for the values of {{formula:09bdbfcb-d916-41d9-b28d-9f60169772f3}} at the grid points. We note, however, that the three variables {{formula:bb5661f9-70aa-4b41-8e2b-588d12663c86}} remain unknown: these are the linear and angular velocities at the base of the flagellum, which couple the motion of the flagellum to that of the head and must be determined self-consistently along with the solution for the tension and normal forces. To this end, the dynamics of the spermatozoon head must be analyzed, as we explain next.Equations (REF )–(REF ) constitute a linear system for the traction field {{formula:6724e176-8e25-427c-a761-b12d55734287}} on the head surface, linear velocity {{formula:a57d366c-3652-4f71-8580-a4243b500477}}",
        "9-b3cf-d8925877d947}} denote the tangential and normal components of the disturbance velocity due to to tractions on the both head and flagellum, which captures non-local hydrodynamic interactions: {{formula:7f5f777e-29b7-4dd6-88e1-f1634efd0928}}\nBoundary conditions: The head of the spermatozoon has no motility and is pushed, dragged, and rotated by the force and torque coming from the flagellum. The free body diagram in Fig. REF (b) illustrates the forces acting on each component of the sperm model. The boundary condition at {{formula:2e7fffe3-ed96-44f1-a5b0-a2ef7054fbee}} is given by the force and the moment balance equations {{formula:3c288b23-ce45-4432-9f8a-926ff5a4ce31}}The distal end of the flagellum is force and moment-free, which implies {{formula:60af5da5-3807-42cd-bd80-",
        " equations (REF )–(REF ) for the flagellum elastohydrodynamics and internal motor kinetics and actuation, coupled to the boundary integral equation (REF ) on the surface of the sperm head. The coupling occurs through the clamped boundary conditions at the head-flagellum junction as well as through the net force and torque balance on the assembly as given in equations (REF )–(). To numerically solve this system we first combine these governing equations to yield a linear system for the unknown contact forces {{formula:a6df34a9-422d-4000-8e76-969614756b2d}} . The combined equations are given as: {{formula:6ac5dbaf-7177-4d27-a294-6eb1ebf2fa67}}{{formula:e45a127d-1fb7-4a48-836c-0ea331450334}}For a given flagellar shape {{formula:16ed8af2-9e44-4b6e-9d92-6b75354cf8cb}} and given bound motor distributions {{formula:7677d9b3"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do integrators, thermostatting, and ab initio calculations contribute to reducing the costs associated with path integral calculations in this research?",
      "expected_output": "Integrators, thermostatting schemes, and ab initio calculations contribute to reducing the costs associated with path integral calculations by increasing efficiency. Efficient integrators and thermostatting schemes address non-ergodicity, while methods like reducing force evaluations or points on the imaginary time path also decrease computational costs. Additionally, improvements in ab initio calculations enable computationally efficient simulations, making on-the-fly first principles path integral molecular dynamics feasible. This combination is beneficial as ab initio calculations aim to reproduce the exact Born-Oppenheimer potential energy surface, which is essential for accurately accounting for nuclear quantum effects.",
      "context": [
        " performed, albeit traditionally with a significant computational overhead.A series of recent developments have dramatically decreased the computational cost of path integral simulations. These have taken the form of the introduction of efficient integrators and thermostatting schemes to address non-ergodicity , , . In addition, methods have been developed to reduce the number of force evaluations or points on the imaginary time path , , , , , . These developments along with improvements in the ability to perform computationally efficient ab initio calculations make on-the-fly first principles path integral molecular dynamics a feasible proposition , , , . Such a combination is natural, since ab initio calculations attempt to reproduce the exact Born-Oppenheimer potential energy surface. Whereas nuclear quantum effects are often implicitly accounted for by empirical potential energy surfaces, which are typically parameterized to reproduce experimental properties, they are completely absent in ab initio calculations unless they are treated explicitly.In this work we introduce a series of developments to allow the extraction of free energy changes upon isotope substitution from path integral simulations. Firstly, we show how the free energy change can be computed efficiently by performing a physically motivated transformation of the thermodynamic integration variables. Secondly, we address the failures of harmonic approximations when applied to liquids and other systems with anharmonic",
        " {{formula:b1a34442-725b-42b4-837c-e2ca3ac7ec70}} decreases with the square root of the number of substituted atoms. This implies that the instantaneous values of {{formula:d15ce200-0bce-4c98-8495-dd9a62d1c486}} for atoms in different molecules are almost perfectly un-correlated – another indication of the local nature of the quantum kinetic energy. However, we stress that one should be careful when performing multiple substitutions, in particular for inhomogeneous systems, and that whenever possible one should benchmark this approximation against the highly-diluted case.Even exploiting our efficient transformation of the integral which requires only simulations at the two extrema the need to perform multiple simulations to obtain free energy changes using direct substitution is undesirable. We now consider our FEP approaches which allow the free energy change to be obtained from a single trajectory of the most abundant isotope.\n“Thermodynamic” free energy perturbation: The TD-FEP estimator in Eq. (REF ) provides an appealing approach to the computation of the kinetic energy of an isotope-substituted system by",
        "# arXiv Paper: 1211.1475\n\nEfficient methods and practical guidelines for simulating isotope effects\nabstract: The shift in chemical equilibria due to isotope substitution is often exploited to gain insight into a wide variety of chemical and physical processes. It is a purely quantum mechanical effect, which can be computed exactly using simulations based on the path integral formalism. Here we discuss how these techniques can be made dramatically more efficient, and how they ultimately outperform quasi-harmonic approximations to treat quantum liquids not only in terms of accuracy, but also in terms of computational efficiency. To achieve this goal we introduce path integral quantum mechanics estimators based on free energy perturbation, which enable the evaluation of isotope effects using only a single path integral molecular dynamics trajectory of the naturally abundant isotope. We use as an example the calculation of the free energy change associated with H/D and 16O/18O substitutions in liquid water, and of the fractionation of those isotopes between the liquid and the vapor phase. In doing so, we demonstrate and discuss quantitatively the relative benefits of each approach, thereby providing a set of guidelines that should facilitate the choice of the most appropriate method in different, commonly encountered scenarios. The"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do undoped crystals' higher light yields and eliminated Cherenkov radiation contribute to improved detector sensitivity in this research?",
      "expected_output": "The higher light yields of undoped crystals at 77 K and the elimination of Cherenkov radiation contribute to improved detector sensitivity by allowing for a lower energy threshold and increased accuracy in detecting non-standard neutrino interactions. For example, the measured light yield of an undoped CsI crystal at 77 K was 33.5 ± 0.7 PE/keVee, which is about three times higher than those achieved with the COHERENT CsI(Na) detector. This increased sensitivity enables more precise measurements and better detection of neutrino oscillations.",
      "context": [
        " the framework of NSIs, hence help in interpolating neutrino oscillation data in general. The key technical advantages of the prototype include much higher light yields of undoped crystals at 77 K compared to those of doped ones at room temperature, and a complete elimination of Cherenkov radiation originated from PMTs that seriously limits the energy threshold of current inorganic scintillator detectors. As one of the initial steps to verify the feasibility of the proposed technique, the light yield of an undoped CsI crystal at about 77 K was measured to be {{formula:fe9ae58c-d9d5-4e2c-bbc0-4d77f27f0492}} PE/keVee in the energy range around [13, 60] keVee. This was about three times higher than those achieved with the COHERENT CsI(Na) detector , and the DAMA and COSINE NaI(Tl) detectors.This work is supported by the National Science Foundation (NSF), USA, award PHY-1506036, and the Grant-in-Aid for Encouragement of Young Scientists (B), No. 26800122, MEXT, Japan. Computations",
        "# arXiv Paper: 2008.00939\n\nProspect of undoped inorganic scintillators at 77 Kelvin for the detection of non-standard neutrino interactions at the Spallation Neutron Source\nabstract: Investigated in this work are sensitivities to non-standard neutrino interactions (NSI) of a prototype detector placed about 20 meters away from the Spallation Neutron Source at the Oak Ridge National Laboratory in two years of data taking. The presumed prototype consists of 10 kg undoped CsI scintillation crystals directly coupled with SiPM arrays operated at 77 K. Compared to the COHERENT CsI(Na) detector, a much higher light yield is assumed for the prototype. An experiment with a cylindrical undoped CsI crystal coupled directly to a photomultiplier tube at about 77 K was conducted to verify the light yield assumption. A yield of $33.5 \\pm 0.7$ photoelectrons per keV electron-equivalent (PE/keVee) was achieved in [13, 60] keVee, which is much closer to the relevant energy region for the NSI search than some of the early studies.\nIntroduction: Forty",
        "-21d01241750a}} 50%, which are already available in the market.However, the high yields were measured at an energy range from 662 to 2614 keVee , , far away from the region that is relevant to the CEvNS detection. There is already evidence of the non-linear scintillation responses of undoped NaI and CsI crystals. A measurement of the light yield at a lower energy region is needed to verify the feasibility of using undoped NaI and CsI at 77 K for CEvNS and NSI detections.Reported in this paper is such a measurement using an undoped CsI crystal at 77 K with an {{formula:f785cb30-dd27-4a32-85a5-798e55a6daf2}} Am source down to 13 keVee. Based on the measured light yield, the potential of a {{formula:e71b425d-24c1-4519-b340-ee4ca4403d3e}} kg undoped CsI prototype detector located {{formula:278a3b4e-1fac-4f6f-95f2-cbfcbdff"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the two-peak hypothesis improve the description of the $B^+K^-$ mass spectrum?",
      "expected_output": "default",
      "context": [
        "# arXiv Paper: 2010.15931\n\nObservation of new excited $B_s^0$ states\nabstract: A structure is observed in the $B^+K^-$ mass spectrum in a sample of proton--proton collisions at centre-of-mass energies of 7, 8, and 13 TeV, collected with the LHCb detector and corresponding to a total integrated luminosity of 9 fb${}^-1$. The structure is interpreted as the result of overlapping excited $B_s^0$ states. With high significance, a two-peak hypothesis provides a better description of the data than a single resonance. Under this hypothesis the masses and widths of the two states, assuming they decay directly to $B^+K^-$, are determined to be $m_1 = 6063.5 \\pm 1.2 \\text{ (stat)} \\pm 0.8\\text{ (syst) MeV},$ $\\Gamma_1 = 26 \\pm 4 \\text{ (stat)} \\pm 4\\text{ (syst) MeV},$ $m_2 = 6114 \\pm 3 \\text{ (stat)} \\pm 5\\text{ (sy",
        "$ $m_2 = 6114 \\pm 3 \\text{ (stat)} \\pm 5\\text{ (syst) MeV},$ $\\Gamma_2 = 66 \\pm 18 \\text{ (stat)} \\pm 21\\text{ (syst) MeV}.$ Alternative values assuming a decay through $B^{*+}K^-$, with a missing photon from the $B^{*+} \\rightarrow B^+\\gamma$ decay, which are shifted by approximately 45 MeV are also determined. The possibility of a single state decaying in both channels is also considered. The ratio of the total production cross-section times branching fraction of the new states relative to the previously observed $B_{s2}^{*0}$ state is determined to be $0.87 \\pm 0.15 \\text{ (stat)} \\pm 0.19 \\text{ (syst)}$.\n: 0.5em"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research on M-branes and their role in producing 5D black holes, and what methods were used to obtain these results?",
      "expected_output": "default",
      "context": [
        " stable and unstable black brane solutions depending on the charge regions of the moduli space.The organization of this work is as follows. In section 2, we present a concise discussion on 5D black objects from M-theory on CY 3-folds. In section 3, we investigate the behaviors of 5D BPS and non-BPS black holes from M2-branes wrapping 2-cycles on a three parameter CICY 3-fold via an economical model involving three ordinary projective spaces. In section 4, we approach the 5D BPS and non-BPS black strings from such a CY 3-fold. In the last section, we provide concluding remarks and open questions.\n5D black branes in M-theory CY compactifications: In this section, we present a concise discussion on the 5D black holes and the black strings using the \\mathcal {N}=2 supergravity formalism explored in the string theory compactification mechanism. In particular, these objects can be obtained from M-theory on CY three-folds using M-brane physics. The CY geometries have been explored first in superstring theory compactifications to generate models with minimal supercharges in 4D space-times , ,",
        " have been explored first in superstring theory compactifications to generate models with minimal supercharges in 4D space-times , , , , , . Alternatively, they have been used also in the compactification of M-theory to provide 5D space-time supersymmetric models. At lower-energy limits, this theory is described by 11D supergravity with \\mathcal {N}=2 supersymmetry producing certain superstring models in 10D via the dimension reduction scenarios , , , . Roughly, this 11D theory contains also two solitonic objects called M2 and M5-branes. Using the compactification mechanism, these objects have been exploited to build 5D black holes and black strings in CY three-fold compactifications, respectively. To approach such black brane solutions, one uses the 5D \\mathcal {N}=2 supergravity formalism via the following Maxwell-Einstein actionS=\\frac{1}{2\\kappa _{5}^{2}}\\int d^{5}x\\left( R\\star \\mathbb {I}-G_{IJ}dt^{I}\\wedge \\star dt^{J}-G_{IJ}F^{I}\\wedge \\star"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between Tree IRPFL's approximation ratio and the Steiner tree problem, and how do the main findings of this research paper compare to existing methods?",
      "expected_output": "The Tree IRPFL problem has a direct relationship with the Steiner tree problem, as the uncapacitated single-day Tree IRPFL can be modeled as a single Steiner tree problem. This allows for a 2-approximation algorithm. The research paper's findings compare to existing methods, such as the 3-approximation by Glicksman and Penn for grouped clients, and the constant approximations by Fukunaga et al. for periodic IRP and IRP on tree metrics. For example, the paper mentions that the Steiner tree approximation method of Goemans and Williamson can be generalized to achieve a 3-approximation for grouped clients.",
      "context": [
        " approximation ratio for problem {{formula:52843994-6587-49c9-9f63-a06da1fc7ee7}} . For uncapacitated single-day Tree IRPFL, the problem can directly be modeled as a single Steiner tree problem: attach a new root node with edges to each facility of cost equal to its opening cost; finding a Steiner tree from this root to all the clients gives the required solution. Thus, this problem has a {{formula:2de5e7c6-9e26-49d1-8b46-98d28b1deb3e}} -approximation algorithm. If clients are given in groups such that only one client per group needs to be served, Glicksman and Penn generalize the Steiner tree approximation method of Goemans and Williamson to {{formula:d0949078-d4ca-4b1d-967e-af32f3ca5f6f}} -approximation, where {{formula:4034e257-c232-495a-b98c-d2a8fc7a1a65}} is the largest size of a group. For",
        "5062}} -approximation by Fukunaga et al . For variants of periodic IRP, Fukunaga et al. provide constant approximations. IRP on tree metrics have a constant approximation . Another special case of IRP is the joint replenishment problem (JRP), which has also been extensively studied , , , , .TreeIRPFL: Another related problem is the Tree IRPFL, which has the same requirements except that the connected components for the daily visits are trees (instead of tours in the regular IRP, or stars in the Star IRP version we study). Tree IRPFL differs from Star IRPFL by allowing savings in connection costs by connecting clients through various other clients who are connected to an opened facility.Single-day variants of Tree IRPFL have been studied extensively. In these problems, there is no holding cost component and thus they trade off the facility location placements with the routing costs from these facilities. We use {{formula:b7b1d207-c68d-4110-830d-92f17b3bee89}} to denote the best existing approximation ratio for problem {{formula:52843994-6587-49c9-9f63-a06"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What techniques can be used to reduce neural network parameters without affecting their performance, and what are the main findings of this research in terms of model compression?",
      "expected_output": "Techniques to reduce neural network parameters without affecting performance include designing resource-efficient networks, such as MobileNet and EfficientNet, and network pruning, which can be either unstructured or structured. The research findings show that low-rank initialization can achieve training time speedups of 1.3x for Automatic Speech Recognition (ASR) and 1.15x for Audio-Visual Speech Recognition (AVSR), with the ASR model having 243M parameters and the AVSR model having 268.5M parameters. The models were trained using the AdamW optimizer with a constant learning rate and different schedulers for ASR and AVSR.",
      "context": [
        " and training time speedups (by 1.3x for ASR and 1.15x for AVSR).\nIntroduction: Training large machine learning (ML) models for computer vision, natural language processing, and speech recognition has become increasingly challenging due to the exponential growth in the number of parameters, scaling from millions to billions , , and even reaching trillions . To expedite the training process and alleviate memory constraints, researchers have explored numerous techniques aimed at reducing the parameter count of modern neural networks.One avenue of research has focused on the design of resource-efficient networks, exemplified by models such as MobileNet and EfficientNet . While these models reduce size, they often come at the expense of performance. Another widely adopted approach is network pruning, which seeks to eliminate redundant or less significant components within a model. Unstructured pruning, for example, removes individual weights and achieves high compression rates, but is not hardware-friendly , , . Conversely, structured pruning, which removes entire structures such as channels or attention heads, offers real speedups on common hardware, though it typically achieves lower compression ratios . {{figure:9b39affa-e5ae-4b74-b39e-7c6160c87262}}Recently, low",
        " comprises a 1D ResNet front-end, followed by a Conformer encoder, a Transformer decoder and a CTC layer, resulting in 243M parameters. Following , our AVSR model comprises a ResNet frontend for audio and video modalities, a multi-layer perceptron for early fusion of multi-domain features, a single Conformer encoder, a Transformer decoder and a CTC layer, resulting in 268.5M parameters.Training details. The models are trained for 75 epochs using the AdamW optimiser . A constant learning rate with square root cooldown scheduler is used for ASR, while a cosine scheduler is used for AVSR. The peak learning rate is 0.0006/0.001 for ASR/AVSR, 15 epochs of cooldown for ASR and a warm-up of 5 epochs for both. The ASR/AVSR models are trained with 32/64 A100 GPUs, respectively.\nResults: {{table:22ce9ae4-4a69-4aff-b118-0404600a1613}}Low-Rank Initialization Comparison. Figure REF illustrates the performance of low-rank ASR models initialized using two different methods: the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What happens to current density as the temperature approaches the critical temperature Tc in the research paper?",
      "expected_output": "As the temperature approaches the critical temperature Tc, the current density behaves similarly for both RTA and BGK collision kernel, with significant effects of inhomogeneity of the electric field in the lower temperature regimes.",
      "context": [
        " describe the behaviour of the current density in the temperatures closer to the transition temperature {{formula:93ad1e52-642b-4cab-a697-c7b1a6516bfe}} . We observe similar temperature behaviour of {{formula:1a7a388d-3914-4647-9ed7-8adbc8f3074c}} for both RTA and BGK collision kernel. The quantitative difference in the results with RTA and BGK are depicted in Fig. REF (right panel). We have also compared the HLL result for {{formula:e8c91192-5c8c-4d66-b76a-e402f5c0f3e9}} with the BAMPS estimation at {{formula:3df6569f-2282-4dbe-a978-9453f9f9eb8d}} .Finally, we have observed that the effects of inhomogeneity of the electric field on {{formula:079e8347-601b-4059-8afd-265fdbc1aa6d}} are significant in the lower temperature regimes. The EoS dependence of the",
        "ula:c5b9324b-5acd-4678-b64b-a0e1279ebfb7}} as described in the Eq. (REF ) is depicted in the Fig. REF . The ratio {{formula:0436152a-7f3f-46d1-8f9f-d4a0965c785c}} is plotted for both RTA and BGK collision terms and we observe a similar behaviour for both the collision integrals. The {{formula:b7cf715f-1757-4249-a6dd-27626e216d0e}} increase with increasing temperature and seen to saturate at higher temperatures. The temperature behaviour of the ratio {{formula:fffa4945-3fa8-435e-8df4-1bc21c79ae8f}} in the presence of the magnetic field indicates that the effects of spacetime inhomogeneity of electric field are significant in the temperature regime near to the transition temperature {{formula:10decb97-b420-4234-828e-a561875a52ee}} whereas the effects are negligible at high temperatures. {{figure:e815"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods do equi-areal mirrors use to minimize distortion, and what are the key findings related to their effectiveness?",
      "expected_output": "According to the paper, equi-areal mirrors use techniques to minimize angular and areal distortion. The key findings related to their effectiveness are not explicitly stated in the provided context, but the paper introduces techniques for developing new mirror surfaces that simultaneously minimize these distortions.",
      "context": [
        "# arXiv Paper: 1411.3694\n\nEquitable Mirrors\nabstract: In this article we explore some finer properties of equi-areal mirrors and introduce techniques for developing new mirror surfaces that simultaneously minimize angular and areal distortion.\nIntroduction: The OSA REV4-1 template is designed to assist authors with creating a two-column manuscript that can be submitted to JOSA A, JOSA B, Optics Letters, and Applied Optics (separate templates are available for other OSA journals, which have not yet migrated to REV).See the “REVTeX 4.1 Author's Guide” for an overview of REV4-1's features . See the “OSA Author Style Guide” for points on OSA journal style , such as use of OCIS codes.\nPreparing a REVTeX Manuscript for Submission to an OSA Journal: Optics Letters authors: Be aware of two major changes for OL papers. Authors now have a firm four-page limit for their papers, and submitted papers must contain a fifth informational page, with complete references. Preamble Use the following preamble to invoke REV4.1 for OSA journals. Use",
        " complete references. Preamble Use the following preamble to invoke REV4.1 for OSA journals. Use the “10 pt” option for JOSA A, JOSA B, and OL; use the “11 pt” option for Applied Optics: \\documentclass[osajnl,twocolumn,showpacs, superscriptaddress,10pt]{revtex4-1} Citations in REV use the natbib package. The osajnl4-1.rtx package will enforce appropriate style for citations callouts: “{{formula:25031fbe-c097-4d4a-9a74-a33a739e7a89}} in this study [1,2].” Use the \\cite{} command for all callouts. Bib may be used to create a file containing the references, whose contents (i.e., contents of .bbl file) can then be pasted into the bibliography section of the .tex file. References must be contained within the .tex file, not a separate Bib file. Compress your .tex manuscript file and all figures (which should be in EPS format)"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the electric Hamiltonian and SU(3) gauge fixing, and how does it impact the research findings?",
      "expected_output": "The electric Hamiltonian is related to SU(3) gauge fixing, as it can be written solely in terms of loop variables after gauge fixing. This is achieved by eliminating the maximal tree variables from the Hamiltonian, resulting in a simplified expression. For example, the electric Hamiltonian can be written as {{formula:b8e82ff0-c9ad-49ad-93c5-2ae02d3f93d1}}, which only depends on loop variables. This simplification is crucial for the research findings, as it allows for a more straightforward analysis of the system.",
      "context": [
        "3b-3cea0ede312c}} .With this expression in hand, we are now able to write the electric Hamiltonian. {{formula:656829d1-a044-43c5-b20b-bce0804c5ce2}}Note that we are able to leave out the generators of the maximal tree variables, in accordance with the discussion in Sec. V of Ref. .The magnetic Hamiltonian can also be written solely in terms of the loop variables: {{formula:027c9acf-0d4a-4173-9817-8276fce95047}}In this expression, if either subscript of {{formula:9f00954f-dabc-4fa2-b2ee-a9ad5926d355}} is zero, then {{formula:686f8986-356f-43d9-8e77-b413816fff31}} is defined to be the unit element.This completes the gauge fixing of the SU(3) Hamiltonian. The general state in the zero-charge subspace is independent of the maximal tree variables, and we have also eliminated these variables from the Hamiltonian. The sole vestige",
        "-634c-4f56-b8ca-3cf384819693}} with an arbitrary function of group elements is the same as that of {{formula:234a6da7-6fed-4441-8c35-fed17fd51f28}} : {{formula:24e77b31-92a7-4901-b53b-1fb65c7bfd4a}}This last point is important, because then the primed analogue of Eq. (REF ) will hold.We now proceed with the second change of variables, which is the link-to-loop change used in the pure gauge case. As a result, the operators in the electric Hamiltonian are {{formula:856bd826-5029-40b1-a215-78b221825239}}We can thus write the electric Hamiltonian, {{formula:b8e82ff0-c9ad-49ad-93c5-2ae02d3f93d1}}The magnetic Hamiltonian is unchanged from the pure-gauge case: {{formula:df3e11a3-614c-4c07-846f-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the third interferometer use to detect photon passage and what are the key findings related to this detection?",
      "expected_output": "The third interferometer uses the output of the fringe pattern to detect photon passage. The key findings are that the fringe pattern detected is different when the photon passes through the inner interferometer compared to when it does not. This difference allows for the determination of whether the photon passed through the inner interferometer. The interaction between the coherent beam and the photon inside the inner interferometer is represented by a Hamiltonian, which includes the creation and annihilation operators for the photons and the interaction strength between the two paths.",
      "context": [
        " Fig. REF . The output of the third interferometer ({{formula:c7e5797c-73d1-443c-908a-7069ee69923d}} and {{formula:a3f42eca-46e7-4f10-8c16-8258c23c0efa}} ) will give us a fringe pattern due to the interference between two coherent fields. The fringe pattern detected by {{formula:74d0158a-845c-4b5d-abef-49061ec404b0}} (or {{formula:7fee9939-bd6d-4dce-9d08-b605399ce043}} ) in the case of the photon passing through the inner interferometer (due to the interaction with the coherent fields) is different from that in the case of the photon not passing through the inner interferometer. Thus from the fringe pattern, we can determine whether the photon passes through the inner interferometerThe arm of the third interferometer that passes through the Kerr-medium carries the field ({{formula:22ebac2e-0a0d-42db-aff",
        "-medium carries the field ({{formula:22ebac2e-0a0d-42db-aff1-b6e457cda1b3}} ). Inside the Kerr-medium, the interaction between the coherent beam and the photon inside the inner interferometer can be represented by the Hamiltonian , , {{formula:4b3218ad-66df-4b75-965b-28fec7ad5f43}}where {{formula:fa323fff-b68d-4498-9f95-d70203a2db1f}} and {{formula:e631138f-e833-456f-8f75-6dda795a7d43}} are the creation and annihilation operators for the photons of {{formula:dda50af2-21d7-4133-b819-e4fe1f7ac386}} , and {{formula:dfd0854e-5b0c-4c5f-91bf-c847dc881d6e}} is the interaction strength between the two paths of the inner interferometer in the Kerr-medium. As the two paths"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between polarization profiles and pulsar types, and how do the methods used impact the key results?",
      "expected_output": "The research paper presents key findings on the polarization profiles of 100 pulsars, including millisecond pulsars, observed at 774 MHz using the Green Bank Telescope. The main findings indicate that millisecond pulsars have complex polarization profiles, often with wide profiles and flat position-angle curves, whereas normal pulsars exhibit simpler profiles. Strong linear polarization is detected in some pulsars, while circular polarization is generally weak. The methods used, including polarimetric observations and rotation measure determination, impact the results by providing insights into the emission mechanisms and the Galactic environment. For example, the paper notes that single pulse polarization observations reveal orthogonal modes of polarization, which are not always evident in integrated profiles. The study's results, such as the lack of correlation between spin-down luminosity and linear polarization percentage, contribute to our understanding of pulsar emission processes.",
      "context": [
        "# arXiv Paper: 0901.0962\n\nPolarization observations of 100 pulsars at 774 MHz by the Green Bank Telescope\nabstract: We report on polarimetric observations of 100 pulsars centered on 774 MHz, made using the Green Bank Telescope, presenting their polarization profiles and polarized flux densities and comparing them with previous observations when possible. For 67 pulsars, these are the first such measurements made. Polarization profiles of 8 millisecond pulsars in our sample show wide profiles and flat position-angle curves. Strong linear polarization, sometimes approaching 100% of the total intensity, has been detected in all or a part of the average pulse profiles of some pulsars. In general, circular polarization is very weak, although it is observed to be extremely strong in the leading component of PSR J1920+2650. Sense reversal of circular polarization as a function of pulse phase has been detected from both core and other components of more than 20 pulsars. Any relationship between the spin-down luminosity and the percentage of linear polarization is not evident in our data at this frequency.\nIntroduction: More than 40 years after the discovery of the first pulsars, there is currently no consensus on how electromagnetic radiation is generated in the pulsar magnet",
        " 6 or 7 orders of magnitude smaller in volume. The total intensity profiles of millisecond pulsars are qualitatively similar to those of normal pulsars, though with a tendency to cover a wider region of pulse phase . In contrast, the polarization profiles of millisecond pulsars , , , are typically much more complex than those of normal pulsars. Polarization observations of millisecond pulsars are relatively scarce, and most are presently available at only one radio frequency (see references above).Single pulse polarization observations show that orthogonal modes of polarization can be emitted at a given pulse phase , . In the integrated polarization profile, transitions between orthogonal modes typically stand out as jumps in the P.A. curve; in general, orthogonally polarized modes are elliptically polarized , . Furthermore, histograms of the orientation of the polarization vector provide evidence of nonorthogonal modes of emission at a given pulse longitude .We have observed 100 pulsars with the Green Bank Telescope (GBT) in order to measure their polarization properties. We also determined the rotation measures (RMs) of these pulsars. The RMs will be presented in a second paper , and, together with an additional 377 RMs, will be used in an analysis of the large-scale Galactic"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between type 1 and type 2 idempotents, and how do they relate to the research findings in this paper?",
      "expected_output": "The main differences between type 1 and type 2 idempotents are their forms, with type 1 being trivial idempotents and type 2 being of the form {{formula:dbe775de-49d3-4f47-85b2-2abc7e435b22}}. According to the research paper, type 1 idempotents are {{formula:26860525-4bf7-482e-bb65-f46ca3a8e4bb}} and {{formula:d607a104-7a64-4453-98eb-8a0216e1496e}}, while type 2 idempotents are derived from the action of an idempotent on a relation, making them non-trivial and more complex in nature.",
      "context": [
        "formula:31fbac74-037b-45eb-a927-4ea6a0beed42}} and {{formula:4e8d1c3c-8498-4145-832f-7a2c1a947155}} .Now if we use (REF ) in (REF ) and (REF ), we find that {{formula:2f818999-9679-4071-9b8b-f69b2b9be9a0}} is an idempotent of {{formula:60f0490b-e27f-47b5-b3f0-64ef1478372a}} , and {{formula:7ff999d7-32c4-456c-aada-0d4f24aeb68f}} is an idempotent of {{formula:e4ab74df-1c34-4b6d-af33-9c321ef22adb}} .If we apply {{formula:18d0eb3c-cb14-465b-b54b-789c8eff517a}} to the relation {{form",
        "-697b9ac00546}} in {{formula:f5cf9215-4fe1-48c8-8c1f-18bee90782a0}} is idempotent if and only if {{formula:f1a10a5f-7c69-4acf-b2df-d867886df2ad}}Therefore, apart from the trivial idempotents {{formula:26860525-4bf7-482e-bb65-f46ca3a8e4bb}} and {{formula:d607a104-7a64-4453-98eb-8a0216e1496e}} , the other idempotents of {{formula:08568b76-5a18-4979-b56e-cc8a45444699}} are of the form {{formula:dbe775de-49d3-4f47-85b2-2abc7e435b22}}The two types of idempotents in (REF ) will henceforth be called type 1 idempotents and type 2 idempotents respectively.We look at the action of an"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the geographical organization of data impact the analysis and what methods are used to achieve this organization in the research paper?",
      "expected_output": "The geographical organization of data in the research paper is achieved by dividing the country into regions, districts, or areas. This organization impacts the analysis by allowing researchers to focus on specific geographical locations and understand the variations in data across different areas. The methods used to achieve this organization include using files such as region.text, milieu.txt, and cluster.txt, which hold variables related to geographical divisions. For example, the person file contains disaggregated data, including variables like region and cluster, which can be used to analyze data at the individual level within specific geographical areas.",
      "context": [
        "S™ in most of the NSI's. When IBM SPSS™ is used, the person file is given in an SPSS™ fold contained the data and the meta-file.Figure REF shows how the data are displayed.{{figure:f9b834d9-4c3f-499c-bb5c-beb6827b356a}}Figure REF provides information of the variables, which are listed in Column 2 called Name. For each variable, the type (categorical, logical, numerical, etc. is precised). For a factor or label variable, the range of the values are given in Column Values.{{figure:e2eaf487-f276-4d81-867e-476dd61dfd38}}For MS Excel ™ files, a separate dictionary files is given in replacement of the IBM SPSS ™ meta-file.It is not excluded that different software packages or other file formats (like CSV) may be used in some NSI's. However, the files organization must be similar and automatic conversions are available.B - Organization of the data.(a) First of all, the country is organized into geographical regions, districts or areas.",
        ": In the 80's, in the 90's and in the first decades of the 21th century, a number of surveys have been conducted. Nowadays, surveys destined to collect longitudinal data are available. But this paper will be devoted to households surveys with the objectives of showing how researchers may be quickly introduced to be ready use such data.\nDescription of Households data: {{formula:4df3bfc8-6f3a-445d-b318-9fdf71a42a49}}A - Organization of the files.The households databases (HDB in short) gather information collected in a sample of households. In each household, all the member respond to the questions. At the arrival, a HDB usually contains three main files.(1) A person file which contains the responses of individuals of all households. This file contains the disaggregated data.(2) A household file which contains aggregated data at the household level provided experts of the NSI's.(3) A meta-file or dictionary which explains the features of each variable.These files are provided in MS Excel™ or in IBM SPSS™ in most of the NSI's. When IBM SPSS™ is used, the person file is given in",
        "862ca-81ac-48fe-889e-c41d2d2417b3}} is the total income of the household divided by {{formula:d6af279d-4c38-4964-a886-3916d824d4e9}} . We notice here that the DMP scale cannot be applied directly to the individual, to the contrary of the Oxford and Fao-Fam scales.\nPractice: This version will focus on MS VB™ codes. We recommend to start by Open the MS Excel™ or the IBM SPSS™, and for each variable we want to use :(1) Copy the associated column(1) Open a text file (with Notepad, Bloc note, etc.)(1) Paste the copied column(1) Name the file and save it.Using this method, we create the files : region.text (for the main area division), milieu.txt, cluster.txt, household.txt, gender.txt, age.txt holding the variables of the same names : region, milieu, cluster, household, gender, age.We also need the variable poswrchief (position with respect to the chief of the household), which"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between log-canonical threshold and Hodge spectral exponents in the context of this research paper?",
      "expected_output": "The log-canonical threshold is the smallest Hodge spectral exponent, denoted as α1, which is calculated as α1 = 1/e1 * (1/n1 + 1/(n1+l1)). This value represents a key point in the distribution of Hodge spectral exponents, with no exponents less than α1. The relationship between the log-canonical threshold and Hodge spectral exponents is fundamental in understanding the behavior of the cumulative difference function, particularly in intervals where the function's sign remains constant.",
      "context": [
        ").Corollary 2.3 The smallest Hodge spectral exponent is the log-canonical threshold\\alpha _1= \\operatorname{lct}(f) = \\frac{1}{e_1} \\left(\\frac{1}{n_1}+\\frac{1}{n_1+l_1}\\right) = \\frac{1}{n_g\\cdots n_2} \\left(\\frac{1}{n_1}+\\frac{1}{n_1+l_1}\\right) = \\frac{1}{e_0} \\left(1+\\frac{1}{1+\\frac{l_1}{n_1}}\\right).The largest Hodge spectral exponent in the interval (0,1) is\\alpha _{\\mu /2}= 1 - \\frac{1}{e_{g-1} w_g} = 1 - \\frac{1}{n_g w_g}.\nCumulative difference function : From Definition REF we have that the cumulative difference function for the case of plane curves is \\phi _f \\colon [0,1) \\longrightarrow \\mathbb {R}",
        "\\frac{1}{n_1+l_1}\\right).Thus, \\#\\lbrace \\alpha _i < \\alpha _1\\rbrace =0 and \\phi _f(r)>0 in the interval r\\in \\bigl ( 0, \\alpha _1 \\bigr ). For the rightmost interval, the largest Hodge spectral exponent is\\alpha _{\\mu /2}= 1 - \\frac{1}{e_{g-1} w_g} = 1 - \\frac{1}{n_g w_g}and thus we have \\phi _f(r)<0 in the interval r\\in \\left[1 - \\alpha _{\\mu /2}, 1\\right).",
        " - 1 + \\sum _{j=1}^g l_j e_j\\right) } \\right)withD_2 = \\left(e_0 + \\sum _{j=1}^g l_j e_j\\right)^2 - 4 \\left(2e_0 - 1 + \\sum _{j=1}^g l_j e_j\\right) \\left(\\frac{1}{4} + \\sum _{j=1}^g \\frac{l_j}{4n_j} \\right) >0.\nIntervals of constant sign at both ends of : These intervals come directly from the first and the last Hodge spectral exponents in the set given in Corollary REF and will not use the explicit expression for \\phi _f(r). For the leftmost interval, the log-canonical threshold is\\alpha _1=\\operatorname{lct}(f) = \\frac{1}{e_1} \\left(\\frac{1}{n_1}+\\frac{1}{n_1+l_1}\\right).Thus, \\#\\lbrace \\alpha _"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between turning points and branch points in the context of this research paper?",
      "expected_output": "In the context of this research paper, the turning points are branch points of the multivalued analytic function, meaning they are points where the function's value can change significantly, leading to different branches of the function. Specifically, the turning points given by the solutions of the equation are branch points of the function, viewed as a multi-valued analytic function of energy.",
      "context": [
        "007c6fdf99}}that we construct below. Here, {{formula:f0faff42-c06a-4db4-af30-34ed6525595d}}is the classical momentum at energy {{formula:742f2a70-ad92-438c-82cc-3a886be277ba}} . The turning points {{formula:28274686-0693-47e3-8c90-ce87ef2b2d29}} , given by the two solutions of {{formula:8933b2ca-a48d-4442-b276-623a08ca3025}} are branch points of {{formula:b3cd8189-e5c6-458a-81bd-0df9f9307c72}} , viewed as a multi-valued analytic function of {{formula:c91a2571-d5c2-4f95-9f50-7e768c01e77e}} . We use the multivaluedness of this function in the analysis below.We consider wave functions {{formula:9e3dcaeb-28d9-40f8-",
        ". Moreover, the asymptotic values of the coefficients as {{formula:c26a9f46-8b78-4376-8c12-027ad8aa4862}} can be controlled, because both these paths are dissipative.We choose two specific branches, {{formula:1ab4f006-5906-45c6-8631-4554416d9b50}} and {{formula:fbc3d44c-f915-431a-bc2e-30b6a959499a}} , of the multivalued function {{formula:7c22fa92-67bd-44ac-86b8-962b4c2a4eaa}} . For {{formula:b32e77b1-8986-4283-b054-b933dfebdfc6}} we place vertical branch cuts below the real axis, extending down from {{formula:1aa93ce2-52e3-467e-ac67-6190ca1395c7}} and {{formula:a127b31b-4b15-4274-8a39-ef08b2",
        "}} which we have to select in a consistent way. Note the turning points {{formula:87fcb8e1-1319-4dbb-b940-c1ba2ac1f298}} are separated for all {{formula:7c9baa30-0972-4b9f-81d7-fccfce244e78}} .We initially confine {{formula:52cafb4a-8ed7-45da-afa9-4cded055abaf}} to the real axis and define {{formula:4b095b71-3590-4f11-a5fd-4f18b238ad03}}We begin our analysis for {{formula:535598f6-ccf2-4d9d-8b76-8146e8c98f8d}} , where {{formula:863d9585-f894-4eb4-9686-f7ee6048ac72}} is the classical momentum.Suppose {{formula:cf932815-0be2-461d-92bb-eea68f57a87f}} satisfies the ordinary differential"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used in this research paper to enable fault-tolerance using CSS codes, and what are the main findings regarding their effectiveness?",
      "expected_output": "The research paper uses CSS code construction to enable fault-tolerance, specifically by combining two classical error correcting codes to address both X and Z type errors. The main findings indicate that by combining the stabilizers generated from the parity check matrices of both codes, a QECC can be formed, with the number of physical qubits in the code determined by the length of the classical codes and the number of logical qubits given by the difference between the length of the codes and the number of parity check equations.",
      "context": [
        " for universal fault-tolerance can be used , , , , .The focus of many fault-tolerant implementations are through the CSS code construction , , . A CSS quantum code is constructed using two classical error correcting codes, each individually used to address {{formula:60811f01-f3bf-45b6-9791-702b9193dc35}} and {{formula:13d110e6-e316-43b7-8b08-1c2afb5f21e4}} type errors. Given that any quantum error can be decomposed in terms of a linear combination of Pauli operators, developing an error correcting code that can address both types of errors will be sufficient for the construction of a QECC.Let {{formula:fb447557-845b-42a5-bedc-921277eda89b}} be a classical error correcting code of length {{formula:f545bf69-465f-44b4-83bd-e116e9b9109a}} that has the associated parity check matrix {{formula:cfbc41e4-7dcc-4aef-90d9-8d7e",
        "formula:cfbc41e4-7dcc-4aef-90d9-8d7e98c2b378}} , where each 0 in the parity check matrix of the classical code is replaced by the two-dimensional identity matrix {{formula:7e3c38e5-5f43-4f98-986e-b424c776504c}} and each 1 in the classical parity check matrix is replaced by the Pauli {{formula:af380382-07e0-42c0-ba16-87347cdbb034}} operator. Similarly, let {{formula:33368590-310c-4dc5-b72f-b1b8be359007}} be a second classical error correcting code of length {{formula:1f59f10f-bdfd-4b54-b014-fae47172bc49}} with an associated parity check matrix {{formula:1691ba62-7812-4a21-b1e1-74423a7ad3b3}} , where each 1 in the classical parity check matrix has been now replaced by the Pauli {{formula:bc",
        "3}} , where each 1 in the classical parity check matrix has been now replaced by the Pauli {{formula:bc418f0a-6951-413a-afed-4446a381a993}} operator. If {{formula:8c39eeee-abba-424a-bbb8-110eb1bd3ee4}} then by combining the stabilizers generated from the parity check matrices of both codes, {{formula:1a503a56-7461-4e3d-811e-5e96cc21190f}} and {{formula:c579bb0a-2fa6-418d-a942-62a1bf145f0f}} , the resulting stabilizer code forms a QECC. The number of physical qubits in the code is {{formula:bf70cdc2-d9d4-41a0-acb4-efbe74fdedb7}} , the number of logical qubits is given by {{formula:7f2611d1-ceab-4204-9e9f-f07954730aa2}} , where {{formula:0c3a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between saddle points and precursor frequencies in the context of this research paper?",
      "expected_output": "The instantaneous oscillation frequencies of the precursors and their local dumping are directly related to the locations in the complex frequency plane of the corresponding saddle points in the integral representation of the signal. The locations of these saddle points vary with space and time and are governed by the saddle point equation, with two pairs of dominant saddle points responsible for the Sommerfeld and Brillouin precursors.",
      "context": [
        " instantaneous oscillation frequencies of the precursors and their local dumping are directly related to the locations in the complex frequency plane of the corresponding saddle points in the integral representation of the signal. Those locations vary with space and time, and are governed by the saddle point equation (SPE) in Eq. (REF ), requiring that the phase function in the integrand in that representation be stationary. Analysis of the equation shows that there are two pairs of dominant saddle points, the distant and the near ones, responsible for the first (referred to as Sommerfeld) and the second (Brillouin) precursors, respectively. The saddle points in each pair are located symmetrically with respect to the frequency imaginary axis, the fact being related to the causality principle.Since SPE seems to be not solvable in a closed form, attempts have been made to solve it in an approximate manner. Brillouin's approach consisted in replacing the complex index of refraction in SPE by its expansion in powers of frequency, and then solving the simplified equation for frequency. As a result, a simple, approximate formula was found relating complex frequency to the distance and time coordinates. In the case of the Sommerfeld precursor, the applicability of this map was",
        "6745cb1}} , the exponent determining the rate of decay is approximately equal to {{formula:b8f387bb-cd24-4d2c-b99d-11e9c73ceef9}} . The contribution to precursor decrease due to natural lowering of the oscillation amplitudes in the Bessel functions is of smaller importance.\nThe location of the saddle point {{formula:35b393bd-0ff2-4440-80ce-909cdf8b7795}}: As seen from (REF ), evolution of the precursor depends on the function {{formula:fc3ca1bb-7cf7-41a8-8dbd-860972dcdc78}} which is a solution to SPE, describing the location of the distant saddle point of (REF ) in the right complex {{formula:9b689c88-dcde-47c5-aab2-7fe20ec7fa56}} half-plane. Since this function does not seem to be expressible in a closed form, approximate formulas for it were found in the literature (Brillouin , Oughstun and Sherman , and Kelbert and Sazonov ). In",
        "# arXiv Paper: physics/0412065\n\nOn Sommerfeld precursor in a Lorentz medium\nabstract: A one-dimensional electromagnetic problem of Sommerfeld precursor evolution, resulting from a finite rise-time signal excitation in a dispersive Lorentz medium is considered. The effect of the initial signal rate of growth as well as of the medium dumping on the precursor shape and its magnitude is discussed. The analysis applied is based on an approach employing uniform asymptotic expansions. In addition, new approximate formulas are given for the location of the distant saddle points which affect local frequency and dumping of the precursor. The results obtained are illustrated numerically and compared with the results known from the literature.\nIntroduction: Fundamental investigations on EM signal propagation in the Lorentz model of a dispersive medium are due to Sommerfeld and Brillouin , . These authors revealed that in addition to the main signal propagating in the medium, two precursors are formed which precede the signal. The front of the fastest (Sommerfeld) precursor propagates in the medium with the velocity of light. The instantaneous oscillation frequencies of the precursors and their local dumping are directly related to the locations in the complex frequency plane of"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the RV data baseline and the observation span of HD 45364, and how does it impact the research findings?",
      "expected_output": "The RV data baseline for HD 45364 spans over 6500 days, with observations taken by the HARPS telescope from December 2003 to September 2017, and additional observations by HIRES from December 2009 to September 2021. The extended observational baseline allows for more accurate orbital fitting and provides insights into the dynamical state of the planet pair, including their masses, inclinations, and potential resonance configuration. The longer baseline helps to reduce uncertainties and biases in the measurements, enabling more robust conclusions about the system's properties.",
      "context": [
        ": The original RV observations of HD 45364 used by were taken by the HARPS telescope over a period of approximately 1600 days from December 2003 to April 2008. Further observations of the system by HARPS have since extended its observational baseline to September 2017, spanning over 5000 days in total. improved the HARPS RV precision by recomputing the RVs from observed spectra using the Spectrum Radial Velocity Analyser pipeline of and correcting a number of systematic errors. The new HARPS data were later published as part of the HARPS-RVBank archive , which we use here. In addition, an upgrade to HARPS's optical fibres in May 2015 changed the instrumental profile and thus the RV offset between the pre- and post-upgrade RV observations . As such, we treat the pre- and post-upgrade RV data as taken from two separate instruments in this work. Finally, we also use 7 RV observations taken by HIRES from December 2009 to September 2021, provided in Table 1 of .Here we report our fits to the full set of RV data available for HD 45634, spanning a total observational baseline of \\sim 6500 days. We begin in Section REF by describing our N-body dynamical model, which we fit under",
        "likelihood. The chains are then evolved for 50,000 steps, generating a total of 2.5 million posterior samples. The results for our RV fit and MCMC sampling are summarized in Figure REF . The top panel shows the 1\\sigma , 2\\sigma and 3\\sigma uncertainties of our fit RV curve together with the observed data. We obtain best-fit instrumental jitter values of 2.15 m/s, 1.49 m/s and 4.43 m/s for the HARPS1, HARPS2 and HIRES telescopes, respectively. {{figure:7bd4ace5-4fab-4d6b-a004-f8bb02bf343f}}The large masses and close orbits of the two planets in HD 45364 give rise to mutual gravitational interactions between the planet pair that are strong enough for our dynamical model to break the usual m\\sin I degeneracy inherent to purely Keplerian models. We therefore let inclination vary as a free parameter in our model, allowing us to place constraints on the planet masses and inclinations. Using our dynamical model, we constrain the absolute masses of HD 45364 b and c to within a factor of \\sim 2 of the median values.",
        ", showed, using a Bayesian model comparison framework, that the available RV data for HD 45364 were fully consistent with the planet pair residing in a zero-libration amplitude resonant configuration that would naturally arise under the effects of smooth migration and eccentricity damping forces.New RV measurements of HD 45364 by the HARPS team, available through the HARPS-RVBank archive , as well as observations from the High Resolution Echelle Spectrometer instrument, obtained by , have significantly extended the observational baseline of the original data set analyzed in the studies described above. Based on orbital fitting of this extended dataset, conclude that the HD 45364 planets are slightly outside of the 3:2 MMR, contrary to the conclusions of previous studies and the theoretical expectations for any planet pair migrated into a compact configuration via planet-disk interactions.However, as emphasized by both and , inferences about the dynamical state of (near-)resonant planet pairs can be particularly sensitive to modeling assumptions. Indeed, show that measurement noise in RV observations tends to produce a bias towards larger libration amplitudes of planets in MMR. This is essentially because orbital configurations with large libration amplitudes occupy significantly more prior volume than low-libration amplitude configurations.In this paper, we re"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main effects of coarse graining on lattice QCD and how do they impact the research findings?",
      "expected_output": "default",
      "context": [
        " discussed. Finally, the critical coupling of the deconfinement transition is determined in both limits. Agreement to about 12% with Monte Carlo results of 2+1d Yang-Mills theory from the literature is observed.",
        "# arXiv Paper: 2112.11178\n\nCoarse graining in effective theories of lattice QCD in 1+1d and 2+1d\nabstract: In the strong coupling and heavy quark mass regime, lattice QCD dimensionally reduces to effective theories of Polyakov loops depending on the parameters of the original Wilson action $\\beta, \\kappa$ and $N_\\tau$. We apply coarse graining techniques to such theories in 1d and 2d, corresponding to lattice QCD at finite temperature and non-zero chemical potential in 1+1d and 2+1d, respectively. In 1d the method is applied to the effective theories up to $\\mathcal{O}(\\kappa^4)$. Using the transfer matrix, the recursion relations are solved analytically. The thermodynamic limit is taken for some observables. Afterwards, continuum extrapolation is performed numerically and results are discussed. In 2d the coarse graining method is applied in the pure gauge and static quark limit. Running couplings are obtained and the fixed points of the transformations are discussed. Finally, the critical coupling of the deconfinement transition is determined in both limits. Agreement to about 12% with"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do spectral-cube surveys utilize immersive 3D visualization to enhance astronomy discovery, and what are the main findings of this research?",
      "expected_output": "Spectral-cube surveys utilize immersive 3D visualization to enhance astronomy discovery by using novel environments like CAVE2, which overcomes desktop-based visualization limitations. The main findings of this research are that CAVE2, combined with a web-based controller interface, enables simultaneous 3D comparative visualization of around 100 spectral-cubes, allowing astronomers to easily organize and apply real-time transforms to the data, and request quantitative information, thereby accelerating the discovery rate in various research scenarios.",
      "context": [
        ".\nLarge-scale spectral-cube surveys and the knowledge discovery process: Current and upcoming large-scale extragalactic surveys aim to study galaxy kinematics, formation and environmental effects by capturing large numbers of spectral-cubesA spectral-cube is composed of two spatial dimensions along with a spectral or a velocity dimension., each containing large numbers of sources. While a large part of the analysis of these surveys will be done using automated pipelines, the human experience remains a key element of the scientific discovery process. To fully comprehend the data, visualization will play a major role. Data visualization is a fundamental, technologically-led process that enables knowledge discovery . {{figure:a707386b-5176-4ffa-b692-b33100c37a73}}The classical desktop-based visualization methodology where one object is examined at a time is highly inefficient for large-scale spectral-cube surveys. With optical instruments such as SAURON, PMAS/PPAK, SAMI, and MaNGA and radio facilities/instruments such as ALMA, APERTIF, ASKAP, LOFAR, and MeerKAT , , data products are growing quickly. As the resolution (voxels or spaxels)",
        "# arXiv Paper: 1610.00806\n\nAn interactive, comparative and quantitative 3D visualization system for large-scale spectral-cube surveys using CAVE2\nabstract: As the quantity and resolution of spectral-cubes from optical/infrared and radio surveys increase, desktop-based visualization and analysis solutions must adapt and evolve. Novel immersive 3D environments such as the CAVE2 at Monash University can overcome personal computer's visualization limitations. CAVE2 is part advanced 2D/3D visualization space (80 stereo-capable screens providing a total of 84 million pixels) and part supercomputer ($\\sim100$ TFLOPS of integrated GPU-based processing power). We present a novel visualization system enabling simultaneous 3D comparative visualization of $\\sim100$ spectral-cubes. With CAVE2 augmented by our newly developed web-based controller interface, astronomers can easily organise spectral-cubes on the different display panels, apply real-time transforms to one or many spectral cubes, and request quantitative information about the displayed data. We also discuss how such a solution can help accelerate the discovery rate in varied research scenarios.\nLarge-scale spectral-cube surveys and the knowledge discovery process: Current and upcoming large-scale extragalactic"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings and methods used to derive angular velocity for near-circular orbits in this research paper?",
      "expected_output": "The main findings of the research paper include the derivation of orbital angular velocity for near-circular orbits, given by a specific formula, and the orbital period, given by another formula, where the semimajor axis plays a key role. The methods used involve evaluating the impact of the Yukawa modification on the perturbation of a given planet by another planet, following standard orbital perturbation methods outlined in the paper, and carrying out calculations for the two-body problem, with the equation of motion given by a specific formula. For example, the paper mentions that for a nearly circular orbit, the orbital angular velocity is given by a formula, and the orbital period is given by another formula, where the semimajor axis is a crucial parameter.",
      "context": [
        "q. (REF ). For a nearly circular orbit, Eq. (REF ) leads to an orbital angular velocity given by {{formula:76202bd0-9989-4ee9-8c95-2618cd91b050}} , and thus to an orbital period given by {{formula:d07e92ca-6827-4021-8d37-1bf59d58e311}}where {{formula:19df214d-f19c-43a3-990a-b16006a8c05b}} is the semimajor axis. One can also evaluate the impact of the Yukawa modification on the perturbation of a given planet by another planet. Following the same method used in to determine the Newtonian effect of a third body, we find that {{formula:08280d33-18a8-4e57-8690-f596ed39167d}} , where {{formula:5724644f-976a-47d7-82fc-77f7841b2905}} and {{formula:d319ac0a-a142-433f-af1d-e9df25158581}} are",
        "b-af5f-ba4a453a5523}}where we have dropped an irrelevant constant in the second line. Since we already know that {{formula:84ebb723-0660-4387-ad41-762492fb5efb}} astronomical units (au), we have expanded the potential in powers of {{formula:460c08ea-d6ba-4996-9b22-cb7e81b694e9}} , retaining only the leading non-trivial correction term. For the two-body problem, the equation of motion is given by {{formula:85f829e5-c43a-468b-b1ec-c91821038de8}}where {{formula:989db0cb-a22b-4425-b290-ded4033d2d02}} is the sum of the two masses. Carrying out the standard orbital perturbation methods as outlined in Section 3.3.2 of , it is a simple exercise to show that the pericenter advance is given by Eq. (REF ). For a nearly circular orbit, Eq. (REF ) leads to an orbital angular velocity given by"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding how parameter changes affect particle interactions in this research paper?",
      "expected_output": "The main findings of the research paper indicate that changes in cosmological parameters, such as those from Planck 2013 to WMAP9, can significantly affect particle interactions. Specifically, the paper finds that decreasing the cosmological parameters from Planck 2013 to WMAP9 results in a 12% decrease in the mean number of interactions per particle by redshift zero. This change is driven by Planck's larger value for a specific parameter, leading to larger critical densities at early times. Additionally, using earlier WMAP results, such as WMAP3, can lead to even more significant changes, including a 33% reduction in interactions due to anomalously low values of certain parameters.",
      "context": [
        "e8f88e43a5}} , which accounts for the different behaviour of fluctuations that are smaller than and larger than the horizon during the radiation and then matter dominated eras. For simplicity we use the zero-baryon CDM model in which {{formula:39f72373-2877-4199-83c3-0b48569d4798}}and {{formula:bfcba446-60b3-41df-85e0-5ecbeb7ef946}} is related to {{formula:995a2327-deca-4f47-8dcd-b0293a50eba5}} by {{formula:7a2540c0-ba81-43d8-b500-5cfa7d1cf67e}}where {{formula:46489bf4-7682-4a76-966b-43f2af841e25}} . We look at the effect of changes to the high-{{formula:23d8a600-0bd7-43c0-b3c5-14fea0a92a10}} power spectrum by integrating the mass function down to",
        " {{formula:ca69e5f0-3f5b-4f27-ba21-f3811d9ffba6}} from D08, we find that changing cosmological parameters from Planck 2013 to WMAP9 decreases the mean number of interactions per particle by redshift zero, {{formula:458faa42-fc52-4abc-a35c-61bc4f6e2795}} , by 12%. This is driven by Planck's larger value for {{formula:3e5c76fd-be29-4a26-8858-36fdff5a1b2b}} , resulting in larger critical densities at early times. Using earlier WMAP results leads to similar changes, except for WMAP3 for which the anomalously low {{formula:3055d536-355c-4573-adcc-c03edde82f0b}} and {{formula:42160d8c-c1b3-4327-9e6c-460366447cb0}} lead to a 33% reduction in {{formula:37f8b461-cf2d-465f-a449-65"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between surjectivity and the kernel of a restriction morphism in the context of this research paper?",
      "expected_output": "According to the research paper, the surjectivity of a restriction morphism is related to its kernel. Specifically, the paper states that the restriction morphism is surjective with a kernel isomorphic to a certain formula. This means that the morphism maps onto its target, but there may be elements in the domain that map to the same element in the target, and these elements form the kernel. For example, the paper mentions that the restriction morphism is surjective with kernel isomorphic to formula:7a6019ef-abe0-4782-8b03-caf4baeded2b. This indicates that the morphism is onto, but its kernel is non-trivial, consisting of elements that map to the zero element in the target.",
      "context": [
        "-9e66-5519aa70cbf9}} , for {{formula:5509fc80-0c58-428a-8b51-e84597928775}} is given by {{formula:15c828de-9dc4-41ad-875a-0ea682fbf4d5}}Using the decompositions (REF ) and (), we can observe that {{formula:d6d443f4-675a-4aca-ab21-19d28601b35c}} By Theorem REF , the restriction morphism {{formula:d3b654c5-df39-43fe-b8d4-79e4a20a9af3}} is surjective. Combining this with the Mayer-Vietoris sequence, we have that the restriction morphism {{formula:e0ed0e7d-9411-49d7-a997-801d4eafd0dc}} is surjective with kernel isomorphic to {{formula:7a6019ef-abe0-4782-8b03-caf4baeded2b}} . Using the decompositions",
        " have {{formula:a1cdefae-7c50-4e24-a099-aaede5ca02e5}}Therefore, {{formula:4cf0c0ce-6258-483f-86e2-2ea32fc117df}} induces a homomorphism {{formula:cd54fc24-0556-435c-a211-65fc817dabc3}} By , we conclude that the homomorphism {{formula:4000c803-26e7-4554-ab33-ff1ee8d125dd}} is an isomorphism such that the induced isomorphism on the associated vector bundles: {{formula:b98e6fac-6624-466b-ac3c-fc8efbad7cd8}}Denote by {{formula:28eeeda0-2f88-451d-81bd-799ea5946ed6}}the restriction of {{formula:f49d3aa5-e933-4d44-9a1a-7e4afeb13b6c}} to the point {{formula:93f182b4-731a-",
        "c29b00009}} , the restriction morphism {{formula:de4bc923-84fd-489b-a297-961ad703509c}} is an isomorphism (Lefschetz hyperplane section theorem). Moreover, {{formula:cbd8d738-0adc-460b-92be-622cc5b164fa}} (resp. {{formula:2d688001-5fc0-4ce2-9f1b-f9df22033ef4}} ) is injective (resp. surjective) with cokernel (resp. kernel) isomorphic to {{formula:c4f4255c-613f-433a-9d81-712c27c1f092}} (resp. {{formula:7381f0dc-2d43-4339-9d43-948361352e64}} ), where notations as in §REF . Using (REF ) and (), this implies that the restriction morphism {{formula:aff96deb-e843-4944-bcf6-96fab65372fc}} The exact sequence (REF"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used to create the Dirac monopole's magnetic field in the research paper?",
      "expected_output": "The research paper uses the operator to create a vector potential of the Dirac monopole, which is oriented in the direction of the Higgs field. Specifically, the operator creates the magnetic field of the Dirac monopole, including the contribution of the Dirac string. Additionally, the 'tHooft tensor is used to define the magnetic current, and the unit vector field is utilized to write the 'tHooft tensor in a convenient form. The paper also mentions the transformation into unitary gauge, where the Higgs field is constant everywhere in space, to construct the string operator.",
      "context": [
        "-d7ba-449b-a76c-1973a293175f}} . The operator {{formula:fb5fbec9-3d83-4df7-8d21-f96a908da690}} creates a vector potential of the Dirac monopole which in the isospace is oriented in the direction of the Higgs field. In particular {{formula:9384c53b-a44a-41c7-bdca-e45ee8855b49}}Note that the operator {{formula:07cf7ee2-f91e-45db-ba8d-ab646a7274d5}} is fully gauge invariant. It does however create the magnetic field of the Dirac monopole including the contribution of the Dirac string. As a result it does not create a magnetic charge, since the magnetic flux due to the Coulomb magnetic field is balanced by the flux of the Dirac string. The role of the operator {{formula:38508d07-43b8-4ee1-b60e-d5b8ebc1afc9}} in eq. (REF ) is to create the magnetic field of the negative Dirac",
        "8ebc1afc9}} in eq. (REF ) is to create the magnetic field of the negative Dirac string alone, and thus cancel the unwanted singular magnetic field in eq. (REF ).To construct the string operator {{formula:7ae0093e-ee32-4731-bc45-8907bd196649}} we turn for inspiration to the classical monopole solution eq. (REF ). This solution is written in the gauge where the vector potential is regular everywhere except at the location of the monopole. One can however transform this into unitary gauge, where the Higgs field is constant everywhere in space. The transformation that achieves this is given by {{formula:1d7d6bab-5ddb-447e-bb54-9e22a1f4d2f0}}where {{formula:37cdf229-a721-49ae-a549-8204142dcc6c}} and {{formula:878bf9ff-3b9a-4ab4-8f26-706004327d75}} are azimuthal and polar angles respectively, {{formula:58398f09-01f0-4",
        "-be7e-cee41d4b28ba}}in terms of the nonabelian field strength tensor {{formula:eba63aa8-93c6-4cdd-a034-57041e9de9ba}}and covariant derivatives of the Higgs field {{formula:67a42f4c-4b44-429e-a4a6-68f799673672}} {{formula:30bec9de-7593-4936-bf37-633cf4188815}}The 'tHooft tensor (REF ) can also be written as {{formula:5b3e6cf8-350f-4922-b220-113673c72f6d}}with the unit vector field {{formula:ec412bc1-59d6-436e-be00-c13a77b25809}} . The magnetic current is then defined as {{formula:0fe12a0e-1b6a-4714-907f-6e0c63462dd4}}This definition is convenient since the magnetic charge density so defined for all smooth configurations is equal to the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in disease severity predictions between the MT-HH and IDS-HH models, and how do these differences impact the research findings?",
      "expected_output": "The main differences in disease severity predictions between the MT-HH and IDS-HH models lie in how they account for varying disease response. The MT-HH model assumes that disease severity is independent of the infector, while the IDS-HH model suggests that the severity of the disease depends on the disease response of the person who caused the infection, and possibly whether the contact was within or between households. These differences impact the research findings by allowing for the comparison of final size distributions of the two models, which can be used to discriminate between the two candidate explanations for why infection response varies. For example, the IDS-HH model may predict a higher risk of severe infection from within-household transmission, which can be compared to the MT-HH model's predictions to determine which model better fits the observed data.",
      "context": [
        " initially completely susceptible. (Inspection of histograms (not shown) of the final proportion of individuals infected suggests that this cutoff is appropriate for separating minor and major outbreaks.)Figures REF and REF show histograms of the numbers of individuals ultimately mildly and severely infected in, respectively, the 9,992 simulations of the MT-HH model and the 9,993 simulations of the IDS-HH model that resulted in major outbreaks. Overlaid on these histograms are probability density functions (scaled so as the area under them matches that of the histograms) of normal distributions with the same mean and variance. The excellent agreement between the histograms and density functions in Figure REF is expected in view of the central limit theorem for the MT-HH model of Ball and Lyne (2001) and in Figure REF this lends credence to the central limit theorem discussed in Section REF above for the final outcome of the IDS-HH model. Though the mean values of these distributions are similar ({{formula:b9f3160f-6d80-44d0-8dea-b80b44cbc7aa}} , {{formula:a80aaa5c-3a33-4420-84ef",
        "0d5b-ae31-438b-95a3-4786a17317e0}} as giving the proportions of households of different sizes, rather than having random household sizes with distribution {{formula:ad276a33-a055-4787-af3c-6d8db5a452ce}} .) In order that minor outbreaks are unlikely, we initiated the epidemics with 10 infectives, each randomly chosen in different households of size 5; in the MT-HH model these individuals may be mild-type or severe-type (with respective probabilities {{formula:914ca125-6869-4d90-b4fa-3a1958981eba}} and {{formula:da563140-95f4-436e-929e-b56041f33270}} ) and in the IDS-HH model we specified that they are all severe cases. For simulations that result in more than 0.15 of the population becoming infected we then recorded the overall final size amongst initial susceptibles and the within-household final sizes amongst households that are initially completely susceptible. (Inspection of histograms (not shown) of the final proportion of individuals infected suggests that this cutoff",
        " the different types in the community are unobserved.The second explanation can be modelled by extending the so-called infector-dependent-severity (IDS) epidemic model of Ball and Britton (2007) to an infector-dependent-severity household (IDS-HH) epidemic model. In the IDS model, the probability of an individual becoming a mild/severe case depends on the disease response of the person who caused that individual's infection. In the IDS-HH model this may also depend on whether the contact causing the infection was a within- or between-household contact; for example, within-household transmission might have a higher risk of leading to severe infection.Once the final size distribution of the IDS-HH epidemic model is obtained, together with known such results for the MT-HH model, it is possible to compare the two distributions. The motivation for the present paper lies in this comparison. In particular we pose (and answer) the following question: can final size data from an epidemic outbreak with varying disease response be used to discriminate between the two candidate explanations for why infection response varies? Except in a few degenerate cases, the answer to the question is yes. This is illustrated numerically by showing that, in the limit"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What do Arrott plots indicate about magnetic transitions in the research paper, and how do the methods used to create these plots contribute to the main findings?",
      "expected_output": "Arrott plots indicate the magnetic transition temperature and the inverse DC susceptibility in the research paper. The plots are created by focusing on the high-field data and fitting it to extract the inverse susceptibility as a function of temperature for different doping levels. The methods used to create these plots, such as fitting the high-field data and exploring the non-linear field dependence, contribute to the main findings by allowing the researchers to characterize the evolution of the fully polarized magnetic state and extract key parameters like the zero-field magnetic transition temperature.",
      "context": [
        "1-a097-9fac19a7fb0e}} from the Arrott plots we focused on the high field data. (The black dots in Figures REF (b) and (d) indicate the magnetic field value above which the fit was performed.) The result are shown in Figure REF , showing the inverse susceptibility (from the intercept of the fits) as a function of temperature for different x. We define T{{formula:f637a611-016c-4419-8ffd-3c1d7372b858}} as the temperature where the inverse susceptibility {{formula:99bf283d-d151-4bf6-a754-bae6ed5d024c}} goes through 0. While no {{formula:249c737f-e415-4467-8b9b-23c00a29455c}} is found for x=0.02, T{{formula:2649dc8c-06f6-4d2e-827f-c537c28a87e8}} increases with doping, starting at x = 0.025. To characterize the evolution of the fully polarized magnetic state as a",
        "11c03}} ){{formula:30ccbe8c-f0c2-4a29-903e-077167098593}} B; additional effects such as canting of the magnetic moments, which may reduce {{formula:10b6bc45-e39c-4069-8a4c-6c6240c7353e}} (and thus elevate {{formula:31862e91-e670-495e-951f-a544a33db8b6}} ) from the value one might expect from a high-field extrapolation of the Arrott curves, may be present. {{figure:0ec31128-026c-46ed-b560-b7e29dde6704}}In order to extract a zero-field magnetic transition temperature T{{formula:23f5ead4-4d73-46a4-a38c-636c98f71a1b}} as well as the inverse DC susceptibility {{formula:cd60d327-9a6c-44a1-a097-9fac19a7fb0e}} from the Arrott plots we focused on the",
        "f14-a76ab0f61482}}Figures REF (a) and (c) show the field dependence of the bulk magnetization. In the paramagnetic high-temperature regime, characteristic linear M(H) is observed; at lower temperatures, however, significant curvature develops. In a mean field picture for an isotropic system, the magnetic contribution to the free energy to lowest order in the magnetization is {{formula:8387316e-7874-4c92-ab5e-468a1338a0c8}} ; it follows that {{formula:d04d7a1b-a766-4ad0-afc2-71d8e4d5ac6b}} . Therefore, Arrott-plots of H/M as a function of M{{formula:87c3c527-7d70-4de8-9f45-d3a293b9ed2b}} allow us to explore the non-linear field dependence quantitatively. In agreement with this picture, the Arrott plots of our M(H) data are straight lines at high temperatures. Two possible scenarios may explain the significant low-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding gauge transformations and their impact on space-time fields?",
      "expected_output": "The research paper shows that gauge transformations can be consistently defined for space-time fields, with two crucial ingredients: keeping all singular terms and using a finite cutoff. This approach ensures gauge invariance and does not introduce new physical degrees of freedom, although it makes space-time effectively infinite dimensional. The paper also finds that space-time fields must be functions of gauge coordinates, which is necessary for defining gauge transformation laws in a consistent way.",
      "context": [
        " increase in the number of physical degrees of freedom. This is a desirable feature.\nConclusions: We have described a general construction that gives gauge invariant equations of motion, the gauge transformation prescription (in terms of loop variables) being the same as in II. This method was outlined in III but many of the details had not been worked out. One of the problems that was left unsolved was whether the map from loop variables to space time fields is unambiguous. In particular it was not obvious that there was a map that correctly reproduced the gauge transformations. The results of the present paper indicate that it is indeed possible to define space time fields and their gauge transformations consistently. There are two crucial ingredients. One is that one has to carefully keep all the singular terms that are normally discarded by “normal ordering”. We have to keep a finite cutoff in order for this procedure to make sense. This is not unexpected - we already know that in order to define off-shell Green functions in this approach, one needs a finite cutoff , , . As shown in , even U(1) gauge invariance of the massless vector is violated when a finite cutoff is introduced in order to go off-shell, and one needs to introduce massive modes to restore gauge invariance.",
        "formula:38f60ef8-a62c-4a70-a31a-474503833d28}} -dependence in the {{formula:a5bbce7d-4ffe-4cb0-88ca-2b0ed6e2125d}} coordinates also. Gauge invariance at the level of loop variables is very easy to see. What was not clear at the time was whether there was a consistent map to space-time fields. Here we show that this is in fact the case. It crucially involves keeping a finite cutoff on the world sheet and also making the space-time fields a function of {{formula:d08666d9-2c1a-4053-b27a-ba0125dafa65}} . Keeping a finite cutoff is required when going off shell , , . In the presence of a finite cutoff there are problems with gauge invariance as discussed in . It was shown there that to lowest order these problems could be resolved by adding a massive mode with an appropriate transformation law. It was also speculated that maintaining exact gauge invariance would be possible if all the modes are kept. This is shown to be true in the present work. We have the",
        "ance would be possible if all the modes are kept. This is shown to be true in the present work. We have the full gauge invariance and it is not violated by a finite cutoff and the construction necessarily requires all the modes.Another feature that emerges from the present work is that the space-time fields have to be functions of the gauge coordinates {{formula:a876c239-b8fa-47bb-9458-cfd12da52371}} . This is forced on us when we require that it be possible to define the gauge transformation laws for the space time fields in a consistent way. This does not introduce any new physical degrees since these can be gauge fixed. Nevertheless it is amusing to note that space-time has effectively become infinite dimensional.In order to make precise contact with string theory one has to perform another step that we do not discuss in this paper. It involves generalizing to the interacting case the dimensional reduction that was done in I (for the free case). Given that the basic technique involves calculation of correlators of vertex operators on the world sheet we are more or less guaranteed that we will reproduce bosonic string amplitudes. What needs to be shown is that the dimensional reduction does not violate the gauge invariance. We reserve"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the statistical effects of single beam on channel power gain in this research, and how do the findings relate to the methods used?",
      "expected_output": "default",
      "context": [
        "c-8c6750a8a491}}\nSingle-direction beam performance: As a comparison, we compute the gain obtained with a single beam pointed towards the largest component that, without loss of generality, we assume to be the first. Therefore, we set the beam coefficients to\\beta ^{(Sing)}_n = \\frac{1}{N} e^{-j \\phi _{n, \\vec{k}_1}}.With this assumption, and again using the definition of array parameter, the channel power gain can be written as\\mathbb {E} \\left[ \\vert H^{(Sing)}(f) \\vert ^2 \\right] = \\mathbb {E} \\left[ \\vert \\alpha _1 \\vert ^2 \\right] + \\sum _{m=2}^M a^2 \\mathbb {E} \\left[ \\vert \\alpha _m \\vert ^2 \\right] .Note that, under the assumption that \\alpha _1 is the largest component, its statistical distribution changes. In fact, if we assume that a generic \\alpha _m has exponentially distributed power with parameter 1 (which is a direct consequence of the Gaussian distribution of \\",
        "^{M} \\alpha _m e^{-j 2 \\pi f (\\tau _{m} - \\tau _{m^{\\prime }})} \\\\ & \\sum _{m^{\\prime } \\ne m} \\frac{1}{N} \\alpha ^*_{m^{\\prime }} \\left( \\sum _{n=1}^{N} e^{-j \\phi _{n, \\vec{k}_{m^{\\prime }}}} e^{j \\phi _{n, \\vec{k}_m}} \\right),where the first summation accounts for the contribution of the M multipath components received by the corresponding beam components, while the other term is the aggregate contribution of the multipath components not aligned with the beam components. Considering a frequency well outside the coherence bandwidth of the channel, we can assume that the phases 2 \\pi f(\\tau _{m} - \\tau _{m^{\\prime }}) are uniformly distributed and independent. With this assumption, the expected channel power gain is&\\mathbb {E}\\left[ \\vert H(f) \\vert ^2 \\right] = \\sum _{m = 1}^{",
        " small, in the order of a few dB. Moreover, despite M is not large enough to show the convergence expected from (REF ), such result shows that the gap will not increase in more complex channels.To evaluate the increase of robustness enabled by the diversity generated by the mrc, we generated the beam for a given channel H(f), then simulated a blockage by removing component m^{\\prime }, where m^{\\prime } is randomly selected between 1 and M, generating the new channelH^{\\prime }(f) = H(f) - \\alpha _{m^{\\prime }} e^{j \\phi _{n, \\vec{k}_{m^{\\prime }}}} e^{-j 2 \\pi f \\tau _{m^{\\prime }}}.We then applied the beam designed for H(f) to the new channel H^{\\prime }(f) and evaluated the snr obtained by the two methods. In Fig. REF we can observe the resulting snr distribution. As expected, we can observe that the tail of the snr obtained with a single beam extends much further than that of mrc, because the single beam approach is much more sensitive too the loss of the component used"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between Hahn echo and critical points in the context of this research paper?",
      "expected_output": "The research paper establishes a relationship between the Hahn echo and critical points in the context of spin-chains, indicating that the Hahn echo can be used as a tool to study quantum phase transitions and measure critical points in experiments. Specifically, the results show that the contributions to the Hahn echo from the first excited states tend to zero with increasing temperature, and the Hahn echo envelope can be written in terms of thermal states and one- and two-particle excitations.",
      "context": [
        " like to notice that the discussion on the finite temperature effect was limited to very low temperatures, because only one- and two-quasiparticle excitations were included. Nevertheless, it is interesting because it also sheds light on the contribution to Hahn echo from the first excited states, which have the same energy as the ground state {{formula:f0b0b1d3-b860-414f-b98d-2ec1f9ad6be3}} of {{formula:a01b5e9a-9b93-4114-affb-1d023c5b8a9e}} at critical points. The results presented in figure 6 show that those contributions tend to zero with {{formula:8edf2105-2f2b-407f-a6e7-0584c84dbdfe}} .In conclusion, by discussing the Hahn spin echo in the spin-{{formula:7cd4966c-2225-4bdb-a056-878028f4d0d8}} particle coupled to critical spin-chains, the relation between the Hahn echo and the critical points was established. The relation not only provides an",
        "-484c-80e6-7dd9c1936923}} ), indicating the change in the ground state of the spin-chain from paramagnetic phase to the others. {{figure:c2bb3b22-d31e-4c84-b6e5-5a19d4c24429}}Up to now, we did not consider the temperature effect. Finite temperature is the regime to which all experiments being confined, but what is the finite temperature effect on the Hahn echo? In the following, we shall consider this problem by studying the contributions of one- and two-particle excitations to the Hahn echo. Taking a thermal state {{formula:4e735f7f-5bab-45be-b6bf-dc10c49b34e5}} {{formula:8a3403c4-cefe-4f0b-bef8-70c667cb91ab}} ) as the initial state of the spin-chain, the Hahn echo envelope can be written as {{formula:03e1b646-06c6-4174-9e60-a0bed1ea4096}}where {{formula:",
        " critical spin-chains, the relation between the Hahn echo and the critical points was established. The relation not only provides an efficient theoretical tool to study quantum phase transitions, but also proposes a method to measure the critical points in experiments. Up to two-particle excitations, we have also studied the influence of thermal fluctuation on the Hahn echo, it would shed light on the low temperature(with respect to the overall energy scale {{formula:238ce701-3a47-48c7-bd1a-bf02cb23d6ea}} ) effects on the Hahn echo.This work was supported by NCET of M.O.E, and NSF of China Project No. 10305002 and 60578014.S. Sachdev, Quantum Phase Transition (Cambridge University Press, Cambridge, 1999).E. L. Hahn, Phys. Rev. 80, 580(1950).A. Osterloh, L. Amico, G. Falci, and R. Fazio, Nature 416, 608(2002).G. Vidal, J. I. Latorre, E. Rico, and A. Kitaev. Phys. Rev. Lett. 90, 2"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does metric learning aid supervised learning in aligning representations with human intuition, and what are the key findings and methods used in this research?",
      "expected_output": "Metric learning aids supervised learning in aligning representations with human intuition by incorporating human-provided triplet judgments to learn human-compatible decision-focused representations. The key findings of this research include that human-compatible representations identify nearest neighbors that are perceived as more similar by humans and allow humans to make more accurate predictions. The methods used in this research involve combining supervised learning with metric learning, using a Convolutional Neural Network (CNN) and a multi-layer perceptron (MLP) to project image inputs into a high-dimension representation space. The research demonstrates substantial improvements in human decision accuracies, with 17.8% improvement in butterfly vs. moth classification and 13.2% improvement in pneumonia classification.",
      "context": [
        " an open question that how to ameliorate these negative influence when designing decision support interactions with case-based reasoning.\nImplementation detail: The architecture of our model is presented in Figure REF . We first encode image inputs using a Convolutional Neural Network (CNN), and then project the output into an high-dimension representation space with a projection head made of multi-layer perceptron (MLP). In our experiments we use one non-linear layer to project the output of the CNN into our representation space. For classifcation task we add an MLP classifier head. We also use one non-linear layer with softmax activation. For triplet prediction, we re-index the representations with the current triplet batch and calculate prediction or loss. We use the PyTorch framework and the PyTorch Lightning framework for implementation. Hyperparameters will be reported in Section for models in the synthetic experiments and in Section and Section for models in the human experiments. {{figure:d1fea212-fcd9-4f6d-b2eb-7c38f4de8bee}}\nComputation resources: We use a computing cluster at our institution. We train our models on nodes with different GPUs including Nvidia GeForce",
        "# arXiv Paper: 2303.04809\n\nLearning Human-Compatible Representations for Case-Based Decision Support\nabstract: Algorithmic case-based decision support provides examples to help human make sense of predicted labels and aid human in decision-making tasks. Despite the promising performance of supervised learning, representations learned by supervised models may not align well with human intuitions: what models consider as similar examples can be perceived as distinct by humans. As a result, they have limited effectiveness in case-based decision support. In this work, we incorporate ideas from metric learning with supervised learning to examine the importance of alignment for effective decision support. In addition to instance-level labels, we use human-provided triplet judgments to learn human-compatible decision-focused representations. Using both synthetic data and human subject experiments in multiple classification tasks, we demonstrate that such representation is better aligned with human perception than representation solely optimized for classification. Human-compatible representations identify nearest neighbors that are perceived as more similar by humans and allow humans to make more accurate predictions, leading to substantial improvements in human decision accuracies (17.8% in butterfly vs. moth classification and 13.2% in pneumonia classification).\nAcknowledgments: We thank the anonymous reviewers for their insightful comments.",
        "ts an intervention to present a small region in the dataset and may downplay the importance of global distribution in human users' decision making process.The policy aims to select the nearest in-class examples but the furthest out-of-class examples. It aims to maximize the visual difference between examples in opposite class, thus require less effort for human users to adopt case-based reasoning for classification. It also helps human users to develop a local decision boundary with the largest possible margin. However, when model prediction is incorrect, the policy end up selecting the furthest in-class examples with the nearest out-of-class examples, completely contrary to what it is design to do, may lead to even over-reliance or even adversarial supports.In general, decision support policies aim to choose a number of supporting examples without considering some global properties such as representativeness and diversity. While aiming to reduce humans' effort required in task by encouraging them to make decision in a local region, the decision support examples do not serve as a representative view of the whole dataset, and may bias human users to have a distorted impression of the data distribution. It remains an open question that how to ameliorate these negative influence when designing decision support interactions with case-based reasoning.\n"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the effect of aggregation on XAS spectra in this research paper?",
      "expected_output": "The main findings regarding the effect of aggregation on XAS spectra in this research paper are that the effect is rather small due to the small transition dipole moments for the 2p to 3d excitations, especially when compared to valence transitions of organic dyes. However, the ligand coordination in various solvents has been shown to have a pronounced influence on XAS and RIXS spectra, making solvent effects an important consideration.",
      "context": [
        "icle excitations.In general, due to the quite small transition dipole moments for the 2p{{formula:d4425e69-f0fb-49d5-b6cb-120ef5bf86cd}} 3d excitations, if compared to the valence {{formula:e5561759-13ae-473f-a06f-2cb982b4b508}} and {{formula:d96b5313-c798-4fd4-830a-4be098f683bc}} transitions of organic dyes, the effect of aggregation on XAS spectra will be rather small. As far as the RIXS spectrum is concerned, Eq. (REF ) points to a dependence on the electronic polarizability. Here, the intermonomer contributions will determine the aggregation effect on the spectrum.The developed protocol was applied to the hemin system forming dimers in water solution, while staying monomeric in other polar solvents. Remarkably, ligand coordination in various solvents has been shown to have a pronounced influence for XAS and in particular RIXS spectra. This is an important result, since solvent effects have not been considered in",
        " XAS and in particular RIXS spectra. This is an important result, since solvent effects have not been considered in the previous experimental studies of hemin X-ray spectra. , , In present work it was found that coordination and aggregation effects on RIXS spectra are of similar magnitude. This could make the unequivocal assignment of aggregation induced features difficult. However, in this proof-of-principle study a direct comparison with experiment was not attempted. First, due to construction of the active space the {{formula:f58b1af3-5dcf-4002-94ad-ca4a8b57dddd}} stacking effect, discussed e.g. in Ref. golnak153058, was not taken into account. Second, it was found that the aggregation induced changes in the RIXS spectrum are depending on the mutual orientation of the monomers in the dimer. Due to the flexibility of the dimer structure in solution a more accurate description would require a rather time consuming sampling of different conformations, i.e. by combining molecular dynamics with the present RIXS calculation.The present Frenkel exciton-like approach to the X-ray spectroscopy of multi-center systems",
        "-40d8-9bbe-1cf1e0b01fb0}}In addition to its high biological relevance as a constituent of the hemoglobin active center, the hemin complex is interesting due to its aggregation behavior in different solvents. Staying monomeric in polar solvents like ethanol or DMSO it forms dimers in water solution. The effect of aggregation was recently addressed by means of soft X-ray Fe L-edge absorption spectroscopy in transmission (XAS) and partial fluorescence (PFY) modes as well as by off-resonant X-ray emission (XES) and RIXS on the example of DMSO and aqueous solutions. , The general shape of the spectra for both cases was quite similar and the pronounced difference in broadenings for RIXS as well as 1.3 eV energy shift in off-resonant XES was found as an indication of aggregation. These effects were, however, solely attributed to the {{formula:f74bf3d4-1b8d-47f2-9bec-0fc39ad07943}} -stacking, although ligand {{formula:97494ef4-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the interaction between black holes and dark matter halos in this research paper?",
      "expected_output": "The main findings of this research paper indicate that black holes (BHs) are not isolated in the universe and likely exist in complex environments, such as within dark matter halos. Studies have shown that supermassive BHs are connected to active galactic nuclei and are surrounded by dark matter halos. Various research works have explored the effects of different dark matter halo profiles on BHs, including the Hernquist, universal rotation curve (URC), and pseudo-isothermal profiles. These studies have found that the presence of dark matter halos can influence the existence of BH horizons, gravitational waves, and geodesics. For example, the paper mentions that the central density of the dark matter halo significantly affects the existence of BH horizons. Additionally, the research suggests that massive BHs may be present in dwarf galaxies, which are often thought to not host BHs at their centers.",
      "context": [
        " in Refs. , , , , , , , , , , , , , , , , , , , , It's implausible that BHs are completely isolated in the universe. It might be possible that they exist in dynamic and complex environments. In particular, there is a strong indication that supermassive BHs are the powerhouses behind active galactic nuclei , . There is also substantial evidence indicating that dark matter envelops most galaxies in a halo . Several important works based on BH-matter composite systems have been carried out for special types of dark-matter halo profiles. For instance, Xavier et al studied shadows of non-rotating BH immersed in a Hernquist-like dark matter profile . Cardoso et al discussed the effect of the Hernquist dark matter halo profile on the propagation of gravitational waves and on the geodesics . Jusufi et al presented a new BH solution surrounded by a dark matter halo in the galactic center of the M87 galaxy from the universal rotation curve (URC) dark matter profile. Konoplya considered a spherical model consisting of a Schwarzschild BH with a piecewise distribution of dark matter around it. In an another work, Konoplya and Zh",
        "schild BH with a piecewise distribution of dark matter around it. In an another work, Konoplya and Zhidenko derived BH solutions in the presence of various dark matter halo profiles. Hou et al studied rotating BH at the centre of the Sgr* galaxy with cold dark matter and scalar field dark matter halos. Yang et al discussed the optical aspects of a rotating BH-dark matter system with a pseudo-isothermal dark matter halo profile. Liang et al discussed the thermodynamic aspects of a BH immersed in a perfect fluid dark matter halo. Carvalho studied the thermodynamics of Einstein–Gauss-Bonnet (EGB) BH surrounded by three different distributions of dark matter halos.The Dehnen density profile , is commonly studied while dealing with dwarf galaxies, which often do not host BHs at their centers. However, recent observations suggest that massive BHs may also be present in these dwarf galaxies. Specifically, it was reported that a supermassive BH (SMBH) with a mass of around 2.00 \\times 10^5 M_{\\odot } resides in the dwarf galaxy Mrk 462 . Additionally, a study on BH-triggered",
        " \\theta d\\phi ^2),withf(r) = 1 - \\frac{2M}{r} - \\frac{4 \\pi r_s^3 \\left(r_s+2 r\\right) \\rho _s}{3 \\left(r_s+r\\right)^2}.This is the effective metric function of our composite dark matter-BH system. Fig. REF represents the plot of the metric function (REF ) as a function of r . It is evident from the plot that the central density of the dark matter halo significantly influences the existence of BH horizons while keeping the core radius fixed at r_s = 0.5 . The horizons can be determined by the condition f(r) = 0 . We see that there exists a unique horizon for each of the combinations of the dark matter halo parameters \\rho _s and r_s. {{figure:d2424a71-9f4d-4a8c-b4fa-80b9345d7784}}\nSingularity Test: It is known that the Schwarszchild BH possess a true physical singularity at r = 0 and coordinate singularity at r ="
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how light-matter interaction influences energy current in this research paper?",
      "expected_output": "default",
      "context": [
        "r}),in the form of a linear coupling between the electric field \\hat{E}({r}) and a polarization density d({r}); both are treated as quantum mechanical operators at this point, a is the cartesian component, and we again assume Einstein summation convention. (Note that the full light-matter interaction in the dipolar representation includes also a polarization self-interaction \\sim \\int d^3r d({r})^2, which however is not relevant for calculating the heat transfer to leading order in the light-matter interaction.) The current density is \\hat{J}_a({r}) = \\frac{d}{dt} d_a ({r}) =\\dot{d}_a ({r}) = i [H,d_{\\alpha }({r})] . The energy current to the material is the time-derivative of the system energy due to the light-matter interaction,\\dot{Q} = i [\\hat{H},\\hat{H}_m] = i [\\hat{H}_I,\\hat{H}_m].Using Eq. (REF ), the energy current is therefore given by,\\dot{Q} = - \\",
        "m].Using Eq. (REF ), the energy current is therefore given by,\\dot{Q} = - \\int d^3r \\hat{J}_a ({r}) \\hat{E}_a({r}).We aim to calculate the energy transfer \\langle \\dot{Q} \\rangle , evaluated in the steady state. We will now proceed to express this heat current in terms of the response functions of the solid (polarizability) and the electromagnetic density of states.\nField fluctuations: The heat transfer does not only depend on the electromagnetic density of states, but also on the field fluctuations, which will be assumed to be thermal. Similar to the response function (REF ), we introduce the autocorrelation functions,D_{a,a^{\\prime }}^>({r},{r}\\,^{\\prime };t-t^{\\prime })&= -i\\langle E_a({r},t) E_{a^{\\prime }}({r}\\,^{\\prime },t^{\\prime })\\rangle _0,\\,\\,\\, \\\\ D_{a,a^{\\prime }}^<({r},{r}\\,^{\\prime };",
        "_{aa^{\\prime }}({r},{r};\\omega ).(Here and in the following we assume Einstein convention for the cartesian indices).The local density of states is directly related to the radiation from a point dipole at position {r} with orientation \\hat{e}: To show this, we write the interaction of the field with an external classical dipole source as H_{\\rm ext}= \\int d^3r \\, p_a({r},t) E_a({r}), where p_a({r},t) is a time-dependent polarization density. The radiative energy loss for an oscillating source p_a({r},t)= \\frac{1}{2}(p_a({r})e^{-i\\omega t} + c.c.) is given by the time-averaged rate of increase in the field energy, W=\\frac{1}{\\tau }\\int _0^{\\tau } dt \\langle dH_{\\rm field}/dt \\rangle , \\tau =2\\pi /\\omega , which in general is obtained from the imaginary part of the response function,W=- \\frac"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in Schönhage's inequality and how does it impact the calculation of tensor volumes?",
      "expected_output": "Schönhage's asymptotic sum inequality is a vast generalization that provides a bound on the volume of a matrix multiplication tensor. The main finding is that it uses the direct sum of tensors and a specific value to replace the volume in applications, allowing for the analysis of tensor volumes. For example, in Theorem 3.2, the asymptotic sum inequality is restated to provide a bound on the volume of a matrix multiplication tensor based on the size of the support of the tensor.",
      "context": [
        "27540cb5}} . A vast generalization of this idea is Schönhage's asymptotic sum inequality : {{formula:4e09e02d-d5c4-46e9-b17c-b987d23a5b58}}This formula uses two pieces of notation we have to explain. First, the volume of a matrix multiplication tensor {{formula:e254f648-1423-49a5-bbc8-fe902df1100e}} is {{formula:4a58146b-cf5a-4088-a45e-fb43aa2931f3}} . Second, {{formula:518f5c14-2da5-44d7-b331-8824ea1f9978}} is the direct sum of the tensors {{formula:10f5ed1d-a77f-4071-b297-f170a49554e9}} . This direct sum is obtained by writing the tensors {{formula:83d3c828-a500-4ca7-b03e-de9fd793a4a3}} using disjoint formal variables,",
        " sake of exposition we ignore these details here.. The value {{formula:94700c39-6f06-4a3d-b119-90103fcc51cb}} is a number {{formula:36478308-60eb-492a-bccc-2b6ea8d2da93}} such that if we take the {{formula:a845aa55-befd-4824-a33b-c75aadd52cca}} th tensor power of {{formula:a4364506-0a97-46e1-bc83-bf0a6aeaca4e}} and zero some variables appropriately, we get a sum of disjoint matrix multiplication tensors {{formula:f735ed18-300a-488f-b6a2-6658b307191c}} such that {{formula:d569ccea-8061-4702-8115-212a9ad9db78}} . It can therefore be used as a replacement for the volume in an application of the asymptotic sum inequality for analyzing the tensor {{formula:31742e18-4522-42",
        "cbee5edf-4baa-4578-acfa-d409fbc414dd}} ({{formula:09427a7f-fa43-47aa-a84c-c3763f1692bc}} ) of positive integers, {{formula:e64cb99c-2a6c-452b-a703-01b4561247f0}}If we define the volume of a matrix multiplication tensor {{formula:ec2fbc1d-f9bb-497d-9b81-1cb9652f2f26}} by {{formula:fe267532-e4f3-4679-9de8-66e05ca235ca}} , the size of the support (set of non-zero entries) of {{formula:2f49fde3-f75c-4764-8b90-d0818c72faab}} , then we can restate the asymptotic sum inequality as follows.Theorem 3.2 (Asymptotic sum inequality (restated)) For every set {{formula:04d28fbc-e01d-4493-b747"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does QLMD use to determine the solar interior equation of state, and what are the key findings of this research?",
      "expected_output": "QLMD uses density functional theory (DFT) to study the electronic structure and the Langevin Equation to perform the ionic trajectory. The key findings of this research are that QLMD can ensure much more accurate results in all conditions, agrees with the AA model with energy-level broadening (AAB) at high temperatures, and can accurately determine the solar-interior equation of state in the pressure range of 58 ≤ P ≤ 4.6×10^5 Mbar at the temperature range of 1 ≤ T ≤ 1500 eV.",
      "context": [
        " the potential between atoms, supplies a powerful and accurate tool, and has been successfully applied in warm dense matter (WDM) , , , . In astrophysics, QMD has been used to study the properties of giant planets and given rise to amazingly satisfying results , , , , , . Very recently, QMD was extended to the field of high energy density physics (HEDP) by considering the electron-ion collision induced friction (EI-CIF) in the procedure of molecular dynamics, and the corresponding model called quantum Langevin molecular dynamics (QLMD) was constructed , . Thanks to this model, one can go into the regime of HEDP from first principles, and the thermodynamic properties of the sun and sun-like stars can be therefore researched based on ab initio method. It also gives us a good tool to investigate the system in which the coupling of ions is important.In this work, we firstly study the EOS of conditions under the solar interior using QLMD. Three densities with different temperatures are chosen, and their EOS are compared with the EOS of the AA model with energy-level broadening (AAB) . QLMD can ensure much more accurate results in all conditions, and agrees with AAB",
        " broadening (AAB) . QLMD can ensure much more accurate results in all conditions, and agrees with AAB at high temperatures. The central pressure of the sun is also calculated using different models, comparing with the data of other models and standard solar models . The important effect of heavy elements on EOS of very dense matter is also investigated at the end.\nQuantum Langevin molecular dynamics: We firstly recall the QLMD method here, where the electronic structure is studied from density functional theory (DFT), and the ionic trajectory is performed using Langevin Equation {{formula:4f024bc3-09b0-4879-b82b-6185dd02eb62}}Where {{formula:8499e624-87c6-47a1-9691-ddd86de07a3d}} is the ionic mass, {{formula:cd5aaa26-33bd-48a3-b84a-073668f43d7f}} is the force calculated in DFT, {{formula:548dc6f1-b91a-4667-8397-81cb3f7a16d3}} is a Langevin friction",
        "# arXiv Paper: 1008.0966\n\nQuantum Langevin molecular dynamics determination of the solar-interior equation of state\nabstract: The equation of state (EOS) of the solar interior is accurately and smoothly determined from \\textit{ab initio} simulations named quantum Langevin molecular dynamics (QLMD) in the pressure range of $58 \\leq P \\leq 4.6\\times10^5$ Mbar at the temperature range of $1 \\leq T \\leq 1500$ eV. The central pressure is calculated, and compared with other models. The effect of heavy elements such as carbon and oxygen on the EOS is also discussed.\nIntroduction: For solar and stellar models, a high-quality equation of state (EOS) is very crucial . It is well-known that some requirements should be satisfied for solar and stellar modeling: thermodynamic quantities would be smooth, consistent, valid over a large range of temperature and density, and incorporate the most important astrophysically relevant chemical elements as well. There are two major efforts for a high-precision and high-accuracy EOS made before, which have been included in the recent opacity recalculations. They are the international Op"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of single receiver limitation on backscatter in this research paper?",
      "expected_output": "The main findings of this research paper are that current backscatter systems require dual receivers to capture the original and backscattered signals, which is not conducive to widespread deployment. The proposed 'double-decker' system addresses this limitation by using a single commercial device for backscatter communication, achieving this by dividing the carrier OFDM symbols into pilot symbols and data symbols, allowing for tag data demodulation without an additional receiver.",
      "context": [
        "-4741-96d8-58207c65dd2f}}For a typical wireless communication system, most of the power consumption is caused by the oscillator that generates the carrier signal. The key idea of backscatter communication is to strip off the extremely energy-intensive high-frequency carrier generator part from battery-free devices. The excitation signal (i.e., carrier) for backscatter communication is provided by an external carrier generator. Backscatter tag reflects the excitation signal and modulates its data on the carrier. The initial backscatter systems usually require dedicated devices to generate excitation signals , , these works are excellent but not practical because of the huge hardware overhead. In recent years, there are many studies to explore leveraging ambient signals for backscatter communication , , , . These backscatter systems can uses the ambient signals as carriers to convey tag data. Therefore, commodity radios can be used as carrier generators and receivers. Along this line, we can envision the prospect of backscatter communication: leveraging the already-deployed commercial infrastructure and blending with conventional wireless communication technology. However, although these state-of-the-art backscatter systems are available on commodity radios, as a matter of fact, they are still",
        "# arXiv Paper: 2408.16280\n\nDouble-decker: Productive Backscatter Communication Using a Single Commodity Receiver\nabstract: Backscatter communication has attracted significant attention for Internet-of-Things applications due to its ultra-low-power consumption. The state-of-the-art backscatter systems no longer require dedicated carrier generators and leverage ambient signals as carriers. However, there is an emerging challenge: most prior systems need dual receivers to capture the original and backscattered signals at the same time for tag data demodulation. This is not conducive to the widespread deployment of backscatter communication. To address this problem, we present double-decker, a novel backscatter system that only requires a single commercial device for backscatter communication. The key technology of double-decker is to divide the carrier OFDM symbols into two parts, which are pilot symbols and data symbols. Pilot symbols can be used as reference signals for tag data demodulation, thus getting rid of the dependence on the dual receiver structure. We have built an FPGA prototype and conducted extensive experiments. Empirical results show that when the excitation signal is 802.11g, double-decker achieves a tag",
        "-of-the-art backscatter systems are available on commodity radios, as a matter of fact, they are still non-productive communications. This is because the current backscatter systems cannot recover tag data directly from the backscattered signal. They have to set an additional receiver to capture the excitation signal for tag data decoding. Obviously, this structure is incompatible with conventional wireless systems and is not conducive to the widespread deployment of backscatter communication.To implement a truly productive backscatter system, we have to address a key challenge: How can we demodulate the tag data with a single receiver? In this paper, we propose double-decker with a new modulation mechanism. As shown in Fig. REF , by making concessions in tag data rate, double-decker sets some areas on the carrier as pilot symbols, which are prohibited from data modulation. By using these pilot symbols as reference signal, double-decker can demodulate the tag data without deploying an additional receiver to capture the excitation signal. We have built a prototype of double-decker with FPGA and analog RF front-end circuits and comprehensively evaluate the prototype of our design. The empirical evaluations show that:Different from previous backscatter systems, double"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the effect of electron puddles on helical edge conductance in this research paper?",
      "expected_output": "The main findings of this research paper are that electron puddles created by doping of a 2D topological insulator can significantly affect its helical edge conductance. A single puddle, modeled by a quantum dot tunnel-coupled to the helical edge, can lead to inelastic backscattering within the edge due to the long electron dwelling time in the dot. The paper finds the resulting correction to the perfect edge conductance and generalizes to multiple puddles, assessing the dependence of the helical edge resistance on temperature and doping level. The results indicate that a lower doping level improves the quality of the edge conductance quantization, and the presence of puddles can be implied by the strong sensitivity of the edge conductance to the potential of a scanning probe.",
      "context": [
        " influence of electron puddles created by doping of a 2D topological insulator on its helical edge conductance. A single puddle is modeled by a quantum dot tunnel-coupled to the helical edge. It may lead to significant inelastic backscattering within the edge because of the long electron dwelling time in the dot. We find the resulting correction to the perfect edge conductance. Generalizing to multiple puddles, we assess the dependence of the helical edge resistance on temperature and doping level, and compare it with recent experimental data.The realization that a boundary separating a topologically-nontrivial insulator from a conventional one should carry delocalized electron states , has led to the prediction of such states in concrete materials and their experimental observation , , . One of the stunning theoretical predictions is that in 2D the zero-temperature electron transport along an edge is reflectionless, as long as time-reversal symmetry is not broken, which should lead to the quantization of the edge conductance .Experiments with HgTe quantum wells of the appropriate thickness confirmed the existence of highly-conducting channels in a nominally insulating state of a heterostructure , , . The Fermi energy {{formula",
        "# arXiv Paper: 1303.1766\n\nHelical edge resistance introduced by charge puddles\nabstract: We study the influence of electron puddles created by doping of a 2D topological insulator on its helical edge conductance. A single puddle is modeled by a quantum dot tunnel-coupled to the helical edge. It may lead to significant inelastic backscattering within the edge because of the long electron dwelling time in the dot. We find the resulting correction to the perfect edge conductance. Generalizing to multiple puddles, we assess the dependence of the helical edge resistance on temperature and doping level, and compare it with recent experimental data.\n: [1]Helical edge resistance introduced by charge puddles Jukka I. Väyrynen Department of Physics, Yale University, New Haven, CT 06520, USA Moshe Goldstein Department of Physics, Yale University, New Haven, CT 06520, USA Leonid I. Glazman Department of Physics, Yale University, New Haven, CT 06520, USA We study the influence of electron puddles created by doping of a 2D topological insulator on its helical edge conductance.",
        "ant atoms may lead to the formation of electron and hole puddles in the quantum well, cf. Fig. REF , similar to the known phenomenon in compensated bulk semiconductors . In the existing measurements doping varied from {{formula:378042f9-c42e-4165-a7d8-be97549ffc9b}} to {{formula:252ecdb0-f5ea-405a-ae5b-5d990b302718}} , , , , , and the results indeed seem to indicate that a lower doping level improves the quality of the edge conductance quantization. Furthermore, the uncovered strong sensitivity of the edge conductance to the potential of a scanning probe may imply the presence of puddles, i.e., spontaneously formed quantum dots, in the vicinity of the edge.In this Letter we elucidate the role of tunneling between an edge and a quantum dot on the edge conductance. Elastic processes involving electron dwelling in the dot do not lead to any backscattering. However, dwelling enhances the inelastic backscattering by increasing the time electrons interact with each other. At temperatures {{formula:5dbf1bd5-37a6-4c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how different reconstruction types affect computational geometry performance in this research paper?",
      "expected_output": "The research paper finds that different reconstruction types, such as 8-parameter and 12-parameter reconstructions, affect computational geometry performance. Specifically, the 12-parameter reconstruction gives better performance in the non-degenerate case, while the 8-parameter reconstruction is used for the degenerate twisted mass solver due to register spilling issues. The paper presents strong scaling results for the multi-GPU asymmetrically preconditioned mixed precision CG solver, demonstrating the performance difference between the two reconstruction types.",
      "context": [
        " Jefferson Lab. (ECC was enabled on the JLab K20 cluster and disabled on the GTX Titan). {{figure:a0fb3280-5d3d-474f-81df-508ccafc6f93}}We start our analysis with single GPU performance for the (asymmetrically preconditioned) dslash operator, which corresponds to the operator entering the LHS of Eq. (2.2). Here we included the plain Wilson case as a reference point. The lattice size for the single-GPU runs was {{formula:d4887742-3559-476e-aadd-e591747f7afd}} and we examined two types of gauge field reconstructions, namely 8- and 12-parameter reconstructions. QUDA allows for storing the gauge-field links in less than the 9 complex numbers needed to store a full SU(3) matrix. In one case, it allows omitting one row of the three, reducing the storage requirements to 6 complex numbers, so-called 12-parameter reconstruction. With 8-parameter reconstruction, the link is decomposed into a linear combination of the eight SU(3) generators and only the coefficients are stored (",
        "}} ) the results are presented in Table 2. {{table:e420b079-1691-4ab5-9f52-0c2c6afb3564}}We conclude that the asymmetrically preconditioned CG solver outperforms the symmetrically preconditioned version and the symmetric case requires further optimization.Finally, we present the strong scaling results for the multi-GPU asymmetrically preconditioned mixed precision CG solver. We consider here a {{formula:7b0bc3c6-bb67-4b44-8a68-f088ee876535}} lattice to demonstrate the best case code scaling. Fig. REF corresponds to the degenerate flavor doublet where we choose 8-parameter reconstruction for the (random) gauge field configuration. {{figure:561e9e08-5d8b-417e-a6c7-84a5b01fa08c}}Fig. REF presents the strong scaling for the non-degenerate case. Here we choose 12-parameter reconstruction as it gives better performance, as can be seen from the comparison in Table 1. The main difference in the CG performance between degenerate",
        "reconstruction, which is attributed to the register spilling. Taking these observations into account, in the following multi-GPU tests we will consider the 8-parameter reconstruction for the degenerate twisted mass solver and 12-parameter reconstruction for the non-degenerate one.Our next goal is to illustrate multi-GPU performance profile of the mixed precision degenerate twisted mass CG solver that is combined with asymmetric and symmetric preconditioning. These runs were performed for {{formula:7d326938-48a9-47f4-9294-667b73ee2a6b}} lattice, with {{formula:985b5001-0ca7-4d66-b946-6328ac63f973}} . Here we provide information on the number of iterations and total solver time depending on number of GPUs and preconditioning type used. For the double-single mixed precision (and the solver tolerance set at {{formula:b1ca4266-73bb-4d69-871f-ccfc29542774}} ) the results are presented in Table 2. {{table:e420b079-1691-4ab5"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the sinusoidal magnetic structure in late-series manganites and how were they discovered?",
      "expected_output": "The main findings regarding the sinusoidal magnetic structure in late-series manganites, such as those with R = Tb, Dy, Y, show a sinusoidal magnetic structure defined by the propagation vector. This was discovered through research on the magnetic properties of these materials, including a new AFM phase observed in late-R HoMnO3 as well as in nickelates, with an up-up-down-down magnetic arrangement. The study found that the AFM-E is the ground-state collinear phase for HoMnO3-like distortions, which increase along the rare-earth series and reduce the in-plane nearest-neighbor FM superexchange interaction.",
      "context": [
        "-A) ordering (i.e. ferromagnetically (FM) coupled {{formula:199c2af2-2374-493e-8100-397ff5b7e2e6}} layers antiferromagnetically aligned along the {{formula:026455ce-907e-4592-9943-7ba8bae6cd15}} direction in the {{formula:5d15f8e7-8f15-4738-89c1-25425f0e4ebe}} setting) is favored, whereas manganites toward the end of the series (R= Tb, Dy, Y) exhibit a sinusoidal magnetic structure defined by the propagation vector ({{formula:bdb76a43-ebc7-4386-8402-ba694de0af18}} ,0,0). Recently, a new AFM phase has been observed in late-R HoMnO{{formula:569ab04c-a28d-4048-b3d7-50d6b1c16aa1}} as well as in nickelates , with an “up-up-down-",
        "8a6bb0b2c23}} and, more in general, to deeply investigate this non-conventional magnetic ordering, focusing in particular on the link between magnetic and structural properties. In closer detail, we consider different collinear magnetic alignments for three highly distorted manganites, HoMnO{{formula:8fa1402e-7e0a-45c8-9ffe-6b32b8a56eb8}} , TbMnO{{formula:80c07006-1c93-4da1-869f-52ff0bbaf81a}} and YMnO{{formula:897d4a95-ad00-46df-803d-e9f5e4e3ceef}} , in order to find the stable magnetic configuration and compare our results for the most studied LaMnO{{formula:6ec96ffd-b3a0-497f-b06c-00bdc21c25c9}} . Surprisingly, we find that the AFM-E is the ground–state collinear phase for HoMnO{{formula:a49a8eaa-61",
        "7e86-cb89-4579-9871-3439f6b5875d}} -like distortions - that increase along the rare-earth series - reduce the in–plane nearest-neighbor FM superexchange interaction along the Mn-O-Mn path. This interaction, along with the delicate interplay with the AFM next–nearest–neighbors superexchange along planar Mn-O-O-Mn paths is ultimately responsible for the stabilization of the peculiar AFM-E spin arrangement. Although the approximations made in this study (such as partially inaccurate treatment of correlation effects and atomic relaxations, etc.) should well be tested in the near future, we hope that these results will stimulate more works - both from theory and experiments - to ascertain the role played by the AFM-E magnetic configuration in the manganites framework. {{figure:0f2ad020-2047-43a4-9599-8a72c0ca3b71}}{{figure:eb42fa75-d457-4d85-a15b-bfb9634e56c9}}{{figure:fcf2a53d-10b1-490"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding how calibration surpasses accuracy in sports betting model selection, and what methods were used to achieve these results?",
      "expected_output": "The research paper found that calibration is a more important metric than accuracy for sports betting model selection, resulting in greater returns on investment. The methods used to achieve these results included training models on NBA data over several seasons and running betting experiments on a single season using published odds. The paper showed that using calibration for model selection led to an average return on investment of +34.69% versus -35.17% when using accuracy, and a best-case return of +36.93% versus +5.56%.",
      "context": [
        "%$ versus $+5.56\\%$). These findings suggest that for sports betting (or any probabilistic decision-making problem), calibration is a more important metric than accuracy. Sports bettors who wish to increase profits should therefore select their predictive model based on calibration, rather than accuracy.",
        "# arXiv Paper: 2303.06021\n\nMachine learning for sports betting: should model selection be based on accuracy or calibration?\nabstract: Sports betting's recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to reliably predict the probability of an outcome, they can recognise when the bookmaker's odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of predictive models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. We show that using calibration, rather than accuracy, as the basis for model selection leads to greater returns, on average (return on investment of $+34.69\\%$ versus $-35.17\\%$) and in the best case ($+36.93\\%$ versus $+5.56\\%$). These findings suggest that for sports betting (or any probabilistic"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the rebuttal effectively address the reviewers' comments and what are the main points it covers to improve the research paper?",
      "expected_output": "The rebuttal effectively addresses the reviewers' comments by providing an opportunity for the authors to rebut factual errors or supply additional information requested by the reviewers. The main points it covers include clarifying any misconceptions, providing more details on the methodology or results, and addressing any concerns raised by the reviewers. For example, authors may include figures or tables to illustrate their points or provide comparisons with other research. The rebuttal must be concise, no longer than 1 page in length, and maintain anonymity to ensure a fair review process.",
      "context": [
        "The author rebuttal is optional and, following similar guidelines to previous conferences, is meant to provide you with an opportunity to rebut factual errors or to supply additional information requested by the reviewers. It is NOT intended to add new contributions (theorems, algorithms, experiments) that were absent in the original submission and NOT specifically requested by the reviewers. You may optionally add a figure, graph, or proof to your rebuttal to better illustrate your answer to the reviewers' comments.Per a passed 2018 PAMI-TC motion, reviewers should refrain from requesting significant additional experiments for the rebuttal or penalize for lack of additional experiments. Authors should refrain from including new experimental results in the rebuttal, especially when not specifically requested to do so by the reviewers. Authors may include figures with illustrations or comparison tables of results reported in the submission/supplemental material or in other papers.Just like the original submission, the rebuttal must maintain anonymity and cannot include external links that reveal the author identity or circumvent the length restriction. The rebuttal must comply with this template (the use of sections is not required, though it is recommended to structure the rebuttal for ease of reading).\nResponse length: Author responses must be no longer than 1 page in length including any references and figures",
        "4093-b0d6-3ee092074855}}To avoid ambiguities, it is best if the numbering for equations, figures, tables, and references in the author response does not overlap with that in the main paper (the reviewer may wonder if you talk about fig:onecol in the author response or in the paper). See template for a workaround.\nIllustrations, graphs, and photographs: All graphics should be centered. Please ensure that any point you wish to make is resolvable in a printed copy of the response. Resize fonts in figures to match the font in the body text, and choose line widths which render effectively in print. Readers (and reviewers), even of an electronic copy, may choose to print your response in order to read it. You cannot insist that they do otherwise, and therefore must not assume that they can zoom in to see tiny details on a graphic.When placing figures in , it is almost always best to use \\includegraphics, and to specify the figure width as a multiple of the line width as in the example below\\usepackage{graphicx} ...\\includegraphics[width=0.8\\linewidth]{myfile.pdf}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the wrapper use to reduce CO2 emissions, and what are the key findings of this research?",
      "expected_output": "The wrapper reduces CO2 emissions by minimizing waiting time for workers, thus saving energy. Key findings include the development of a Python wrapper that supports distributed computing setups, allowing researchers to easily wrap their objective functions and reduce CO2 emissions, with an example of reducing about 6kg of CO2 in an experiment on JAHS-Bench-201.",
      "context": [
        " As a computer consumes about 30Wh even just for waiting and 736~\\mathrm {g} of \\mathrm {CO}_2 is produced per 1kWh, the experiment of BOHB on JAHS-Bench-201 would have produced about 6~\\mathrm {kg} of \\mathrm {CO}_2. Therefore, researchers can reduce the corresponding amount of \\mathrm {CO}_2 for each experiment. The main limitation of our current wrapper is the assumption that none of the workers will not die and any additional workers will not be added after the initialization. Besides that, our package cannot be used on Windows OS because fcntl is not supported on Windows.\nConclusions: In this paper, we presented a Python wrapper to maintain the exact order of the observations without waiting for actual runtimes. Although some existing packages internally support the similar mechanism, they are not applicable to multiprocessing or multithread setups and they cannot be immediately used for newly developed methods. On the other hand, our package supports such distributed computing setups and researchers can simply wrap their objective functions by our wrapper and they can directly give to their optimizers. We describe the basic usage of our package and demonstrated that our package significantly reduce",
        " While this problem is naïvely addressed by making each worker wait for the runtime the actual DL training would take, each worker must wait for a substantial amount of time in this case, and hence it ends up wasting energy and time.To address this problem, we developed a Python wrapper that automatically sorts out both the order of evaluations and the allocation of jobs to each worker in a fraction by forcing each worker to wait internally. In this paper, we describe how our package solves the problem and how to use this wrapper. Our package can be easily used for existing parallel optimizers such as BOHB ( ), DEHB ( ), and SMAC3 ( ) and we provide the example codes of how to pass our wrapper to these existing optimizers at https://github.com/nabenabe0928/mfhpo-simulator/.\nBackground: In this section, we describe the potential problems during MFO on tabular or surrogate benchmarks and explain why our wrapper would be useful. Throughout the paper, we assume minimization problems of an objective function f({x}): \\mathcal {X}\\rightarrow \\mathbb {R} defined on the search space \\mathcal {X}\\mathcal {X}_1 \\times \\mathcal {"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main criteria that must be met according to the research paper's findings and methods?",
      "expected_output": "The main criteria that must be met are: (1) achieving a high optical quality factor, (2) realizing a significant optomechanical coupling rate, and (3) operating at a low enough temperature to reach the quantum ground state, as demonstrated in the research paper with the novel micro-optomechanical device.",
      "context": [
        "8572-5fb8161a3b53}} is the Boltzmann constant and {{formula:8a1a10d9-a56e-4299-a8d7-f97118aafa3f}} is the reduced Planck constant). So far, no experiment has demonstrated all three requirements simultaneously. Criterion (1) has been achieved , , ; however, the performance was limited in one case by laser phase noise and in the other cases by absorption in the cavity , . Other, independent, experiments have implemented only criterion (2) , , , . Finally, criterion (3) has been realized in several cryogenic experiments , , , , however not in combination with both (1) and (2). {{figure:bce6f080-ef0a-4ffb-8e87-5e8c9985d8d4}}We have designed a novel micro-optomechanical device that enables us to meet all requirements at the same time. Specifically, we have fabricated a Si{{formula:1ef4b6c2-5cc3-4fa1-8c16-5b4c37833dd2}} N{{formula:8d",
        " design are dominated by the Si{{formula:56d1f30f-4366-4936-93ac-ddf7c25d88dc}} N{{formula:15da0a2a-33c0-45ce-84b1-2ae39f831ae5}} , which is known to exhibit superior performance in particular at low temperatures, where {{formula:b5a3d661-85cc-4667-b488-448804526539}} -factors beyond {{formula:0eeb3b25-716a-471f-b2c3-5310db88db3e}} have been observed at millikelvin temperatures .We operate our device, a {{formula:40ffc822-98c8-49d0-a753-d2784d9ca334}} microresonator, in a cryogenic {{formula:99fde8f5-3889-44b1-ae39-91ebc2ed6a9a}} He environment at {{formula:7fb6aba2-89ad-42b5-8748-db3760c901",
        " final occupation number {{formula:bda8a5d9-530c-4bd5-a205-a6291b15796d}} , that is, well into the quantum ground state , . The experimentally achievable temperature is obtained as the equilibrium state of two competing processes, namely the laser cooling rate and the coupling rate to the thermal (cryogenic) environment. In essence, laser cooling is driven (in the ideal resolved-sideband limit and at detuning {{formula:3f6e23be-b74f-43bd-8161-442baed92313}} ) at a rate {{formula:50a51fda-ed8f-4244-a7e5-fe49bdb23b40}} ({{formula:6313096a-4328-4292-943e-9915d970dc32}} is the effective optomechanical coupling rate, as defined in ref. ), whereas mechanical relaxation to the thermal environment at temperature {{formula:17ca4211-5514-46e9-adbc-bde8a07867b0}} takes place at a rate ({{formula:833fab6c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.6
      }
    },
    {
      "input": "What are the main findings regarding the relationship between EYM/EYMH black holes' magnetic fields and their mass, charge, and angular momentum in this research paper?",
      "expected_output": "The main findings regarding the relationship between EYM/EYMH black holes' magnetic fields and their mass, charge, and angular momentum are that these black holes possess non-trivial magnetic fields outside their regular event horizon and are not uniquely characterized by their mass, angular momentum, electric charge, and magnetic charge. The hairy black hole solutions of SU(2) Einstein-Yang-Mills (EYM) and Einstein-Yang-Mills-Higgs (EYMH) theory have non-trivial magnetic fields, and their properties cannot be determined solely by their mass, angular momentum, and charges.",
      "context": [
        "ee-c90452373526}} : at the Kaluza-Klein value {{formula:a46d3289-6b4f-4544-8c46-10face0024f4}} stationary non-static black holes appear, whose horizon is non-rotating , , and beyond {{formula:6b9058d3-67d4-4a6a-bd90-95a09d22f779}} even counterrotating black holes arise, whose horizon angular velocity and global angular momentum have opposite sign .The EM uniqueness theorem, on the other hand, does not readily generalize to theories with non-Abelian gauge fields coupled to gravity . The hairy black hole solutions of SU(2) Einstein-Yang-Mills (EYM) and Einstein-Yang-Mills-Higgs (EYMH) theory possess non-trivial magnetic fields outside their regular event horizon and are not uniquely characterized by their mass, their angular momentum, their electric and magnetic charge , , , , , , . Futhermore, black hole solutions arise, which are static and not spherically symmetric, showing that Israel's theorem , does not generalize to non-Abelian",
        " their angular momentum {{formula:de79875c-0809-46a4-a7e4-1e66943f3b8e}} , their electric charge {{formula:9a2c8fa4-9103-416e-8203-c6f738a44b8f}} , and their magnetic charge {{formula:bafd0910-3046-458d-832c-60168b65b1ea}} , . Following Wheeler this uniqueness theorem of EM theory is often expressed as “EM black holes have no hair”.In many unified theories, including string theory, dilatons appear. When a dilaton is coupled to EM theory, this has profound consequences for the black hole solutions. Not only do charged static Einstein-Maxwell-dilaton (EMD) black hole solutions exist for arbitrarily small horizon size , but also the staticity theorem of EM theory does not generalize to EMD theory for arbitrary dilaton coupling constant {{formula:b58e6c50-f422-45ad-9dee-c90452373526}} : at the Kaluza-Klein value {{formula:a46",
        "5-a2ba-42f3-9d36-3ae8087483e2}}\nConclusions: We have considered non-perturbative globally regular and black hole solutions of EYMHD theory. These stationary axially symmetric solutions are asymptotically flat. The solutions are characterized by two integers, {{formula:d6a1931a-d9bd-4227-a246-8e459c70e856}} and {{formula:4669be9b-5721-4180-b128-92f53c1a475a}} , related to the number of monopoles and antimonopoles in the solutions, and to the magnetic charge of the monopoles, respectively . The black hole solutions carry non-Abelian hair outside their regular horizon.The globally regular solutions do not rotate, when they carry a global magnetic charge. Only solutions with no global magnetic charge can possess angular momentum, which is then quantized in terms of the electric charge . The globally regular solutions satisfy a simple mass formula {{formula:b5241a46-dccf-49b5-821a-decfff9e2dcd}}The presence of"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods were used to maximize the worst-user's mutual information in this research, and what were the key findings?",
      "expected_output": "The research used traditional alternating approaches and machine learning (ML) techniques to maximize the worst-user's mutual information. The key findings included the development of an optimization problem that allocated transmit power among active links to maximize mutual information under link outage constraints. Specifically, the problem aimed to ensure a minimum SINR requirement and tolerable outage probability for each user. The research also utilized a lemma to express the outage probability in closed form for an interference-limited scenario.",
      "context": [
        " ) is a non-convex problem with fractional objective and the nonlinear probabilistic constraints, and hence the exactly optimal solution is non-trivial. In the following, we will first develop an acceptable solution to the problem following traditional alternating approaches, and then following ML techniques. {{figure:aeec7637-ec2a-4ab2-9cc4-f90b8127a2d0}}For ease of exposition, let us consider an interference-limited system such that the noise term in (REF ) can be ignored. Thus, the SINR in (REF ) reduces to {{formula:7dedbeff-7952-42e4-ae2b-21a36c6d6b52}}Then the chance SINR outage requirement in () can be expressed as {{formula:34d31f0d-c949-4641-a644-e6d2baded17f}}Note that the outage probability {{formula:d47eb4cb-10f3-4c16-9277-83ddbba3551a}} can be interpreted as the fraction of time the {{formula:82d39411-6895-",
        "3551a}} can be interpreted as the fraction of time the {{formula:82d39411-6895-41d1-83f8-020f33c2a017}} th transmitter/receiver pair experiences an outage due to fading. Interestingly, it has been shown analytically in , that the outage probability (REF ) can be expressed in closed form using the following Lemma.Lemma 1 If {{formula:9b6ada8c-1ba3-41be-aa15-8f475a7ba290}} are independent random variables following exponential distribution with means {{formula:e6f012ff-c257-4222-a950-44d253edd471}} , then we have {{formula:3457d2de-2eb7-4452-b62c-1398c621cda0}}The proof follows similar lines as in . Therefore, we omit the proof here for brevity.Using Lemma REF for an interference-limited scenario, we can express the outage-probability in (REF ) as {{formula:f8466bce-d6d1-493a-ba07-d4da8",
        "ally allocating transmit power among the active links such that the worst-user's mutual-information is maximized under link outage constraints. Accordingly, we formulate the following optimization problem: {{formula:fbc994ed-97dd-46c1-9e42-0f6f0a536803}}where {{formula:886986ff-8cca-40a9-b13e-c4266fbed20b}} and {{formula:92e5e4ab-654a-4b09-accb-8eba4b798256}} are the maximum and minimum transmit power budgets of the users, respectively, {{formula:18809890-635e-4782-8d0c-fbca2dabfca7}} is the minimum SINR requirement that needs to be maintained by each user to ensure reliability, and the tolerable outage probability is denoted by {{formula:1a433054-14f2-4c59-a428-bd1d338c2899}} . Note that problem (REF ) is a non-convex problem with fractional objective and the nonlinear probabilistic constraints, and hence the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding wall-crossing morphisms and their relationship to alpha-stable pairs on P^2 in this research paper?",
      "expected_output": "The main findings of the research paper are that the moduli spaces of alpha-stable pairs on P^2 are related by smooth blow-up and blow-down morphisms as the parameter alpha is altered. Specifically, when the degree d is 4 or 5, the moduli spaces at each wall are related by a smooth blow-up morphism followed by a smooth blow-down morphism. The blow-up centers can be described geometrically as a configuration of points on curves, which are projective bundles over the product of the moduli spaces of lower degree alpha-stable pairs.",
      "context": [
        "# arXiv Paper: 1210.2499\n\nModuli Spaces of $\\alpha$-stable Pairs and Wall-Crossing on $\\mathbb{P}^2$\nabstract: We study the wall-crossing of the moduli spaces $\\mathbf{M}^\\alpha (d,1)$ of $\\alpha$-stable pairs with linear Hilbert polynomial $dm+1$ on the projective plane $\\mathbb{P}^2$ as we alter the parameter $\\alpha$. When $d$ is 4 and 5, at each wall, the moduli spaces are related by a smooth blow-up morphism followed by a smooth blow-down morphism, where one can describe the blow-up centers geometrically. As a byproduct, we obtain the Poincar\\'e polynomials of the moduli space $\\mathbf{M}(d,1)$ of stable sheaves. We also discuss the wall-crossing when the number of stable components in Jordan-H\\\"{o}lder filtrations is three.\nMotivation and Results: In the moduli theory, for a given quasi-projective moduli space {{formula:c",
        " contraction such that the divisorial part can be described by stable pair space with various Euler characteristics.As corollaries, we obtain the Poincaré polynomials of {{formula:396d753e-a109-4120-a07e-f8dcc4a6bab8}} by using those of the relative Hilbert schemes of points on the plane curves (§5).To prove part (1), we first find flipping locus at each wall by using the stability conditions. It turns out the blow-up centers can be described as a configuration of points on curves. In particular, they are projective bundles over the product of the moduli spaces of {{formula:ba2f01f2-c1a3-4383-a9df-a24f881f2182}} -stable pairs of lower degrees. After blowing up the moduli space along such loci, by performing the elementary modifications of pairs (Definition REF ), we construct a flat family of pairs that are stable on the other side of the wall, which in turn gives a birational morphism. This morphism is shown to be a smooth blow-down morphism by analyzing the exceptional divisor and",
        " sense. In the following two subsections, we will show that each flipping space are related by smooth blow-up and followed by smooth blow-down morphisms (Theorem REF and Theorem REF ). By using the same technique, we also relate the {{formula:c4120a9c-7fb5-4d10-9adc-3918ca204890}} -stable pair spaces with Hilbert polynomial {{formula:2ac3cc70-f92e-4350-b686-df6393efc2a8}} by smooth blow-up/down morphisms. In Theorem REF , we will present the results without proof. This results will be essential to compute the Betti numbers of the moduli space {{formula:fe64af09-9451-4c50-8526-596c0e344fe2}} (Corollary REF ).\nWall-Crossing for {{formula:8cdd3569-0902-4e11-b983-7ca15da81d2d}}: The aim of this subsection is to provide a proof of Theorem REF below. We prove that the moduli spaces of"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does vs_n play in the process of vertical key generation, and how does it impact the overall outcome?",
      "expected_output": "vs_n plays a crucial role in the process of vertical key generation as it specifies the number of vertical transformations to be applied. This directly impacts the overall outcome by determining how many times the algorithm will randomly select a vertical operation function and generate a corresponding key, thereby influencing the final vertical key.",
      "context": [
        " keys, vs_{n} specifies the number of vertical transformations to be applied. For each transformation, the algorithm randomly selects a vertical operation function, vs_{op}, and subsequently generates a corresponding key based on the chosen vertical operator. The term vs_{op} represents the specific transformation function employed, with the experimentally identified functions being {addv, inflate, tilt, dx-fixp, gaussian, laplace}. For comprehensive details of these functions, refer to Table REF . The component vs_{opt} pertains to the parameters of the transformation. It primarily involves the random matrix used in the transformation function and the associated hyperparameter settings. {{table:c7d7ea8c-56b8-4e89-90d5-7f988caee718}}Horizontal Key Generation. The horizontally generated key functions as a mapping table, correlating old and new IDs. This concept, termed “horizontal shaking”, essentially involves the random permutation of an ordered sequence. The structure of a horizontal key is defined as hs_{key}: {hs_{tab}}, where hs_{tab} represents a mapping table constructed through a random process. This approach underpins the fundamental methodology of horizontal shaking in",
        "keys}=\\lbrace vs_n, vs_{ops}[], vs_{opts}[]\\rbrace hs_{keys} = \\lbrace hs_{tab}\\rbrace Output: keys = \\lbrace vs: vs_{keys}, hs: hs_{keys}\\rbrace Process: Step1: Vertical key generation: i \\leftarrow 0, vs_n o \\leftarrow rand(0, len(vs_{ops}) vs_{op} \\leftarrow vs_{ops}[o] vs_{opt} \\leftarrow vs_{op}.keygen() + vs_{opts}[o] Step2: Horizontal key generation: seq \\leftarrow range(0, len(vocab)) hs_{tab}\\leftarrow permute(seq) Step3: Key Combination: key=\\lbrace vs_{key}, hs_{keys}\\rbraceVertical Key Generation. The vertical key in our framework is denoted as vs_{key}: \\lbrace vs_n, vs_{op}, vs_{opt}\\rbrace . During the generation of vertical keys, vs_{n} specifies the number of vertical transformations to be applied. For each transformation, the algorithm randomly selects a",
        " is not directly used in the text generation process, it is only connected to the classifier of the upper layer to achieve its task-specific classification work. Shaking can be directed only at the input layer without transforming the output layer. The shaking operation involves the manipulation of the weights in the representation layer, with two primary objectives: 1) To alter the representation of the original model, and 2) To differentiate the weights of the transformed model from those of the original. To fulfill these objectives, two model shaking are employed: Model Horizontal Shaking and Model Vertical Shaking.Model Vertical Shaking refers to the process of altering the weights within the model's representation layer, which is shown in Algorithm REF . The primary aim of this process is to disrupt the association between the transformed and original vectors, thereby obscuring the relation between the transformed vector and its original counterpart. Vertical shaking typically involves multiple iterations, each employing a transformation operator and a unique key. The key for vertical shaking is defined as vs_{key}:{vs_{op}, vs_{opt}}, where vs_{op} denotes the transformation function utilized. Further details about the Vertical Key are elaborated in Section REF and Algorithm REF . Upon determining the operator for a given round, the client implements"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main applications of Markov's inequality in research and how is it used to analyze data?",
      "expected_output": "Markov's inequality is a fundamental concept in probability theory, and its main applications in research include analyzing the probability of rare events and bounding the tails of distributions. In the context of the given research paper, Markov's inequality is used to prove Lemma 20, which provides a bound on the probability of a certain event. For example, the paper uses Markov's inequality to show that the probability of a Bernoulli random variable exceeding a certain threshold is bounded. This is a key step in the proof, as it allows the authors to derive a lower bound for a certain probability. Overall, Markov's inequality is a powerful tool for analyzing data and understanding the behavior of random variables.",
      "context": [
        "b-45e5-adf4-696029279b4c}} be non-negative real numbers such that {{formula:567a9e71-0922-4300-ad3e-8166926ed54e}} . For {{formula:dbe2c3b4-2c0b-42ab-937c-a79b75f82107}} , write {{formula:c9a24a98-4644-4bf8-990c-6ff1bdb4b105}} . The following lemma is an easy consequence of Markov's inequality.Lemma 20 With the notation above, we have, for any {{formula:0fdc0f6d-439c-4adf-9215-5cf11238ef7e}} , {{formula:21aa9e7e-3317-4441-9012-4de570322bf1}}Consider the Bernoulli random variables {{formula:8c67aef1-9cd0-443a-b0a1-210e7c5759da}} for {{formula:11ac4159",
        "443a-b0a1-210e7c5759da}} for {{formula:11ac4159-3abe-4fc8-a8f6-84a2e90a0fb0}} so that {{formula:4cb6e432-6a77-4ddf-8921-9c46440caf13}} and {{formula:1dfb085b-8343-4a99-8823-a228046047ca}} Then it follows from Markov's inequality that {{formula:a0b36ef5-cd7e-4082-86c9-d6d90d68ff4d}} This finishes the proof.\nAdditive combinatorial tools: Let {{formula:17aa72af-63ad-4f26-aaa9-a4d4f1fbf827}} be bounded subsets of {{formula:461e251a-5d42-4de7-b4c2-65892548360c}} . We look at them at scale {{formula:5fea86ef-544d-4ca6-b2ca-021",
        "c-3563-4c27-be46-4c04bcb4005b}} such that for all {{formula:1a3887be-262c-4f9c-a83a-b771bf5f50a4}} , the probability that there exists {{formula:1c44ab8e-60cf-49d1-adae-42cda50ffb98}} satisfying {{formula:ce3bb5a0-2842-422f-9422-aa3776fc57c3}} and {{formula:5a8969c9-295a-4cae-9bc9-809c1fabf6f3}} is at most {{formula:36f6ee16-2fa3-4080-ac0a-ffbfaba0168c}} . The rest of the proof consist of proving a lower bound for the same probability. First, {{formula:81d88f03-0287-4e43-aa52-d307801d5956}} with probability at least {{formula:457fe954-5b41-451c-b89f-075a75"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the size of impurities affect the bond length in the materials studied in this research paper?",
      "expected_output": "The size of impurities affects the bond length in the materials studied, with smaller impurity atoms resulting in shorter bond lengths. For example, 3d impurities have smaller atomic radii and shorter bond lengths, while 4d and 5d impurities have larger atomic radii and longer bond lengths. Specifically, the bond length for 3d impurities decreases across the period as the atom size decreases, except for Co and Ni, which show a modest increase. In contrast, the bond lengths of 5d impurities are similar, around 1.97 Å, with small variations.",
      "context": [
        " as for 3d. The most stable impurities in each row occur for cobalt in 3d, then in the iron group, in ruthenium (4d) and osmium (5d). {{figure:d01f3d5f-f9ac-450c-a9c8-83a8dcf91068}}For all the impurities the distances between the three nearest neighbour carbon atoms remain almost the same, with differences less than 0.004 Å, so that the symmetry of the vacancy in graphene is well preserved. We calculated the average impurity-carbon atom bond length, as shown in Fig. REF (b). The bond length for the 3d impurities decreases across the period as the atom size decreases, except for the last two magnetic elements, Co and Ni, for which the bond length shows a modest increase. A similar trend is found for the bond lengths of the 4d impurities, but with larger values than for the 3d elements. However, the bond lengths of the 5d impurities are similar around 1.97 Å and show only small variations, within a range of 0.05 Å.The average bond length is closely related to the size of the impurity",
        ", within a range of 0.05 Å.The average bond length is closely related to the size of the impurity atom size. 3d impurities have a smaller atomic radius, which is correlated with their shorter bond lengths in graphene. Because 4d elements have a larger atomic radius than 3d elemnts, we expect greater bond lengths, in agreement with our results. The difference between the 3d and 4d case can be explained by the influence of the 4p electrons; in 4d elements these electrons screen the effective nuclear charge felt by the d electrons. Lower down in the periodic table, the difference in size between 4d and 5d elements is much less than between 3d and 5d elements. Note that for tungsten and platinum, the 4d and 5d order of the carbon-impurity distances within the group is actually reversed. A similar behaviour affecting the 4d and 5d impurities is caused by the so-called lanthanide contraction , , . For 5d elements, the 4f levels have filled up before the addition of 5d electrons. After filling the 4f levels the electrons generally move far away from the nucleus, implying a poor shielding of the effective nuclear charge affecting the 5d electrons. The result",
        " should be noted that only 3d atoms Fe, Co and Ni are ferromagnetics in bulk, according to the Stoner criterion. The wider bands of 4d and 5d atoms prevent them from having bulk ferromagnetic order at room temperature. However, at the nanoscale it is possible to find magnetic clusters for other species apart from these three traditional magnets , , , , , . As a result, the magnetic behaviour of these atoms in graphene needs careful consideration. Furthermore, the fact that atom size increases when going following the periodic table down a group implies that size effects must play an important role when introducing impurities into graphene, because the atomic size determines the distances with the neighbouring carbons. Geometry appears the key to determining the magnetic state in some cases, revealing close relationship . Where unexpected magnetic behaviour has been found, we have this explained in terms of a shell-adding phenomenon such as lanthanide contraction. It seems that 4d and 5d impurities show less independence from graphene than 3d impurities, meaning that spin polarization may be better intertwined with the graphene bands.\nComputational details: We carried out density functional theory (DFT) calculations on 4d and 5d substitutional impurities in graphene, using SIESTA"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods do lattice simulations use to determine the GW spectrum, and what are the key findings of this research?",
      "expected_output": "Lattice simulations use methods such as averaging many smaller simulations to determine the GW spectrum, which is equivalent to running one large simulation but more easily parallelized. The key findings of this research include the calculation of the GW spectrum from strongly supercooled phase transitions, studying the scaling of gradient energy in two-bubble collisions, and calculating the GW spectrum by performing many-bubble simulations in thin-wall approximation. The results show that the GW spectrum lies somewhere between the envelope and bulk flow approximations at high frequencies, and the low-frequency behavior is difficult to resolve. The research also studies the case of a complex scalar field with a U-symmetric potential, finding that the gradient energy quickly reaches a scaling behavior.",
      "context": [
        "-9876-f295a8b2f50a}} directions that correspond to the normal vectors of the cube faces. In order to accurately determine the GW spectrum, we calculate the average spectrum over multiple simulations.Averaging many smaller simulations is equivalent to running one much larger simulation but much more easily parallelised. {{figure:7634b972-217a-4816-806f-88f12340be77}}In Fig. REF the red and blue solid curves show, respectively, our result for the spectral shape of the GW spectrum in the U{{formula:aabf687f-9f6e-40b3-bf00-8ec6b0af4cfc}} symmetric case, and the case where all bubbles have equal complex phase, {{formula:7545b100-db11-4e44-91f0-e15952f636d8}} , which corresponds to having a real scalar field. The green and yellow dashed curves, for comparison, the result in the envelope and bulk flow approximations. The results are obtained by taking geometric mean over 40 simulations with around 200 bubbles each, while the error bands show the corresponding geometric standard deviations.We",
        "5d-4f87-9e67-6616b0a93178}} after the collision as a function of the bubble radius {{formula:78f2327d-b7e9-42b9-8aa1-d00f216f8f15}} . The bulk flow approximation results in a GW spectrum that turns from {{formula:982f104c-3d97-4be0-b3f5-d8e659ade35e}} behaviour to {{formula:70096b29-854a-4f2b-b39f-72bde064aaba}} at around the same frequency at which the spectrum in the envelope approximation peaks.Recently the GW spectrum was calculated in 3D lattice simulations , , . These simulations are very difficult because of the large separation between the characteristic length scales in the problem, that is the size of the growing bubble and its thinning wall. Due to numerical limitations in such simulations it is impossible account for realistically large bubble wall velocities. However, the GW spectrum can still be computed. At high frequencies it was found that the spectrum lies somewhere between the envelope and bulk flow approximations. The low-frequency behaviour",
        " high frequencies it was found that the spectrum lies somewhere between the envelope and bulk flow approximations. The low-frequency behaviour of the spectrum is especially difficult to resolve and a {{formula:2f184522-88e3-4ff2-bf32-014ca432f190}} behaviour is typically assumed.In this paper we approximate the GW spectrum from strongly supercooled phase transitions by first studying the scaling of the gradient energy in two-bubble collisions by lattice simulations and then calculating the GW spectrum by performing many-bubble simulations in thin-wall approximation. In this way we can efficiently perform large simulations with realistic behaviour of the GW source. In contrast to the earlier works, we study also the case of a complex scalar field with a U{{formula:a51acc2d-ee67-4c93-a75b-6a057038642e}} symmetric potential which is often realized in particle physics models See e.g. Refs. , , , , , , , for studies where GW signal was studied in these kind of models.. We find that in this case the gradient energy quickly reaches an {{formula:887ad981-1c2a-433b-bc"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of superclusters' high galaxy densities on cluster mergers, and what methods were used to obtain these results?",
      "expected_output": "The main findings regarding the impact of superclusters' high galaxy densities on cluster mergers are that they result in high peculiar velocities of galaxies, increasing the probability of cluster-cluster collisions. The methods used to obtain these results include X-ray observations from Einstein Observatory and EXOSAT, optical data from the Automatic Plate Measuring facility at Cambridge, and X-frequency radio observations. For example, the study of 17 clusters from the core region of the Shapley Supercluster used these methods to analyze cluster mergers at various evolutionary stages.",
      "context": [
        " of galaxies known to us ( ). Rich superclusters are also appropriate systems for studying major cluster mergers. The galaxy number density in superclusters is {{formula:36b20847-a041-431f-bdbd-6eac22c64fd1}} 10 times that in the field on length scales of {{formula:45bdab3b-fd5d-4070-bd21-16711e707cbb}} 10h{{formula:d0342e04-71c6-4175-a038-d99ac40d8c2a}} Mpc ( ). This results in high peculiar velocities of the galaxies in superclusters, which increases the probability of cluster{{formula:c397a250-dd13-45f0-ad54-3b67f9900687}} cluster collisions ( ). One such example of a rich supercluster environment is the central region of the Shapley Supercluster (SSC). , by using X-ray observations from Einstein Observatory and EXOSAT, and optical data from Automatic Plate Measuring (APM) facility at Cambridge, studied 17 clusters from the core region of the SSC. The X",
        "-frequency radio observations. The northwestern extension in A3532 seems either a part of the WAT or an unrelated diffuse source in A3532 or in the background. There is an indication that the cool core in A3532 has been disrupted by the central AGN activity. A reanalysis of the redshift data reinforces the close proximity of the clusters. The excess emission in the overlapping region appears to be a result of tidal interactions as the two clusters approach each other for the first time. However, we can not rule out the possibility of the excess being due to the chance superposition of their X-ray halos.\nIntroduction: Clusters of galaxies are believed to form hierarchically by the merger of smaller groups and clusters. Major cluster mergers, which are believed to be the most energetic events in the Universe ( ), involve two clusters of similar masses ( ). Many clusters are often found to form large concentrations, called superclusters ( ), which are the largest (size{{formula:36f0b548-5418-4367-ba6b-1e0fab96dc04}} -100 Mpc) systems of galaxies known to us ( ). Rich superclusters are also appropriate systems for studying major cluster mergers. The galaxy number",
        "diameter 74 h{{formula:bbd9bbd0-8a93-40d6-9bb1-78ad3316e004}} Mpc) of the SSC. The optical data also revealed a large deviation from the Hubble flow within SSC which suggested that it might be a nearly bound system. By using the X-ray observations from the ROSAT All Sky Survey (RASS), discovered 14 new cluster candidates, and also observed that the over-density of clusters in the SSC outskirts is mainly due to an excess of low X-ray luminosity clusters. This led to a suggestion that the whole region is still accreting small clusters with low luminosities from the outskirts ( ).The SSC hosts two major cluster complexes dominated by A3558 and A3528. This concentration of clusters is a rich system for observing cluster mergers at various evolutionary stages ( ). The A3558 complex comprises three rich clusters of galaxies viz., A3556, A3558, and A3562, and several other poor clusters and groups, while the A3528 complex is formed by the galaxy clusters A3528, A3532 and A3530. The properties of the A3558 complex have been extensively studied using"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do game forms map strategies to outcomes in the context of this research paper, and what are the key findings related to this concept?",
      "expected_output": "In the context of this research paper, game forms map strategies to outcomes through a mapping g : X × Y → Ω, where X and Y are finite sets of strategies for Alice and Bob, and Ω is a finite set of outcomes. The key findings related to this concept include the ability to compute a lexsafe Nash Equilibrium (NE) in an arbitrary game using an oracle that satisfies certain requirements, and the introduction of game correspondences as a way to define game forms. Specifically, Theorem 7.1 states that given an oracle satisfying requirements (I,II,III), a lexsafe NE of Alice or Bob exists and can be computed in time polynomial in |Ω|. For example, in Figure REF, game forms are represented by tables with rows, columns, and entries labelled by x ∈ X, y ∈ Y, and ω ∈ Ω, respectively.",
      "context": [
        " which size may be logarithmic in the size of g. We assume that this oracle solves in polynomial time any two-person game (g,u^0) in which payoff u^0 is zero-sum and takes only values \\pm 1; oracle \\mathcal {O} tells us who wins and determines a winning strategy. Based on this assumption, we provide an algorithm computing a lexsafe NE in an arbitrary game (g,u) in time polynomial in the size of \\mathcal {O}.In the last section we consider four examples of such oracles from different areas of game theory and show that all four satisfy the above assumption.\nGame forms: In this paper we consider finite, not necessarily zero-sum, normal form games of two players, Alice and Bob. They have finite sets of strategies X and Y, respectively. A game form is a mapping g : X \\times Y \\rightarrow \\Omega , where \\Omega is a finite set of outcomes. Several examples are given in Figure REF , where game forms are represented by tables with rows, columns, and entries labelled by x \\in X, y \\in Y, and \\omega \\in \\Omega , respectively",
        "cO)$ generated by $\\cO$ is tight and that an arbitrary {\\em win-lose game} $(g;u,w)$ (in which payoffs $u$ and $w$ are zero-sum and take only values $\\pm 1$) can be solved, in time polynomial in $|\\cO|$. These assumptions allow us to construct an algorithm computing two (one for each player) lexsafe NE in time polynomial in $|\\cO|$. We consider four types of oracles known in the literature and show that all four satisfy the above assumptions.\nIntroduction: Here we outline main results. Precise definitions will be given later.Consider a finite n-person game in normal form representing it as a pair (g,u), where u is the payoff function of n players and g is the so-called game form. Respectively, the latter can be viewed as a game without payoffs, which are not given yet. Such approach is standard and convenient: game form g “is responsible\" for structural properties of game (g,u), which hold for any payoff u. For example, game form g is called Nash-solvable if game (g,u) has",
        " a minimal winning strategy of Alice (in the original game) in at most |\\Omega _A| steps. We can speed up the above procedure using dichotomy. The same works for Bob.Theorems 1–3 immediately imply the following statement.Theorem 7.1 Given an oracle \\mathcal {O} satisfying requirements (I,II,III), a lexsafe NE of Alice (of Bob) exists and can be computed in time polynomial in |\\mathcal {O}|.\nGame forms and game correspondences: A game correspondence is defined as an arbitrary mapping G : X \\times Y \\rightarrow 2^\\Omega \\setminus \\lbrace \\emptyset \\rbrace , that is, G assigns a non-empty subset of outcomes to each situation.Given G, define a game form g \\in G, choosing an arbitrary outcome g(x,y) \\in G(x,y) for each situation (x,y). Conversely, given a game form g : X \\times Y \\rightarrow \\Omega , define a game correspondence G setting G(x,y) = g(x)\\cap g(y). Then, obviously, g \\in G."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do LSSMs address observational noise in X-ray data to improve the accuracy of their findings?",
      "expected_output": "LSSMs address observational noise in X-ray data by explicitly modeling it, allowing for the estimation of the hidden autoregressive process. This is achieved through the use of two equations: the system or dynamical equation and the observation equation. The LSSM is a multivariate description that provides a more accurate representation of the underlying dynamics, as seen in the analysis of Active Galactic Nuclei (AGN) observed with EXOSAT, which found evidence for a relationship between the relaxation timescale and the spectral hardness.",
      "context": [
        " appear less steep and the spectral slope will be underestimated (Deeter and Boynton 1982; Deeter 1984). Consequently a model is required which operates in the time domain and avoids any misleading systematic effects. As an alternative approach, we have used the Linear State Space Model (LSSM). The LSSM is a generalization of the autoregressive (AR) model invented by Yule (1927) to model the variability of Wolf's sunspot numbers.The AR model expresses the temporal correlations of the time series in terms of a linear function of its past values plus a noise term and is closely related to the dynamics of the system. LSSMs generalize the AR processes by explicitly modelling observational noise. The variable {{formula:3e433cf7-21fc-4a3b-a19a-b77dae64a90d}} that has to be estimated cannot be observed directly since it is covered by observational noise {{formula:ecd60b15-b0e3-4a09-b00b-7d8a1cc68607}} . The measured observational variables {{formula:7d476f57-dc19-47c1-a4c0-4ad163d56",
        "# arXiv Paper: astro-ph/9704003\n\nAnalyzing X-ray variability by State Space Models\nabstract: In recent years, autoregressive models have had a profound impact on the description of astronomical time series as the observation of a stochastic process. These methods have advantages compared with common Fourier techniques concerning their inherent stationarity and physical background. If autoregressive models are used, however, it has to be taken into account that real data always contain observational noise often obscuring the intrinsic time series of the object. We apply the technique of a Linear State Space Model which explicitly models the noise of astronomical data and allows to estimate the hidden autoregressive process. As an example, we have analysed a sample of Active Galactic Nuclei (AGN) observed with EXOSAT and found evidence for a relationship between the relaxation timescale and the spectral hardness.\nIntroduction and Mathematical Background: Irregular X-ray variability is a common phenomenon of Active Galactic Nuclei (AGN). The power spectra of the observed lightcurves exhibit an `observational noise floor' at high frequencies, an increase of power towards low frequencies ({{formula:971a3018-ae02-4960",
        "ula:7d476f57-dc19-47c1-a4c0-4ad163d56ce6}} do not generally agree with the system variables {{formula:0f455f15-7cf0-497e-8444-1938622471ab}} . Thus an LSSM is defined with two equations, the system or dynamical equation (REF ) and the observation equation (). {{formula:a9daef47-ae15-4306-aca5-b96e1b039c28}}This definition is a multivariate description, which means that the AR[p] process is given as a {{formula:4d019e5f-30f5-43d4-a428-99b96d467ffd}} -dimensional AR process of order one, with a matrix {{formula:f98f7956-2630-4434-9449-281c8989dca7}} that determines the dynamics. The matrix {{formula:fbded0c7-d738-489c-97c0-ffc9185707f0}} maps the unobservable dynamics to the observation. The terms {{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main effects of altering the cosmological constant on the power spectrum in this research paper?",
      "expected_output": "According to the research paper, altering the cosmological constant has no effect on the power spectrum. However, for a specific list of pulsars at the right angular separation, the signal significance grows enormously for certain values and does not follow a power law. In contrast, when the pulsars are at an angular separation away from the peak, the signal follows a power law and there are no differences between the different values of the cosmological constant.",
      "context": [
        " indicated that no differences at all were observed in the power spectrum when the value of {{formula:c071dedf-6513-4dfd-9110-b6339f2bb838}} was changed and that, as expected , , , the signal follows a power law {{formula:9c6ce362-6e60-438b-b25f-4abf223d6d40}} . However, let us take a closer look at the dependency on the frequency for a short list of pulsars located at the right angular separation to observe the peak. We have already seen the significance grows notoriously in this angular region. Figure REF (middle) shows the frequency dependence of the signal for fifteen pulsars at the right spot with respect to Source 1. As it can be clearly seen, the signal significance grows enormously again for {{formula:8c2b42af-e003-4f8a-a4a1-74a9fdff69ae}} and apparently does not follow a power law. For the same short list of pulsars and for {{formula:0c544894-70ce-4f91-ac76-092d6dd909a",
        ":c584f9e7-f76f-4062-ac48-982ba3251ff1}} to these sources. This could be a realistic calculation once real sources are considered.The results obtained are plotted in Figure REF . In both cases a very noticeable peak is observed at the expected angle. {{figure:39f69b23-9583-4272-931f-b25b23f18a21}}The reason why the peak for Source 2 is lower than the peak for Source 1 is that Source 2 is located close but not at the precise angular separation of a real cluster of pulsars. This is meant to illustrate that even in that case a significant enhancement of the signal can be achieved.Finally, the dependency of {{formula:e32eaa90-a5b2-46c9-a141-ae22f1a8686e}} on the frequency {{formula:52b85a56-952d-40f9-b2b4-9bae00eff791}}has also been investigated. Some of our preliminary checks indicated that no differences at all were observed in the power spectrum when the value of {{formula:c071dedf",
        "ula:0c544894-70ce-4f91-ac76-092d6dd909a6}} the signal falls back to smaller values and its envelope shapes towards a power law. In Figure REF (top) we also present the same plot for fifteen pulsars located at an angular separation of around {{formula:2c54ccae-8501-4d2c-beaa-2ca40737fac2}} rad, that is away from the peak. In this case we see no differences between the different values of the cosmological constant as well as a clear power law behavior. The magnitude of the signal is compatible with that of the fifteen pulsars at the peak separation when {{formula:ccfbf98c-fb5b-48a6-b37b-084d202f6106}} . {{figure:0eb6be6a-da92-4d3a-8a63-ccf994c78a77}}\nMeasuring the cosmological constant: We have seen in the previous that there is an enhancement in the timing residual for a particular value of the angle {{formula:146b77bb-cf0d-4e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how linear strategies enable effective signaling, and what methods were used to obtain these results?",
      "expected_output": "The main findings show that linear strategies in players' private types enable effective signaling in dynamic LQG games with asymmetric information. The methods used to obtain these results include a sequential decomposition methodology and a backward-forward algorithm. Specifically, the backward algorithm involves solving an algebraic matrix equation, while the forward algorithm consists of Kalman filter recursions. These methods allow for the calculation of Gaussian beliefs and the update of mean and covariance through Kalman filtering equations, which depend on the players' strategies, thus facilitating signaling.",
      "context": [
        " results in to find non-signaling equilibria of dynamic LQG games with asymmetric information.Recently, we considered a general class of dynamic games with asymmetric information and independent private types in and provided a sequential decomposition methodology to find a class of PBE of the game considered. In our model, beliefs depend on the players' strategies, so our methodology allows the possibility of finding signaling equilibria. In this paper, we build on this methodology to find signaling equilibria for two-player dynamic LQG games with asymmetric information. We show that players' strategies that are linear in their private types in conjunction with consistent Gaussian beliefs form a PBE of the game. Our contributions are:(a) Under strategies that are linear in players' private types, we show that the belief updates are Gaussian and the corresponding mean and covariance are updated through Kalman filtering equations which depend on the players' strategies, unlike the case in classical stochastic control and the model considered in . Thus there is signaling , . (b) We sequentially decompose the problem by specializing the forward-backward algorithm presented in for the dynamic LQG model. The backward algorithm requires, at each step, solving a fixed point equation in `partial",
        "# arXiv Paper: 1606.04960\n\nSignaling equilibria for dynamic LQG games with asymmetric information\nabstract: We consider a finite horizon dynamic game with two players who observe their types privately and take actions, which are publicly observed. Players' types evolve as independent, controlled linear Gaussian processes and players incur quadratic instantaneous costs. This forms a dynamic linear quadratic Gaussian (LQG) game with asymmetric information. We show that under certain conditions, players' strategies that are linear in their private types, together with Gaussian beliefs form a perfect Bayesian equilibrium (PBE) of the game. Furthermore, it is shown that this is a signaling equilibrium due to the fact that future beliefs on players' types are affected by the equilibrium strategies. We provide a backward-forward algorithm to find the PBE. Each step of the backward algorithm reduces to solving an algebraic matrix equation for every possible realization of the state estimate covariance matrix. The forward algorithm consists of Kalman filter recursions, where state estimate covariance matrices depend on equilibrium strategies.\nIntroduction: Linear quadratic Gaussian (LQG) team problems have been studied extensively under the framework of classical stochastic control",
        " these notions of equilibrium consists of a strategy and a belief profile of all players where the equilibrium strategies are optimal given the beliefs and the beliefs are derived from the equilibrium strategy profile and using Bayes' rule (whenever possible), with some equilibrium concepts requiring further refinements. Due to this circular argument of beliefs being consistent with strategies which are in turn optimal given the beliefs, finding such equilibria is a difficult task. To date, there is no known sequential decomposition methodology to find such equilibria for general dynamic games with asymmetric information.Authors in studied a discrete-time dynamic LQG game with one step delayed sharing of observations. Authors in studied a class of dynamic games with asymmetric information under the assumption that player's posterior beliefs about the system state conditioned on their common information are independent of the strategies used by the players in the past. Due to this independence of beliefs and past strategies, the authors of were able to provide a backward recursive algorithm similar to dynamic programming to find Markov perfect equilibria of a transformed game which are equivalently a class of Nash equilibria of the original game. The same authors specialized their results in to find non-signaling equilibria of dynamic LQG games with asymmetric information.Recently, we"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings and methods used to determine the unique solution of a dual problem in this research paper?",
      "expected_output": "The main findings of the research paper include the unique solution of the dual problem, which is a maximizer of the energy functional, and the strong duality relation that applies. The methods used to determine this solution include appealing to the Fenchel-Young identity and using convex optimality relations. For example, the paper states that the dual problem admits a unique solution, denoted as formula:689a1d66-2110-4f4e-ba2c-bacd61154f9a, which is equivalent to formula:2422dd94-adfa-4b1a-a816-9f20-9bf3128343b5 by the Fenchel-Young identity. Additionally, the discrete dual problem also admits a unique solution, denoted as formula:ceb5c3d4-ed3c-449b-9c10-269c8186058e, with a discrete strong duality relation.",
      "context": [
        "87fa1520}} if {{formula:2fd3b449-b769-487f-9b68-a57cc9f6f32e}} and {{formula:049f95e8-cef5-46ec-aba3-4a91e4cba914}} else. Appealing to , the dual problem admits a unique solution {{formula:689a1d66-2110-4f4e-ba2c-bacd61154f9a}} , i.e., a maximizer of (REF ), called the dual solution, and a strong duality relation, i.e., {{formula:80b15b4e-7304-4512-a9fe-a45fb6b09b41}} , applies. In addition, there hold the convex optimality relations {{formula:61273f89-038d-4edb-8f4d-ec456c9a2b4b}}Note that, by the Fenchel–Young identity, cf. , (REF ) is equivalent to {{formula:2422dd94-adfa-4b1a-a",
        "a16-9f20-9bf3128343b5}}Appealing to , the discrete dual problem admits a unique solution {{formula:ceb5c3d4-ed3c-449b-9c10-269c8186058e}} , i.e., a maximizer of (REF ), called the discrete dual solution, and a discrete strong duality relation, i.e., {{formula:bfc1a937-edaa-4511-95ac-208c0f7a90b2}} , applies. In addition, cf. , there hold the discrete convex optimality relations {{formula:953698b6-6511-456d-87a2-a48ccb7beac2}}Note that, by the Fenchel–Young identity, cf. , (REF ) is equivalent to {{formula:815fbb96-c7b4-49d8-8654-f10e8635181b}}Moreover, cf. , the unique solution {{formula:b8499b51-09e0-41a9-b093-33026e9313d1}} of the dual problem",
        ", i.e., is the unique minimizer of the {{formula:fef64bb4-a065-40c9-b13e-9666bf7de9d6}} -Dirichlet energy.Dual problem. In the case (REF ), proceeding as, e.g., in , one finds that the dual problem consists in the maximization of the energy functional {{formula:eba21fc9-3f1b-4771-81ca-a6440c6746af}} , for every {{formula:92116591-3cde-4c03-b01f-cafccafb15d0}} defined by {{formula:bde85858-cc26-4712-adc4-23bb4246e786}}where {{formula:2eee7c6d-6833-4de0-9cb2-79f01fe7341c}} is defined by {{formula:27613d70-56ae-45c3-9458-6d5d87fa1520}} if {{formula:2fd3b449-b769-487f-9b"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do non-symmorphic symmetries play in protecting nodal line Fermi surfaces, and how do the researchers demonstrate this protection in their study?",
      "expected_output": "Non-symmorphic symmetries play a crucial role in protecting nodal line Fermi surfaces by enforcing a four-fold degenerate nodal line FS on a Brillouin zone boundary plane. The researchers demonstrate this protection using a tight binding model derived for perovskite iridates, showing that the two non-symmorphic symmetry operations lead to two pairs of bands, whose partners within each pair are exchanged when the Bloch bands disperse from one BZ boundary to another, resulting in a protected four-fold degenerate nodal line FS.",
      "context": [
        " class of topological semimetals is characterized by a closed loop of FS called nodal line FS , , , , , , , , , . These semimetals named as topological nodal line semimetals have recently been proposed in various materials, including a three-dimensional graphene network , Ca{{formula:ba25a28a-169a-4a58-8f9f-ec810f7c6c43}} P{{formula:4baf6e05-ef30-4e6d-9788-eae32d897c61}} , Cu{{formula:3e35b4b4-6e2c-430c-9443-330635272351}} PdN , and orthorhombic perovskite iridates , . However, in graphene material, Ca{{formula:dd3abebe-7198-41e6-922c-53d4c3eb66b5}} P{{formula:e8bf0b62-a72c-485f-b7ae-d4c103c724fc}} , and Cu{{formula:467e9438-5",
        ". Due to the two non-symmorphic symmetry operations, two pairs of such bands exist, and their partners within each pair should be exchanged when the Bloch bands disperse from one BZ boundary to another. This enforces a four-fold degenerate nodal line FS protected by non-symmorphic symmetries on a BZ boundary plane. Using a tight binding model derived for perovskite iridates {{formula:06f35d6e-6c31-4113-aab3-8074830a8e2a}} IrO{{formula:926b6305-dadc-4f49-b667-19470897e30a}} ({{formula:0dca359e-c430-4252-a34f-023f8bba2b5c}} being alkali earth elements) , we further show how Dirac point or line of Weyl FS appears when TRS is broken. Surface states of this topological crystalline semimetal with and without TRS are also presented.\nA sufficient condition for nodal-line semimetal with TRS: First we provide a generic condition for a four-fold degenerate",
        "orphic symmetries. When TRS is broken, four-fold degenerate Dirac points or Weyl ring FS could occur depending on a direction of the magnetic field. On a certain surface double helical surface states exist, which become double Ferm arcs as TRS is broken.\nIntroduction: Recently intense interest has been drawn to novel topological semimetallic phases, in which the systems support non-trivial band crossing points in crystal momentum space. Such studies have been motivated by the discovery of topological insulators with bulk energy gap and conducting surface modes protected by TRS , , , , , , , , , , . A list of topological semimetals, which is an extension of topological insulators to metallic phases, has been growing in theoretical communities, and some members in the list have been experimentally confirmed. , , , One group of topological semimetals is characterized by FS points. This includes Weyl semimetal with chiral fermion , , , , , and three-dimensional (3D) Dirac semimetals with surface Fermi arc states , , . Another class of topological semimetals is characterized by a closed loop of FS called nodal line FS , , , , ,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on spectral slope variations on 67P/CG and how were they obtained?",
      "expected_output": "The main findings of the research on spectral slope variations on 67P/CG show that the spectral slope changes locally over time, with some regions like Seth and Imhotep experiencing a decrease in reddening of up to 25%, while others like Ma'at and Serget see an increase of up to 7%. These findings were obtained through the analysis of 14 GCPs reported in Table REF and by comparing maps in Fig. REF, which demonstrate that the spectral slopes vary across the surface and are dynamic quantities changing with time depending on heliocentric distance and gaseous activity.",
      "context": [
        " to MTP9 (corresponding to a time-interval of about 4 months or 0.69 AU in heliocentric distance), the spectral slope shows local changes: from the comparison of the maps in Fig. REF and from the analysis of the 14 GCPs reported in Table REF , it turns out that on some regions, like Seth and Imhotep the reddening has a maximum decrease of about 25{{formula:ecf1aca5-ac36-4e7b-9408-67d7d22619c5}} . On other regions, like Ma'at and Serget, a significant increase, up to 7{{formula:f1e3e9d0-40e9-4044-abdf-352a84e6a1fd}} and {{formula:b5d425b8-9d47-4694-910b-868f7f96dcb3}} , respectively, is observed. Finally, the slope remains almost constant on Aker and Ash. and have also made a comparison of the VIS spectrophotometric parameters measured by VIRTIS and OSIRIS, respectively, on 67P/CG and previously observed com",
        "rophotometric parameters measured by VIRTIS and OSIRIS, respectively, on 67P/CG and previously observed comets, like 1P/Halley, 19P/Borrelly, 103P/Wild, 9P/Tempel and 81P/Wild. A direct comparison among these objects is quite difficult because different space missions have collected data at different heliocentric distances and with variable phase angle coverages. Certainly Rosetta data clearly show that 67P/CG spectral slopes vary across the surface and are dynamical quantities changing with time depending upon the heliocentric distance and level of the gaseous activity. {{figure:2657c768-6b4d-48d7-ae3e-2668c7179d43}}\nInfrared spectral slope {{formula:87c5f8d6-6fa8-4281-838c-8ed0c3f9cb82}} cylindrical map: The measured infrared slope {{formula:1dfcaf87-42d1-4ae9-99a3-a0e1136f808a}} varies between 2.5 and 6.0 {{formula:c79b9254",
        "e1136f808a}} varies between 2.5 and 6.0 {{formula:c79b9254-7996-47a4-919f-cce7764d2515}} . The distribution of the slope on the surface as shown in Fig. REF resembles the visible slope one: lower values are observed on Hapi and around the Imhotep plain, in particular around GCP E area. The strongest reddening, equal to 6 {{formula:fa899bc5-a8e4-48ba-8d48-8edefdb938ac}} , is measured in MTP6 on Anuket region, towards the Hathor and Serget boundary. Moving from MTP6 to MTP9 the {{formula:4110dd09-fcb5-4f92-8c5d-bdc863f7092b}} slope values remain almost constant (see Table REF ) with the exclusion of Serget and GCP E, where an increase of {{formula:00b4bb4f-c24d-40a3-916e-e5c298182f1e}} and a decrease of about {{formula:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how the AR regime affects shot noise in N-TI-S contacts, and what methods were used to obtain these results?",
      "expected_output": "The main findings show that the Andreev reflection (AR) regime in N-TI-S contacts gives rise to effective charge doubling in shot noise measurements, but with a surprisingly reduced Fano factor of approximately 0.22, compared to the normal state Fano factor of about 1/3. These results were obtained through transport and shot noise measurements in lateral N-TI-S contacts, where the devices were found to be in the elastic diffusive transport regime, and the AR regime was identified in the absence of a magnetic field.",
      "context": [
        " related to a finite thermal conduction of the proximized, superconducting TI owing to a residual density of states at low energies.\nDifferential resistance in a wide bias range: In all N-TI-S devices studied the differential resistance, {{formula:29a344d7-4ddd-4f75-ae4e-14fd27dbb76d}} , behaves similarly to the reference N-TI-N device and exhibits no AR related features. This is verified in Figs. REF a and REF b for two representative devices in a wide bias range. Just like in the reference N-TI-N device, see Fig. REF c, the small zero bias feature in {{formula:1dc7e201-82cb-4bf6-8e38-6fed47e1fab2}} develops into a pronounced resistance peak in a magnetic field {{formula:45d3b3d0-a69b-4aff-80d9-c70bf4ea378c}} T. This behavior is qualitatively consistent with a scenario of competing quantum corrections, weak anti-localization and Altshuler-Aronov, among which the former is suppressed",
        "# arXiv Paper: 1604.00164\n\nAndreev reflection in s-type superconductor proximized 3D topological insulator\nabstract: We investigate transport and shot noise in lateral N-TI-S contacts, where N is a normal metal, TI is a Bi-based three dimensional topological insulator (3D TI), and S is an s-type superconductor. In normal state, the devices are in the elastic diffusive transport regime, as demonstrated by a nearly universal value of the shot noise Fano factor $F_{\\rm N}\\approx1/3$ in magnetic field and in reference normal contact. In the absence of magnetic field, we identify the Andreev reflection (AR) regime, which gives rise to the effective charge doubling in shot noise measurements. Surprisingly, the Fano factor $F_{\\rm AR}\\approx0.22\\pm0.02$ is considerably reduced in the AR regime compared to $F_{\\rm N}$, in contrast to previous AR experiments in normal metals and semiconductors. We suggest that this effect is related to a finite thermal conduction of the proximized, superconducting TI owing to a residual density of states at",
        " competing quantum corrections, weak anti-localization and Altshuler-Aronov, among which the former is suppressed by a perpendicular magnetic field and both are suppressed by a high bias owing to dephasing, see, e.g. Ref. . {{figure:f5a811a4-6cea-48ca-92e5-d1f8c1e7302e}}\nElectron-phonon energy relaxation: As discussed in the main text, at large biases, {{formula:a7b3ab6f-e5a6-4bb8-9f28-b6ddeb06bbac}} mV, the data deviate below the {{formula:77b94072-0ed4-4c4e-829f-6aa522493cc9}} fit, both in zero and finite {{formula:7b7b39bf-fe15-45df-acf3-c2191e2abf1f}} -field, which is a result of shot noise suppression via electron-phonon (e-ph) energy relaxation , . We have checked that for {{formula:65a4e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the local field play in enhancing emission current, and how does it impact the overall findings of this research paper?",
      "expected_output": "The local field plays a crucial role in enhancing emission current by determining the distribution of electrons emitted from a single emitter. According to the research paper, the local field at the emitter surface is enhanced due to the curvature of the emitter, and this enhancement can be calculated using the formula provided. The paper states that the local field is proportional to the field at the emitter apex and the distance from the apex, and this relationship allows for the prediction of the net emission current and the distribution of electrons. For example, the paper mentions that the tip of the emitter is locally parabolic, which affects the local field and the resulting emission current. Overall, understanding the local field is essential for predicting the emission current and the distribution of electrons, and the research paper provides a theoretical framework for making these predictions.",
      "context": [
        " the net emission current or the distribution of electrons (that are emitted from a single emitter comprising of the apex and its neighbourhood) with respect to launch angle, the total and normal energy .Our aim here is to provide a partial rectification of this lacuna for the class of smooth axially symmetric curved emitters placed on a planar surface in situations where the asymptotic (away from the emitter apex) electric field {{formula:1df4c267-9aec-422a-9979-51ba97c839db}} is uniform and aligned parallel to the emitter axis. Under such circumstances, a recent result shows that the local field {{formula:5fc5ddf0-cf67-4c7b-ba44-252f7c77998b}}where {{formula:7afa3382-3891-4601-a29a-b7e2a7bc64a8}} is the field at the emitter apex, {{formula:a4d62d6c-353d-4b3f-8c9c-500cce586182}}and {{formula:615691ac-12bb-4ede-89",
        "}} for a point on the emitter surface. Thus a Taylor expansion at the apex yields {{formula:0011f5f0-954c-448f-9fb4-921bb050c5a9}}since {{formula:31b3c0cf-61a7-496b-863c-fb5ab1a76200}} . Thus, the tip is locally parabolic. Using Eq. REF and assuming that {{formula:d9ebf2bb-a98b-4b90-a5d4-60dee5fb5703}} is known, an ideal theoretical prediction for field-emission current from a single vertically aligned emitter can in principle be made.Equation REF also allows us to address the issue of distribution of electrons from a single emitter. A joint distribution of the net emitted electrons with respect to generalized angle {{formula:17074e75-4521-494f-a683-415f1beca25d}} , the total energy {{formula:2c6dca3e-ec10-4cce-a771-b00a4ae15a8a}} and the normal",
        " of the Fowler-Nordheim law serves as the basis of modern analytical field emission formulae , , , . It includes the image-charge correction and makes allowance for curved emitters by replacing the electric field on the planar emitter surface by the (enhanced) local electric field {{formula:94f2872f-9f1e-4026-9966-96170a45e525}} at a point {{formula:80ea6f52-9c2c-4bc3-89c0-00714da9e00d}} on the curved emitter surface. This quasi-planar formalism serves as a reasonable approximation to the current density for even high aspect-ratio curved field emitter when the local field at the emitter surface is not too low. The bottleneck in most theoretical predictions lies in not knowing much about the enhanced electric field {{formula:617af2ba-90d5-4fff-8b7e-656ed16a6acd}} or its local variation. In the absence of such information, it is impossible to predict the net emission current or the distribution of electrons (that are emitted from a single emitter comprising of the apex and its neighbourhood"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do vision-based techniques aid seizure detection, and what are the main findings of this research in terms of accuracy and reliability?",
      "expected_output": "Vision-based techniques aid seizure detection by analyzing video recordings of patients to identify stereotypical behavioral and motor changes associated with seizures. According to the research, these techniques have proven valuable in patient monitoring, particularly in noisy environments like epilepsy monitoring and intensive care units. The main findings suggest that vision-based systems can successfully detect semiology manifestations of seizures, including motor alterations, abnormal lack of movement, and abrupt changes in posture. While the research does not provide specific accuracy and reliability metrics, it highlights the potential of deep learning and computer vision techniques in improving seizure detection methods and refining semiology analysis for predicting seizure type and cerebral localization.",
      "context": [
        " a key aspect of semiology analysis, aims to identify different actions from sequences of observations across diverse environmental conditions. The applications of such recognition methods driven by deep learning and computer vision are wide-ranging, contributing to advancements relating to human health and performance. Applications include assessment of human development, optimization of human performance, neuromuscular rehabilitation, prevention of musculoskeletal injury, gait analysis, and motor assessment , . Existing literature extensively explores the causal relationship between diseases and abnormal body actions such as atypical head poses and movements, emotions, eye movements, as well as upper and lower limb movements , .\nVision-based seizure semiology analysis: There is an increasing interest in leveraging video analysis techniques for the automated analysis of movements associated with motor stereotyped manifestations such as those from Parkinson's disease, tremors, peripheral neuropathy, Huntington's chorea, ataxia, dystonia, and epilepsy , , , . Vision-based techniques, in particular, have proven valuable in patient monitoring . While general vision-based motion analyses excel in uncluttered settings, they can be unreliable in noisy environments like epilepsy monitoring and intensive care units. These settings pose unique challenges, including varying lighting conditions throughout recording sessions, environmental occlusions (e.g",
        " divided into five major sections. Section elucidates concepts and methods in quantifying behavior and behavioral sequences from a neurobiological/neuroscience perspective. Then, the section explores the application of computational neuroethology tools in evaluating these behavioral patterns. In Section , we provide a technical overview of human action recognition (HAR) and current approaches for vision-based seizure semiology analysis. Specifically, Section REF , introduces skeleton-based HAR approaches, while Section REF focuses on frame-based HAR models. In Section , we introduce the current applications of computer vision and deep learning techniques for semiology analysis. Details regarding the existing approaches along with the computer vision techniques are categorized according to the motion framework employed. Section offers guidance on incorporating existing approaches along with the computer vision techniques they employ for clinical use through a multi-stream framework for semiology analysis. Finally, in Section , we clarify limitations and offer new perspectives on the current state of semiology-based seizure analysis.\nHuman movement analysis in the epilepsies: from traditional clinical observation to quantitative and computational neuroethology: The evolution of technology has played a pivotal role in shaping our understanding of human behavior, especially in the context of conditions such as motor epilepsies. Computational tools have become integral in uncovering",
        "-observe events, reducing documentation efforts, and extending assessment capabilities to areas with limited expertise. The main applications of these could be (1) improved seizure detection methods; (2) refined semiology analysis for predicting seizure type and cerebral localization. In this paper, we detail the foundation technologies used in vision-based systems in the analysis of seizure videos, highlighting their success in semiology detection and analysis, focusing on work published in the last 7 years. Additionally, we illustrate how existing technologies can be interconnected through an integrated system for video-based semiology analysis.\nIntroduction: Seizure semiology involves the analysis of seizure clinical manifestations, which may include stereotypical behavioral and motor changes, among them tonic, clonic or complex motor patterns. The clinical expression of seizure represents the primary symptomatic challenge in epilepsy. Various semiologic manifestations of seizures are potentially measurable using video analysis: motor alterations (e.g. as seen in automatic behaviors, hyperkinetic seizures, or tonic-clonic seizures); an abnormal lack of movement (typical of some seizures with prominent loss of responsiveness to the environment); or an abrupt change in posture that constitutes a risk factor for falls (e.g. in atonic seizures). Alongside electrophysiological recordings and neuro"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between stellar spots and spike, and how do researchers study this phenomenon in the context of the research paper?",
      "expected_output": "According to the research paper, the spike in the hump & spike feature is assumed to be rotational modulation induced by stellar spots co-rotating with the stellar surface. Researchers study this phenomenon by analyzing the Kepler light curves of 162 intermediate-mass stars that present a similar feature in their Fourier spectra. They investigate whether the rotational modulation is due to stellar spots caused by magnetic fields or due to Overstable Convective modes resonantly exciting g-modes. The study finds a clear anti-correlation between the spike amplitudes and stellar mass, and possibly a correlation with stellar age, consistent with the dynamo-generated magnetic fields theory.",
      "context": [
        "4f990c30}}{{figure:ab7b4da2-6d82-4561-ba5c-3ac4c6065f73}}\nConclusions and future work: In this work, we analysed the Kepler light curves of 213 stars that present a similar feature in their Fourier spectra called hump & spike. Out of this sample, we have selected 162 stars that did not present obvious photometric signs of being in a binary system (e.g. no transits or ellipsoidal variation). We aimed to validate the initial theoretical interpretation of the origin of the spike , which is assumed to be rotational modulation induced by stellar spots co-rotating with the stellar surface. We tested the theory regarding the presence, origin and strength of magnetic fields on our sample of stars under the assumption that the hump represents unresolved Rossby modes and the spike stellar spots. In the present work, we have concentrated only on the spike in the hump & spike feature. The 'hump' will be the focus of future work currently in preparation.In addition, we have also discussed another possible phenomenon that could cause rotational modulation in the light curves of our stars, as suggested by and . In this context, the spike would be due",
        "# arXiv Paper: 2301.04974\n\nRotational modulation in A and F stars: Magnetic stellar spots or convective core rotation?\nabstract: The Kepler mission revealed a plethora of stellar variability in the light curves of many stars, some associated with magnetic activity or stellar oscillations. In this work, we analyse the periodic signal in 162 intermediate-mass stars, interpreted as Rossby modes and rotational modulation - the so-called \\textit{hump \\& spike} feature. We investigate whether the rotational modulation (\\textit{spike}) is due to stellar spots caused by magnetic fields or due to Overstable Convective (OsC) modes resonantly exciting g~modes, with frequencies corresponding to the convective core rotation rate. Assuming that the spikes are created by magnetic spots at the stellar surface, we recover the amplitudes of the magnetic fields, which are in good agreement with theoretical predictions. Our data show a clear anti-correlation between the spike amplitudes and stellar mass and possibly a correlation with stellar age, consistent with the dynamo-generated magnetic fields theory in (sub)-surface convective layers. Investigating the harmonic behaviour, we find that for 125 stars neither of the two possible explanations",
        "'s evolution is around the stellar age, (2) the length scale of surface magnetic structures should be at least 20 per cent of the radius of the star, (3) the higher stellar rotation, the stronger the stellar magnetic field. With respect to the large-scale and slowly evolving features, the characteristics of failed-fossil magnetic fields are opposite to those of dynamo-generated magnetic fields predicted by Cantiello2019, but for both scenarios the rotation should play an important role. One could test both scenarios by performing an ensemble analysis on a homogeneous sample of stars that show magnetic activity signs.For this study, we use the, so-called, hump & spike stars, which suggested could show signs of magnetic activity in the form of spots. The common characteristic of these stars is a feature in the Fourier spectrum (see Fig. REF for some examples), interpreted to be unresolved Rossby modes (the hump) mechanically excited by deviated flows caused by stellar spots (the spike) at intermediate to high latitudes . Rossby modes were first reported in intermediate-mass stars by , as they studied the core rotation of a sample of \\gamma Dor stars. They have been reported in other works such as: , .\nAlternative explanation for rotational"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on how morphology and orthography complexities affect Arabic sentiment analysis, and what methods were used to achieve these results?",
      "expected_output": "The research found that morphology and orthography complexities in Arabic affect sentiment analysis, particularly with non-standard Arabic text like tweets. To address this, the study used character-level, word-level, and Ch5gram-level approaches to increase feature extraction. The results showed that word-level and Ch5gram-level methods achieved better sentiment classification results, with accuracy reaching 0.9424 for the Main-AHS dataset and 0.9568 for the Sub-AHS dataset. The study suggests using pre-trained word representation models like word2vec, GloVe, and Fasttext for future improvements.",
      "context": [
        " to good sentiment classification results even when the dataset size is small. El-Beltagy et al. enhanced the performance of a sentiment classification using a particular set of features.\n3. Datasets: There is a lack of Arabic sentiment datasets in comparison to English. In this paper, four datasets (where one is a subset of another) will be used in the experiments. Each used only two sentiment classes, i.e., Positive and Negative sentiment.\nArabic Health Services Dataset (Main-AHS and Sub-AHS): This is our own Arabic sentiment analysis dataset collected from Twitter. It was first presented in and it has two classes (positive and negative). The dataset contains 2026 tweets and it is an unbalanced dataset that has 1398 negative tweets and 628 positive tweets. We call this dataset Main-AHS, and we selected a subset of this dataset, called Sub-AHS, which was introduced in . The Sub-AHS dataset contains 1732 tweets, with 502 positive tweets and 1230 negative tweets.\nTwitter Data Set (Ar-Twitter): The authors of have manually built a labeled sentiment analysis dataset from Twitter using a crawler. The dataset contains 2000 tweets with two classes (Positive and Negative) and",
        " sentiment analysis dataset from Twitter using a crawler. The dataset contains 2000 tweets with two classes (Positive and Negative) and each class contains 1000 tweets. The dataset covered several topics in Arabic such as politics, communities and arts. There are some tweets in the available online dataset are missing and, hence, the used size of the dataset in our experiments is 975 negative tweets and 1000 positive tweets.\nArabic Sentiment Tweets Dataset (ASTD): The authors of presented a sentiment analysis dataset from Twitter that contains over 54,000 Arabic tweets. It has four classes (objective, subjective positive, subjective negative, and subjective mixed). However, in this paper only two classes (positive and negative) will be used and the numbers of negative and positive tweets are 1684 and 795 respectively, giving a total of 2479 tweets.\n3. CNN-LSTM Arabic Sentiment Analysis Model: The fundamental architecture of the proposed model is shown in Figure REF and it outlines the combination of the two neural networks: CNN and LSTM. There are no accurate tools for preprocessing Arabic text, especially non Standard Arabic text like most of the tweets. There are many forms for a single word in Arabic, for example Arabic words are different based on",
        " analysis because of the complexities of morphology and orthography in Arabic. We used character level to increase the number of features for each tweet, as we are dealing with short messages, which was not an ideal option for our model. However, using Word-level and Ch5gram-level have shown better sentiment classification results.This approach has improved the sentiment classification accuracy for our Arabic Health Services (AHS) dataset to reach 0.9424 for the Main-AHS dataset, and 0.9568 for the Sub-AHS dateset, compared to our previous results in which were 0.92 for the Main-AHS dataset and 0.95 for the Sub-AHS dateset. Future work will use some pre-trained word representation models, such as word2vec , GloVe , and Fasttext for the embedding layer."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the variables T_t and T_r relate to each other through the parameter lambda in the context of this research paper?",
      "expected_output": "The variables T_t and T_r relate to each other through the parameter lambda by the equation lambda = m / (2 k_B T), where lambda_t and lambda_r are associated with the translational temperature T_t and rotational temperature T_r, respectively. This indicates that lambda_t and lambda_r are inversely proportional to T_t and T_r, respectively.",
      "context": [
        " }} \\int _{0}^{\\infty } {(\\cdot ) {\\rm d}{\\varepsilon _v}},and K_r and K_v(\\lambda ) denote the number of rotational and vibrational degrees of freedom, respectively. \\lambda _t, \\lambda _r, \\lambda _v, \\lambda _{tr}, \\lambda _M are associated with the translational temperature T_t, rotational temperature T_r, vibrational temperature T_v, the translation-rotation average temperature T_{tr} and the fully relaxed temperature T_M, respectively by \\lambda = m / (2 k_B T), where m is molecular mass, k_B is the Boltzmann constant. It should be noted that the number of vibrational degrees of freedom K_v(\\lambda ) is determined by the vibrational temperature in each equilibrium state, i.e.,K_{v}(\\lambda )=\\frac{4\\Theta _{v}k_B\\lambda /m}{e^{2\\Theta _{v}k_B\\lambda /m}-1},where \\Theta _v is the characteristic temperature of vibration for diatomic gases,",
        "r} {{\\xi }}^{2}} \\frac{4 \\lambda _{v}}{K_v(\\lambda _v)} e^{-\\frac{4 \\lambda _{v}}{K_v(\\lambda _v)} \\varepsilon _{v}},where {c} = {u} - {U} denotes the peculiar velocity, and {{c}}^2 = (u - U)^2 + (v-V)^2 + (w-W)^2 and {{\\xi }}^2 = \\xi _{1}^2 + \\xi _{2}^2. The intermediate equilibrium state g_{tr} has the same temperature \\lambda _{tr} of molecular translation and rotation, but a different temperature \\lambda _v for vibration, which indicates complete energy exchange between translational and rotational degrees of freedom, and a frozen process of vibrational energyg_{tr}=\\rho \\left(\\frac{\\lambda _{tr}}{\\pi }\\right)^{\\frac{3}{2}} e^{-\\lambda _{t r} {{c}}^2} \\left(\\frac{\\lambda _{t r}}{\\pi }\\right)",
        " \\cdot {c} \\lambda _1 \\lambda _2(1 - \\sigma )}{K_v\\left(\\lambda _2\\right) \\rho }\\left(\\frac{4 \\lambda _2 \\varepsilon _v}{K_v\\left(\\lambda _2\\right)} - 1\\right),where \\sigma depends on the inter-molecular potential, and the translational heat flux {q}_t, rotational heat flux {q}_r, and vibrational heat flux {q}_v are{q}_t&=& \\frac{1}{2} \\int {c}^2 f {\\rm d}{{\\Xi }}, \\\\ {q}_r&=& \\frac{1}{2} \\int {c} {\\xi }^2 f {\\rm d}{{\\Xi }}, \\\\ {q}_v&=& \\int {c} \\varepsilon _v f {\\rm d}{{\\Xi }}.For a uniform flow, the thermal flux relaxation rate corresponding to the model equation Eq. (REF ) is given by\\frac{\\partial {q}_t}{\\partial t}&="
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the research paper use asteroseismology to rule out companions, and what methods are employed to achieve this?",
      "expected_output": "The research paper uses asteroseismology to rule out companions by analyzing the frequency-power spectrum of p-mode oscillations. According to the paper, if a bound companion with the same density as Kepler-65 existed, it would have produced a second set of oscillations in the frequency-power spectrum, which was not detected. The paper employs the asteroseismic detection prediction code from Chaplin et al. (2011b) to determine that a bound companion with a density up to 6 times that of Kepler-65 would still have shown detectable oscillations, thus ruling out such companions. This method allows the researchers to constrain the possibility of a companion orbiting Kepler-65.",
      "context": [
        " would have been detectable in the optical spectrum, the spectrum of p-mode oscillationsThe asteroseismic analysis allows us to rule out the presence of a bound companion having the same density as Kepler-65, since we would have detected a second set of oscillations in the frequency-power spectrum, overlapping in frequency with the oscillations of Kepler-65. In fact, given the observed background noise level, and using the asteroseismic detection prediction code in Chaplin et al. (2011b), we can rule out a bound companion having a density up to {{formula:6be10bde-9a00-46bb-b25b-0001bb1f1f5f}} -times that of Kepler-65 (since it would still have shown detectable oscillations). (i.e., by contributing signatures of its own oscillations), or through excessively diluted transit depths, and then compute the integrated probability of the allowed phase space. This is beyond the scope of this study.\nDetection of a TTV signal for Kepler-65d: The detection of transit timing variations have proven to be useful for validating planets as well as constraining the masses of the transiting planets. We performed a transit-timing analysis of",
        " et al. 2012) for Kepler-65d of approximately 10 {{formula:dd006d63-3447-43ed-9af9-e5a12f6b22ba}} .We conclude that Kepler-65 is indeed transited by a system of three planets, based on the low false-alarm probability for each individual transit, the unlikely coincidence that would be required for a spurious system to produce the observed trend of transit durations versus orbital periods, the agreement between the photometric and asteroseismic estimates of the mean stellar density, and the detection of a physically reasonable TTV signal for at least one of the planets. However, as in many other cases, we acknowledge the fact that we cannot completely rule out the unlikely companion scenario, in which one or more of the planets orbits a fainter, bound companion having a higher density than Kepler-65.cccccccc TTV signals of Kepler-65 0ptPlanet {{formula:c0d3a823-2146-4c7e-82b2-b7d2a26d3fc3}} Error scale Period Amplitude {{formula:5a079934-eafb-4e32-802d",
        " other two planets. That planet d is singled out in this test is consistent with the hypothesis that all three planets orbit Kepler-65, as this planet has the longest orbital period and is closest to the largest and presumably most massive planet (Kepler-65c; see Section REF and Table REF ), factors which enhance the amplitude of the TTV signal. Its orbital period is 1.39 times that of Kepler-65c, making it near a 7:5 ratio. This has been observed in many other multiplanet systems (Fabrycky et al. 2012).Even though the TTV signal of Kepler-65c did not satisfy the Bayesian information criterion for detection, the measured TTV period of the best-fitting sinusoid is close to the value expected from the formula of Agol et al. (2005), which for the c and d pair is 50 days (in agreement with the detected period). With more data one might be able to establish this signal more securely. Using the 0.9-min amplitude of this hypothetical signal as a reference, one would estimate a mass (or upper bound; see Lithwick et al. 2012) for Kepler-65d of approximately 10 {{formula:dd006d63-3447-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does background understanding impact parameter inference in the context of this research paper, and what are the key findings related to this topic?",
      "expected_output": "Understanding the instrumental background is key to any parameter inference in gamma-ray astrophysics, as it allows for accurate modelling and subtraction of background radiation from the telescope and satellite. The research paper highlights the importance of robust and reliable background modelling with SPI, which is achieved by combining contributions from the instrumental continuum and instrumental lines. The absolute levels of these contributions are determined through a maximum likelihood fit, enabling accurate parameter inference. For example, the paper models the background as a combination of instrumental continuum and lines, with their absolute levels determined through a fit, allowing for precise estimation of parameters such as the normalisation and scale radius of the Galactic centre.",
      "context": [
        "-c427-4a6b-be9f-454c65e1b1c0}}In Eq. (REF ), the absolute flux normalisation, {{formula:58eae602-af09-4c21-a609-bf72a53782f8}} , is determined through a maximum likelihood fit, and {{formula:53bdee36-0771-49c9-96e3-0710d570be13}} is a free parameter (see below). Understanding the instrumental background in {{formula:dba5c208-3c22-4454-abc3-658a08034abf}} -ray astrophysics is key to any parameter inference. Details about robust and reliable background modelling with SPI can be found in and . Here, the background is modelled as a combination of contributions from the instrumental continuum, {{formula:0503f8f0-1253-4f91-94c2-5f06251aff57}} , as well as instrumental lines, {{formula:8185eb38-0bd2-47ec-8502-f4e62c808624}} , and their absolute levels are determined through a",
        " in 3D space, with {{formula:a7e63562-03f2-49cd-b738-d771c1460bbf}} resembling the Galactic centre, in units of {{formula:13d1dd95-c3b6-4905-9aa3-01ad50563e4f}} , {{formula:1dc8a092-96f4-43de-8073-dd03804d6bcb}} is the normalisation, inherent to the total galactic 1.8 MeV luminosity and the received or measured flux at the point of the observer, {{formula:23f012fe-5f37-425c-b795-4ac278181cd8}} and {{formula:37c69a64-b3bf-42d5-98e2-c8df888cec48}} are the exponential scale radius and scale height, respectively, and {{formula:57550a4a-a78d-482c-9461-083d77eb0b14}} is the Galactocentric radius.In order to obtain an image from the the distribution in Eq. (REF ), we",
        ". REF , the basics about the spectrometer on board the International Gamma-Ray Astrophysics Laboratory (INTEGRAL/SPI), the chosen data set, and model fitting in its raw photon-count data space are explained.\nInstrument, data, and fit: The spectrometer SPI on board ESA's INTEGRAL satellite measures photons in the energy range between 20 and 8000 keV with a 19-element high-purity Ge detector camera. It uses a coded-mask technique to distinguish between emission from the sky and typically high instrumental background radiation from the telescope and satellite themselves. SPI has a field of view of {{formula:f026a75c-8331-49b1-aec9-c6c093ff4720}} and an angular resolution of {{formula:7a21db4d-344f-47fd-9f61-9a7045128991}} . The data space is thus a size-19 vector of photon counts at different energies for individual pointed observations (`pointings') of typically 30 min duration. Because we study one of the strongest {{formula:3a4eb78d-8163-4e75-9b5b-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role did the Academy play in driving the goals of meridian measurement, and how did their objectives influence the research methods and outcomes?",
      "expected_output": "The Academy played a crucial role in driving the goals of meridian measurement by outlining a plan to re-measure the Lacaille-Cassini arc of the Paris meridian from Dunkerque to Perpignan, and extending the measurements down to Barcelona. Their objective was to improve the knowledge of the meridian length, which led to the adoption of the provisional meter by decree on August 1st, 1793. The Academy's objectives influenced the research methods by necessitating several campaigns at different latitudes to determine the meridian, and their outcomes resulted in the establishment of the meter as a standard unit of length, despite being perceived as a French imposition by other countries, such as Great Britain and the USA.",
      "context": [
        "omogeneity) the circularity of the equator comes from symmetry arguments, determining the meridian requires necessarily, as it had already been done in the middle of the 18th century, several campaigns at different latitudes.Frankly, we find that naïveness acquires an unintentional humorous vein at page 8 of the document , when, after having claimed a few pages earlier that the choice of the meridian is `democratic', the Paris meridian going from Dunkerque to Barcelona is presented as almost unique to perform the proposed measurement:That could be the reason why the world best selling encyclopedia erroneously reports that the meter “was originally defined as one ten-millionth of the distance from the equator to the North Pole on a line running through Paris.” This could be just a minor flaw due to superficiality, but it could also be a heritage of the anglo-saxon reaction to what was perceived as a French imposition.One cannot find neither in Europe nor in any other part of the world, unless to measure a much wider angle, a portion of meridian that satisfies at the same time the condition to have the extreme points at sea level, and that of crossing the forty-fifth parallel, if one does not take the line",
        "es 44/100 present Paris measure [443.44 lignes], and this approximation is such that its error does not exceed one tenth ofligne [0.2 mm], that is sufficient for the ordinary use in the society. This unit will take the name meter. ( , p. 5)The so called provisional meter, with all the resulting units of volume, weights and surfaces , , was adopted by decree the 1st August 1793.\nThe 1792-1798 meridian mission: The Rapport sur le choix d'une unité de mesure outlined a plan to realize the new system of measures. One of the items of the plan was to re-measure the Lacaille-Cassini arc of the Paris meridian from Dunkerque to Perpignan,More precisely, the lowest latitude was about Collioure, a small town close to the Spanish border. also extending the measurements down to Barcelona (a total of almost ten degrees in longitude, for a length of 1075 km — see the map in Ref. to get an idea of “cette belle entreprise” ).The motivation of the Academy in support of the enterprise was to improve the knowledge of the meridian length",
        " have the extreme points at sea level, and that of crossing the forty-fifth parallel, if one does not take the line that we propose, or as well another more western meridian from the French coast, until the Spanish one. (Ref. , p. 8)(Their precise Swiss neighbors would have no chance to reproduce the meter standard in their country!) As a matter of fact, the choice of the meridian appeared to other countries, especially Great Britain and USA, as a imposition of the Paris meridian. All previous attempts of cooperation towards an international standard of length were frustrated, and still now we suffer of communication problems.As a side remark, we would like to point out that even the very interesting Alder's book The Measure of All Things , that has been very useful to us in this research, is a proof the dreams of the académiciens are still far from coming true. In fact most lengths are given in feet and inches, used by to American and British readers, but hard for the others, especially when, in translation, unavoidable mistakes happen, as the 25 feet of page 188, that becomes 672 meters at page 289 of the Italian edition. Nothing compared to the Mars Climate Orbiter disaster, but this is symptomatic of the troubles"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in common causes between classical and random field models, and how do these differences impact the research findings?",
      "expected_output": "The main difference in common causes between classical and random field models is that classical particle models have a single source as the common cause of correlated events, whereas random field models have distributed causes. This difference impacts research findings as it makes the assumption of common causes in random field models unreasonable, leading to the violation of Bell inequalities. For example, in a random field model, correlations evolve from earlier times, making it a straw man theory when constrained to be local. The violation of Bell inequalities can be taken to show that there are no simply localized particles, but it does not rule out a random field model.",
      "context": [
        " on a notion of common cause that is well-founded for a classical particle model, but is completely unmotivated for a random field. For a classical particle model, two particles come from a single source, which is the common cause of two correlated events. For a random field (and for a quantum field), the correlations of the field at the time of measurement evolve from correlations at earlier times. There is a distributed cause, not a common cause.Bell also constrains the dynamics to be local, which has generally been seen as the only way to satisfy Lorentz invariance. The kernel {{formula:b33f27ad-8427-423e-b384-fab2507f886e}} is therefore forbidden because it's nonlocal, even though it's a Lorentz invariant, exponentially decreasing nonlocality that a sensible classical random field theory ought to be comfortable enough with — or anyway as comfortable as we are with the quantum field that generates the same probability density over the phase space.Random field theory constrained this much is a straw man of a theory. The violation of Bell inequalities can reasonably (though not without any qualification) be taken to show that there are no simply localized particles that would justify an assumption that there are pervasive",
        " not without any qualification) be taken to show that there are no simply localized particles that would justify an assumption that there are pervasive common causes, but the violation does not rule out a random field model.An alternative discussion of the Bell inequalities asserts that Bell inequalities are about consistency in the sense of probability theory and measurement incompatibility, not about Einstein locality (for example, , , , , , , and see Karl Hess and Walter Philipp in this volume for a historical discussion of papers in the statistics literature going back to the 1950's). If we construct probability distributions {{formula:27cf475a-b346-4b43-bc69-26b47feb4887}} , {{formula:b94c871a-b956-4409-bd6c-e4aec4e74f46}} , {{formula:620cfbd5-fb94-49c7-8e2b-5fbde21c43c8}} , and {{formula:f6209726-ac88-45f3-8bbe-c822bcfd0f49}} , where {{formula:8ee8c256-96f1-43bd-9169-65",
        ", but then they just need more experimental effort to set up. Experiments that violate Bell inequalities are quite hard to set up — they take a lot of free energy — so it is somewhat disingenuous of Bell, and many others since, to say pejoratively that it requires a \"conspiracy\" for classical physics to obtain a violation of the Bell inequalities, insofar as this suggests that the experimenter had little to do with setting up favourable conditions for obtaining the experimental result.A quantum field state presented as a Wigner quasi-probability over phase space at the time of a measurement determines the Wigner quasi-probability over phase space at future and past times to precisely the same extent as a classical random field, so there is exactly as much \"conspiracy\" in a quantum field model as there is in a continuous random field model. We should not conclude that quantum field theory is unreasonable, but that a continuous random field theory is as reasonable.To derive Bell inequalities for a random field, Bell introduces a priori constraints on what the initial conditions in the past are allowed to have been. Bell's constraints are based on a notion of common cause that is well-founded for a classical particle model, but is completely unmotivated for a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What effects does low temperature have on the string sector during the heating process, and how do these effects impact the overall results?",
      "expected_output": "At low temperatures, the system stays stuck in the zero-string sector during the heating process because the single-spin-flip dynamics is extremely ineffective in changing the number of strings. This is due to the high energy cost of moves that take the system out of the current sector, making the probability of acceptance of such moves extremely small. As a result, the system cannot transition to the high-temperature phase until the temperature is sufficiently high, as seen in Fig. REF.",
      "context": [
        " even for this small system, equilibration is not obtained for temperatures lower than about 0.3. We also examined the states obtained by starting the system {{figure:7c0d8058-8f6c-4c1c-b433-251be4b87443}}{{figure:1994e48a-bfc8-48d2-9a5f-4924b924abad}}REF , the system reached the zero-string ground state. On heating, the system continues to be in the zero-string sector until at some temperature value it jumps to the high-temperature phase. On the other hand, a slow cooling from the high temperature phase leads to the lowest energy state in the {{formula:c23ac07d-9e12-4118-8cce-f25bebb08108}} sector and the true zero-string ground state is not reached.These results can be understood as follows. As discussed in the preceding section, the ground state lies in the zero-string sector, and the excitations within {{formula:1f9fa483-effb-4074-8c38-71b337215478}} from the ground state correspond to",
        "483-effb-4074-8c38-71b337215478}} from the ground state correspond to the formation of an even number of strings. The single-spin-flip dynamics is reasonably efficient in exploring the states within a sector with a fixed number of strings. However, at low temperatures, it is extremely ineffective in changing the number of strings. In fact, even with zero external field, the single-spin-flip dynamics at zero temperature is non-ergodic and only samples states within a given sector. At finite temperatures, the only way to change the number of strings is through moves which take the system out of {{formula:c42fe340-34e7-479b-a21e-7c4131697c28}} . These moves cost energy of order {{formula:efb54612-7c9e-499d-a992-e642a2945c75}} . At low temperatures, the probability of acceptance of such moves becomes extremely small. Thus in Fig. REF , during the heating run, the system starts from the ground state in the zero-string sector and stays stuck in it till the temperature is sufficiently high. At high temperatures, the {{formula",
        " the zero-string sector and stays stuck in it till the temperature is sufficiently high. At high temperatures, the {{formula:d43686d8-3098-4c28-875c-4d10518c868b}} sector is most probable (note that at very high temperatures, the string picture is no longer valid) and during the cooling run, the system starts from this sector and again stays stuck in this sector since the dynamics cannot reduce the number of strings. Thus the cooling curve basically shows equilibrium properties within the {{formula:d5b460e7-9285-464a-b5a4-e025a73404d7}} sector.We have verified the above picture by an exact numerical evaluation of the staggered magnetization for a {{formula:200b0d76-3175-4ce1-b1c0-55ebcbc06ebc}} system. This is done by numerically computing the two sums that occur in the expression {{formula:53af9c95-5639-474b-a537-d82605f0d20b}}where {{formula:de257675-1b64-4ac3-a828"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the HGRL method handle complex patterns in graph structures, and what are its key findings in addressing these challenges?",
      "expected_output": "The HGRL method handles complex patterns in graph structures by utilizing hyperbolic spaces, which have an exponential growth property, allowing for more efficient modeling of tree-like or scale-free graphs with hierarchical organizations. Key findings include the ability of hyperbolic shallow models and hyperbolic neural networks to effectively embed complex graph patterns, and the development of hyperbolic graph neural networks that can learn representations of graph data with complex structures, such as those with power-law distribution or hierarchical organizations.",
      "context": [
        "# arXiv Paper: 2211.04050\n\nHyperbolic Graph Representation Learning: A Tutorial\nabstract: Graph-structured data are widespread in real-world applications, such as social networks, recommender systems, knowledge graphs, chemical molecules etc. Despite the success of Euclidean space for graph-related learning tasks, its ability to model complex patterns is essentially constrained by its polynomially growing capacity. Recently, hyperbolic spaces have emerged as a promising alternative for processing graph data with tree-like structure or power-law distribution, owing to the exponential growth property. Different from Euclidean space, which expands polynomially, the hyperbolic space grows exponentially which makes it gains natural advantages in abstracting tree-like or scale-free graphs with hierarchical organizations. In this tutorial, we aim to give an introduction to this emerging field of graph representation learning with the express purpose of being accessible to all audiences. We first give a brief introduction to graph representation learning as well as some preliminary Riemannian and hyperbolic geometry. We then comprehensively revisit the hyperbolic embedding techniques, including hyperbolic shallow models and hyperbolic neural networks. In addition, we introduce the technical details of the current",
        ", including hyperbolic shallow models and hyperbolic neural networks. In addition, we introduce the technical details of the current hyperbolic graph neural networks by unifying them into a general framework and summarizing the variants of each component. Moreover, we further introduce a series of related applications in a variety of fields. In the last part, we discuss several advanced topics about hyperbolic geometry for graph representation learning, which potentially serve as guidelines for further flourishing the non-Euclidean graph learning community.\nIntroduction: Graph-structured data are ubiquitous in the real-world applications, ranging from social networks, recommender systems, knowledge graphs to chemical molecules. Despite the effectiveness of Euclidean space for graph-related learning tasks, its ability to encode complex patterns is intrinsically limited by its polynomially expanding capacity. Although nonlinear techniques assist to mitigate this issue, complex graph patterns may still need an embedding dimensionality that is computationally intractable. As revealed by recent research that many complex data shows non-Euclidean underlying anatomy, for example, the datasets with tree-like structure (e.g., hierarchies, power-law distribution) extensively exists in many real-world networks, such as the hypernym structure in",
        " spaces and manifolds. If not, we also provide intuitive descriptions for all of them during the tutorial. The expected audience size is 200. No special technical equipment is needed and some way of taking notes is suggested.\nOutline: The tutorial is mainly organized and refers to the recently released survey papers and but with more emphasis on the challenges by presenting the potential or newly developed solutions to address them. The outline is sketched as following: Introduction (Min, 30 min) , , An overview of graph representation learning Brief introduction of Riemannian geometry The motivation hyperbolic graph representation learningHyperbolic graph representation learning (HGRL) (Menglin, 30 min) , Hyperbolic shallow models Hyperbolic neural networks Hyperbolic MLR Hyperbolic RNNHyperbolic Graph Neural Networks Hyperbolic feature transformation Hyperbolic neighborhood aggregation Hyperbolic non-linear activation Applications (Menglin, 45 min) HGRL for recommender systems , , , , HGRL for knowledge graph , HGRL for other applications , Advanced Topics (Min, 60 min) Complex structures , , , , Evolving interactions , Geometry-aware Learning Trustworthy and scalability , , , Video records and the slides are now accessed via the tutorial homepage"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the Legendre transformation play in removing action divergence, and how does it impact the overall results of the research?",
      "expected_output": "The Legendre transformation plays a crucial role in removing the divergence from the action. According to the research paper, it is used to reexpress the action and remove the divergence, resulting in a finite value. Specifically, the prescription for removing the divergence is to perform a Legendre transformation, as shown in the formula. This transformation leads to a simplified expression for the action, which upon regularization, yields zero. The use of the Legendre transformation is essential in obtaining a finite and meaningful result, and its impact on the overall results of the research is significant.",
      "context": [
        " chain of substitutions. {{figure:66eca42e-757d-469c-8479-578f6ec6e7d2}}We have plotted {{formula:eac7e1ce-e4cb-4fbc-aaef-8c57298da762}} in figure REF .It remains to compute the action of the solutions. Using (REF ) we can express the action as twice the “prime” terms, i.e. those involving derivatives by {{formula:c9580d05-abea-416a-b223-ea42ff5b7253}} . We express everything in terms of {{formula:cb33bd42-d210-4822-a7a5-6f603c4c543b}} and {{formula:596785b1-3005-4378-b576-1c7564a464db}} , the latter we use to reexpress the integration over {{formula:88ea8dc5-000d-40ce-b1df-555954da0bc8}} by integration over {{formula:0e308eb5-f7c7-49",
        "54da0bc8}} by integration over {{formula:0e308eb5-f7c7-49d2-b1ee-dc28d84b89dd}} . {{formula:c3440f59-ff0a-45b2-a762-4ed53bae5f71}}The integral is simple to evaluate. The result is {{formula:bbc2e6d2-1886-452a-be58-abf47440011c}}The prescription for removing the divergence from the action is to perform a Legendre transformation , as follows {{formula:b41a75fb-976a-40c3-9a26-a35fa2cf91d9}}where we are using the coordinates defined in (REF ). We then find, using (REF ) and (REF ), {{formula:03d4797b-4040-4266-b586-ffaea4cd39ea}}and so {{formula:b1f29b74-78a7-4a6d-81b8-b2b56a89b040}}We therefore have that {{formula:738b57",
        "6ac646cf2}} . In order to simplify the presentation we give the explicit solution in terms of {{formula:8b10d216-e420-469d-a117-530199e46fad}} here, and then continue with the cases {{formula:f433694a-2364-4342-b391-277e2d4153c8}} below. The solution for {{formula:e87b3202-87a5-4899-ac62-c25148de720b}} is {{formula:d8b0abb1-6cb7-4076-9db3-45db94c1336e}}and leads to the action {{formula:971e25d7-b5f7-40ea-9787-1654b1d2d30d}} , which upon regularization as in (REF ) below, yields zero.Continuing with cases {{formula:d31578ba-f479-4b1f-b458-15559668ee38}} , the solution (REF ) wraps one half of an {{formula:76d0e0df-1d02-4f"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between macroscale electricity and microscopic charge density, and how do the methods used impact the key results?",
      "expected_output": "The research paper establishes a relationship between macroscale electricity and microscopic charge density, showing that the relationships between macroscopic fields are identical in form to the relationships between their microscopic counterparts. The methods used, based on a general theory of structure homogenization, allow for the derivation of expressions for polarization current without invoking quantum mechanics. Key findings include that the bulk-average electric potential vanishes in a macroscopically-uniform charge-neutral material and that symmetry is scale-dependent, meaning anisotropy of the microstructure does not necessarily imply the existence of macroscopic vector fields. The paper also shows that all relationships between observable macroscopic fields can be expressed mathematically without introducing polarization and electric displacement fields, which are not observable.",
      "context": [
        "# arXiv Paper: 2403.13981\n\nElectricity at the macroscale and its microscopic origins\nabstract: I define the fields that describe electrical macrostructure, and their rates of change, in terms of the microscopic charge density, electric field, electric potential, and their rates of change. To deduce these definitions, I lay some foundations of a general theory of structure homogenization, meaning a theory of how any observable macroscopic field is related to spatial averages of its microscopic counterpart. I apply homogenization theory to the fields that appear in the vacuum theory of electromagnetism and deduce that the relationships between macroscopic fields are identical in form to the relationships between their microscopic counterparts. Without invoking quantum mechanics, I derive the expressions for polarization current established by the Modern Theory of Polarization. I prove that the bulk-average electric potential, or mean inner potential, vanishes in a macroscopically-uniform charge-neutral material, and I show that when a crystal lattice lacks inversion symmetry, it does not imply the existence of macroscopic $\\vec{E}$ or $\\vec{P}$ fields in the crystal's bulk. I point out that symmetry is scale-dependent. Therefore",
        " or $\\vec{P}$ fields in the crystal's bulk. I point out that symmetry is scale-dependent. Therefore, if anisotropy of the microstructure does not manifest as anisotropy of the macrostructure, it cannot be the origin of a macroscopic vector field. The macroscopic charge density is isotropic in the bulks of most materials, because it vanishes at every point. Therefore, regardless of the microstructure, a macroscopic electric field cannot emanate from the bulk. I find that all relationships between observable macroscopic fields can be expressed mathematically without introducing the polarization ($\\vec{P}$) and electric displacement ($\\vec{D}$) fields, neither of which is observable. ~\n: arrows >=latexexternal60150[scale=2.5,tdplotmaincoords](O) at (0,0,0);1 2 2 [thin,tdplotrotatedcoords,-, densely dotted, gray] (0,0,0) – (0.8,0,0) node[anchor=west]; [thin,tdplotrotatedcoords,,-, gray] (0,0,0) – (",
        "8,0,0) node[anchor=west]; [thin,tdplotrotatedcoords,densely dotted,-, gray] (0,0,0) – (0,0.8,0) node[anchor=west]; [thin,tdplotrotatedcoords,-, , gray] (0,0,0) – (0,0,0.8) node[anchor=south]; [ultra thick, black, dotted ,->] (0,0,0) – (1,0,0) node[anchor= east]|\\varphi _3\\rangle ; [ultra thick, black, ,->] (0,0,0) – (0,1,0) node[anchor= north west]|\\varphi _2\\rangle ; [ultra thick, black, ->] (0,0,0) – (0,0,1) node[anchor=south]|\\varphi _1\\rangle ; 7 14 14 [ultra thick,tdplotrotatedcoords,->, dotted, blue] (0,0,0) – (1,0,0)"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does functor exactness play in the vanishing cycle decomposition, and how do the main findings of this research relate to its methods and key results?",
      "expected_output": "The functor exactness plays a crucial role in the vanishing cycle decomposition, as it restricts to an exact functor, allowing for the preservation of exact sequences. The research findings relate to the methods by utilizing the exactness of the functor to prove the existence of the isomorphism and to deduce the general case from the specific case. The key results, such as the Thom-Sebastiani isomorphism and the vanishing cycle decomposition theorem, rely on the exactness of the functor to establish the relationships between the different constructs.",
      "context": [
        "19cd1374-7c28-4528-a97a-76256a6d8c5c}} -dimensional fibres. Then the natural map {{formula:7630a0aa-1f9e-4071-8f9f-ea493f241a3b}} is an isomorphism, as is the natural map {{formula:1b0277ec-49d2-4e5c-a90b-c4b15baa37b6}} . (Exactness): The functor {{formula:8ec438d2-6bc9-4cb7-8c31-5e1defaca27a}} is exact, i.e. it restricts to an exact functor {{formula:c1ecd869-d044-4b52-9a45-7ccb35723ff9}} . (Vanishing cycle decomposition theorem) Let {{formula:b9965700-083d-475a-8a4c-b9449517d20b}} be pure, let {{formula:ad0e0ed7-9bb6-48df-928d",
        " sheaf on {{formula:e495c90e-71e4-4a2d-b0f5-507026d7239d}} via the direct image. Then we define the functor on {{formula:7771e6ee-e97d-4004-921e-198a934a1e7f}} {{formula:a3a78adb-4d1c-4b6c-8a3d-ffc8c490363f}} and we may alternatively define {{formula:b466c312-e7c4-426e-b9c2-eaecaf7b2162}} . By the Thom–Sebastiani isomorphism is given by the natural map {{formula:8b7d1f79-a3c4-4565-b381-ad96a7b65aaf}} induced by the inclusion {{formula:9cf8bbdf-ff0c-40ee-be50-17c49901a569}} and the {{formula:e31c2074-c68c-45ad-94e8-0f8be",
        "3-8701-a8a5c0bc5535}}and {{formula:db1ac2c6-0658-4051-b220-5432627588a9}}As in the proof of Theorem REF we only need to prove the case for which {{formula:e3da010f-cff7-447b-a69b-fcbcf00a4fde}} and then we can deduce the general case from the fact that monodromic vanishing cycle functors are exact, and commute with proper maps and the relevant symmetric monoidal structures. We first prove that the isomorphism (REF ) exists. Fix a {{formula:baf2dcbe-4be5-40d0-9ecf-4183b5126336}} . Then if we complete {{formula:e45ba0c4-5730-4dc6-9211-1495ee158603}} to a total ordering {{formula:f70c430d-91bc-4995-a1af-e022e8263718}} of {{formula:1a941619-153e-46ea-950"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on evolving EoS models and their impact on cosmography?",
      "expected_output": "The research on evolving EoS models finds that using a model-independent parametrization of the dark energy EoS allows for the estimation of cosmographic parameters and their derivatives. For example, the present-day values of these quantities can be written as a function of the EoS parameter, enabling numerical expressions for cosmography in quiessence scenarios. The study also shows that the higher-order cosmographic parameters are poorly restricted by current cosmological data, but the equations can be used to place restrictions on the form of the EoS function. Additionally, the cosmographic approach can help discriminate between different models of dark energy.",
      "context": [
        "adf-dce0b957c079}} , we can obtain numerical expressions for {{formula:f4bc4a8a-0ccd-483d-8423-0294cdd221ad}} , {{formula:1e919bf8-eb5c-4d78-9bca-422154626306}} and {{formula:565845c2-1201-45da-8077-fa4f55fd5fd9}} for this case of dark energy model with EoS in evolution.This procedure to estimate {{formula:f19c0756-1229-468c-8c0a-13010fc27ebd}} and its derivatives resorting to a model independent parametrization of the dark energy EoS allows us to write the present-day values of these quantities as a function of {{formula:811911eb-5a67-44fd-b89a-4d65ee10e04c}} , so that, using values for these parameters, numerical expressions could be obtained for {{formula:64adf345-b23e-40a6-ac83-9ff5",
        "f1842d0}} , we can obtain numerical expressions for the {{formula:b5470c81-8c35-4b02-9098-339986384503}} cosmography in this scenario of quiessence.\nEvolving dark energy models: Finally, we will consider here a general model of dark energy with evolving EoS, that is, {{formula:ec47d06e-a4bc-4665-84e2-61825438681a}} . Furthermore, in agreement with some of the most recent analyses , we only concentrate on evolving dark energy models with {{formula:cf0c68f4-5e75-4abd-8175-fc030dbee412}} . The cosmographic parameters for such models are {{formula:23e37fb9-79ee-4361-950e-dfd4f73cb891}}Inserting the values of {{formula:6df13253-3391-4f85-bf78-4b13542600b5}} given in (70) into (REF )-(REF ), we have the {{formula:1c10d2c4-",
        "da88-4c98-a58e-83c35f2ac2f6}} . This is important in view of the fact that the higher order cosmographic parameters are poorly restricted by current cosmological data , , . We have also shown here how these equations can be used together with the observational values of {{formula:43f814eb-ee0e-43c2-b0c5-995ba77ffc00}} to place restrictions on the form of the {{formula:2da2a2ec-af4b-483e-9b21-54ac6cc2d84c}} function in a similar way to what was done for the {{formula:2225c04f-3acc-4c9f-a2ca-ea56d9c446e0}} theory , , . In addition, the Equations (REF )-(REF ) have been rewritten within various dark energy scenarios. This can be useful due to the fact that as aforementioned the cosmographic approach can help to discriminate between different models of dark energy , , , .Finally, it is worth mentioning that although cosmography is a robust tool to distinguish among competing cosmological models"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings and methods used in the research paper related to formula ef5696bb, and what are the key results that can be applied in practice?",
      "expected_output": "The research paper focuses on determining an appropriate set of constraints for formula ef5696bb. The main findings include the use of quadratic constraints in the traditional Plebanski formulation, which leads to two separate sectors of solutions. However, the paper also explores a linear redefinition to identify a general solution, but notes that additional conditions are needed. A key result is that imposing three additional constraints implies that formula 5416f22b is the same function for all variables, which can be absorbed by a pointwise rescaling to complete the identification. The rescaled field plays the role of an orthonormal tetrad, determining physical lengths and angles.",
      "context": [
        "97e}} by {{formula:6cd4a92d-d976-470a-8206-ba97f388ab9c}}(note that this is just a linear redefinition). We will focus on this case in the following and try to determine an appropriate set of constraints for {{formula:ef5696bb-78e1-4024-94fe-e49632203647}} .In the traditional (Plebanski) formulation , one uses quadratic constraints for {{formula:e46da30c-a941-4fc8-a56d-348f27394466}} , {{formula:834e69e4-770a-48c1-a6cf-eed34bb9ce39}}which have two separate sectors of solutionsThis is under the additional assumption that {{formula:0b2be498-76e5-46e0-a694-02b40b778dac}} ! {{formula:3e2a2554-cd74-4be5-ada8-21d5a0ba95f1}} configurations are not geometric at all, leading to additional potential difficulties in quantisation",
        "ula:dd96311c-673b-4253-bc40-9d1c3df1fb22}} are functions on spacetime. One could try a linear redefinition {{formula:2c990f72-b790-4420-82ac-4948977b744f}} to identify this general solution {{formula:5a464a9f-f19c-43c9-b426-c0fcc960d0fa}}with {{formula:36e8d44c-67f8-4bb1-ace9-1aef60a5ddce}} , but this is not possible in general; one needs additional conditions.In , we show that imposing the additional three constraints {{formula:a27e6d3a-11b1-4aba-a250-157209f311b3}}implies that {{formula:5416f22b-45a0-46ac-be79-6286a2332197}} is the same function {{formula:9c06c111-a709-40b1-b564-fc85bd634b0e}} for all {{formula",
        "a709-40b1-b564-fc85bd634b0e}} for all {{formula:7c393922-46d9-44f0-bf14-5d2fd98320a7}} , {{formula:6d05601d-0989-46f9-8127-3fda2db6a3d3}} , which can be absorbed by a pointwise rescaling {{formula:bee53c7d-14b4-4334-b7df-c7e3ab063ee7}} to complete the identification {{formula:b329d771-34cc-4ab2-954d-8d114301c2b0}} . Note that not the field {{formula:53fe1a22-0f76-4632-bc6d-2b52a7378d4d}} we have introduced to define the constraints (REF ), but the rescaled {{formula:30426fec-81e3-4332-8fb2-aa0801b05ac2}} will play the role of an orthonormal tetrad determining physical lengths and angles. In"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on auto-equivalence functors and how do they relate to the action of G on C?",
      "expected_output": "The research paper discusses the concept of auto-equivalence functors and their relation to the action of a finite group G on a category C. The main finding is that an action of G on C consists of an auto-equivalence functor for every element in G, and that this action is equivalent to a strict G-action on some equivalent category. For example, the paper states that every G-action on C is equivalent to a strict G-action on some equivalent category, which implies that the equivariant category is a useful tool for studying the action of G on C.",
      "context": [
        " example in Section REF respectively.\nGroup actions on categories: Let {{formula:80a1d7c3-8db9-44d9-9ba0-4567a50c5c49}} be a finite group and let {{formula:bb80351c-f222-4fda-83a8-4fc5dbc8a478}} be a category.\nDefinition: An action {{formula:424aaee2-c85f-442f-bf57-5b0a262884b1}} of {{formula:db232a0e-a98c-4d70-b97a-4deef7c9fab0}} on {{formula:1578a760-4212-4778-9b2d-90298d43abe7}} consists offor every {{formula:f8276a31-c8f0-4060-8ba7-c14a50ed9a38}} an auto-equivalence {{formula:0d599cb0-db95-4e6b-bdf0-107901848cb7}} , for every pair {{form",
        "031a8-0765-4d23-babd-12af054c87f1}} on {{formula:fca9d5a1-6cf8-4b1b-8b34-465d6e6d99ce}} is equivalent to a {{formula:7cbcf7cc-7504-4df7-b096-de156804c039}} -action {{formula:af59bd24-6bcc-4610-93c5-c924e9e12075}} on {{formula:52bb1156-17ee-4d30-abbd-cc9bccbacb2b}} if we have an equivalence in {{formula:36608e71-7087-48ce-857e-552943423411}} , i.e. {{formula:32b43cfc-c6ec-4f2a-a9bb-c86b5cc5804d}} . Because of this, by passing to an equivalent category one can (and we often will) assume that the action is strict.The equivariant category {{formula:",
        "09845d3c0475}} . In particular, this implies that {{formula:f1bbad47-f1e3-44a0-a7ad-9b90709bad79}} . By every {{formula:2fe3e00d-8ae8-4a6e-b2c7-0ffc6d03b044}} -action on {{formula:05e3fa1a-44a9-424f-8a1e-4ee825bc2da6}} is equivalent to a strict {{formula:a43ab12b-8ff0-41cf-a6d5-349eb4f135f0}} -action on some equivalent category {{formula:219032c5-1310-4a85-a5de-529e0db2a34d}} . Here we say that a {{formula:57230ab2-320a-43b5-9cd7-8322290c04c3}} -action {{formula:b8c031a8-0765-4d23-babd-12af054c87f1}} on"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in model parameters and how do they impact the research findings?",
      "expected_output": "The main difference in model parameters is that existing approaches result in a separate set of model parameters for each item, increasing the number of model parameters and storage space requirement, whereas the proposed approach uses a single shared model for all items, reducing the number of parameters and storage space needed. This is achieved through meta-training on the union of training datasets for all items and using a masking procedure to ensure training loss is backpropagated over valid score classes only.",
      "context": [
        "4f-514af6759fcf}} for an item is simply the cross entropy, i.e., negative log-likelihood loss : {{formula:c73b7bfb-d6bc-46c6-a853-724bd75b317a}}while the objective across all 20 items is given by: {{formula:113831e1-f527-447b-acbb-09ae75c11dd4}}In contrast, existing approaches result in a separate set of model parameter {{formula:facce74b-a686-4680-8dce-781dc93383e9}} for each item, which increases the number of model parameters and thus storage space requirement by {{formula:4e0866e3-8a81-42f0-a1ea-6f88bd5b5518}} .\nExperimental Evaluation: In this section, we detail our experiments on the training dataset provided by the NAEP AS challenge. We conduct both quantitative evaluations on scoring accuracy of our approach and qualitative evaluations on the error types and fairness aspects of our approach.\nDataset Details: Each item in the NAEP AS Challenge has responses",
        "1724ad064c1a}} as explicit options in the input to make the model aware of valid scores as seen in the answer template in Figure REF . Furthermore, we mask out the invalid score classes for each response before computing the softmax. This masking procedure ensures that training loss is backpropagated over valid score classes only, enabling us to meta-train a single shared model for all items.\nMeta-training.: We train our AS model on the union of training datasets for all items {{formula:d242cf20-b9ed-4d0a-a79f-5523d146e260}} . Let {{formula:561c2375-2050-4c0f-a2de-0bff2318b5c5}} represent the model parameters to be learned, i.e., BERT parameters which are fine-tuned after initialization from their pre-trained values and the classification layer parameters which are learned from scratch. The in-context learning objective {{formula:91e8dfb4-2171-487c-9a4f-514af6759fcf}} for an item is simply the cross entropy, i.e., negative log"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does ECDL use to stabilize spectroscopy light and what are the key findings related to its effectiveness in this application?",
      "expected_output": "The ECDL uses the Pound-Drever-Hall method to stabilize spectroscopy light. This method stabilizes the laser to a cylindrical cavity made of ultra-low expansion glass (ULE). The key findings related to its effectiveness in this application include achieving laser linewidths below a certain threshold and observing drifts of the stabilized laser frequency below a specified value, indicating high stability and accuracy in the spectroscopy measurements.",
      "context": [
        "8-4d07-4e69-9006-b01bbe649751}} transition. The spectroscopy light at {{formula:421ef3f8-3b66-4996-a321-4748b658ad3b}} is generated by an extended cavity diode laser (ECDL), which is stabilized by the Pound-Drever-Hall method to a cylindrical cavity made of ultra-low expansion glass (ULE). We use a fiber EOM and an offset sideband locking technique to shift the laser frequency relative to the cavity resonances, which are spaced by {{formula:d6d90510-1cb7-4cf1-93a5-4eb1e2a87f8f}} . The ULE cavity is temperature stabilized to the zero-crossing temperature of the coefficient of thermal expansion and we achieve laser linewidths below {{formula:3ae98c6d-74db-4f0d-84b1-b23fe85098ec}} , and observe drifts of the stabilized laser frequency below {{formula:ce2f635a-8203-4ac9-83af-e9ceefd",
        "832b637c-d624-4ca4-816c-2d2e610bc4c7}} to the resonance of {{formula:ee17063b-f2fe-4ed0-98a0-3f6aeab83104}} and conduct a {{formula:e6de89f3-101f-45a7-88b0-d674221c5e27}} measurement immediately before and after each measurement of one of the other isotopes. This way we can detect and account for drifts of the ULE cavity, which is the optical frequency reference in our setup. As result we obtain the following isotope shifts for {{formula:a6f5826f-53d8-4a2a-9f81-a401c73c580f}} in Eq. (REF ) and {{formula:578037a6-d276-4189-a200-4c5bbc3bf067}} in Eq. (REF ) relative to {{formula:e8a79f27-d39b-4598-aae8-4dfab38b8f0e",
        "a79f27-d39b-4598-aae8-4dfab38b8f0e}} : {{formula:dc67a50f-1f7b-4f03-b548-43f143260405}} {{formula:caa0c358-97b7-4d8a-94f9-ae3c6ed91f6e}}The error budget is summarized in Table REF and takes contributions from ULE cavity FSR uncertainties, ULE frequency drifts, RF measurement uncertainties and fit errors into account. By using a wavelength meter (High Finesse WSU-30), we determine the absolute frequency of the transition for {{formula:33391919-bfff-45ae-b0cd-67e4176a6ec6}} to be: {{formula:2f861d4c-b8bc-4e4c-bb26-34c80c92374c}}corresponding to a wavenumber of: {{formula:ea81e692-1914-4662-9c85-0cece8c710ba}}The accuracy of the rubidium calibrated"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding proton localization in neutron star matter, and what methods were used to achieve these results?",
      "expected_output": "The main findings of the research paper are that proton localization is likely to occur in the interior of neutron stars, and this phenomenon is plausible and worth further research. The methods used to achieve these results include the extended Thomas-Fermi approach and the Wigner-Seitz approximation of a spherically symmetric cell to minimize energy and find the exact solution for proton wave function and neutron background distribution. The analysis of three different nuclear models, AV14+UVII, SLy4, and UV14+TNI, suggests that proton localization can occur at lower thresholds than previously thought.",
      "context": [
        "c99a619d7ddb}} determined by the proton fraction which is fixed by the {{formula:6c626496-43c7-4cf8-baa5-40833eb1f233}} -equilibrium occurring in neutron star matter. By minimization of the energy in the finite-size Wigner-Seitz cell we found exact solution for proton wave function and neutron background. It turn out that proton localization still occurs for all the presented models. The localization threshold is slightly lower than in the previous work where the one-parameter method for energy minimization was used . In this work we have investigated, in a rigorous way, the earlier ideas of the proton localization and have shown the phenomenon is plausible and worth further research. Interesting issues involve: the crystallization of protons as has been shown earlier , the influence of temperature on the neutron threshold density for proton localization and consequences for neutron star cooling , .\nACKNOWLEDGMENTS: We are grateful to Marek Kutschera and Adam Szmagliński for helpful feedback at the early stage of this work and fruitful discussions.",
        "# arXiv Paper: 1602.07511\n\nExact solution of equations for proton localization in neutron star matter\nabstract: The rigorous treatment of proton localization phenomenon in asymmetric nuclear matter is presented. The solution of proton wave function and neutron background distribution is found by the use of the extended Thomas-Fermi approach. The minimum of energy is obtained in the Wigner- Seitz approximation of spherically symmetric cell. The analysis of three different nuclear models suggests that the proton localization is likely to take place in the interior of neutron star.\nIntroduction: The interior of a neutron star contains the densest forms of matter in the Universe. The central density is as high as 5 to 10 times the nuclear equilibrium density {{formula:b8347a4a-2e3c-4665-ad04-95de4c2e7a46}} . Most of the mass of the star is placed in its liquid core covered by a thin crust ({{formula:ddb6cea5-57d5-471a-b15c-05c78e672ac4}} for typical neutron star) whose bottom edge is located at around 0.5{{formula:",
        "5f}} , and leading, at the same time, to not very high proton fraction at higher densities: AV14+UVII , SLy4 . The last one is very common in the description of the neutron stars as it well reproduces the properties of nuclei, the nucleon-nucleon scattering data, and it correctly recovers saturation point properties. Moreover, in our analysis we also included UV14+TNI model taken from . Although the maximum neutron star mass for this model is smaller than {{formula:d6e58951-ca42-4712-9dc7-294b165872a7}} , it was interesting to test it for very small proton fractions at high density because it could be relevant for the properties of proton localization. In the Fig.REF the proton fraction in {{formula:d7f30fbd-b38d-4fa9-b030-d8fe25c6b6ad}} -equilibrated matter is shown for the three selected nuclear models.The present paper is organized as follows. A short review of the the variational method for finite-size Wigner-Seitz cell is presented in Sec.II. In Sec"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of quantum and thermal fluctuations on two-level systems in this research paper?",
      "expected_output": "default",
      "context": [
        " part also contributes equally is averaged out here, as because, \\omega goes to \\omega _0 at the resonance, and electromagnetic field oscillates many times within a single Rabi cycle as \\omega _0\\gg \\omega _\\gamma . incident on the two-level system.\nEffect of quantum fluctuations and thermal fluctuations in free space: Let us now consider the system in thermal radiation field in free spaceHere free space is an idealization of a big black-body cavity. where all possible frequencies of the incident light are there with 2 independent arbitrary polarizations. While averaging over the polarization states results a factor 1/3 in the r.h.s. of Eqn.(REF ), contribution of the thermal radiation of all possible frequencies leads to an integration in the r.h.s. of Eqn.(REF ) over \\omega with the weight-factor u(\\omega ) which is to be followed from Planck's distribution formula (or Bose-Einstein statistics for photons) with adequate corrections for the vacuum energy density. Thus we recast Eqn.(REF ), asP_{2\\rightarrow 1}(t)=\\frac{|\\langle \\psi _1|\\",
        " a fixed direction of propagation and time averagedHere, time averaging is taken in the very short time scale of 1/\\omega _0. energy density u=\\frac{1}{2}\\epsilon _0E_0^2) incident on the two-level system we need not to take averaging over the directions of polarizations as done in Eqn.(REF ), and can recast Eqn.(REF ) from Eqn.(REF ), asR_{2\\rightarrow 1}(t)=-R_{1\\rightarrow 2}(t)=\\frac{\\mu _{12}^2u}{\\epsilon _0\\hbar ^2}\\frac{\\sin (\\Omega t)}{\\Omega }.Here, we are not considering the effect of vacuum fluctuations to restrict discussions only on monochromatic field, though it needs to be included for another project to explain experimental observations with injected laser light , . Now, if we define the B coefficient (B(t)=|R_{1\\rightarrow 2}(t)|/[u/\\Omega ]=|R_{2\\rightarrow 1}(t)|/[u/\\Omega ]) for a",
        "ga )) in u(\\omega ) incidentally represents the average contribution for the thermal photons at the temperature T and responsible for stimulated emissions, the second term (u_q(\\omega )) accounts for the correction due to the zero-point energy of the thermal radiation field.By the definition, \\Omega ranges form -\\sqrt{\\omega _0^2+\\omega _\\gamma ^2} to -\\omega _\\gamma and \\omega _\\gamma to \\infty following the avoided crossing as \\omega varies from 0 to \\omega _0 and \\omega _0 to \\infty . The integrand in Eqn.(REF ) is picked at the resonant frequency, \\omega =\\omega _0 (i.e. \\Omega =\\pm \\omega _\\gamma ), so that most of the integration comes from \\omega close to \\omega _0. Further considering \\omega _0 to be fairly large (i.e. \\omega _0\\gg \\omega _\\gamma which is compatible to the rotating-wave approximation), we recast Eqn.(REF ), asP_{2\\rightarrow"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research on right-handed neutrinos and how do they contribute to lepton asymmetry?",
      "expected_output": "The research paper finds that adding right-handed Majorana neutrinos to the Standard Model breaks the conservation of lepton number and allows for the generation of primordial lepton asymmetry through their out-of-equilibrium decay. This asymmetry can be enhanced if two heavy right-handed neutrinos are nearly degenerate in mass. The paper also explains that heavy right-handed Majorana neutrinos can account for the smallness of light neutrino masses via the see-saw mechanism. For example, the decay of heavy right-handed Majorana neutrinos can generate lepton asymmetry, which is a crucial step in understanding the matter-antimatter asymmetry in the universe.",
      "context": [
        "e6-afc5-42921c45d0c8}} is conserved and no asymmetry in {{formula:3c59734e-dc27-4cd4-b0b5-3d5759803476}} can be generated. However, adding right-handed Majorana neutrinos to the SM breaks {{formula:86aa3320-e5c4-4322-b87d-7ac5dc18a498}} and the primordial lepton asymmetry may be generated by the out-of-equilibrium decay of heavy right-handed Majorana neutrinos {{formula:08a10fef-301f-4908-a322-1d5fc84cc823}} (in the supersymmetric version, heavy scalar neutrino decays are also relevant for leptogenesis). This simple extension of the SM can be embedded into GUTs with gauge groups containing {{formula:01385146-5798-4197-ba74-747c0fbadd41}} . Heavy right-handed Majorana neutrinos can also explain the smallness of the light neutrino masses via the see-saw mechanism , , .",
        "on asymmetry for a wide range of the parameters . In the case of {{formula:09151e65-5828-4f93-94ac-c6bf7d02cad8}} , {{formula:679a2532-40a0-4f5c-977b-30d21149ccd0}} -violation may be provided by the complex Yukawa couplings between the right-handed and the left-handed neutrinos and the scalar Higgs.\n{{formula:10823703-376c-4d5a-9259-585be5831725}} -Violation in the CKM Matrix of the Standard Model.: Since only the left-handed electron is {{formula:6b576975-c6e7-4302-9bbe-e59a0f060a49}} gauge coupled, {{formula:2a6bb84a-7c9f-44c1-8c9f-d5d925c618d4}} is maximally broken in the SM. Moreover, {{formula:f43a004f-2bd6-4beb-8b36-fd",
        "-loop self-energy. In particular, it has been observed that {{formula:bb00f227-4d91-4265-b01b-e838a8ad035c}} -violation may be considerably enhanced if two heavy right-handed neutrinos are nearly degenerate in mass . It is also important to remember that large lepton number violation at intermediate temperatures may potentially dissipate away the baryon number in combination with the sphaleron transitions. Indeed, {{formula:61273981-ae0b-447b-b08b-708429e5b09d}} interactions of the form {{formula:5f654665-b115-41b2-8d00-5b04ac72c38e}}where {{formula:9be20f7a-faec-451c-a367-f5c43066eec7}} is the mass of the light left-handed neutrino, are generated through the exchange of heavy right-handed neutrinos. The rate of lepton number violation induced by this interaction is therefore {{formula:4c31b6d9-00f7-47b8-b"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about Bose-Einstein condensation and its relation to quantum phenomena?",
      "expected_output": "The research paper discusses the phenomenon of Bose-Einstein condensation and its relation to quantum phenomena, specifically the collapse and revival of coherent quantum states. The main findings include a phenomenological theory of mode collapse-revival in confined Bose gas, which takes into account fluctuations of the number of quanta of oscillations. The theory provides a general framework for understanding the effect and allows for direct comparison with experimental results. For example, the collapse time for a mode of frequency is given by a specific formula, which depends on the frequency and other parameters of the system.",
      "context": [
        "z, 03.75FiThe recent discovery of Bose-Einstein condensation of alkali atoms confined in magnetic traps has opened a new important field of investigation of quantum phenomena on a macroscopic scale. The most interesting possibility is to observe specific quantum phenomena which have no classical analogy. One such phenomenon is the collapse and revival of coherent quantum states recently observed for an atom in a electromagnetic field in a resonant cavity . The problem has a developed theoretical background, and for a comprehensive discussion see in Ref. .Several significant works have been carried out recently about manifestation of this phenomenon in Bose-condensed gases. In paper properties of a ground state are discussed. Kuklov et. al. considered dephasing of a finite amplitude oscillation in a finite Bose system because of atom occupation number fluctuations in the oscillation. They suggested that this mechanism can explain the observed damping of oscillations and used a microscopic model to calculate the collapse time and they concluded that the theory can give a reasonable order of magnitude of this time compared to the damping observed in experiments , . In the authors considered the dephasing because of the zero-point fluctuations of occupation numbers of atoms. Since the authors have withdrawn the paper we don't discuss it in detail. Note",
        "-point fluctuations of occupation numbers of atoms. Since the authors have withdrawn the paper we don't discuss it in detail. Note only that we believe that zero-point fluctuations cannot result in the effect under consideration.In this paper we will present a general phenomenological theory of collapse-revival of a highly excited mode of oscillation of a superfluid Bose system. We will take into account not fluctuations of occupation numbers of atoms but rather fluctuations in the numbers of quanta of oscillations of the mode. (This is by no means the same for low-frequency oscillations.) We believe that this choice of variables is more proper then used in . The physical meaning of the effect is that a coherent oscillation is a linear combination of stationary states of a system and the number of quanta is a natural specification of a stationary state of an oscillator. In this way one doesn't meet the troublesome problem of diagonalization of the Hamiltonian. Of course a problem can be solved in any variables and a microscopic theory always gives more information about some mechanism of the effect. From another side, the phenomenological approach is more transparent and gives direct possibility for comparison with an experiment.According to general theory the effect in consideration is based on the fact that transition frequencies for stationary component of",
        "\n: [preprint,aps,prd]revtexPhenomenological theory of mode collapse-revival in confined Bose gasL.P. Pitaevskii Department of Physics, Technion, 32000 Haifa, IsraelandKapitza Institute for Physical Problems, 117454 Moscow, RussiaA phenomenological theory of mode collapse-revival for a system with a weak nonlinearity is presented. The theory takes into account fluctuations of the number {{formula:73359b11-6e27-4339-9d11-3c23986c5fc0}} of quanta of oscillations. The collapse time {{formula:97dd1417-a70c-4a42-b2bd-10595e03b163}} for a mode of frequency {{formula:1c0f0d22-5018-49fb-84e4-27ee5e23eee0}} turns out to be {{formula:255d89db-5a53-4dae-bca9-e39a30602f2b}} , where {{formula:fac06085-9e11-4cdb-8bfe-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on the ferromagnetic QCD vacuum's effect on EM LEC bounds, and how were these findings obtained?",
      "expected_output": "The main findings of the research on the ferromagnetic QCD vacuum's effect on EM LEC bounds are that the obtained bounds are explicitly independent of the low-energy scale, providing a complete and model-independent prediction at low energies. These findings were obtained by taking into account the explicit symmetry breaking sources and using the charge separation convention, which results in LEC estimates that depend on the QCD renormalization scale and are gauge dependent. The researchers also found that their bounds are numerically compatible with theoretical estimates based on QCD, and that the LEC combinations appearing in their bounds are gauge independent. Additionally, the condition for the LEC was derived as a necessary condition for the QCD physical vacuum to be ferromagnetic, and the positivity condition on the quark condensate was used to probe the vacuum.",
      "context": [
        " the explicit symmetry breaking sources. The chiral limit can be taken at the end of the calculation. The bounds obtained are explicitly independent of the low-energy scale {{formula:1fe3cd1f-8627-4342-b93d-5672c558e06e}} , providing then a complete and model-independent prediction at low energies. However, when this low-energy representation is compared with theoretical estimates based on QCD, one has to take into account that due to the convention used in the charge separation, the estimated LEC depend on the QCD renormalization scale {{formula:3b227a3f-4eb4-42b1-b5f5-5705614a1246}} , as well as being gauge dependent. Our bounds are numerically compatible with those estimates, based on sum rules, resonance saturation and QCD-like models, within the stability range of {{formula:b250ac3e-faf1-4f26-8915-a559731fe625}} where those approaches are reliable. Furthermore, the LEC combinations appearing in our bounds are gauge independent. We believe that our results can be useful in view of the few estimates of the EM",
        "107265fed}} at which the LEC on the left-hand side are evaluated as long as the same scale is used on the right-hand side. Thus, it provides a well-defined low-energy prediction, expressed in terms of meson masses. The LEC on the left-hand side could be estimated by fitting low-energy processes or theoretically from the underlying theory, with all the related subtleties commented above.The condition (REF ) and the corresponding ones for {{formula:3560abd4-9263-41c2-bdea-f92a946f9969}} that will be derived in section REF are obtained as a necessary condition that the LEC should satisfy if the QCD physical vacuum is ferromagnetic. This positivity condition on the quark condensate probes the vacuum by taking the mass derivatives (REF ) through the external source method so that the quark masses have to be kept different from zero and in that way the explicit symmetry breaking corrections are revealed in the condensate. If one is interested in the chiral limit, it must be taken only after differentiation, i.e., directly in eq.(REF ). In that case, it is not justified to perform the additional",
        "79-b641-d73309062d18}} , the ratios (REF )-() are finite and independent of the scale {{formula:f2d7dde3-a1a6-43d5-868f-882cb699e151}} , of {{formula:fc3d7115-f1da-472f-a2b5-3f784feb9ffe}} and of the not-EM LEC, so they are free of contact ambiguities.As in section , we want to explore the consequences of the ferromagnetic nature of the physical QCD vacuum under explicit chiral symmetry breaking for the EM LEC. Here also the charge-mass crossed terms in the fourth order lagrangian (REF ) give explicit breaking contributions to the quark condensate coming now from the isoscalar, isovector and strangeness part of the charge matrix. For physical quark charges, demanding that the ratios (REF )-() are greater than one we obtain the following EM bounds, to leading order in the chiral expansion and in {{formula:983cf3b4-9704-49d4-9cc4-6e6c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the application of stochastic analysis and Bernoulli's inequality, and how do the methods used relate to the key results?",
      "expected_output": "The research paper applies stochastic analysis and Bernoulli's inequality to study complex Brownian motion. The main findings include the use of Gauss's divergence theorem and Taylor's theorem to derive elementary estimates. For example, Lemma 4.1 states Bernoulli's inequality for real exponents, which is used to prove the existence of a complex value. The methods used relate to the key results by providing a mathematical framework to analyze the behavior of complex Brownian motion.",
      "context": [
        "6}}Here we use techniques of stochastic analysis. Alternatively, one could obtain (REF ) using Gauss's divergence theorem. See for instance .Let {{formula:77dd2cff-4015-48d4-a3e3-debfd02be6e5}} be a complex Brownian motion started at the origin and {{formula:ce1f7f9e-4d3a-4643-98e1-e26353588250}} . We then have {{formula:96205277-4c66-416b-9020-618aff22ca7d}}Here we collect and prove the elementary estimates used in section . We begin with a special case of the classical Bernoulli inequality for real exponents.Lemma 4.1 (Bernoulli's inequality) Let {{formula:1fdf7d3e-9394-4099-a1ef-783d9c55a257}} and {{formula:9348e09a-2294-4997-b67d-676a98ea81a9}} . We have {{formula:51f78c90-8d52-436d-",
        "4c66-416b-9020-618aff22ca7d}}Here we collect and prove the elementary estimates used in section . We begin with a special case of the classical Bernoulli inequality for real exponents.Lemma 4.1 (Bernoulli's inequality) Let {{formula:1fdf7d3e-9394-4099-a1ef-783d9c55a257}} and {{formula:9348e09a-2294-4997-b67d-676a98ea81a9}} . We have {{formula:51f78c90-8d52-436d-96bd-58048a382292}}Taylor's theorem, applied to {{formula:5ae7acdc-2efe-4d3b-827c-0229158946f2}} , guarantees the existence of a {{formula:0e79532c-f94a-463d-be93-7b2cfb1ea9f3}} , such that {{formula:76472a43-5082-4aad-be71-7527f2c3e896}} Using",
        "-4c66-416b-9020-618aff22ca7d}}Here we collect and prove the elementary estimates used in section . We begin with a special case of the classical Bernoulli inequality for real exponents.Lemma 4.1 (Bernoulli's inequality) Let {{formula:1fdf7d3e-9394-4099-a1ef-783d9c55a257}} and {{formula:9348e09a-2294-4997-b67d-676a98ea81a9}} . We have {{formula:51f78c90-8d52-436d-96bd-58048a382292}}Taylor's theorem, applied to {{formula:5ae7acdc-2efe-4d3b-827c-0229158946f2}} , guarantees the existence of a {{formula:0e79532c-f94a-463d-be93-7b2cfb1ea9f3}} , such that {{formula:76472a43-5082-4aad-be71-7527f2c3e896}}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the researchers establish a relationship between temporal correlations and spatial correlations in their study, and what methods do they use to measure these correlations?",
      "expected_output": "The researchers establish a relationship between temporal correlations and spatial correlations by showing that the correlation of a particle at two different instants is equivalent to the spatial correlation of two subsystems at two different places. They use the Choi theorem to prove that the effect of temporal correlation is a quantum channel, and thus completely positive. Specifically, they utilize orthonormal bases and maximally entangled states to demonstrate this relationship, as seen in equations such as Eq. (7), which can be re-written to prove the sufficiency of the statement.",
      "context": [
        "4940215}} and {{formula:63b7cab8-6c06-472c-a55c-3d0e696ae5b9}} is stored in the spatial bipartite state {{formula:21ab119c-098c-44a4-b5c9-b31dac9fe739}} .As a consequence, the correlation of a particle W at two different instants is just the spatial correlation of two subsystems at two different places. That is, temporal correlations are equivalent to spatial correlations.On the other hand, it is known that a positive trace-preserving linear map is a quantum channel if and only if it is completely positive. In order to show that the effect of temporal correlation {{formula:49da2a01-5ba4-4aba-b3cb-cd8d741b20be}} is a quantum channel, we only need to prove that it is completely positive. With Choi theorem, we know that {{formula:2546fdb8-831f-4dd4-83c6-36a97d1f2534}} is completely positive if and only if the matrix {{formula:0",
        "799ba}} is a completely positive trace-preserving linear map. That is, {{formula:a0d71cd6-a215-4941-a248-a0ecd95305da}} is a quantum channel.Firstly, we show that temporal correlations are just spatial correlations.Let {{formula:e336cd8b-e9ad-4bcc-b27b-797533aa9e49}} and {{formula:e85b8e46-c0d7-4450-9eaf-67e446fafb3b}} be orthonormal bases of {{formula:a9e7400a-683d-4ebb-a371-7ca8a567ddb6}} and {{formula:e1ae8094-8245-43b0-946a-7d49394ca72d}} , respectively. Then, {{formula:549eea02-65a4-4135-bce9-f33bfd0fdd6a}} and {{formula:92063eb6-7844-44be-9bcd-e7511fb3a752}} are",
        " if and only if {{formula:6d1e6acd-3011-4334-be5c-14337842d085}} , the effect of temporal correlations, is a quantum unitary channel.To prove this statement, note that Eq. (7) can be re-written as {{formula:15d10943-f985-4d4d-afd6-1d40875dd5e6}}Thus, if {{formula:256c8e49-43ee-479d-863c-80f1af059951}} is a quantum unitary channel, it is obvious that {{formula:05775fbb-9c7d-45ee-b6bc-8343730d4ba5}} is a maximally entangled state and the sufficiency is proved.On the other hand, if {{formula:59127b43-b71e-4770-a763-33b17a06e068}} is a maximally entangled state, then we can find orthonormal bases {{formula:006501d4-3981-4f98-9b3e-35592c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does ACER use to analyze physics problem-solving, and what are the main findings of their research in this area?",
      "expected_output": "The ACER framework uses a four-component approach to analyze physics problem-solving: Activation, Construction, Execution, and Reflection. This framework helps identify key difficulties in students' work, such as applying mathematical tools in physics. The main findings of the research suggest that ACER is a productive analytical tool for investigating student difficulties with physics concepts, like boundary conditions, and can inform the development of interview protocols to target specific aspects of the problem-solving process.",
      "context": [
        " on students' explicit discussion of the details of their work. Another framework developed by Bing to analyze upper-level students' use of mathematics was epistemic framing . Epistemic framing is the students' unconscious answer to the question: “What kind of activity is this?” A student's framing can be identified by examining the justifications and proof students offer to support their mathematical claims . Earlier research on investigating students' understanding includes diSessa's work on epistemology in physics , , Sherin's work on symbolic forms , , and Collins et.al's work on Epistemic forms and epistemic games .blackOne significant difference between these theoretical frameworks and ACER is that these frameworks usually require detailed analysis of students' in-the-moment reasoning through video or audio data, while ACER can be used to analyze written solutions. Connections between these, and other, theoretical frameworks and ACER is discussed elsewhere . For the purpose of this study, we are interested in identifying the points (nodes) in students' work where key difficulties appear. blackACER provides a mechanism to identify these nodes using students' written solutions. The framework also informed the development of interview protocols that targeted blackmore aspects of the problem-solving process, including those",
        " using mathematical tools in physics. blackThe use of BCs, however, would more appropriately be characterized as the application of a physics concept/principle rather than the use of a mathematical tool. Thus, this is a slight extension on the original use of ACER. However, blackthe overarching goal of the ACER framework was to assist in the process of investigating mathematically sophisticated physics problem solving . Thus, since we have found ACER to be a productive analytical tool when thinking about students' difficulties with BCs, and the theoretical grounding of the framework does not preclude the use of a conceptual (rather than mathematical) tool, we have opted to utilize the framework here.blackSeveral other frameworks have been applied to investigations of student difficulties. For example, the framework of epistemic games presented by Tuminaro was developed to analyze students' problem solving behavior in terms of locally coherent goal-oriented activities. These games both guide and limit what knowledge students think is appropriate to apply at a given time. Identifying these games provides a way of parsing students' tacit expectations about how to approach solving physics problems . This framework was developed for introductory students and relies on students' explicit discussion of the details of their work. Another framework developed by Bing to analyze upper-level students' use",
        "Sec. ), and end with some implications for teaching, the limitations of the study and suggestions for future work (Sec. ).\nAnalytical Framework: blackTo guide and structure our analysis of student difficulties with BCs in the context of EM waves, we utilize an analytical framework known as ACER (activation, construction, execution, reflection) . The ACER framework is an analytical tool that blackwas developed to characterize student difficulties with upper-division problem solving by organizing the problem-solving process into four general components: Activation of the tools, Construction of the models, Execution of the mathematics, and Reflection on the results. These components appear consistently in expert problem solving , and are explicitly based on a resources view on the nature of learning , . Since the particulars of using mathematical and physical tools to solve upper-division physics problems are highly context-dependent, ACER is designed to be operationalized for specific physics topics. The operationalization of the ACER framework for BCs will be discussed in Section. , and additional details about the ACER framework can be found in Ref. .ACER was blackoriginally designed to characterize students' difficulties using mathematical tools in physics. blackThe use of BCs, however, would more appropriately be characterized as the application of a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of RSB on the stability of the XY model, and what methods were used to obtain these results?",
      "expected_output": "The main findings of the research paper are that the fixed point previously found by Cardy and Ostlund in the glass phase is unstable to Replica Symmetry Breaking (RSB), and the susceptibility associated with infinitesimal RSB perturbation in the high-temperature phase diverges as the temperature approaches the critical temperature. The methods used to obtain these results include constructing controlled RG recursion relations that allow for RSB, and analyzing the Hamiltonian of the 2D vortex-free XY model in a random field. The researchers found that the replica symmetric flow and the CO fixed point are unstable to a small RSB, providing analytical evidence that RSB occurs in finite dimensional models.",
      "context": [
        " Le Doussal Laboratoire de Physique Théorique de l'Ecole Normale Supérieure, 24 Rue Lhomond, F-75231 Paris Cedex, France Thierry Giamarchi Laboratoire de Physique des Solides, Université Paris–Sud, Bât. 510, 91405 Orsay, France We study the 2D vortex-free XY model in a random field, a model for randomly pinned flux lines in a plane. We construct controlled RG recursion relations which allow for replica symmetry breaking (RSB). The fixed point previously found by Cardy and Ostlund in the glass phase {{formula:6ddc77c1-52f7-4a90-bec5-4d14b4a60d87}} is unstable to RSB. The susceptibility {{formula:d01f5f83-82fa-40f9-8a5d-b523c9364d06}} associated to infinitesimal RSB perturbation in the high-temperature phase is found to diverge as {{formula:0fd84502-db63-4ba3-b49c-2218667a",
        "# arXiv Paper: cond-mat/9409103\n\nReplica Symmetry Breaking Instability in the 2D XY model in a random field\nabstract: We study the 2D vortex-free XY model in a random field, a model for randomly pinned flux lines in a plane. We construct controlled RG recursion relations which allow for replica symmetry breaking (RSB). The fixed point previously found by Cardy and Ostlund in the glass phase $T<T_c$ is {\\it unstable} to RSB. The susceptibility $\\chi$ associated to infinitesimal RSB perturbation in the high-temperature phase is found to diverge as $\\chi \\propto (T-T_c)^{-\\gamma}$ when $T \\rightarrow T_c^{+}$. This provides analytical evidence that RSB occurs in finite dimensional models. The physical consequences for the glass phase are discussed.\n: [prl,preprint,aps,tighten]revtexReplica Symmetry Breaking Instability in the 2D XY model in a random fieldPierre Le Doussal Laboratoire de Physique Théorique de l'Ecole Normale Supérie",
        "a168-4d75-9960-9568e280300f}} the replica symmetric flow and the CO fixed point, are unstable to a small RSB. This is to our knowledge the first physical model where this effect can be demonstrated in a controlled way.The Hamiltonian of the 2D vortex-free XY model in a field of random amplitude and direction, reads: {{formula:dbd2b73c-6e9a-4048-a9a4-039914640d93}}with {{formula:f95fdb26-0ba7-4a76-b0fc-3fb69bf9e749}} and {{formula:4643ff18-4728-4a5a-91bc-5ea78ffc8d9f}} are two Gaussian white noises. This model also describes flux lines with displacements {{formula:6e86cb10-2dbd-40e9-be9a-5ca238216ee3}} and average spacing {{formula:5691ace3-ff01-4027-a1f4-9712bac3503d}} in {{formula:ef"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does physical argument support PL and PLC relations, and what are the main findings of this research in terms of methods used and key results?",
      "expected_output": "The physical argument supports PL and PLC relations by deriving a relation at maximum light, which leads to a more accurate distance estimator. The main findings of this research are that a PLC relation based on maximum light has a dispersion about 10% smaller than its counterpart using mean light. Additionally, using observations at both maximum and mean light results in a significantly smaller dispersion. The research used a sample of Cepheids in the Large Magellanic Cloud and compared the variance of different distance indicators. Key results include a reduction in dispersion of over 50% when using a PLC relation with both maximum and mean light, and over 30% when applied to a large group of equidistant Cepheids in a distant galaxy, such as IC4182, M81, and M100.",
      "context": [
        " distant galaxy. Recent HST observations of IC4182, M81 and M100 already provide suitable candidate data sets for this relation.\n: []laa",
        "# arXiv Paper: astro-ph/9504033\n\nOn improved Cepheid distance estimators\nabstract: In this paper, we derive a physical argument for the existence of Period-luminosity and period-luminosity-colour relations at maximum light. We examine in detail a sample of Cepheids in the Large Magellanic Cloud, and compare the variance of some PL and PLC type distance indicators based on mean and maximum light. We show that a PLC relation based on maximum light leads to a distance estimator with a dispersion about $10 \\%$ smaller than its counterpart using mean light. We also show that a PLC type relation constructed using observations at both maximum and mean light has a significantly $( > 50 \\%)$ smaller dispersion than a PLC relation using either maximum or mean light alone. A comparable $( > 30 \\%)$ reduction in the dispersion of the corresponding distance estimator, however, in this case requires the relation be applied to a large $( n > 30)$ group of equidistant Cepheids in, e.g., a distant galaxy. Recent HST observations of IC4182, M81 and M100 already provide suitable candidate data sets for this"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the self-consistent Green's function play in calculating electron density, and how does it affect the accuracy of the results?",
      "expected_output": "The self-consistent Green's function plays a crucial role in calculating electron density. According to the research paper, it is used to calculate the electron density, which is found to be a certain percentage of the exact electron density for a given system. For example, in the case of the system described by formula 1b91d86d-a996-44ac-a83c-4b1fafe967fd, the electron density calculated with the truncated Green's function is a certain percentage of the exact electron density. The accuracy of the results depends on the self-consistent Green's function, as it affects the calculated electron density. The paper shows that the electron density calculated with this truncated Green's function should be zero, indicating the importance of the self-consistent Green's function in achieving accurate results.",
      "context": [
        "ula:cfca8c0c-9d67-4bc2-88d1-fe0269ed6947}} of the exact density (for the {{formula:442bdf60-22c8-45b2-b216-f42899cbbbb9}} system). Furthermore, we calculated the self consistent Green's function without one of the Hartree-Fock self energy diagrams which is zero when inserted in {{formula:166edd84-6849-42eb-979c-35aa9db14a44}} diagrams. The electron density which is calculated with the help of this Green's function is {{formula:f2834b7d-af26-43d4-b300-f34bff15f064}} of the exact electron density (for the {{formula:1b91d86d-a996-44ac-a83c-4b1fafe967fd}} system). It was shown by us in that the electron density calculated with this truncated Green's function should be zero. At last we obtained that the asymptotics of the Green's function in the Hartree-Fock approxmation by taking into",
        "-4f5f-9fef-b2d366c8e428}}Now we calculate the density of the system by using the self-consistent Hartree-Fock Green's function. With the help of {{formula:c4a04f1b-f5c3-4847-83de-65d228c44fb5}} we get for the electron density {{formula:aae7fed5-5677-4fef-a145-1c1e3404e39f}} ({{formula:67ae7ba7-cad7-4890-90aa-8b71ece121b4}} is an infinite small positive parameter) with {{formula:35bc8a79-3722-4c92-b597-ba0c9a007c8f}} for the {{formula:f6108cd2-d035-445c-9e25-23dda42e3c51}} system we get {{formula:8c8f80f0-c3be-4127-bc14-eccdeccb9aab}} of the electron density. In contrast to this"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about the relationship between conjugation equations and semisimple Lie algebras, and how do the methods used contribute to the key results?",
      "expected_output": "The research paper presents a significant finding that the conjugation equation can be obtained in the general case with a semisimple Lie algebra. Specifically, it shows that the conjugation equation is equal to the product of certain elements, where these elements are derived from the decomposition of the longest Weyl group element as a product of simple reflections. The methods used, such as the decomposition of the longest Weyl group element, contribute to the key results by providing a way to express the conjugation equation in terms of simpler elements. For example, when the algebra is denoted by a specific formula, it is an algebra over the ring of formal power series in a variable, with generators that can be explicitly defined. Overall, the paper provides a detailed analysis of the relationship between conjugation equations and semisimple Lie algebras, shedding light on the underlying mathematical structures.",
      "context": [
        " that he obtained the conjugation equation (REF ) in the general case (that is, with {{formula:d2bbe75f-b322-4f5c-91d5-6e815eb2fcae}} replaced by a semisimple Lie algebra {{formula:cf309efb-ad38-4e11-baaa-a541b0dee046}} ). Then {{formula:2172df27-c676-4a2f-ba0e-02dffcbc91e9}} is equal to the product {{formula:7add0ec8-c71c-4a89-a511-fc136662e27e}} , where {{formula:55f579c4-c105-4602-9fd6-9bcf325dd60d}} is a decomposition of the longest Weyl group element as a product of simple reflections, and {{formula:118eb1ce-f71d-40d7-90ac-4df0bae43ce2}} , with {{formula:7b477f30-d49a-4c35-bb69-7",
        "2b-2e0bb8213240}} . In the case where {{formula:3c481437-49af-437c-8e9b-6c9927c6ab42}} , this algebra is denoted by {{formula:f81d54ad-ace6-408c-9416-8692ad1dbdc4}} . We will also denote it by {{formula:c59759a1-8406-45f0-8912-d168e00d2a05}} . It is an algebra over the ring of formal power series in the variable {{formula:7a32c23f-aa4d-4dea-8fe5-2edc135bb543}} , with generators {{formula:79c71e7f-2046-4c35-b9f3-7dd22e8569e8}} ({{formula:9bb6d7c8-9de5-4cd5-a199-c938b3a309cd}} ), {{formula:dbaa92bc-ed5d-4cc6-ae8f-7a78"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the friction constant and noise levels, and how does the fluctuation-dissipation theorem explain this connection?",
      "expected_output": "The relationship between the friction constant and noise levels is connected by the fluctuation-dissipation relationship. According to the theorem, the friction constant and noise are related in such a way that the energy dissipated by friction is balanced by the energy introduced by the noise. This relationship is expressed as {{formula:cb3ed70f-087b-4a3e-8f8a-1a43af013798}}, which shows that the friction constant {{formula:3fcc3b9c-957e-490c-b725-83903a374515}} and the noise {{formula:c8c27954-f98f-4b53-90b6-d1ffe054b52a}} are connected, allowing for the calculation of thermodynamic quantities and the complete dynamical trajectory.",
      "context": [
        "b3}} is the coordinate, {{formula:d574c74a-fef4-4f95-bf7c-dc0f5036596e}} is its velocity, {{formula:dee70b42-171a-4411-9fcc-8f59d0a6f6ed}} is its mass, {{formula:20d1c677-f28b-42cb-a80f-cce2158b6aae}} is a net deterministic force acting on {{formula:5fcb94f6-d735-4ea4-bfca-f8e54289e91c}} . The friction constant {{formula:3fcc3b9c-957e-490c-b725-83903a374515}} and the noise {{formula:c8c27954-f98f-4b53-90b6-d1ffe054b52a}} are connected by the fluctuation-dissipation relationship {{formula:cb3ed70f-087b-4a3e-8f8a-1a43af013798}}",
        "b2-7705-4f9a-b206-3fc84c141574}} . In order to calculate both the complete dynamical trajectory and important thermodynamic quantities, one needs the velocity {{formula:3cba7023-5558-4f73-9263-17d22bf620a5}} explicitly expressed as well. Consistent with the method, we replace {{formula:8fdf3990-e328-42d8-8058-44b0094652b8}} in Eq. () by inserting Eq. (REF ), such that {{formula:c6dd7376-8ee8-47ce-bc76-203a8fdd38b5}}Thus, Eqs. (REF ), (REF ), and (REF ) constitute the identical SV form of the VE expressions.\nLeap-Frog G-JF: This version of the Verlet method for Langevin equations comes with some flexibility in how the method is expressed. We take the starting point with the SV form given in Eq. (REF ) and introduce a reasonable definition of the half-step velocity {{formula:78c8f72d-d5"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the derivative of the given formula impact the overall outcome of the research, and what are the key findings related to this specific calculation?",
      "expected_output": "The derivative of the given formula, specifically the derivative of {{formula:2e2f5ff9-a78f-42e8-9fd4-b87da1a72ee6}}, impacts the overall outcome of the research by showing that {{formula:de3839f5-14b3-4328-9931-9f5da0e7347b}} depends on this derivative rather than on {{formula:8de2de11-7cee-4611-b401-b5a2c2790393}} itself. A key finding is that the imaginary part of {{formula:9fae3fcc-0d71-476e-b713-2a19bb8400a6}} is proportional to {{formula:5633e023-335b-4f05-a469-99ca387164c4}}, indicating a specific relationship between these variables. This calculation is crucial as it affects the understanding of production amplitudes and meson distributions, highlighting the complexity of QCD factorization due to infrared divergences.",
      "context": [
        " found in {{formula:1879a6f2-0117-439c-87f4-872d35936e59}} production , but at variance with the model proposed in . Integrating eq.(REF ) by parts shows that {{formula:de3839f5-14b3-4328-9931-9f5da0e7347b}} depends on the derivative of {{formula:2e2f5ff9-a78f-42e8-9fd4-b87da1a72ee6}} rather then on {{formula:8de2de11-7cee-4611-b401-b5a2c2790393}} itself. The imaginary part of {{formula:9fae3fcc-0d71-476e-b713-2a19bb8400a6}} is, for example, proportional to {{formula:5633e023-335b-4f05-a469-99ca387164c4}}It can be argued that {{formula:80e8e773-141e-4537-8a88-918e9513214d}} is proportional",
        " production amplitudes on the meson distributions {{formula:4c84a7cc-99e4-4ce8-8d3c-6fb4be37701f}} , {{formula:5e4514d6-b1a5-4e8d-8732-c6924a33447f}} and {{formula:6916878a-fa0d-40d3-99e4-8752c2546658}} : {{formula:4ddc42d2-2c61-4a25-9449-4a58fe76b201}}Both, {{formula:e66664e3-075d-420f-b3fd-f319c34cca8e}} and {{formula:50e9acaa-f51f-42dd-bd6e-70ba84c3b4f2}} are divergent due to the behavior of the integrands at {{formula:9de0af58-e97c-4866-963f-debd98bd7a0b}} . These infrared divergences make QCD factorization impossible , . Currently no QCD"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the key differences between MMFBM's continuous memory modulation and MFBM's reset memory structure, and how do these differences impact the models' performance?",
      "expected_output": "The key differences between MMFBM's continuous memory modulation and MFBM's reset memory structure lie in how they handle time-dependent memory exponents. MMFBM continuously modulates its long-range memory based on the history of the process, whereas MFBM resets its memory structure at each time step, only considering the instantaneous value of the memory exponent. This difference significantly impacts the models' performance, particularly in capturing viscoelastic or persistent anomalous diffusion in changing environments. For instance, after a step-like change in the memory exponent, MMFBM's mean-squared displacement (MSD) scaling exponent can be influenced by the value of the exponent before the change, whereas MFBM's MSD scales solely based on the current exponent value. This is evident in Figure REF, which shows a discontinuous jump in MFBM's trajectory at the switching time, in contrast to MMFBM's continuous path.",
      "context": [
        "# arXiv Paper: 2303.01551\n\nMemory-multi-fractional Brownian motion with continuous correlations\nabstract: We propose a generalization of the widely used fractional Brownian motion (FBM), memory-multi-FBM (MMFBM), to describe viscoelastic or persistent anomalous diffusion with time-dependent memory exponent $\\alpha(t)$ in a changing environment. In MMFBM the built-in, long-range memory is continuously modulated by $\\alpha(t)$. We derive the essential statistical properties of MMFBM such as response function, mean-squared displacement (MSD), autocovariance function, and Gaussian distribution. In contrast to existing forms of FBM with time-varying memory exponents but reset memory structure, the instantaneous dynamic of MMFBM is influenced by the process history, e.g., we show that after a step-like change of $\\alpha(t)$ the scaling exponent of the MSD after the $\\alpha$-step may be determined by the value of $\\alpha(t)$ before the change. MMFBM is a versatile and useful process for correlated physical systems with non-equilibrium initial conditions in a",
        " the change. MMFBM is a versatile and useful process for correlated physical systems with non-equilibrium initial conditions in a changing environment.\nLévy's Riemann-Liouville-FBM: A well-known representation of fractional Brownian motion (FBM) is attributed by Mandelbrot to Paul Lévy . It is given by the Holmgren-Riemann-Liouville fractional integral B_{\\alpha }(t)=\\sqrt{\\alpha }\\int _0^t(t-s)^{(\\alpha -1)/2}dB(s)B(t) is standard Brownian motion and \\alpha \\in (0,2]. It is easy to show that the MSD of B_{\\alpha }(t) yields as \\langle B_\\alpha ^2(t)\\rangle =t^{ \\alpha }. This exact equality follows from the choice of the prefactor in definition (REF ) .With the increments of RL-FBM for disjoint intervals [t,t+\\delta ] and [t+ \\Delta ,t+\\Delta +\\delta ],B^\\delta _\\alpha (t)=B_",
        "^{b-1}ds.RL-FBM has non-stationary increments at any given time t, i.e., it does not solely depend on the time lag \\Delta .\nMFBM: A direct generalization of FBM to multifractional Brownian motion (MFBM) is to replace \\alpha by an explicitly time-dependent function \\alpha (t). In comparison to mathematical literature (see, e.g., , , ) we base the generalization on RL-FBM (REF ) with the square-root prefactor,Y(t)=\\sqrt{\\alpha (t)}\\int _0^t(t-s)^{(\\alpha (t)-1)/2}dB(s).In MFBM, the long-range correlations are reset, as only the instantaneous value of \\alpha at time t is considered in (REF ), and the MSD scales like\\langle Y^2(t)\\rangle =t^{\\alpha (t)}.Trajectories for MFBM and MMFBM for a step-like protocol are shown in Fig. REF . Evidently, the path of the conventional MFBM has a discontinuous jump at the switching time while the path of MMFBM"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the main goal of agents in solving optimization problems and how do they achieve it?",
      "expected_output": "The main goal of agents in solving optimization problems is to cooperatively seek the global optimal solution. They achieve this by only using local computations and communication among agents, where each agent possesses exclusive knowledge of its local objective function and communicates directly with other agents if they are connected over an undirected graph.",
      "context": [
        "bcae5}} . Two agents {{formula:6187c093-8035-4477-af1b-48b6d3d9f018}} and {{formula:41ac36ec-70d3-49b0-bc45-7b63fd2a0860}} can only communicate directly with each other if the edge {{formula:28642f08-42d9-410c-b554-0771217592c8}} .Then, we equivalently reformulate the optimization problem (1) as follows: {{formula:05d979f6-2ae1-4029-9dbc-4547c3bb9479}}where {{formula:c1333597-47e6-4943-81f1-f75280e6d901}} . Therefore, the global optimal solution, {{formula:799485a3-46ff-49af-a3e2-b71b230e20c0}} , to problem (2) is equal to {{formula:643b1366-2729-4581-a681-b232267bc653}} .Assumption 1 The graph",
        " is to seek the global optimal solution {{formula:1acd3041-34a6-408b-96c0-36d37504f22d}} to (1) via only local computations and communication among agents.Let {{formula:b3dc626a-c848-4199-ba3c-409ad15fe918}} agents be connected over an undirected graph, {{formula:16c27e2c-73ef-4674-8f27-53edc4b5b8e9}} , where {{formula:2fddebe5-bf66-4469-a37e-40e13c49bb4c}} is the set of agents, {{formula:a5ea3817-5a77-4e7f-9d26-050e38f2f4b7}} is the collection of edges, and {{formula:b35d2e45-1b60-46d6-b687-cb40486236dd}} indicates the weighted adjacency matrix where the weight {{formula:4b7c13b0-0b77-4317-",
        "f7b4}} . A nonnegative vector is called stochastic if the sum of its elements equals to one. A nonnegative square matrix is called row- (column-) stochastic if its rows (columns) are stochastic vectors, respectively. We abbreviate independent and identically distributed to i.i.d..\nProblem Formulation: Consider a network containing {{formula:61441e29-d506-4919-846a-da3d7abd1e68}} agents and all agents aim at cooperatively solving the optimization problem as follows: {{formula:dbebb56c-a869-4e8e-9b67-da30e62976a7}}where agent {{formula:020f9965-c5cd-4d4a-9171-a13d3b2a0654}} possesses exclusive knowledge of its local objective function {{formula:ffeba7f3-6515-46a6-848c-f7c25e9b2bba}} . The goal is to seek the global optimal solution {{formula:1acd3041-34a6-408b-96"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does Theorem 2.1 relate to Ricci flow bounds and what are the implications of this relationship for the main findings of the research paper?",
      "expected_output": "Theorem 2.1 relates to Ricci flow bounds by establishing universal constants that depend only on the initial manifold, providing bounds for the Ricci curvature and diameter under the normalized Ricci flow. This relationship implies that the Ricci curvature is controlled, and the diameter of the manifold is bounded, which has significant implications for understanding the behavior of the manifold under the Ricci flow. For example, Lemma 2.3 provides evolution equations for the normalized Ricci flow, which are used to derive the bounds in Theorem 2.1. The bounds have implications for the main findings of the research paper, such as understanding the long-time behavior of the Ricci flow and the properties of the manifold.",
      "context": [
        "6d-43f4-bcce-ff51004e16b4}} such that under the normalized Ricci flow, {{formula:0f7847ee-9220-4e9e-bc51-2244ffc751df}}Proof. By the evolution equations, {{formula:be078bbd-49e0-44f5-9539-82c113f88624}}so {{formula:294b0b3c-b501-4f6b-ac81-b231bea1b5aa}}Using the upper bound for the sectional curvature of {{formula:2bc9bcac-59f5-44f9-a4cd-8c5459418c49}} and Klingenberg's Lemma (Theorem 5.10 of ), the injectivity radius {{formula:9f5d0093-ee08-471f-be85-235ebe83a0ad}} of {{formula:a12e66ac-af0a-42aa-b071-874a3c05b391}} satisfies {{formula:a9de99a8-d504-",
        "7a-9c46-fbf23429e866}} . The following evolution equations for the normalized Ricci flow were established by Hamilton in .Lemma 2.3 Under the normalized Ricci flow, {{formula:515f022f-8a9b-4b82-82e4-b3015007f726}}where {{formula:d1ff0693-40d1-4fdf-bd99-e2a1a15156df}} , and {{formula:1ab7c730-4de3-40b1-81d8-5c7c7814eac8}} .Now we are ready to proveTheorem 2.1 There exist some universal constant {{formula:90145bc5-5ccc-4dc7-b86d-de0c3ab7e8ec}} and {{formula:434b55e2-5432-4901-a00d-ca9715c23af4}} depending only on {{formula:94591daf-0f6d-43f4-bcce-ff51004e16b4}} such that under the normalized Ric",
        "4-4385-8adc-e50aebe05ff8}} . The other inequality {{formula:1c29ca5a-b4b3-4bd4-bb13-4d37a6687e07}} can be derived by the same way.On the other hand, by Lemma REF , the Ricci curvature of {{formula:e09dae91-eab1-440a-b15e-3225407bd7ca}} is no less than {{formula:659c61be-d2d0-4dec-80c1-cd5fc807a1ff}} , thus the diameter of {{formula:a0052516-5021-40d4-90a2-a89ad0968a5d}} is smaller than {{formula:50f2112c-1e4a-47bd-ab91-925b8ab94787}} . Since {{formula:ff6b713c-ab78-46b6-a6a3-f319b9a77523}} , we have {{formula:1602ad20-6b40-4e60"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do constants play in exciton decoherence, and how do they affect the overall findings of the research paper?",
      "expected_output": "The constants, specifically the deformation potential coupling constants, play a crucial role in exciton decoherence as they represent the dominating interaction leading to decoherence. These constants are given by a formula that involves crystal density, normalization volume of the phonon system, and deformation potential constants for electrons and holes. For example, the material parameters used in the research paper include a crystal density of 5360 kg/m^3, a longitudinal sound velocity of 6420 m/s, and an exciton energy of 1.5 eV. The constants affect the overall findings of the research paper by influencing the exciton decoherence, which is essential in understanding the behavior of excitons in the material.",
      "context": [
        "}} in the interaction term denote the deformation potential coupling constants , , which is the dominating interaction leading to exciton decoherence. They are given by {{formula:76855351-8433-42b4-b71d-86ef2aa0124f}}Here {{formula:2e8e5a31-faff-4578-94ff-30b11b17746d}} is the crystal density, {{formula:97120551-5910-4c9a-a788-f71a2da282cb}} is the normalization volume of the phonon system, {{formula:11415160-bee4-4711-8887-f21eb6212f7a}} and {{formula:3bc0a2cb-a4e6-4a6e-81b3-45cec0c0f648}} are deformation potential constants for electrons and holes, and {{formula:57346be9-b6b2-4826-b103-acb0224a9d61}} are excitonic wave functions.The Hamiltonian can be diagonalized exactly by means of the Weyl operator",
        "-ca06-485d-9fe8-fd5028a916e4}}where {{formula:093075ca-723d-4c38-9f3d-76a08aa0b6b9}} is the energy of the exciton, {{formula:dd6e7be3-6473-4b94-9495-d0c30bf176dc}} are phonon energies corresponding to phonon creation and annihilation operators for wavevector {{formula:b936cd73-f2af-4fde-8fe2-8bfaec193ca1}} , {{formula:e5e6fa2c-7a73-41f3-af6b-b2a41c378c41}} and {{formula:a56ea844-1069-4cb6-8ed0-ba2c5b3f082f}} . {{formula:2577628b-2aa6-4b8d-acde-38d745856a8a}} in the interaction term denote the deformation potential coupling constants , , which is the dominating interaction leading to exciton deco",
        " which form the exciton) wave functions {{formula:ae398bc8-bc20-4835-9d85-551b16e5e8ed}} as Gaussians with 5 nm width in the {{formula:cd2562c8-5963-4d14-91d0-47b7cd276989}} plane and 1 nm along {{formula:b4a44ce2-bd07-46e2-af78-67f5570445d4}} . The material parameters used are {{formula:b079ad97-8379-44c1-b1b8-c306c4c63e5d}} eV, {{formula:31414865-f4fa-46c0-ad63-9f28e5f60341}} kg/m{{formula:6d8b03bb-f73d-4b35-894a-51ab7c61343d}} , and {{formula:251558ed-5805-4ef3-8844-151b54131d79}} m/s. The shifted exciton energy is not needed (but would have"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Einstein summation in simplifying index notation and how does it impact the methods used in this research paper?",
      "expected_output": "The Einstein summation convention simplifies index notation by assuming repeated indices are summed over. In this research paper, it allows for concise representation of equations, such as the Langevin equation, which is discretized as {{formula:4aa0df03-f70f-426f-962c-8c8e1985baa2}}. This convention impacts the methods used by enabling the derivation of conditional mean equations, like {{formula:6ad268d0-4d6b-4638-a96e-dcad97404c5d}}, and facilitating the analysis of models, including the extended Kalman filter, which is known to fail for certain models, as shown in Figure REF.",
      "context": [
        " the Einstein summation convention is adopted, i.e., all repeated indices are assumed to be summed over, so that {{formula:15083b7a-ddf7-452d-82b3-1605c6a760de}} . Also, {{formula:294384e3-c7eb-4335-922a-d22add0e24d0}} is written as {{formula:1d6017f3-111a-4c0c-a061-cf5d67f85cfb}} .Note that the change in Equation REF when {{formula:ac2220e2-7a19-4dc9-9a7e-6463e277e534}} and {{formula:acf1853f-3f52-4a5f-a5ec-fc87b6daa9a0}} are replaced with {{formula:4ebf9341-65e7-4ddb-9195-84081e321b2b}} and {{formula:4a4cb7b4-b604-4e81-9438-5230921bfa",
        " model {{formula:797d2084-c8b2-45e5-8cb5-c883bc4a9915}}Here {{formula:3228ce74-e445-437b-989c-4007e42501f9}} and we consider two values for {{formula:5ede484c-7880-45bf-a4e5-14ad5642f40a}} , namely {{formula:0fa154b2-2530-4af4-a0de-2e4ff1d04560}} and {{formula:7d952db1-15d7-449f-8cd8-0975284c5add}} . This example was studied in and the extended Kalman is known to fail for this model.{{figure:b0d970f8-f926-49d2-ac9c-5ee72df1ad60}}Conditional mean {{formula:6ad268d0-4d6b-4638-a96e-dcad97404c5d}} computed for the measurement sample of Figure REF .{{figure:5d760684-2380-",
        "-b180-92643ec0cdf2}} equi-spaced points and define {{formula:00b9ed1d-5e4d-44d5-a838-a001b944b9f2}} by {{formula:605896af-03ad-4995-8dba-faf4dbdc9fa9}} , or {{formula:af6a2f00-922e-4e54-9e54-91965e1755a7}} . Then, in discrete-time, the most general discretization of the Langevin equation is {{formula:4aa0df03-f70f-426f-962c-8c8e1985baa2}}where {{formula:bfd57dc3-6357-4b15-b41f-945323b0a9a7}} , and {{formula:52a82665-e422-4345-be59-01eff85a390c}}In this section, the Einstein summation convention is adopted, i.e., all repeated indices are assumed to be summed over, so that {{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of TDHF theory regarding damping in collective vibrations, and how do the methods used in this research lead to these key results?",
      "expected_output": "The main findings of TDHF theory regarding damping in collective vibrations are that it includes damping due to the incoherent 2p-2h decay in the form of a non-Markovian collision term. The methods used in this research, such as linearizing the extended TDHF theory around a finite temperature equilibrium state and expanding small deviations in terms of finite temperature RPA modes, lead to these key results. For example, calculations based on these approaches are partially successful in explaining the broadening of the giant dipole resonance with increasing excitation energy, as seen in the dipole strength distribution in certain nuclei.",
      "context": [
        " out calculations using the phenomenological finite range Gogny force and the zero range Skyrme force , .The small amplitude limit of the extended TDHF provides an appropriate basis for investigating collective vibrations, in which damping due the incoherent 2p-2h decay is included in the form of a non-Markovian collision term . Then, a description for small density fluctuations, {{formula:8d03e6f7-6088-422c-9ce0-7a0ff2242139}} , is obtained by linearizing the extended TDHF theory around a finite temperature equilibrium state {{formula:642365ca-73f2-4164-b886-c91703db6be3}} , , {{formula:4f690a7e-2c82-431b-a30a-ded47c3b8ee3}}where {{formula:336089d6-0295-4517-9d2b-2556fd078b6c}} represents small deviations in the effective mean-field potential. The right hand side describes a non-Markovian quantal collision term in which the quantity {{formula:fc164bd6-",
        " particular at low temperatures , , , , (ii) Damping due to the coupling with incoherent 2p-2h states which is usually referred to as the collisional damping , , , , and the Landau damping modified by two-body collisions , , . Calculations carried out on the basis of these approaches are partially successful in explaining the broadening of the giant dipole resonance (GDR) with increasing excitation energy. In particular, the model based on the coupling with thermal shape fluctuations in adiabatic approximation provides a reasonable description for the dipole strength distribution in {{formula:4cc89000-8c14-4cd8-9dab-d1a1d79184f3}} and the {{formula:97ba8ef3-cd07-417b-89b5-5b84c02da8d8}} nuclei , , . In this work, we do not consider the coherent contribution to damping, but investigate the collisional damping at finite temperature due to decay of the collective state into incoherent 2p-2h excitations in the basis of a non-Markovian transport approach. In order to assess how much of the total width of giant resonance",
        " side describes a non-Markovian quantal collision term in which the quantity {{formula:fc164bd6-b54c-48b5-917d-ab8a0fc9b66f}} denotes the distortion matrix associated with the single particle density matrix, {{formula:0298e82b-7b0d-402e-9654-a600e11c4076}} . It is convenient to analyze collective vibrations by expanding the small deviation {{formula:67a01389-46ea-43aa-8c5f-3c81d4aec021}} in terms of finite temperature RPA modes {{formula:826db9e7-15e4-42d7-8b78-3c0b8a5a12a9}} and {{formula:2d7d82c1-95e1-4fc8-bd47-41baf93fc445}} , {{formula:e294cc0b-926b-4ffa-bdf9-29bbb87d05eb}}where {{formula:ac95424a-6ef6-4e1c-a23"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does RESSCAL3D++ use to refine its predictions and what are the key findings of this refinement process?",
      "expected_output": "RESSCAL3D++ uses an update step to refine its previous predictions, leading to a significant reduction in the cost of scalability from 2.1% to 0.6% and from 2.0% to 0.2% on the S3DIS and VX-S3DIS datasets, respectively. The key findings of this refinement process are that RESSCAL3D++ is able to correct erroneous predictions, such as clutter predictions in bookcases, and even outperform non-scalable baselines in some cases. Additionally, the refinement process allows for a reduction in inference time at the highest scale, ranging from 15.6-63% depending on the acquisition time.",
      "context": [
        " 7031 samples derived from 168 distinct rooms within the S3DIS dataset. To ensure a sufficiently broad field-of-view, only offices, conference rooms, and auditoria are utilized from the original S3DIS dataset. The data is organized into different areas, mirroring the structure of the original dataset. The scanning pattern employs a Lissajous pattern with frequencies 1.1 and 1.8 mHz. The dataset includes annotations for 11 different classes: floor, wall, column, window, door, table, chair, sofa, bookcase, board, and clutter.\nProposed method: Overview of RESSCAL3D. In our prior work, RESSCAL3D , we introduced an architecture designed to handle resolution-scalable input data for semantic segmentation. In brief, it utilizes multiple branches that operate on different input resolutions. When low-resolution data becomes available, scale 1 can promptly generate an initial semantic prediction, even while higher resolution data is still being acquired. This allows for extremely early predictions and facilitates early decision-making. The subsequent branches process only the new, additional points and leverage features from previous scales in a fusion module to enhance performance. An important consequence of this approach is that RESSCAL3D processes all input points",
        " an important improvement which allows to reduce the cost of scalability from the 2.1% in RESSCAL3D to 0.6% at the highest scale. Secondly, while RESSCAL3D shows results on a subsampled dataset, we also evaluate on a novel dataset which emulates the behavior of the resolution scalable 3D sensor from VoxelSensors. The presented dataset, dubbed VX-S3DIS, is the first dataset leveraging resolution scalable point streams. We have devised an approach to jointly acquire and semantically segment a 3D scene. The proposed method is illustrated in Figure REF . Summarized, our main contributions are as follows:The introduction of an update step which allows to refine previous predictions, leading to a significant reduction in the cost of scalability from 2% to 0.2% on VX-S3DIS with a negligible effect on the inference time. The presentation of the first semantic segmentation dataset, dubbed VX-S3DIS, that mimics the working of a resolution-scalable 3D sensor, allowing intra-scan processing. Exhaustive experimentation of the proposed method on two datasets, showing the potential of the method for joint acquisition and processing on VX-S3DIS,",
        "SCAL3D is unable to refine previous predictions which are therefore also present in the final predictions at the highest scales. RESSCAL3D++ on the other hand is able to correct those, leading to a more consistent and correct prediction at the highest scales. This is particulary noticeable for the erronous clutter (black) prediction in the bookcase (pink) on the righthand side. Also noteworthy is that for this visual example, RESSCAL3D++ even outperforms the non-scalable baseline. One reason could be that the processing on different resolution levels allows to increase performance in some cases.VX-S3DIS semantic segmentation As a second experiment, we apply the proposed method on our novel dataset VX-S3DIS. A similar evaluation strategy as for the S3DIS semantic segmentation is followed. From Figure REF can be noticed that all RESSCAL3D++ allows an important improvement over RESSCAL3D with the reduction of the cost of scalability from 2.0% to only 0.2%. In terms of inference latency, Figure REF shows that, depending on the acquisition time, a reduction in inference time at the highest scale can be reduced by 15.6-63"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does Multimodal-CL use to predict stroke risk and what are its main findings?",
      "expected_output": "Multimodal-CL uses three different modules to predict stroke risk: the first module uses ResNet to process raw fundus photographs, the second module transforms retinal images into probabilistic vessel masks and then uses ResNet, and the third module constructs a vessel graph representation and uses Graph Neural Networks (GNNs). The main findings suggest that retinal images could be a cost-effective modality for improving stroke predictions, and the results are clinically meaningful, motivating future research on the relation between retinal and cerebral vessels.",
      "context": [
        " in vessel graphs in a hierarchical manner.In conclusion, we proposed novel approachs to integrate the fundus photographs in a contrastive learning framework. Our findings suggest that retinal images could be a cost-effective modality for improving stroke predictions. We believe these results are clinically meaningful and would motivate future research exploring the relation between retinal and cerebral vessels as well as using graphs as a compact representation of retinal vessels.",
        " This contrastive learning method is referred to as Multimodal-CL-prob. {{figure:d504df04-3bd1-47bd-80cd-967a3439ccfc}}\nRetinal Vessel Graph: As illustrated in Fig. REF , the third module exploits graph-level representation for retinal vessels. Pre-processing steps generate binary vessel segmentations using Automorph and extract vessel graphs and features using Voreen tools . Finally, the graphs are processed by a graph encoder consisting of graph attention layers and a global pooling layer to produce the feature embedding. We name this contrastive learning method Multimodal-CL-graph. {{figure:75dabae3-09e6-4b6d-a2de-1c946e1f4dbd}}\nDataset: Our analyses were performed on UK Biobank, which is a prospective cohort study of over 500,000 individuals' data from across the United Kingdom . This massive database includes patients' information as well as fundus images. To demonstrate the effectiveness of the contrastive pre-training, our goal was to predict future risk of stroke. Therefore, participants who have stroke diagnosed after data collection were",
        " is associated with MRI markers and CVD phenotypes in elderly participants . Recent works focus on early diagnosis and risk management of CVD using retinal images with DL algorithms .To leverage the huge amount of unannotated data and assess the risk of CVDs more efficiently, we propose a contrastive learning framework on fundus photographs (FPs) and clinical information in tabular format. Our framework is developed upon a general contrastive learning model using tabular and imaging data . In addition, we provide three different modules to extract embeddings from fundus photographs. The first module, based on ResNet , takes the raw fundus photographs as input. The second module transforms retinal images into probabilistic vessel masks, which are then processed by a ResNet model. Finally, the third module constructs a vessel graph representation from the fundus photograph and uses Graph Neural Networks (GNNs) to learn the feature embedding. This is the first time, to the best of our knowledge, that retinal images are represented as vessel graphs and further utilized in a contrastive learning framework. We show the effectiveness of the contrastive pre-training by comparing it against other baseline approaches for stroke prediction. Our code is publicly available at https://github.com/yuq"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how ASR errors impact the performance of lexical and dense retrievers in this research paper?",
      "expected_output": "The main findings are that ASR errors significantly impact the performance of both lexical and dense retrievers, causing a dramatic drop in retrieval performance. The research shows that using data augmentation with ASR noise to train a dense retriever can increase robustness against ASR noise, but typo-robust dense retrieval approaches are less effective. Additionally, natural ASR noise is found to be a more challenging setting than synthetic ASR noise, and users' accents play an important role in retrieval performance.",
      "context": [
        " users' accents play an important role in retrieval performance. Finally, to study a real-world scenario with voice variation and non-native speakers voicing questions, we also build a new test set where the questions have natural ASR noise. This set consists of 700 questions voiced by human annotators.We aim to answer the following research questions: RQ1 What is the impact on the performance of lexical and dense retrievers when questions have ASR errors? RQ2 Are typo-robust dense retrieval approaches also robust against ASR noise? How competitive are they against dense retrieval trained via data augmentation with ASR noise? RQ3 Do certain accents affect the effectiveness of the retriever more than others? RQ4 Does natural ASR noise affect the robustness of dense retrievers more than synthetic ASR noise?Our main contributions can be summarized as follows: (i) we provide two large-scale datasets where questions have synthetic ASR noise to facilitate research (evaluation and training of new models) on passage retrieval for spoken QA, (ii) we create a new challenging test set that contains 700 questions with natural ASR noise, (iii) we show how lexical and dense retrievers are not robust against ASR noise and propose data aug",
        " in this direction.Since there is no available dataset for passage retrieval where questions have ASR noise, we simulate ASR noise by automatically transcribing synthetically voiced questions. We then compare the robustness of lexical and dense retrievers by evaluating them against questions with and without ASR noise. Preliminary results showed that neither lexical nor dense models are effective against questions with ASR noise leading to a significant drop in retrieval performance. We find that using data augmentation with ASR noise to train a dense retriever is a promising approach for increasing robustness against ASR noise. However, the generation of such synthetic data is time-consuming and limited to the languages/accents supported by the text-to-speech system. To this extent, we explore if typo augmentation (faster and not bound to specific accents/languages) can alleviate these limitations. Our experimental results show that typo robust dense retrievers can increase robustness against ASR noise to some extent; however, ASR data augmentation remains a significantly more effective approach. Since users can have different local accents, we experiment with multiple accents of the same language and unveil that users' accents play an important role in retrieval performance. Finally, to study a real-world scenario with voice variation and non",
        " natural ASR noise decreases the retrieval performance, natural ASR noise appears to be a significantly more challenging setting. Despite the fact that the distribution of synthetically generated ASR noise and keyboard noise differs from that of natural ASR, we notice that DR combined with data augmentation holds better results than DR alone. We find that DR with synthetic ASR data augmentation outperforms its typo counterpart. Unfortunately, synthetic ASR and typo noise are inefficient while they do not generalize well to natural ASR noise. {{table:85fdedd2-d0b1-40d5-9391-7440ee40eb12}}\nConclusions: In this work, we study the impact of speech recognition errors in passage retrieval for spoken QA. We showcase that the effectiveness of lexical and dense retrievers drops dramatically when dealing with transcribed spoken questions. Moreover, we explore how typo-robust dense retrieval approaches perform against questions with ASR noise. Even though they can increase robustness compared to standard training on clean questions only, dense retrieval trained via data augmentation with ASR noise is a more effective approach. Finally, we compare the effect of synthetic vs. natural ASR noise and find that the latter is a significantly more challenging setting. Unfortunately,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of anisotropy on linewidth in Kondo lattices, and how do the methods used in this research paper support these results?",
      "expected_output": "The research paper finds that despite strong magnetic anisotropy, a collective spin mode of Kondo ions and conduction electrons enables a narrow linewidth in Kondo lattices. The methods used, such as comparing the linewidth-resistivity relation of pressurized Yb and analyzing the effect of Co-content on anisotropy, support these results. For example, at high Co-contents, narrow lines are allowed by an anisotropy being much smaller than for low Co-contents, consistent with a constant or slightly decreasing relaxation rate. These findings are supported by figures in the paper, such as Fig. REF, which shows the ratio of anisotropy and the linewidth behavior towards low temperatures.",
      "context": [
        "9526-289ac2f5f709}} . In this respect it is important to mention also the effect of anisotropy of the exchange interactions to the linewidth. It has been shown that a collective spin mode of Kondo ions and conduction electrons enables a narrow linewidth in a Kondo lattice despite a strong magnetic anisotropy. Hence, despite the anisotropy (as seen in the ratio of {{formula:c5e478db-7adc-4bff-b0ac-03d2be0344df}} , Fig. REF ) is largest at {{formula:d98ed03a-21dc-4b61-83a8-68bdc1987b0f}} , there the linewidth reaches its smallest value, supporting the collective spin mode model. Note, that with increasing {{formula:c00ca91b-120e-4464-8d78-d78762b5532a}} the increasing contribution of disorder to the linewidth should be considered as well. A discrimination of Kondo- and disorder contributions was qualitatively accomplished by a comparison of the linewidth-resistivity relation of pressurized Yb",
        " from an increasing influence of the RKKY interaction. Moreover, at high Co-contents, narrow lines are also allowed by an anisotropy {{formula:b77053e1-6692-4ff6-8e8e-2ba2af393b64}} being much smaller than for low Co-contents. This is consistent with a constant or slightly decreasing (for {{formula:bd5ee87a-0885-47a1-b6e0-c77fa9a1b524}} ) relaxation rate, see Fig. REF . Again, these relations between the ESR linewidth and the interplay of Kondo effect, RKKY interactions and anisotropy support the theoretical basis describing the spin dynamics in terms of a collective spin mode of Kondo ions and conduction electrons. Next, we consider the linewidth behavior towards low temperatures. There, broadening contributions from magnetic fluctuations become important even far above the magnetic transition temperature because the resonance relaxation is very sensitive to the effect of critical spin fluctuations. The type of magnetic order underlying the magnetic phase diagram in Fig. REF depends on the Co-content. As a consequence, characteristic differences in the low-temperature",
        "-c8c5370b4443}} , where it even changes sign and becomes positive, indicating the dominance of ferromagnetic exchange interactions. As documented in the middle frame of Fig. REF this behavior is well correlated with the in-plane linear temperature slope of the linewidth, {{formula:3061f022-7b66-42e1-94e7-617b6ac3f074}} . For low {{formula:12591c92-97a4-43e1-989a-f454f94aab54}} we interpret this correlation, similar to {{formula:2109b371-8cc3-4a6a-a43c-de97d1e85950}} , by a decreasing Kondo interaction up to {{formula:6d0c6985-9bf5-4997-9970-7f2b5c680eaf}} .At this point it is worth to note that in compounds with no Kondo lattice behavior (very small Kondo energy scales) such as YbRh or YbCo{{formula:42be1632-1db8-43ee-bd0a-8"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the cosmic ray fraction and star formation rate in NGC 3256 over a 10 million year period?",
      "expected_output": "The paper does not mention the cosmic ray fraction, but it does discuss the relationship between the cluster formation rate (CFR) and the star formation rate (SFR) in NGC 3256. According to the paper, the CFR has increased by a factor of 9 during the past 100 Myr, and the SFR is approximately ten times higher than normal, with a value of 22.9% ± 7.3-9.8 for the fraction of stars found within bound clusters after the embedded phase of cluster formation.",
      "context": [
        " both panels the cluster formation rate declines for clusters older than the completeness limit, due in part to older clusters fading and becoming undetectable.Focussing on the high mass cut sample, we see that the cluster formation rate (CFR) has increased by a factor of {{formula:9f3f2e9b-1cb7-4420-9bf5-bd51fad7dcf5}} during the past {{formula:f060b03f-703a-4635-b200-4cbd8ab79589}} Myr. This increase is expected due to the ongoing galactic merger which is presumably the cause of the ongoing starburst . This interpretation is also supported by the present SFR of NGC 3256 which is {{formula:91535339-e58a-4bdb-9eef-6c241f56943f}} M{{formula:306bd1bd-4ed4-4fc5-a6d8-f8aeb7d21893}} yr{{formula:174cbb11-ce44-4b37-bd65-94c8d1214853}} , which is approximately ten times higher",
        " clusters will be a (possibly complicated) combination of the fraction of stars formed in clusters, the star-formation history of the galaxy, the cluster disruption time-scale/rate, and the difference in the amount of extinction towards clusters and the field.In this paper we attempt to improve the situation by deriving ages and masses for clusters in the galaxy NGC 3256, which in turn is used to calculate a cluster formation rate (CFR) over the last ten million years. Comparing this inferred CFR to the star formation rate (SFR, measured by H{{formula:211acae7-0396-4d29-a9c3-789a6a353aaa}} fluxes or infrared luminosities) we compute the fraction of stars in clusters younger than ten million years which have survived the embedded phase intact, a value hereafter referred to as {{formula:94db6779-171f-4d34-8a73-9d2100ea3aa1}} . We pay particular attention to the possible sources of uncertainty which may affect these calculations. Using other data sets of cluster ages and masses for different galaxies we perform the same calculation for an additional 6 galaxies in an attempt to find how {{formula:3",
        "# arXiv Paper: 1002.2894\n\nOn the Fraction of Star Clusters Surviving the Embedded Phase\nabstract: In this paper we derive ages and masses for 276 clusters in the merger galaxy NGC 3256. This was achieved by taking accurate photometry in four wavebands from archival HST images. Photometric measurements are compared to synthetic stellar population (SSP) models to find the most probable age, mass and extinction. The cluster population of NGC 3256 reveals an increase in the star formation rate over the last 100 million years and the initial cluster mass function (ICMF) is best described by a power law relation with slope $\\alpha = 1.85 \\pm 0.12$. Using the observed cluster population for NGC 3256 we calculate the implied mass of clusters younger than 10 million years old, and convert this to a cluster formation rate over the last 10 million years. Comparison of this value with the star formation rate (SFR) indicates the fraction of stars found within bound clusters after the embedded phase of cluster formation, $\\Gamma$, is $22.9% \\pm^{7.3}_{9.8} $ for NGC 3256. We carried out an in-depth analysis into"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the limitations of SCA in addressing atom-field entanglement, and how do these limitations impact the research findings?",
      "expected_output": "The SCA has limitations in addressing atom-field entanglement, as it assumes the coupled atom-field system remains in a product of coherent states during time evolution. This assumption allows for decoupling of the equations of motion, but it cannot account for entanglement. As a result, the SCA misses the collapse of Rabi oscillations entirely, which is significant for longer times. For example, in Fig. REF, the SCA agrees with the exact and perturbative result for short times, but for longer times, it fails to capture the entanglement-generated collapse of Rabi oscillations.",
      "context": [
        "9a08569cb2e7}}The SCA equations of motion become exact in the classical field limit, where they reduce to Eq. (REF ).From the equations of motions we can observe a potential problem of the SCA that arises from the generation of atom-field entanglement in the true Dicke dynamics. The change of the field state in Eq. (REF ) is {{formula:4a9b3294-5dba-45d4-a55c-7993036800c2}} , and the influence on the atoms {{formula:54ebd51a-fd78-4402-b66b-ee1f9a7a9d32}} . Therefore, the dynamically relevant time scale in SCA is {{formula:b337440a-5619-4952-ae9b-379238349f65}} , just as the scaling of the entangling time {{formula:80a1456e-6dee-4d32-87ce-76ad523b48dc}} from Eq. (REF ). We should thus expect that for time scales on which the SCA differs from the simpler Eq. (REF",
        " (REF ). We should thus expect that for time scales on which the SCA differs from the simpler Eq. (REF ) significant atom-field entanglement has been generated for which the SCA cannot account.We consider exemplarily the limit {{formula:511efb00-4d66-46d8-bfb4-4308e49d3f0b}} of a classical field with negligible energy quantization. In Fig. REF we compare the SCA with the exact and perturbative result for the dynamics, and with the simplified Eq. (REF ) of a driven atomic ensemble. For short times (left panels) all four descriptions agree and describe Rabi oscillations, which are characteristic for classical field dynamics. For longer times (right panels), we see that significant entanglement is generated over the first few field periods even in the most simple Rabi case {{formula:9c785ff4-13ab-43e5-95e0-8d240cc70db7}} . The SCA cannot account for entanglement and accordingly misses the collapse of Rabi oscillations entirely. We note that the SCA simply reproduces the result also obtained with the simpler Eq. (",
        " (SCA) (see, e.g., Ref. for a discussion). The SCA is based on the assumption that the coupled atom-field system remains in a product {{formula:091857f3-adb2-4ee8-b2a8-416565e51037}} of coherent states during time evolution. This assumption allows for decoupling of the equations of motion for the atomic and field expectation values.After decoupling, the atomic state evolves again similar to a spin in a magnetic field {{formula:e144602b-b5d2-4ed5-a2eb-a8650a23c794}} . Now, however, the field state evolves as for an oscillator with an additional external force {{formula:2dc18ff3-604d-43ea-ab96-b0b9527d2f0d}} that accounts for the back-reaction from the atomic ensemble. The corresponding SCA equations of motion are {{formula:d240b88e-bb44-4836-890e-9a08569cb2e7}}The SCA equations of motion become exact in the classical field limit, where"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the curvature at the peeling front and the peeling force, and how do the researchers measure this in their study?",
      "expected_output": "The researchers found that the curvature at the peeling front is directly related to the peeling force. They measured this by extracting the edge of the outer side of the tape from images and fitting the shape of the midplane to the elastica equation, which allowed them to calculate the curvature at the peeling front. For example, they plotted the curvature at the peeling front as a function of the loop size and found that it increases steeply prior to the unlooping, indicating a strongly nonlinear increase of the peeling force.",
      "context": [
        " . The curvature at the peeling front {{formula:8116d2b7-75f7-4373-bcd1-c5bab4fccb14}} is determined on both sides of the symmetry plane and average values are reported (values that differ by more than 10% are discarded).To investigate the robustness of the phenomenon we applied a broad range of peeling velocities ({{formula:e461e9a1-51ec-4a57-9086-0c908add9355}} to {{formula:0a0f3a62-ebc1-4756-ac00-3e2f2ba066d7}} mm/s), and used three different types of commercial tapes that we indicate by tape A, B, C. Tape D is a thicker tape, formed by sticking two layers of tape A together (details given in Table REF ). Most of the quantitative results will be presented for tape A, which has a thickness {{formula:43496e22-9b4e-4afb-a700-96c2ddf80bba}} and a width of {{formula:29d0c",
        "56d97e8}} (see SI Movie S.3). We characterize the evolution of the process by measuring the loop size and the curvatures on both sides of the contact zone. These curvatures are key quantities of the problem: they are known to offer a direct access to the peeling force whenever the tape – outside the contact region – is accurately described by the elastica , , , . The curvatures are therefore extracted from the experimental profiles through fits to the elastica equation. We extract the edge of the outer (non-adhesive) side of the tape from these images, and find the shape of the midplane by correcting for the thickness of the tape. The shape of the tape can indeed accurately be fitted as an elastica, and allow us to measure the loop size {{formula:7f7312d8-29c4-4a4f-b136-86ea08f3e165}} and the curvatures at the edge of the contact region {{formula:f0818d40-0c6f-434b-b578-f340e8241463}} . The curvature at the peeling front {{formula:8116d2b7-75f7-4",
        " when the size of the contact falls below ten times the tape thickness. At this moment, the large loop begins to decrease in size and follows the sequence shown in Fig. REF . During this process, the size of the tenuous adhesive zone gradually decreases, until it finally ruptures (cf. SI Movie S.2).Surprisingly, we find that during the experiment the curvature at the peeling front ({{formula:7a10a58e-01de-4c9c-8641-b7421c1dfef4}} ) increases, which implies a strongly nonlinear increase of the peeling force. This increase can be seen upon careful inspection of Fig. REF , and is further quantified in Fig. REF , where we plot the curvature {{formula:82acb814-6c20-406f-b893-4c3a734f9622}} at the peeling front as a function of the loop size {{formula:d057c198-ef5f-400c-a22a-8a6065b0b496}} . Initially, the curvature remains approximately constant, but it steeply increases prior to the unlooping. Such"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research paper on Minkowski-Bouligand dimension and how does it relate to measuring image intricacy?",
      "expected_output": "The research paper uses the Minkowski-Bouligand dimension to measure the intricacy of images. The main finding is that this dimension can be used to quantify the level of detail in an image at different scales. The paper explains that the Minkowski-Bouligand dimension is calculated by covering the image with a grid and applying a renormalization measure to each box, which indicates whether the box is necessary to cover the pattern. The number of boxes needed to cover the image at a certain scale gives an idea of the image's intricacy. For example, the paper mentions that the Minkowski-Bouligand dimension is obtained from the formula D_MB = -lim ε→0 [log(N(ε))/log(1/ε)], where N(ε) is the number of boxes necessary to cover the image at scale ε. This concept is related to surface roughness and can be used to analyze the complexity of images.",
      "context": [
        " is the number of obtained copies.For calculating the fractal dimension of images, however, we use the Minkowski-Bouligand dimension. It consists of covering the image with a grid, and, for each box of the grid applying a renormalization measure \\mu that indicates whether that box is necessary to cover the pattern under analysis. The most used measure is simply the presence of a filled pixel. Our images, however, are gray-scale. Therefore, we normalized the images as: M\\rightarrow \\left(M-\\min (M)\\right)/\\left(\\max (M)-\\min (M)\\right) and used a measure \\mu _B=\\min \\left(1,\\sum _{b\\in B}b\\right) that counts the total intensity inside a box. The number of boxes necessary to cover the image at some scale \\epsilon gives an idea of the intricacy, or level of detail of the image at that scale. Similarly to the Hausdorff-Besicovitch, the Minkowski-Bouligand dimension is obtained from:D_{MB}=-\\lim _{\\epsilon \\rightarrow 0}\\frac{\\log \\left(",
        " Fermi level and its effects should be more pronnounced. Potentials estimated by CNN and ES are rougher, and follow a similar trend for transmission values smaller than approximately 2. The roughnesses of potentials estimated by ES, however, show a smaller variability, which is an expected result. Changing the transmission less than unity should not cause a significant movement of the background potential closer to the Fermi level.\nFractal Dimension: The fractal dimension is a related concept to the surface roughness. While surface roughness measures irregularities and deviations from a smooth potential, the fractal dimension measures how a pattern fills space at different scales in a self-similar fashion. The Hausdorff-Besicovitch dimension can be understood through a similar transformation. For example, if amplifying an object by a factor n, we obtain n^D copies of the same object, we say that D is its dimension. Mathematically, the Hausdorff-Besicovitch dimension is given by:D_{HB}=\\frac{\\log (N)}{\\log (n)},where N is the number of obtained copies.For calculating the fractal dimension of images, however, we use the Minkowski-",
        "\\% between the estimated and expected potentials for Gaussian fields.\nResults: Figure REF shows a typical SGM image and the corresponding background potential using the techniques studied in this work. Surprisingly, the background estimates exhibit notable discrepancies depending on the employed method. Since no ground truth is available, finding the method that gives the most appropriate result becomes a formidable task. {{figure:f9014ba4-6eac-4796-af07-c5b532ed6a3e}}\nImage Entropy: To quantify the estimation difference, we computed the image entropy for the whole set of images for each estimation technique. The image entropy is defined as:S=-\\sum _{\\langle image\\rangle }p_i\\log _2(p_i),where p_i is the probability of finding a pixel of the image with intensity i. This probability was computed from the histogram of the image using 256 bins.The importance of using the image entropy to qualify differences in estimation relies on the fact that the image entropy quantifies the level of complexity in the distribution of the intensities within the image. For example, if it is equally probable to find any intensity in a binary image, p_i=1/"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods were used to collect and filter issues, PRs, and commits in the research paper?",
      "expected_output": "The researchers used several methods to collect and filter issues, PRs, and commits. They started by searching for relevant words in the title, description, and comments of issues and PRs, and in the commit notes of commits. They then sorted the resulting words by frequency and removed the long tail of words appearing in less than 10 artefacts. The remaining words were manually analyzed and flagged for relevance to Deep Learning (DL). The researchers also used the GitHub API to retrieve issues and PRs labeled as bug, defect, or error, and mined the change log of repositories to identify commits with messages containing patterns such as 'fix' and 'bug'. Additionally, they defined a vocabulary of relevant words related to DL and excluded artefacts that did not contain any of these words. They also manually inspected a random sample of 100 elements to remove false positives.",
      "context": [
        " of the mined issues, PRs and commits. For commits, we searched for the relevant words in their commit note, whereas for issues and PRs we considered title, description, and all comments posted in the discussion. We sorted the resulting words by frequency (i.e., number of artefacts in which they appear), and we removed the long tail of words appearing in less than 10 artefacts. This was done to reduce the manual effort needed to select the words relevant for DL from the resulting list of words. Indeed, even assuming that one of the automatically discarded rare words were relevant for DL, this would have resulted in missing at most nine documents in our dataset. The remaining 3,076 words have been manually analysed: we split this list into five batches of equal size, and each batch was assigned to one author for inspection, with the goal of flagging the DL relevant words. All flagged words were then discussed in a meeting among all authors in which the final list of 105 relevant words was defined. The list is available in our replication package , and includes words such as layer, train, tensor. After excluding all artefacts not containing at least one of the 105 relevant words, we obtained the final list of commits (1,981), and of",
        " remaining projects sorting them by the number of relevant commits (i.e., commits matching the pattern described above). The 100 selected projects account for a total of 8,577 issues and PRsand 28,423 commits.Before including these artefacts in our study, we manually inspected a random sample of 100 elements and found many (97) false positives, i.e., issues/PRs/commits that, while dealing with fault-fixing activities, were unrelated to issues relevant to the usage of the underlying DL framework (i.e., were about generic programming bugs). Thus, we decided to perform a further cleaning step to increase the chance of including relevant documents in the manual analysis. We defined a vocabulary of relevant words related to DL (e.g., “epoch”, “layer”), and excluded all artefacts that did not contain any of these words. Specifically, we extracted the complete list of 11,986 stemmed words (i.e., “train”, “trained”, and “training” were counted only once as “train”) composing the vocabulary of the mined issues, PRs and commits. For commits, we searched for the relevant words in their commit note, whereas",
        ", and false positives (i.e., projects containing the search strings in one of their Python files but not actually using the related framework). This process left us with 121 TensorFlow, 175 Keras, and 268 PyTorch projects.For each of the retained 564 projects, we collected issues, PRs, and commits likely related to fixing problems/discussing issues. For issues and PRs, we used the GitHub API to retrieve all those labelled as either bug, defect, or error. For commits, we mined the change log of the repositories to identify all those having a message that contained the patterns : (“fix” or “solve”) and (“bug” or “issue” or “problem” or “defect” or “error”).Then, for each framework, we selected a sample of 100 projects for manual analysis. Instead of applying a random selection, we selected the ones having the highest number of issues and PRs. For frameworks for which less than 100 projects had at least one relevant issue/PR, we selected the remaining projects sorting them by the number of relevant commits (i.e., commits matching the pattern described above). The 100 selected"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the proposed approach use to optimize deformable registration and what are the key findings of this research?",
      "expected_output": "The proposed approach uses a combination of optimization techniques, including a generic optimization model and a series of learnable architectures, to optimize deformable registration. The key findings of this research include the development of a deep learning-based framework that integrates the advantages of conventional deformable registration methods and recent deep learning-based approaches, and the proposal of a novel bilevel self-tuned training strategy that allows for efficient search of task-specific hyper-parameters. The approach is shown to achieve state-of-the-art performance on 3D volume datasets, including image-to-atlas registration on brain MRI data and image-to-image registration on liver CT data, with diffeomorphic guarantee and extreme efficiency.",
      "context": [
        " the underlying anatomical structure. The informed priors may help to render the registration method more robust and stable, but it is challenging to introduce priors flexibly under different clinical scenes. These limitations make purely optimization-based registration methods hard to obtain the solutions efficiently and flexibly. The goal of this work is to learn a powerful solver to aggregate a variety of mechanisms to address the deformable registration problem efficiently. We first introduce a general optimization model to formulate deformable image registration, inspired by which we present our optimization propagation framework with a series of modules on multi-scale feature space.Fundamental optimization formulation of registration. Given a source image {{formula:7cabd5f3-554f-4f7e-83f8-9a7d2a1cc44e}} and a target image {{formula:200d341d-2a24-4db5-9215-e0364e741da1}} with a spatial domain {{formula:2acfb7f6-d796-4ce6-b3c0-46e531a7bf9e}} , specifically, we aim at minimizing the following constrained optimization model: {{formula:aa73ef60-89",
        " We conduct two groups of image registration experiments on 3D volume datasets including image-to-atlas registration on brain MRI data and image-to-image registration on liver CT data. Extensive results demonstrate the state-of-the-art performance of the proposed method with diffeomorphic guarantee and extreme efficiency. We also apply our framework to challenging multi-modal image registration, and investigate how our registration to support the down-streaming tasks for medical image analysis including multi-modal fusion and image segmentation.\nIntroduction: Registration plays a critical role in medical image analysis, which transforms different images into one common coordinate system with matched contents by finding the spatial correspondence between images . It is fundamental to many clinical tasks such as image fusion of different modalities, anatomical change diagnosis, motion extraction, and population modeling. Traditional image registration is formulated as an optimization problem to minimize image mismatching between a target and a warped source image, subject to transformation constraints. Deformable registration method computes a dense correspondence between image pairs . The high degrees of freedom for the solution space (deformation maps) and great variations on source/target image pairs are major challenges for this issue.Conventional deformable registration techniques aim at solving the optimization problem and offer rigorous theoretical treatments. However",
        "# arXiv Paper: 2004.14557\n\nLearning Deformable Image Registration from Optimization: Perspective, Modules, Bilevel Training and Beyond\nabstract: Conventional deformable registration methods aim at solving an optimization model carefully designed on image pairs and their computational costs are exceptionally high. In contrast, recent deep learning based approaches can provide fast deformation estimation. These heuristic network architectures are fully data-driven and thus lack explicit geometric constraints, e.g., topology-preserving, which are indispensable to generate plausible deformations. We design a new deep learning based framework to optimize a diffeomorphic model via multi-scale propagation in order to integrate advantages and avoid limitations of these two categories of approaches. Specifically, we introduce a generic optimization model to formulate diffeomorphic registration and develop a series of learnable architectures to obtain propagative updating in the coarse-to-fine feature space. Moreover, we propose a novel bilevel self-tuned training strategy, allowing efficient search of task-specific hyper-parameters. This training strategy increases the flexibility to various types of data while reduces computational and human burdens. We conduct two groups of image registration experiments on 3D volume datasets including image-to-atlas registration on brain MRI data"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the algorithm generalize the model A_{h, t} and what are the implications of this generalization on the overall performance?",
      "expected_output": "default",
      "context": [
        "psi \\rangle , [v])_{\\Gamma }Using the partition of \\Omega and Green's first identity on the left-hand side gives interior terms and boundary terms. The interior terms are as we want them. For the boundary terms, only the \\Gamma -terms remain since v|_{\\partial \\Omega } = 0. Combining terms with common boundary, using Lemma REF and the regularity of \\psi shows (REF ).Consider the domain partition and its corresponding broken Sobolev space presented in the premise of Lemma REF . We define the symmetric bilinear form \\mathcal {A} that generalizes the appearence of A_{h, t} defined by (REF ) to this setting by\\begin{split} \\mathcal {A}(w, v) := & \\sum _{i=1}^N (\\nabla w, \\nabla v)_{{\\Omega _i}} - (\\langle \\partial _{n} w \\rangle , [v])_{\\Gamma } - (\\langle \\partial _{n} v \\rangle , [w])_{\\Gamma } \\\\ &",
        " + \\langle A \\rangle [B] + (\\omega _- - \\omega _+)[A][B]Using the definitions and evaluating both sides shows the identity.Lemma 9.2 (Partial integration in broken Sobolev spaces) For d = 1, 2, or 3, let \\Omega \\subset \\mathbb {R}^d be a bounded domain and let \\Gamma \\subset \\Omega be a continuous manifold of codimension 1 that partitions \\Omega into the subdomains {\\Omega _1}, \\cdots , {\\Omega _N}. For \\psi \\in H^2(\\Omega ) and v \\in H^1_0({\\Omega _1}, \\cdots , {\\Omega _N}), we have that(-\\Delta \\psi , v)_{\\Omega } = \\sum _{i=1}^N(\\nabla \\psi , \\nabla v)_{{\\Omega _i}} - (\\langle \\partial _n \\psi \\rangle , [v])_{\\Gamma }Using the partition of \\Omega and Green's first",
        " \\psi )_- on {\\Omega _O} for a non-discrete function such as \\psi . Using this, we combine \\mathcal {A} with Lemma REF to get the following corollary:Corollary 9.1 (Partial integration in broken Sobolev spaces with bilinear forms \\mathcal {A}) For d = 1, 2, or 3, let \\Omega \\subset \\mathbb {R}^d be a bounded domain and let \\Gamma \\subset \\Omega be a continuous manifold of codimension 1 that partitions \\Omega into the subdomains {\\Omega _1}, \\cdots , {\\Omega _N}. For this setting, let the symmetric bilinear form \\mathcal {A} be defined by (REF ). For \\psi \\in H^2(\\Omega ) and v \\in H^1_0({\\Omega _1}, \\cdots , {\\Omega _N}), we have that(-\\Delta \\psi , v)_{\\Omega } = \\mathcal {A}(\\psi , v)Lemma 9.3 (A scaled trace inequality"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the dual formulation function and how is its root computed?",
      "expected_output": "The dual formulation function is defined by a positive linear combination of scaled versions of a given function. Its root is computed by solving a nonlinear equation on a specified interval. The function has a single inflection point and is strictly decreasing, with its properties derived from observations and calculations. For example, setting a specific value for a variable yields a particular equation, from which the function's behavior can be understood. The root computation involves finding the solution to this nonlinear equation, which can be achieved through further analysis and calculations, as demonstrated in the given formulas and equations.",
      "context": [
        " and {{formula:6a79dc23-697c-48b9-98ca-dd9f0dd3d714}} . The function {{formula:54fbae2a-1279-4c5c-89ea-b30b16b135e6}} will play a crucial rule, and to gain a better understanding of its properties, we make the following observations. Set {{formula:d84241e6-0583-445f-a213-b5c1ba7c5aef}} to obtain {{formula:e9570683-867c-4762-9764-2a43640fa391}}from which it follows that {{formula:f0624ff2-9dfe-46d7-b2f6-e31bd7b39ed2}} . These calculations show that {{formula:8f331474-f913-41e8-814f-f4a8553b57ed}} is a strictly decreasing function with a single inflection point at {{formula:57140151-b5a0-4abf-8e96-92015b35701",
        " . A starting point {{formula:4435ba85-ec07-438c-a8f7-baf9062da95c}} is therefore given by {{formula:b4cd678a-3016-4c7f-868d-fb930b1c28aa}}Although it is not the focus of this example, we conclude by briefly mentioning the function value computation of {{formula:1d0eb7f2-041c-4300-a457-e8f3a8ebf0ef}} . From {{formula:332b2414-d0e8-4e4d-a549-b9f94d33b298}} , we obtain {{formula:4c74aaa8-2a3c-485a-89b2-2cef8d369cc9}} , so that {{formula:dd16e302-c0e0-4627-b569-83bc9584cb08}} is the solution of a nonlinear equation on {{formula:a6037649-0cc6-4ba3-beef-81fe94bd88e9}} . Setting",
        "427d1dd4e13}}We are now ready to state the problem to be solved, which is to compute the root of the function {{formula:4273c2bc-17cc-4b10-ba71-26b2f4250fb2}} , originating from the dual formulation of the problem, defined by {{formula:f7a8477d-037e-4159-adfb-e6fb225134ee}}where {{formula:977e7549-d0d4-418a-a754-7a33cff2e051}} , on the interval {{formula:25642745-9bc0-41c6-845d-ab959646202d}} , with {{formula:58105aa8-e19e-4bd4-9f80-e65ac66338a1}} . Because it is a positive linear combination of scaled versions of {{formula:3f48d0d7-fc7c-4bca-b164-7af7eab8aa60}} , the properties of {{formula:f86634db-c438-48e6"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the intersection of curves and pseudo-discs in wedges, and how do the methods used in this research impact the key results?",
      "expected_output": "The main findings of the research paper indicate that in each wedge, every line in a specific set intersects a pseudo-disc at least twice, leading to a contradiction. The methods used involve analyzing the connected components of the wedge divided by the pseudo-disc and examining the intersection of curves. For example, the pseudo-disc divides the remainder of the wedge into two connected components, and the points in the boundary of a specific set are connected by two curves that intersect the pseudo-disc at least twice. This leads to the conclusion that certain lines cannot intersect specific components of the wedge, as it would result in crossing a pseudo-disc four times.",
      "context": [
        "97b4f4}} divides the remainder of the wedge into two connected components – the part `above' it and the part `below' it. Now, consider the points {{formula:f03a91ce-79a3-470e-a39e-dcb2a3914cc3}} . In the boundary of {{formula:2103520c-44c8-40e1-938a-ffbc4850a98d}} , these points are connected by two curves. As these points are in different connected components w.r.t. {{formula:daf28ef9-b1bd-4f0c-8b11-29c0775f9678}} , each of these curves intersects {{formula:9dafade2-cbdc-4eba-b33a-9083342cc1b3}} at least twice, which means that {{formula:42dbb8f5-7615-4a06-b719-d2c5f030ee1c}} intersect at least 4 times, a contradiction.Second, we claim that in each wedge, every line in {{formula:",
        " The pseudo-disc {{formula:ccead554-dadd-49bd-89a5-96d7aa97b4f4}} divides the remainder of the wedge into two connected components – the part `above' it and the part `below' it. Now, consider the points {{formula:f03a91ce-79a3-470e-a39e-dcb2a3914cc3}} . In the boundary of {{formula:2103520c-44c8-40e1-938a-ffbc4850a98d}} , these points are connected by two curves. As these points are in different connected components w.r.t. {{formula:daf28ef9-b1bd-4f0c-8b11-29c0775f9678}} , each of these curves intersects {{formula:9dafade2-cbdc-4eba-b33a-9083342cc1b3}} at least twice, which means that {{formula:42dbb8f5-7615-4a06-b719-d2c5f030ee1",
        "-d284a0acc1af}} as well. Like above, {{formula:8476cbce-32c7-4d65-8db8-bdf33510b992}} divides the wedge (without it) into two connected components. By the same argument as above, {{formula:dae020a5-200e-4681-af58-191b222e10c0}} cannot intersect the component below {{formula:1079992c-73a6-406a-84a0-e4ee05f41ad7}} (as otherwise, it would cross {{formula:bc0ad0d3-7e00-4a32-b0d6-fb940ed838a8}} four times). Similarly, {{formula:af6850f6-775b-4a06-a037-0053343d33b2}} cannot intersect the component above {{formula:62630730-9684-4179-97c2-6874d702bcf9}} . Thus, either {{formula:2d860e49-4462-446b-b1f7-9f5361"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings and methods used in the research paper regarding LoRA, Compacter, and PEFT, and how do these models relate to each other?",
      "expected_output": "The research paper explores the use of LoRA and Compacter, two Parameter-Efficient Fine-Tuning (PEFT) methods, in combination with fixed-parameter models such as CodeT5 and CodeLlama. LoRA involves decomposing weight matrices into low-rank matrices, while Compacter uses a different approach to achieve efficient fine-tuning. The paper aims to study the performance of these PEFT methods, with results to be published later. The models used include T5, CodeT5, Llama 2, and CodeLlama-base, with experiments focused on comparing the performance of PEFT methods within the same base models.",
      "context": [
        " configurations in this study. We use HuggingFace's peft library and AdapterHub platform to facilitate composing all model configurations. Throughout the experimentation process, all model configuration undergoes fine-tuning with a learning rate of 5e - 5 for a total of 50 epochs for smaller datasets (i.e., CoNaLa and R dataset) and 10 epochs for CodeSearchNet, with early stopping mechanisms in place to ensure optimal performance.Our approach aligns with original papers , , following the established guidelines for setting hyperparameters specific to these PEFT methods. For LoRA, we maintain a dimension rank of r = 4, while for Compacter, we use a value of n = 4. Despite the considerable contrast in parameter budgets between the models, we opt to employ identical PEFT hyperparameters due to practical limitations.\nResults: This paper is a Registered Report and results will be published at a later stage. We will include tables and compare the results with plots if needed. We will also report on the number of parameters needed when fine-tuning the models with and without PEFT methods, in addition to the required training time in each approach. For all results, the percentage of the improvement and/or time reduction will be included.\nDisc",
        " and experiment with them more easily. Compared to Llama, Llama 2 was trained on 40% more training data and allows for a larger context length. Choosing Llama2 over Llama provides us with a fairer comparison in RQ2, given that CodeLlama is based on Llama2.We will employ T5 and the base version of CodeT5 with 220 million parameters and the 7 billion parameters Llama 2 and CodeLlama-base in all of our experiments. It is worth mentioning that our experiments are mainly focused on the comparison of the performance of PEFT methods within the same base models, not across different based models with varied parameter budgets, hence, the difference in the number of parameters among the models is not a threat to the validity of our experiments. Our implementation of the base models involves initializing model implementations similar to HuggingFace's for T5 and Llama 2 by publicly available CodeT5 and CodeLlama checkpoints.\nPEFT Methods: In our experiments, we will employ LoRA and Compacter in combination with the fixed-parameter models, CodeT5 and CodeLlama (and T5 and Llama 2 for RQ2), across all experiments. We choose to incorporate LoRA",
        " for fine-tuning is scarce.PEFT encompasses several methods, such as inserting small trainable feed-forward layers into a fixed pre-trained Transformer-based model , only updating the bias parameters while freezing the rest of a model's parameters , decomposing weight matrices into approximate low-rank matrices , introducing Kronecker multiplication with weight matrices and utilizing low-rank matrices , concatenating embedding weight matrices to fixed models , , and adding efficient element-wise multiplication to the feed-forward and attention layers of models to rescale weight matrices . In this work, we will study LoRA and Compacter . Though LoRA was previously experimented with within SE, the value of Compacter is not well-known for SE tasks.LoRA. Proposed by , LoRA draws inspiration from IntrinsicSAID and introduces a more straightforward method for executing low-rank fine-tuning. In LoRA, the parameter update process for a weight matrix \\delta W involves decomposing it into a product of two low-rank matrices W_A and W_A:\\begin{aligned}\\delta W = W_A W_B \\end{aligned}Where W_A \\in \\mathbb {"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of this research paper regarding the impact of formatting errors on paper submissions, and what methods were used to analyze this effect?",
      "expected_output": "The research paper does not specifically investigate the impact of formatting errors on paper submissions. However, it does provide a list of common violations of final version rules that have resulted in papers being returned to authors for corrections, such as incorrect parentheses for in-text citations, incorrect capitalization, and missing periods after middle initials. The paper also provides advice on best scholarly practices for referencing URLs and citing arXiv preprints, including the use of version numbers and listing all authors. The methods used to analyze the effect of formatting errors are not explicitly stated in the provided context, but the paper does discuss the importance of following proper formatting guidelines to avoid publication delays.",
      "context": [
        "TeX or BibLateX are invoked if the hyperref package is used and the doi field is employed in the corresponding bib entries. The DOI itself will not be separately printed out in that case. Include the version number when citing arXiv preprints, even if only one version exists at the time of writing. For example,Bibtex entries for and corresponding to the depicted output can be found in the supplied sample file tacl.bib. We also cite the peer-reviewed versions , , as required. note the “v1” in the following. Joshua Goodman. 2001. A bit of progress in language modeling. CoRR, cs.CL/0108005v1. An alternative format is: Rebecca Hwa. 1999. Supervised grammar induction using training data with limited constituent information. cs.CL/9905001. Version 1.\nAppendices: Appendices, if any, directly follow the text and the references. Recall from Section that appendices count towards the page limit.\nIncluding acknowledgments: Acknowledgments appear immediately before the references. Do not number this section.In , one can use \\section* instead of \\section. If you found the reviewers' or Action Editor's comments helpful,",
        "# arXiv Paper: 2209.03392\n\nInvestigating Reasons for Disagreement in Natural Language Inference\nabstract: We investigate how disagreement in natural language inference (NLI) annotation arises. We developed a taxonomy of disagreement sources with 10 categories spanning 3 high-level classes. We found that some disagreements are due to uncertainty in the sentence meaning, others to annotator biases and task artifacts, leading to different interpretations of the label distribution. We explore two modeling approaches for detecting items with potential disagreement: a 4-way classification with a \"Complicated\" label in addition to the three standard NLI labels, and a multilabel classification approach. We found that the multilabel classification is more expressive and gives better recall of the possible interpretations in the data.\nCourtesy warning: Common violations of final version rules that have\nresulted in papers being returned to authors for corrections: Avoid publication delays by avoiding these.Violation: incorrect parentheses for in-text citations. See §REF and Table REF . Violation: URLs that, when clicked, yield an error such as a 404 or go to the wrong page. Advice: best scholarly practice for referencing URLS would be to also include the date last accessed. Violation: non-",
        ". Advice: best scholarly practice for referencing URLS would be to also include the date last accessed. Violation: non-fulfillment of promise from submission to provide access instructions (such as a URL) for code or data. Violation: References incorrectly formatted (see §REF ). Specifically: Violation: initials instead of full first/given names in references. Violation: missing periods after middle initials. Violation: incorrect capitalization. For example, change “lstm” to LSTM and “glove” to GloVe. Advice: if using BibTex, apply curly braces within the title field to preserve intended capitalization. Violation: using “et al.” in a reference instead of listing all authors of a work. Advice: List all authors and check accents on author names even when dozens of authors are involved. Violation: not giving a complete arXiv citation number. Advice: best scholarly practice would be to give not only the full arXiv number, but also the version number, even if one is citing version 1. Violation: not citing an existing peer-reviewed version in addition to or instead of a preprints Advice: When preparing the camera-ready, perform an additional"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do limited datasets impact traffic prediction model accuracy, and what methods can be used to improve model performance with restricted data?",
      "expected_output": "Limited datasets impact traffic prediction model accuracy by leading to overfitting and degrading the generalization ability of the model. To improve model performance with restricted data, methods such as using pretrained Large Language Models (LLMs) like GPT, GLM, and LLaMA can be employed. These models can be fine-tuned with minimal data to adapt to downstream tasks like traffic prediction, data imputation, and incident identification, thereby improving accuracy and resilience to overfitting.",
      "context": [
        " good accuracy. This reliance on extensive datasets presents a challenge when dealing with limited historical traffic data scenarios.\nPretrained LLMs: In recent years, propelled by the evolution of computational devices and the emergence of vast text corpora, a multitude of Transformer-based pretrained LLMs have demonstrated remarkable capabilities in tackling diverse natural language processing tasks . Researchers found that model performance could be improved by stacking modules, so they further investigated the scaling effect by increasing the parameter scales to larger size. When the parameter size exceeds a certain level, these language models not only achieve significant performance improvements, but also exhibit some unique capabilities that are not available in small-scale models, such as the few-shot learning capability .OpenAI proposed Generative Pre-Trained Transformer (GPT) based on the Transformer decoder, which shows a strong capability by training on a large amount of corpus data. The basic principle of GPT is to compress various types of knowledge into a decoder-only Transformer model through linguistic modeling so that the knowledge can be memorized and act as a general-purpose task solver. Based on this idea, GPT-2 , GPT-3 , and GPT-4 with increasing number of parameters have been gradually released, and they",
        "-2 , GPT-3 , and GPT-4 with increasing number of parameters have been gradually released, and they not only perform well in a variety of NLP tasks, but also show very good performance on some specialized tasks that require domain adaptability. General Language Model (GLM) is the first open-source LLM optimized for Chinese-English bilingual training. The main architecture of GLM consists of a stack of Transformer decoders which apply an autoregressive blank infilling technique for self-supervised training for a wide range of tasks. Large Language Model Meta AI (LLaMA) is an open and efficient base LLM released by Meta AI. LLaMA normalizes the inputs of each Transformer layer instead of the outputs, and removes the absolute position embedding, replacing it with the rotational position embedding at each layer. Bidirectional Encoder Representations from Transformers (BERT) is a language model based on Transformer encoders. It cannot perform exact generation but can reconstruct the original data using bi-directional contextual information and is therefore commonly used for content understanding tasks.We believe that the nature of the time series prediction task is a generative task on the predicted sequences. Therefore, generative LLMs",
        " comprehensive traffic spatio-temporal dataset in most regions. This constraint thereby limits the widespread application of certain data-driven traffic prediction models. Furthermore, these models are often trained only for specific tasks, which may lead to overfitting and thereby degrade the generalization ability of the model. In summary, it is still challenging to develop models that are resilient to overfitting and capable of delivering accurate predictions in areas with limited historical traffic data.Pretrained Large Language Models (LLMs), which have rapidly emerged in recent years, offer a promising solution to the aforementioned challenge. Pretrained LLMs are deep learning models trained on large-scale high-quality generalized datasets to capture universal patterns and information. LLMs are widely recognized for generative tasks due to their capabilities of powerful few-shot learning and cross-modality knowledge transfer . Endowed with an extensive array of parameters and a wealth of pre-existing knowledge, LLMs have found applications across a diverse range of domains, notably including transportation. These models exhibit remarkable potential for swift adaptation to a variety of downstream tasks, such as traffic prediction, data imputation, and incident identification. This adaptability is facilitated through the process of fine-tuning, which requires only minimal data to significantly extend the models' capabilities, as depicted"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research paper regarding the mono-energetic equation's impact on neutron transport, and how do the methods used support these findings?",
      "expected_output": "The research paper discusses the mono-energetic neutral-particle transport equation and its impact on neutron transport. The main findings include the equation's fundamental role in models of neutral particle transport and its prerequisite for solving more complicated physical models. The methods used, such as the nonsymmetric AMG method and approximate ideal restriction, support these findings by providing a means to solve the equation efficiently. For example, the crooked pipe problem demonstrates the application of these methods in a steady-state test problem for thermal radiative transport with isotropic scattering.",
      "context": [
        " however, it remains a computationally expensive and challenging problem. The simplified steady state, mono-energetic neutral-particle transport equation for the spatially and angularly dependent angular flux, {{formula:ae31242d-6d1e-481e-9b44-8797a75d284e}} , is given by {{formula:90513d35-a18a-4540-a21b-e67d946f4fcd}}Here, {{formula:a99986b1-ccff-4ee3-9eb7-483ba61b142c}} are the spatially dependent scattering and total cross sections, respectively, and we assume isotropic scattering for simplicity. This equation is fundamental to models of neutral particle transport, such as neutron or photon transport, and also a prerequisite for solving more complicated physical models such as thermal radiative transfer. The left-hand side of (REF ) is a 3D advection-reaction equation in direction {{formula:a4f055ee-684a-412f-b8b8-36964dcf435b}} and the right-hand side an integral-operator coupling over direction. More complex physical",
        "44ee-9711-2a9f58387ac4}} , {{formula:c44fbd0a-2ddc-4635-abfc-286b87f52542}} , and the off-diagonal operators {{formula:82358bf8-a125-49aa-91b7-5b6dbb754ae2}} and {{formula:66edc75c-57cc-4d49-8a8d-363a66e2a15e}} correspond to scattering and quadrature weights, respectively.One of the standard approaches to solve (REF ) in transport simulations is to update the scalar flux based on the current angular flux, {{formula:8a91f950-3121-4360-92a8-c8a012ee1e7e}}for all directions {{formula:8521f043-3ebb-488c-b84a-6e1347f88498}} . With updated scalar flux for each direction, the angular flux is then updated by summing the scalar flux over quadrature weights {{formula:29ae3e59-2db4-4512",
        " not present unaccelerated (fixed-point) results. The code does not have a traditional parallel sweeping implementation, but angular flux problems are solved using the nonsymmetric AMG method based on approximate ideal restriction (AIR) , in the hypre library , as studied in .\nThe crooked pipe problem: The first problem we consider is the so-called crooked-pipe problem, originally introduced in and discussed as a benchmark for DSA in . This is a steady state test problem for thermal radiative transport with a single energy group, and purely isotropic scattering throughout the domain. The domain is surrounded by vacuum, has a uniform isotropic radiation field, and has an inward isotropic source {{formula:b165a88c-287f-47f9-8ef3-6864a3e789f3}} times stronger than the radiation field. In , the problem is introduced with two scattering cross sections. Here we modify the domain to have five regions, shown in Figure REF , to allow for a larger variety of heterogeneities. Scattering cross section {{formula:795d57ed-9ad0-4900-a9dd-c09eb124ad12}} is defined to be piecewise"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the key properties of 0-homogeneous functions as discussed in Proposition 2.4 and how do they relate to the main findings of the research paper?",
      "expected_output": "According to Proposition 2.4, a 0-homogeneous function is smooth, has mean value zero on the unit sphere, and can be decomposed into a singular part and a Cantor part. The singular part is related to a countably rectifiable Borel set and is singular with respect to the Lebesgue measure. For example, the function verifies the constraint up to a negligible set, which is obtained by slicing and using notations for one-dimensional sections of BD functions. This proposition is crucial in understanding the main findings of the research paper, which discusses the properties of 0-homogeneous functions and their applications.",
      "context": [
        "-41eb-a1a8-949c0c844b26}} , {{formula:f7ea066b-5c34-4fb6-b420-2be991b964b1}} is the singular part, and {{formula:8b6f3f14-14e1-4d29-b011-9eb134f9debc}} is the Cantor part and vanishes on the Borel sets that are {{formula:eb3f26e3-696f-4c64-a7c7-38e5831f8eec}} -finite with respect to {{formula:8c6d145e-3d49-411a-9856-1058d72c0bb8}} (see ).Hereinafter we will use the following Proposition proved in .Proposition 2.4 Let {{formula:0958b2cd-9b02-4d9f-b6d0-805bb2e220c4}} be a 0-homogeneous function, smooth and with mean value zero on the unit sphere {{formula:b8d0b757-2809-49",
        "00-3257-4800-a447-71e97c027900}} of {{formula:674b10cc-f62b-45ec-b26c-d22585f3a664}} is a countably {{formula:e318ea5a-a0bc-4e61-95e3-a0871137f292}} -rectifiable Borel set and the following decomposition of the measure {{formula:29db33df-c325-4496-91d5-4f47c2b04543}} holds {{formula:8e14d736-d3df-4609-a438-46aaa1dbce64}}where {{formula:4b15d1b5-ce4b-4c96-92d9-da1b7adabe80}} , {{formula:dd2fb77f-b651-4e42-a61e-d25337216da0}} , {{formula:f9a7996a-212d-40c7-afe6-bee5d8cea97e}} and {{formula:400102d7-",
        "ula:fa832cea-3d57-4201-a730-a751711b6d7b}} . Therefore {{formula:6fdff934-451a-4d12-836f-d25d11817bdc}} is singular with respect to the Lebesgue measure. So, by Theorem REF , we have that {{formula:df7e1447-2fd8-4434-8229-3952a881119c}}Now let us prove that {{formula:4a55626a-4447-4368-ba06-73cc94a2748e}} verifies the constraint {{formula:6c5560a8-86f6-4a29-9668-2637cd4fe53a}} up to a {{formula:d5bd541a-e6d5-45a4-8726-2268c60acf5a}} -negligible set. This is obtained by slicing. To this aim we recall the notations for one-dimensional sections of BD functions.Given {{formula:cd821bd3-3dee-4f"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how disorder strength affects the transport properties of nanowires in this research paper?",
      "expected_output": "The research paper finds that the disorder strength significantly affects the transport properties of nanowires. For weak disorder, the Majorana zero-bias peak (ZBCP) is robust and immune to disorder. However, as the disorder strength increases to intermediate levels, the finite nanowire exhibits intricate behaviors highly dependent on details such as disorder configurations and wire length. Specifically, the ZBCP can appear in the local conductance spectrum below the putative topological quantum phase transition (TQPT) at certain voltages. The paper also notes that strong disorder can destroy the topological phase and the resulting ZBCP.",
      "context": [
        ", the SOC strength \\alpha is 0.5 eVÅ, the parent SC gap \\Delta _0 is 0.2 meV, the effective coupling between SC and SM \\gamma is 0.2 meV, the disorder strength \\sigma is 0.3 meV for weak disorder, 2.5 - 5 meV for intermediate disorder, and 10-30 meV for strong disorder, T=0, and L varies from 0.5 microns to 10 microns as shown in our results. With these choices, the pristine TQPT is at V_{\\text{Z}c} = 1.41 meV. We assume one subband occupancy in the nanowire. We note that our choice for the disorder strength in our short-range model is consistent with the corresponding long-range Coulomb disorder choice in Ref. because our disorder of \\sim 3 meV for our discretization scale of 10 nm in Eq. (REF ) corresponds roughly to the correlated disorder of 0.5 meV used in Ref. . We calculate the transport properties using Blonder-Tinkham-Klapwijk formalism , which constructs the S matrix for the scattering process of an incoming electron at energy E at each",
        " to \\mu so that the corresponding term in Eq. (REF ) becomes \\mu - V(x). The uncorrelated nature of our choice for disorder is somewhat artificial since a natural correlation length is imposed by the lattice cutoff (10 nm here) we introduce in diagonalizing Eq. (REF ). The typical lattice cutoff is of the order of 10-30 nm, which is approximately consistent with the microscopic disorder estimates in SM-SC systems , . One advantage of our model is that the problem depends on only one disorder parameter, strength, allowing a better characterization of disorder effects. One should think of Eq. (REF ) simply as a model of disorder, with values of the disorder strength \\sigma chosen to correspond approximately to the disorder in SM-SC nanowires, but we make no efforts to compare with the experimental data, instead focusing on how changing disorder from weak to strong modifies the theoretical results for a qualitative understanding.The parameters chosen for our calculations corresponding approximately to experimental systems are as follows: the effective mass m is 0.015~m_e, the chemical potential \\mu is 1 meV, the SOC strength \\alpha is 0.5 eVÅ, the parent SC gap \\Delta _0 is 0",
        "3 meV) is larger than the pristine induced gap (0.2 meV) at zero magnetic field. This is the robustness or immunity expected for a topological system.\nIntermediate disorder: Although MZMs are robust in the presence of weak disorder, the situation becomes more complex as the strength of disorder increases. In this section, we focus on the intermediate level of disorder to show that the finite nanowire exhibits intricate behaviors that are highly dependent on details such as disorder configurations and wire length. Specifically, we choose two distinct disorder configurations (Figs. REF - REF and Figs. REF - REF ), varying the wire length from extremely short (L= 0.5 microns) to very long (L= 10 microns). We believe that the Microsoft samples are in the intermediate disorder regime, but we do not know the experimental disorder configuration and strength, making it difficult to say anything definitive, quantitatively.We start with the extremely short wire (L=0.5 micron) as shown in Fig. REF . We find that a ZBCP, which may even be correlated from both ends, can appear in the local conductance spectrum below the putative TQPT at V_\\text{Z"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods do metasurfaces use to split polarized light and what are the key findings related to this phenomenon?",
      "expected_output": "Metasurfaces use a subwavelength rectangular lattice of cylindrical silicon Mie resonators to split polarized light. The key finding is that the polarization splitting arises from the anisotropic permittivity and permeability of the metasurface due to the two-fold rotational symmetry of the rectangular unit cell, allowing for efficient reflection of one polarization while transmitting the other with high polarization efficiency, low loss, and low profile.",
      "context": [
        "ics Laboratory, SRI International, Menlo Park, California 94025, United States You Zhou Interdisciplinary Materials Science Program, Vanderbilt University, Nashville, Tennessee 37212, United States Zhi Gang Yu Institute for Shock Physics, Washington State University, Pullman, Washington 99164, United States Ivan I. Kravchenckou Dayrl P. Briggs Center for Nanophase Materials Sciences, Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, United States Parikshit Moitra Optoelectronics Research Centre, University of Southampton, Southampton SO17 1BJ, United Kingdom Srini Krishnamurthy Applied Optics Laboratory, SRI International, Menlo Park, California 94025, United States Jason Valentine Department of Mechanical Engineering, Vanderbilt University, Nashville, Tennessee 37212, United StatesPolarization beam splitters, devices that separate the two orthogonal polarizations of light into different propagation directions, are one of the most ubiquitous optical elements. However, traditionally polarization splitters rely on bulky optical materials, while emerging optoelectronic and photonic circuits require compact, chip-scale polarization splitters. Here we show that a subwavelength rectangular lattice of cylindrical silicon Mie resonators functions as a polarization splitter",
        "# arXiv Paper: 1610.04040\n\nMetasurface polarization splitter\nabstract: Polarization beam splitters, devices that separate the two orthogonal polarizations of light into different propagation directions, are one of the most ubiquitous optical elements. However, traditionally polarization splitters rely on bulky optical materials, while emerging optoelectronic and photonic circuits require compact, chip-scale polarization splitters. Here we show that a subwavelength rectangular lattice of cylindrical silicon Mie resonators functions as a polarization splitter, efficiently reflecting one polarization while transmitting the other. We show that the polarization splitting arises from the anisotropic permittivity and permeability of the metasurface due to the two-fold rotational symmetry of the rectangular unit cell. The high polarization efficiency, low loss, and low profile make these metasurface polarization splitters ideally suited for monolithic integration with optoelectronic and photonic circuits.\n: Metasurface polarization splitterBrian A. SlovickCorresponding author: brian.slovick@sri.com Applied Optics Laboratory, SRI International, Menlo Park, California 94025, United States You Zhou Interdisciplinary Materials Science Program",
        " we show that a subwavelength rectangular lattice of cylindrical silicon Mie resonators functions as a polarization splitter, efficiently reflecting one polarization while transmitting the other. We show that the polarization splitting arises from the anisotropic permittivity and permeability of the metasurface due to the two-fold rotational symmetry of the rectangular unit cell. The high polarization efficiency, low loss, and low profile make these metasurface polarization splitters ideally suited for monolithic integration with optoelectronic and photonic circuits.In general, polarizers can be divided into two types: absorptive polarizers and polarization beam splitters. Absorptive polarizers use wire grids, dichroic materials, or nanoparticle composites to absorb the rejected polarization. , Although they provide high degrees of polarization, because the rejected polarization is absorbed, both polarizations cannot be analyzed simultaneously. This makes them unsuitable for applications such as optical quantum computing, where polarization splitters are needed to produce quantum bits, or qubits, by dividing single circularly-polarized photons into a superposition of vertical and horizontal polarizations. , , , Alternatively, polarization beam splitters preserve the rejected polarization either by reflection or diffraction. Commercially available polarization beam"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between GAR, DPR, and BPR in the context of this research paper, and how do their methods and key results compare?",
      "expected_output": "The main differences between GAR, DPR, and BPR are their approaches to retrieval in open-domain QA. GAR expands the query by adding relevant answers, passage titles, and sentences, while DPR represents questions and passages as dense vectors. BPR integrates a learning-to-hash technique into DPR to represent the passage index using compact binary codes. Key results show that the proposed approach outperforms GAR by 3.1% and 2.9% on Top-5/Top-20 accuracy on the NQ dataset and outperforms DPR by 0.6 and 1.0 points on Top-100 accuracy on the two datasets, while requiring 96% less index storage space. For example, the paper states that 'we bridge this gap and outperform GAR by 3.1% and 2.9% on Top-5/Top-20 accuracy on the NQ dataset' and 'our approach outperforms DPR by 0.6 and 1.0 points on Top-100 accuracy on the two datasets'.",
      "context": [
        "erini BM25 to retrieve top-1000 candidate passages. All the retrieved documents are then re-ranked according to Eq. (REF ). Further details are included in the Appendix REF . {{table:94480320-816e-4402-b128-129798f1e5e5}}{{table:ee8e256e-4db4-4b3f-a25a-ecefcc906392}}\nRetriever: Retrieval in open-domain QA is traditionally implemented with sparse vector space model BM25 , based on exact term matching. DPR implements retrieval by representing questions and passages as dense vectors. GAR proposes to expand the query by adding relevant answers, the title of a passage and the sentence where the answer belongs. It also fuse the results from its own and from DPR (GAR+DPR). To make a fair comparison, we extend our generation target from the answer context only (Ours-single) to include both the answer and the passage title (Ours-multi). We also report the fusion results with DPR. SEAL use BART model to generate ngrams then map to full passage with FM index.\nReader: DPR employs an extractive reader model",
        " minimum score of the augmented query's top-1000 list as the default score for the missing passage from that augmented query. We then average the retrieval score for each passage in the pool according to Eq. (REF ) and re-sort the order of the fused passages. For fair comparison with GAR , we additionally fine-tune an answer generation model and a title generation model. We perform the same fusion steps above for all three generation models, and we linearly interpolate their fusion results by searching the best weighting on the development set using Bayesian Optimization .\nComparison with Memory Efficient DPR techniques: We compare our approach with other memory efficient neural retrieval models in Table REF . show that the DPR could be furthered compressed to trade accuracy off against speed and storage. However, the accuracy of DPR could drop significantly if compressed to the same storage level of the lexical index. BPR integrates a learning-to-hash technique into DPR to represent the passage index using compact binary codes. The index size of BPR is slightly smaller than ours approach, but we achieve higher retrieval accuracy on both two datasets. We also include DrBoost , a dense retrieval ensemble trained in stages. DrBoost outperforms ours approach on NQ dataset, while taking {{",
        "We evaluate our approach on two established benchmarks: Natural Questions and TriviaQA . Our baseline model GAR trails behind its dense retrieval counterpart DPR by a large margin when retrieving a small number of passages. We bridge this gap and outperform GAR by 3.1% and 2.9% on Top-5/Top-20 accuracy on the NQ dataset. Compared with DPR, our approach outperforms it by 0.6 and 1.0 points on Top-100 accuracy on the two datasets, while requiring 96% less index storage space. The accuracy can be further improved by 3.4% on the basis of DPR's performance when fusing the documents retrieved from DPR and our method together. Furthermore, our retrieval performance also successfully transfers to downstream question answering tasks, where our methods increase by 3.2% and 0.8% Exact Match score compared with the DPR and GAR retrieved documents.\nContextual Clue Sampling and Filtering: We employ a sequence-to-sequence model that takes the question as input and generates the contextual clues for the answer as target. As our model, we use BART-large , but note that the generator can be replaced with any other sequence-to-sequence model. Contextual clues are the sentences"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main properties of halo stars that provide insights into galaxy formation, and how do researchers typically study these properties to understand the formation and evolution of galaxies?",
      "expected_output": "The main properties of halo stars that provide insights into galaxy formation include their stellar abundances, kinematics, and star-to-star scatter in relative abundances. Researchers typically study these properties by combining spectroscopic studies of a large sample of halo stars with well-defined selection criteria, with investigations of stellar dynamics. For example, the [alpha/Fe] ratio, which compares the abundance of alpha elements like Mg, Si, and Ca to iron, can be used to distinguish between different stellar populations and provide clues about the formation history of the galaxy. By analyzing the scatter in [alpha/Fe] and other abundance ratios, researchers can gain insights into the supernovae, star-formation processes, and gas dynamics that occurred in the early galaxy.",
      "context": [
        " Halo stars occurred in smaller systems with different star formation rates. We verify the finding by Gratton et al. (2003b) that stars that do not participate in the rotation of the galactic disk show a lower mean and larger spread in [alpha/Fe] than stars participating in the general rotation. The latter stars also seem to show some correlation between [alpha/Fe] and rotation speed. We finally advocate that a spectroscopic study of a larger sample of halo stars with well-defined selection criteria is very important, in addition to the very considerable efforts that various groups have already made. (Abridged)\nIntroduction: One important method of studying the formation and early evolution of galaxies is to explore the properties of the remains of these early events in the Milky Way system. Since the pioneering work by Eggen, Lynden-Bell & Sandage (1962), investigations of stellar abundances and stellar dynamics have been combined in a great number of studies in attempts to reconstruct the formation history of the stellar populations. While the homogeneous collapse scenario of Eggen et al. predicted metallicity and age gradients related to the kinematics, the globular clusters were found to show a considerable spread in metal abundances, independent of their distance from the Galactic",
        "atics, the globular clusters were found to show a considerable spread in metal abundances, independent of their distance from the Galactic centre (Searle & Zinn, 1978). This instead suggested a scenario of galaxy formation where individual protogalactic systems evolved independently before merging into a larger system. There are a number of indications that the infall and accretion of material after the first collapse phase has played an important role in our Galaxy: the existence of high-latitude and high-velocity interstellar clouds, the relatively small proportion of metal-poor solar-type stars (“the G-dwarf problem\"), the large scatter in the age-metallicity relation (if this relation exists at all) for the Galactic disk, and the finding of the presently merging Sagittarius dwarf.More recently, important clues have come from abundance differences in different stellar populations. Thus, Gratton et al. (1996, 2000) and Fuhrmann (1998) have traced significant [{{formula:6cd2ef48-6d67-46fa-b94e-086523c519ca}} /Fe] ratio (i.e., e.g. Mg, Si, and Ca vs. Fe) differences between the",
        " /Fe], and a second population with small or retrograde orbital velocity with larger scatter in the latter respects. The star-to-star scatter in relative abundances, such as in [{{formula:09b5bf37-b176-404c-b80d-b73f92df3ef2}} /Fe] at a given [Fe/H] for halo stars may also be used to obtain information on the supernovae, star-formation processes and gas dynamics in the early Galaxy, since supernovae with different initial masses produce different relative amounts of the heavy elements (see Nissen et al. 1994, Karlsson & Gustafsson 2001, Carretta et al. 2002, Arnone et al. 2004, Cayrel et al. 2004).In order to study the star-to-star scatter, as well as the possible existence of fine structure in the halo-population and the transition to the thick disk population in terms of abundances and kinematics, the present project was initiated with observations more than a decade ago. About half of our programme stars have more recently been independently studied. Our results give checks or support of results obtained by others and add some knowledge of significance for the understanding of Halo"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How did seeing conditions impact spectroscopic observations in this study, and what methods were used to account for their effects on the results?",
      "expected_output": "The seeing conditions impacted spectroscopic observations by introducing variations in the measured kinematics and stellar population parameters. To account for these effects, the researchers excluded exposures taken in worse seeing conditions, such as the last exposure of the series for galaxy FS29, where the seeing changed from 0.6 arcsec to almost 1.8 arcsec FWHM. They also considered the effect of seeing on the measurement of the Line-of-Sight Velocity Distribution (LOSVD) and adjusted their analysis methods to fit the LOSVD and population parameters simultaneously, which helped to suppress the metallicity-velocity dispersion degeneracy.",
      "context": [
        ". Checking the ambient conditions from the ESO meteo monitoring, we found that these exposures were taken in worse seeing conditions than requested.Only for 3 galaxies did the derived kinematics and stellar population parameters differ significantly from frame to frame. In one case - FS29 - the seeing changed from 0.6 arcsec to almost 1.8 arcsec FWHM over the series of exposures. We excluded therefore the last exposure of the series, in order to preserve the high resolution in the co-added spectrum. For the other 10 galaxies the seeing variations were, although measurable, not significant.We also notice that many exposures (especially for FORS2 observations) were taken with a seeing smaller than the slit width. As shown by the different instrumental broadening measured on the twilight spectra (uniform lightening of the slit) and on the calibration stars, this has a significant effect on the measurement of the LOSVD. As we will see later, our analysis methods fits in the same time the LOSVD and the parameters of the population. In the present case, the velocity and velocity dispersion are likely biased and we will not discuss them. However, in order to suppress the metallicity – velocity dispersion degeneracy , the LOSVD will",
        "437d1444f3c8}} value.The spectroscopic observations were carried out during two ESO-VLT service mode runs. The group dEs were observed in April – May 2005 using FORS2 and the Fornax dEs were observed in December 2005 – January 2006 using FORS1. The setup of the observations is summarized in Table REF . Exposure times are listed in Table REF . During the observations, the seeing conditions varied from FWHM = 0.5 to 0.8 arcsec. We also observed three velocity and metallicity standards for each run; two of which belong to the Lick/IDS and the MILES stellar libraries . The data reduction, described in detail below, was performed using the ESO-MIDAS data reduction software package. {{table:f6f9c854-9882-446f-9973-8c1dd313ff71}}\nFORS2 observations: The calibration data include bias frames, dome flat-field frames, and wavelength calibration frames which were taken during the day before and after the observations. During twilight of each night, spectrophotometric standard stars were observed, using a 5 arcsec slit. For three nights we did not have",
        " discuss them. However, in order to suppress the metallicity – velocity dispersion degeneracy , the LOSVD will be free parameters of our analysis.\nExtraction of 1D spectra: For each object, we extract two 1D spectra integrated within simulated elliptical apertures with major axes equal to 0.5 arcsec and 1 effective radius, R{{formula:49865bdd-c5f8-44e9-8ee5-c14269c86d14}} .Assuming the slit width to be negligible, a spectrum at a distance {{formula:af1861bd-63fd-4aa7-aea9-52d1a6084b1a}} along the slit is supposed to be representative of the semi-ellipse with major axis {{formula:289bcacb-6500-4c45-809a-97e7ef2727be}} and with ellipticity {{formula:a8d73659-692e-4044-9ec8-492735f66a90}} (for which we take the galaxy's mean ellipticity). If each pixel measures {{formula:fcb0f26a-c8c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding power-law forms in the inertial and dissipation ranges, and how were these findings obtained?",
      "expected_output": "The main findings regarding power-law forms in the inertial and dissipation ranges are that the spectra show inertial-range power-law forms consistent with specific formulas, and two power-law scaling regions are observed: one in the dissipation range and one in the inertial range. These findings were obtained through simulations of a square cuboid domain with collocation points, and the results are presented in plots showing the power-law-scaling regions, such as the blue-shaded region for the inertial range and the cyan-shaded region for the dissipation range.",
      "context": [
        "ae5df35a1}} ) (Fig.REF (a)) and (b) a square cuboid simulation domain with {{formula:2ec64c91-2f5f-4bd6-8f88-bfe34c8e351c}} collocation points and {{formula:6f3c648b-0a1e-4245-9cb7-f20e63a8aaef}} (Fig.REF (b)). Note that {{formula:a7b63ff8-75ed-458f-b764-adfc8d0f744d}} is strongly suppressed relative to {{formula:0c4fb3cd-734f-4934-9d75-faaed6be2e4a}} ; furthermore, these spectra show the inertial-range (blue-shaded region) power-law forms that are consistent with {{formula:f3834af7-3426-41c9-847e-8d92dbc72a63}} and {{formula:6171ece4-3680-4e2f-98a3-9dae9a",
        "-eccf382554e3}} for different values of {{formula:437f80c2-23d5-4317-858a-e67b01d04c07}} and {{formula:1015114b-5856-4b6a-af73-b6a7acb33f3d}} for runs {{formula:08b7d1b8-3293-43bb-87a3-95727733b612}} , {{formula:cd5c4925-13d8-4427-a234-409bf29f0835}} , and {{formula:ba0213e6-e577-47ca-b83f-375fb954370e}} , respectively. These plots show two power-law scaling regions: in the dissipation range (cyan-shaded regions) {{formula:be10771f-e328-4491-b4b7-6e16a0ebb718}} ; in the inertial range (green- or red-shaded regions) {{formula:02a67459-0c98-461e-b07f-6",
        " plots of Figs.REF (a) and (b), we present for coflow ST, the CPDFs {{formula:06bc00c8-8a24-4f53-ac5d-7c482e9fb654}} and {{formula:1e8b0db9-3715-4eeb-8e1f-c4a3f1e461d0}} , respectively, where {{formula:6c396dba-aa0c-430a-9da8-352aa3b92212}} . Both these CPDFs show power-law-scaling regions: {{formula:99ab3412-abfd-4091-85b9-4c2fd0afdf7e}} , for {{formula:e1b8e14e-1440-44cd-8a83-d466545fce1e}} , with {{formula:e4da3714-6237-40f1-9930-fef233e72396}} , i.e., the PDF {{formula:8e25d338-c538-4cc9-9f"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on spin implications on Lie superalgebra reductions and how do the methods used impact the key results?",
      "expected_output": "The research paper explores the implications of spin on Lie superalgebra reductions, focusing on situations where the remaining generators have spin 0. The methods used involve looking up Lie superalgebras with even subalgebras of a specific form and considering the classical reduced algebras. The key results include the identification of extended superconformal algebras and the characterization of the reduction by a particular subalgebra of the original Lie algebra. For example, the paper mentions the well-known superconformal algebra and the affine Lie algebra, highlighting their roles in the reduction process. The conformal dimensions of the generators are also discussed, with the constant 1 being the conformal dimension of the WZNW currents of the unconstrained affine Lie algebra.",
      "context": [
        "formula:159c3c28-d4a6-4853-85ab-bd39781766d4}} . The generators of the {{formula:d674b896-d478-4346-8805-48b916f1f31b}} itself have spin 1 and indeed give rise as usual to the energy momentum tensor after reduction. We see that we are looking for situations where the remaining generators have spin {{formula:ba8f7e2b-5f8e-4b4f-94c2-799eef2cf4ac}} .It is easy from the known theory of Lie superalgebras to look up the ones having even subalgebras of the form {{formula:896c3f9d-28e7-4b80-8735-57ac5052cd2e}} . The result is given in table (REF ). Previously we have considered these in some detail after two of us considered the classical reduced algebras . {{table:72e831d1-de97-40d1-9cf1-af4132250be6}}The well known superconformal algebr",
        "6147e3a8d511}} , is a weight label of that representation, or indeed, as we shall see, an odd root of the Lie superalgebra; (iii) there are the generators of the affine Lie algebra {{formula:aec8252d-1d3a-424c-b486-9be517ffa5ba}} , with conformal dimension 1; (iv) all other generators are composites of the above.It follows that within the framework of hamiltonian reduction of Lie superalgebras which we are going to adopt, we need consider situations where a certain {{formula:fba065de-230e-4c7d-b91e-f46b2d67f090}} subalgebra is embedded so that the generators of the subalgebra, {{formula:46d73d36-6cdd-454c-a0d8-6025c8e00095}} , have spin 0. That immediately implies that we are looking for situations where there is a subalgebra of the form {{formula:159c3c28-d4a6-4853-85ab-bd39781766",
        " that directly. The reduction is characterized by a particular {{formula:9936c973-47aa-4819-9685-0dab6de1406e}} subalgebra of the original Lie algebra. The generators of the original Lie algebra occur as spin multiplets of that {{formula:305f5ae2-7dd4-47f3-a50b-169f9c04497f}} subalgebra with spins {{formula:98cc764d-003c-4145-b335-c0a35825cab2}} , say. Then the conformal dimensions of the generators of the ensuing {{formula:4c0b576a-f33e-42ed-a415-bab65902027b}} -algebra are simply the set of numbers, {{formula:b5e27573-1f51-4f7e-b145-d126f11e16ec}} , the constant 1, being the conformal dimension of the WZNW currents of the unconstrained affine lie-algebra.In our case we want to consider the set of extended superconformal algebras, which"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on charmonium-nucleon potential and its effect on nuclear-bound states?",
      "expected_output": "The research on charmonium-nucleon potential found that the potential is weakly attractive at short distances for both the ηc and J/ψ states. This attraction is crucial for exploring nuclear-bound charmonium states, such as the ηc-N or J/ψ-N bound state. The study used lattice QCD simulations to calculate the potential and found that it is indispensable for understanding the formation of charmonium bound to nuclei. For example, the potential between hadrons can be calculated from the equal-time Bethe-Salpeter amplitude through the effective Schrödinger equation, as demonstrated in the recent success of the ηc-N potential from lattice QCD.",
      "context": [
        "a017f3e54eef}} system is slightly stronger attractive than the {{formula:2e786db1-1615-4ec5-a6c2-e0f5a505a48f}} -{{formula:adb9bda9-9601-480d-a334-66583020e226}} system. This should be accounted for by the dynamics associated with the structure of quarkonia.In order to make a reliable prediction about the nuclear-bound charmonium, an important step in the future is clearly an extension to dynamical lattice QCD simulations in the lighter quark mass region. Such planning is now underway. Once we obtain a realistic potential, we will proceed exploring the nuclear-bound charmonium state with theoretical inputs of the charmonium-nucleon potential by using the exact few-body calculation.We would like to thank T. Hatsuda for helpful suggestions and fruitful discussions. We also thank to T. Misumi for his careful reading of the manuscript. T.K. is supported by Grant-in-Aid for the Japan Society for Promotion of Science (JSPS) Research Fellows (No. 22-7653). S.S. is supported",
        "2036-4939-be75-62c5be40c599}} is indispensable for exploring nuclear-bound charmonium states like the {{formula:045c4c77-791b-4646-ace6-7b93f05d31fa}} -{{formula:c28a3e06-35bf-4cda-9573-768c29bc50cb}} or {{formula:ec9982de-d6ba-4f2e-9300-df0fbefde552}} -{{formula:ee0899b0-5200-4071-9a52-5debf0965ca2}} bound state in few body calculations .We recall the recent great success of the {{formula:ff45ccdf-e6f4-44c7-8676-57d4be66fde1}} -{{formula:bf395f82-1ff2-40eb-a55c-7749371d93e2}} potential from lattice QCD . In this new approach, the potential between hadrons can be calculated from the equal-time Bethe-Salpeter (BS) amplitude through",
        "# arXiv Paper: 1009.3332\n\nCharmonium-nucleon potential from lattice QCD\nabstract: We present results for charmonium-nucleon potential $V_{c\\bar{c}-N}(r)$ from quenched lattice QCD, which is calculated from the equal-time Bethe-Salpeter amplitude through the effective Schr\\\"odinger equation. Detail information of the low-energy interaction between the nucleon and charmonia ($\\eta_c$ and $J/\\psi$) is indispensable for exploring the formation of charmonium bound to nuclei. Our simulations are performed at a lattice cutoff of $1/a\\approx 2.1$ GeV in a spatial volume of $(3 fm)^3$ with the nonperturbatively $O(a)$ improved Wilson action for the light quarks and a relativistic heavy quark action for the charm quark. We have found that the potential $V_{c\\bar{c}-N}(r)$ for either the $\\eta_c$ and $J/\\psi$ states is weakly attractive at short distances and"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do Cartan angles play in the context of REF, and how do they influence the main findings of this research paper?",
      "expected_output": "In the context of REF, Cartan angles play a crucial role in determining the partition function, which depends only on the values of these angles at the final point of all trajectories. The research paper highlights that there are two Cartan angles in the non-compact boson sector and one angle in the compact fermion sector, denoted as θ, φ, and ψ. These angles influence the main findings of the paper, as the first three moments of conductance can be expressed by taking derivatives of the partition function with respect to these angles. For example, the paper states that the partition function can be evaluated using the transfer-matrix technique, which involves mapping the path integral to the evolution operator of the corresponding quantum Hamiltonian in imaginary time. The result of this evolution is the heat kernel, which is governed by the Schrödinger equation with a Hamiltonian derived from the action. The Cartan angles are essential in this process, as they determine the values of the partition function and, subsequently, the moments of conductance.",
      "context": [
        " to choose a representation that brings {{formula:7cbef779-9a95-4626-a4aa-57d45638fd5b}} to explicitly diagonal form. In class D with {{formula:04e69db6-db36-4fdb-88a0-65d2d9242957}} replicas there are two Cartan angles in the non-compact boson sector and only one angle in the compact fermion sector. We will denote these angles as {{formula:c57f5e4f-909d-4811-bc64-b25b55b350a0}} , {{formula:8fbb00ed-0587-49be-9668-cc463428891e}} , and {{formula:55cc7af6-704c-4256-a043-503c778c532c}} . Partition function REF depends only on the values of these angles at the final point of all trajectories. Once the partition function is known we can readily express first three moments of conductance by the following derivatives : {{formula:dfc90a64-9ed8-477c-",
        " Cartan angles {{formula:aa346ce5-b879-480d-8a61-ca6389bbef17}} .\nTransfer-matrix approach: Evaluation of the partition function (REF ) can be accomplished within the transfer-matrix technique . It amounts to mapping the path integral to the evolution operator of the corresponding quantum Hamiltonian in the imaginary time {{formula:df71d253-dd3c-41ae-ab64-6b2ff4449616}} . We can thus equate the partition function to the resulting wave function after the evolution {{formula:53a41709-f92e-418d-a993-344b18e831fc}}The result of such an evolution is usually termed heat kernel. Changing of {{formula:1d7198dc-22c2-42cd-b101-87f916802d9a}} with {{formula:71b152ef-aaee-4208-97b6-c89bba11a4aa}} is governed by the Schrd̎inger equation with the Hamiltonian derived from the action (REF ), {{formula:7b47bf32-a94c-4464",
        " of conductance by the following derivatives : {{formula:dfc90a64-9ed8-477c-aa54-33111280bf39}}Here every derivative is taken at the point {{formula:360343e6-f986-4372-95eb-1bc72be5391d}} .Strictly speaking, in the evaluation of the partition function (REF ) we consider trajectories {{formula:98c29b16-ce66-4a94-ad55-006454ce85bc}} starting at {{formula:066d0074-5f47-49f7-af06-931159788da1}} and ending at {{formula:56384c41-fab4-44bf-af71-a42e786f3e70}} with certain non-zero values of the Cartan angles {{formula:a7f4fdc4-671b-48e2-bd11-3dd84a5aac36}} . Such trajectories are not closed and the WZW term in the action is not well-defined for them as explained above. This issue can be resolved in the following way. We augment"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings related to inequalities in this research paper and how do the authors apply them to their methods and results?",
      "expected_output": "The research paper applies several inequalities, including Hölder's inequality, Jensen's inequality, and the Cauchy-Schwarz inequality. Hölder's inequality is used to replace the sum with a sum of squares, while Jensen's inequality is used to prepare for symmetrization and move the expectation inside the square root. The Cauchy-Schwarz inequality is used to replace the L1 norm with an L2 norm. The authors also invoke the independence of Rademacher variables to eliminate cross terms and apply Proposition REF to get the final estimate. For example, the paper states that 'it follows that' certain estimates hold for all variables, and 'the independence of the Rademacher variables implies that the cross terms cancel'. These inequalities are crucial in deriving the final estimates and results in the paper.",
      "context": [
        "fcc9-43e0-be8a-5a1136199509}} We treat the second term in the same manner. Use Hölder's inequality to replace the sum with a sum of squares and invoke the independence of the Rademacher variables to eliminate cross terms: {{formula:ff1d8b19-e276-47b3-88c6-3c09237819d7}} Since {{formula:c6fb28b0-4d85-4f16-b396-c1777e61e9a9}} , it follows that {{formula:77bb4b72-8872-42b9-961a-c900d47978a1}} for all {{formula:aafd062b-b6ee-4e4a-a223-1d3ad31baf4e}} , and {{formula:721dd431-14eb-437e-bd64-a69f241e6355}} Introduce these estimates for {{formula:a4c713a9-c8bc-41b5-9893-c7d3eb8a0ca3",
        "-333bc5e0c6a2}} By the subadditivity of the maximum, {{formula:2f8ab3c1-b664-4a27-8d65-8887258591ec}} where we have invoked the fact that {{formula:76e2db34-749d-4e4f-911c-5cdb5b68804a}} has the Rademacher distribution. Apply Proposition REF to get the final estimate of {{formula:dd5c9c91-83d5-4b14-aa99-fa6a95f078cb}} : {{formula:35be5df6-54cb-4395-98e0-847d75ea646e}} Now consider the last term in (REF ). Use Jensen's inequality to prepare for symmetrization: {{formula:751abd15-2b4a-45d4-b953-7455dc97c040}} Apply Proposition REF to the expectation of the inner sum to see {{formula:1adc8f41-7809-4929-9ff7-9cf224fb1ecb}}",
        "-4d56-96b6-36bb2701e3c3}} is a positive diagonal matrix that satisfies {{formula:c07df7ec-e2d0-42de-90e6-34ae6b001fa4}} .Apply Theorem REF to get {{formula:56a539b9-cf79-4cc2-8612-6a3db737bcdb}} Expand the first term, and use Jensen's inequality to move the expectation with respect to the Rademacher variables inside the square root: {{formula:7327214b-4bc5-4812-86ae-e9ec0fb2ee38}} The independence of the Rademacher variables implies that the cross terms cancel, so {{formula:765751a2-0ba1-481c-91d3-34fa89ddde2e}} We use the Cauchy–Schwarz inequality to replace the {{formula:873ff1e8-5a13-41b7-ad18-9d0077619641}} norm with an {{formula:b606b43a-7744-4ba6-9"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the uniqueness of zero function solutions in this research paper, and how do the methods used contribute to these results?",
      "expected_output": "The research paper finds that the solution to the given equation is unique if a certain condition holds for every function. This uniqueness is proven by assuming the condition holds and then showing that it leads to a specific solution. The methods used, such as combining references and using specific formulas, contribute to these results by providing a step-by-step approach to proving the uniqueness of the solution. For example, the paper uses the formula 2a6d24aa-6572-46aa-b926-a7fe667e9deb to define the zero function and then applies other formulas to derive the uniqueness of the solution.",
      "context": [
        "-zero function {{formula:2a6d24aa-6572-46aa-b926-a7fe667e9deb}} satisfying {{formula:4419f3f1-f9b5-4f8d-a784-24d18f7655e5}} Moreover, it holds {{formula:7592156b-b5c1-4376-8271-e65629691dd2}} If we assume that for some {{formula:85cc8328-9f0a-4a89-84f5-5b8d918719ad}} {{formula:92ed7bdd-8cb6-453b-88de-864a4a906790}} holds for every function {{formula:22ef4c92-cc26-4261-b92e-98b992664b16}} , then the solution {{formula:ddce4a24-4654-461f-95e5-e526a635ac89}} of (REF ) is unique. Moreover, there exists a constant {{formula:c1a17930-3919-4645-ac1",
        "bda3-196a-4a1a-ab36-4a2a92c16282}} and {{formula:0828dcb1-34b9-41e7-a071-b60e00ee372e}} only. Combining (REF ) and (REF ) we get {{formula:592b2765-9e4c-4661-b9c8-5e2943fdb067}}This, together with (REF ) implies {{formula:942f2374-4f95-426c-8a3f-9ede02fc9915}} .Thus, it remains to prove that {{formula:868a574e-ebea-4e35-9e7f-d17b56d81fef}} holds for every function {{formula:2e95c66a-b26c-4637-b6d6-0c4e62d6ae94}} satisfying {{formula:19784c28-6a7c-4135-b061-31a196932a5d}} . For any partition {{formula:e3058d7b",
        "-4186-ba91-c37f2af5bfee}} satisfies (REF ). Further, assume that {{formula:f6f8652e-f175-41ad-8e36-2d720b0ba0f1}}holds for any {{formula:497d679f-01b1-4ae7-8358-bf8ca6589c9b}} . If there exist constants {{formula:84ad0c25-c579-4971-a2db-7d9d1366a859}} , {{formula:0d8985b7-7b46-4b1b-9724-6826cf0d9e08}} and {{formula:79b4128b-d186-452b-bd11-f04fc9214b46}} , such that for any function {{formula:7a6d43fd-2e99-4cb0-a8f0-5e3937cbbb0b}} with {{formula:531d4266-76fa-4a9b-ae88-53b2a792e61b}} we"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the effect of strain on orbital polarization in nickelates, and how do the researchers' methods contribute to these results?",
      "expected_output": "The main findings of the research paper indicate that strain significantly affects the orbital polarization in nickelates. Compressive strain increases the occupation of a specific orbital, while tensile strain does not reverse the polarization as expected from the Madelung picture. The researchers used first-principles density functional theory calculations to investigate the electronic structure and orbital occupation, paying special attention to the surface Ni state. Their methods allowed them to compare the electronic structure and orbital properties of thin films and heterostructures to their bulk counterparts, shedding new light on the interpretation of recent experiments.",
      "context": [
        " approach , . For example, both in film and in heterostructures of LNO, compressive strain make the {{formula:612d41e2-5f30-4617-8161-44b8b3b4c154}} orbital more occupied, as one would expect. However, tensile strain does not make the polarization go the other way which is in a clear contradiction to Madelung picture.In this paper, we investigate the electronic structure and orbital occupation by using first-principles density functional theory (DFT) calculations. Special attention has been paid to the surface Ni state as its contribution might be significant in thin film and in surface-sensitive measurements. The electronic structure and orbital property are compared to the results of heterostructure and bulk counterpart. Translational symmetry breaking along the direction perpendicular to the sample and the reduced coordination at the surface are found to strongly change the orbital polarization with respect to the bulk. The effects of strain are also studied. Our results shed new light on understanding the available experimental data. {{figure:a2aab6cc-04ab-4976-a56c-5b8b3cf96aae}}\nComputational Details: For the band-st",
        ": La, Pr, Nd,...etc.) that remains metallic at all temperatures , . As a result, thin film LNO and LNO-based heterostructures have been studied under various strained conditions to control the correlation strength and the metal-insulator phase transition , , , in an effort to find possible high temperature superconductivity and conductivity enhancement . Strain affects most strongly the orbital character of the nickelates , , , , . Recently, several papers have focused on how changes in orbital polarization in heterostructures and thin films affect the electronic structure and thereby the macroscopic material properties , , , , , , .An important issue is the relationship between strain and the orbital polarization in various structural classes of nickelates. For low-spin trivalent nickelates, the orbital polarization {{formula:3d83e44e-07ba-45f1-a942-9de3ff40688a}} can be defined simply as the difference of the two {{formula:024260e4-d0d0-4fbe-a431-2f5c281fc352}} orbital occupations (without renormalization factor) {{formula:b96a9c52-bdb8-",
        " the single layer case (Fig. REF (a)), the polarization of which is {{formula:9774ccdf-2624-4239-a3f3-23ee6a069e1d}} . In addition, it is noted that the {{formula:843bb5a8-5e79-4551-9c6c-15aad44043ae}} DOS at the Fermi level is reduced in Ni(2). Large DOS in Ni(1) as also observed in single-layer case may indicate the possible unstability of the surface and the other reactions such as chemisorptions or stoichiometric changes.Our results shed new light on the interpretation of recent experiments. It is found from XLD measurement that compressively strained LNO thin film has negative orbital polarization whereas the tensile strain does not produce the sign reversal in the polarization , which is in a contradiction to the simple electrostatic Madelung picture assuming the same energy levels both for {{formula:8f5ea15b-6003-439b-91b5-1e7bf8616a52}} and {{formula:2fe85df7-45e2-4294-b"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used in the research paper to demonstrate how encoding prevents degree loss in polynomial, and what are the main findings regarding its effectiveness?",
      "expected_output": "The research paper uses two methods to prevent degree loss in polynomial encoding: leaving the last d positions in the plaintext polynomial empty and using a key-switching key to transform the polynomial. The main findings are that these methods effectively prevent degree loss, allowing for accurate computation of cosine similarity between face vectors. For example, the paper mentions that the direct method of filling all coefficients slots in the plaintext polynomial can result in a loss of valid similarity, but the proposed encoding method avoids this issue.",
      "context": [
        "brace {a}_0,\\cdots ,{a}_{n_u-1}\\rbrace to CS. CS maintains an indicator ind, which tells DP the start vacant position in the last stored ciphertext. Rather than directly encrypting these vectors and sending them to CS, DP first encodes the data based on our proposed encoding method and then performs BFV encryption over the encoded data. When receiving the indicator ind from CS, DP divides its vectors into three parts. The first part contains \\delta -ind vectors where \\delta = \\lceil \\frac{N-d}{d} \\rceil represents the maximum number of face vectors that can be encoded into a polynomial. Since CS is allowed to merge the last ciphertext with a newly come one, DP organizes the first \\delta -ind vectors into a matrix {\\mathsf {A}}_0 \\in \\mathbb {Z}^{\\delta \\times d} by padding ind zero vectors before these real samples. This matrix is encrypted as ct_0. When CS receives ct_0, it can merge it to its last stored ciphertext ct_{la} by simply performing a homomorphic addition Eval.Add(ct",
        " merge it to its last stored ciphertext ct_{la} by simply performing a homomorphic addition Eval.Add(ct_{la},ct_0). The second part contains e \\delta vectors, and the last part contains f vectors where f < \\delta and n_u = \\delta -ind + e \\delta + f. DP encrypts matrices {\\mathsf {A}}_1, \\cdots , {\\mathsf {A}}_e in the second part separately using BFV and sends them to CS. Unlike the first part, for the last part, DP first pads \\delta -f zero vectors to the remaining vectors, then encrypts it as ct_{e+1} and sends it to CS. In the last, DP updates the indicator ind = \\delta -f and sends it to CS for further use.\nEvaluation Process: The evaluation process happens between a verifier and a CS. Specifically, as shown in Algorithm REF , given a face vector, the verifier first encodes it into a polynomial. Then the verifier encrypts the polynomial using the public key pk and sends it to CS. CS gets a key-switching key",
        "}} and then transforms it into the polynomial \\pi ({\\mathsf {A}}). Then DP encrypts this polynomial using BFV as ct_i and sends it to CS. The verifier encrypts the queried face vector {b} as ct and sends it to CS. The cosine similarity is computed by multiplying these two ciphertexts ct_i and ct, whose underlying plaintext polynomial is exactly \\pi ({\\mathsf {d}}). As mentioned, the plaintext space of BFV scheme is taken from R_t = \\mathbb {Z}_t/(x^N+1), which means the maximum degree of a plaintext polynomial is N. The direct method is we fill all the coefficients slots in the plaintext polynomial when considering encoding our face vectors database. However, this might result in a loss of valid similarity. The reason is that the valid value in the product will be dropped (module reduced to a position with a degree less than N) if its associated degree is greater than N, which means we will get the wrong distance between the last face vector in the matrix and the queried image. Our idea is to leave the last d positions in"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does MPDAG represent causal inference in the context of this research paper, and what are its key findings and methods?",
      "expected_output": "MPDAG represents causal inference by identifying the total effect of one variable on another through a set of rules and formulas, such as the truncated factorization formula. The key findings include the ability to identify causal effects from observational data under certain conditions. The methods involve using MPDAGs to represent causal relationships and applying formulas to estimate the effects, as seen in the research paper's discussion of the g-formula and manipulated density formula.",
      "context": [
        "-b6b7-17e7c81eb717}} for continuous treatments and {{formula:b78144aa-0184-413a-8731-ad8a48cea869}} for binary treatments; see, e.g., . For the common definitions, the total effect of {{formula:afc49f23-bc72-4d10-8877-f42a26a5053d}} on {{formula:0e87367f-14b2-4025-ac37-d596ce068c9a}} is identified in MPDAG {{formula:6317d8da-7d09-420c-92e7-76d905c30a5c}} if and only if {{formula:98619197-f35d-4066-b5a6-acacee5936bc}} can be identified from any observational density {{formula:19e3b159-5d45-4622-8371-8136ddbb6aeb}} consistent with {{formula:055b663f-fc6e-4a63-9319-173a8e0f0",
        "5168caea}} s represented by {{formula:9bfd1edc-3984-4ff7-baa4-e1aa29918fca}} . An MPDAG {{formula:99743092-da9b-44ae-89ea-a0e89964723a}} is said to be represented by another MPDAG {{formula:390ed437-3bee-4b0a-9f14-65fbef615b69}} if {{formula:afaf45fd-1dc6-4507-bf0d-20b69def6c86}} .Causal MPDAGs. An observational or interventional density is consistent with MPDAG {{formula:acc9a275-901d-4fc9-b4ac-42cc880a70ec}} if it is consistent with a DAG {{formula:0d0f8e89-932f-4481-b70b-7bf9d996c4ed}} in {{formula:ac4681cb-a22f-4942-a32e-320550823db8}} . An MPDAG",
        "79-abca-0838efabf7bc}} if there is an observational density {{formula:a53ed17d-8b03-442d-bda0-cb3845eb85df}} consistent with {{formula:76d56701-3f5d-42cd-b350-e3bf09f62ee8}} such that {{formula:044c8055-a1df-4b24-a39e-2cd29a534f91}}for values {{formula:a65b11e3-92c6-4a27-b671-f0069ada8a07}} of {{formula:c086e7db-cc2e-4d17-95cf-94202fdf5512}} that are consistent with {{formula:d12184c8-f5c8-4be8-be61-25dfd0c359b9}} . eq11 is known as the truncated factorization formula , manipulated density formula or the g-formula .A DAG {{formula:34e54514-e2d4-4211-a75b-bee13"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of an increased Ekman number on bifurcation types in this research paper?",
      "expected_output": "The main findings of the research paper indicate that by increasing the Ekman number at a fixed magnetic Prandtl number, the bifurcation diagrams transition from supercritical to subcritical and eventually to isola bifurcations. This means that as the Ekman number increases, the nature of the bifurcation changes, affecting the stability and behavior of the dynamo solutions. For example, at higher Ekman numbers, the paper observes a subcritical bifurcation, which may exhibit unusual features such as re-stabilization of the purely hydrodynamical state.",
      "context": [
        "aae4-85d369a80da0}}By increasing the Ekman number at fixed magnetic Prandtl number, we therefore successively described supercritical, subcritical and isola bifurcation diagrams. We further extended our study to {{formula:71343732-ac70-4753-aaba-930f802cca84}} and {{formula:2b10b360-bbce-4393-bee9-0c7228b22f7d}} (figure REF .b) and obtained a subcritical bifurcation.\nInterpretation: We have studied different bifurcations obtained for Ekman number values ranging from {{formula:270dbfa8-d43d-4bdb-bd83-9579e2d7129a}} to {{formula:703b1af9-fbb7-462c-883e-48d1b441c8c2}} , magnetic Prandtl number values from {{formula:37714286-8a80-419b-8e44-1c57abc162fe}} to 6 and Rayleigh number values from {{formula:24f276c9-",
        "1c57abc162fe}} to 6 and Rayleigh number values from {{formula:24f276c9-5807-49fc-802f-736aa14246f5}} to {{formula:1cb1dade-1033-4b19-a063-61563f8870f0}} . In this parameter regime, the evolution of the bifurcation diagram is summarised in figure REF . For a given Ekman number, a supercritical bifurcation (top plot) is obtained for a sufficiently high value of the magnetic Prandtl number. By decreasing its value, it is possible to obtain a subcritical bifurcation (middle plot), which may exhibit unusual features, such as re-stabilization of the purely hydrodynamical state. An unstable branch therefore must exist, it could be connected, for larger values of the Rayleigh number, to the stable branch corresponding to dynamo solutions. If the magnetic Prandtl number is further decreased, the range of Rayleigh number for which the non dynamo solution is unstable vanishes. An isola is then obtained (bottom plot), in this situation the purely hydrodynamical solution is always stable. The very",
        "6de2a5d-bc41-4350-adbe-b3f5d727dbab}} other points were maintained in time for up to {{formula:c2b84226-239f-48cd-895e-02875d37df66}} . We have seen previously that the dynamo bifurcation is subcritical for an Ekman number {{formula:062ff4af-2f39-409d-8d90-2b52c65e1182}} and a magnetic Prandtl number {{formula:594e9a4d-7d54-4879-9727-5ba8f304b1b2}} . Let us now turn to {{formula:5bcb65a4-c1a5-4dfc-af44-3454bec847e1}} , the resulting bifurcation diagram is presented on figure REF .a and corresponds to a supercritical bifurcation. {{figure:c2c0de32-5353-42e8-aae4-85d369a80da0}}By increasing the Ekman number at fixed magnetic Prandtl number"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how major mergers trigger AGN activity and what methods were used to obtain these results?",
      "expected_output": "The main findings suggest that major mergers trigger AGN activity when the mass ratio of the merging haloes is larger than a certain threshold, which is lower than the value generally used in the literature. The methods used to obtain these results include the semi-analytic code developed by previous researchers and the extended Press & Schechter formalism to describe the merging history of dark haloes. Additionally, phenomenological prescriptions were adopted to model the feeding of the central BH and the physical processes of the AGN activity.",
      "context": [
        " of the more massive halo, and consequently induce star formation and BH mass accretion. Notice that this threshold is lower than the value of {{formula:dbfa3fc2-e324-404e-883e-7828158b23b4}} generally used in the literature, but it is not low enough to reproduce the observed faint AGN number counts, as we will describe later. So, an higher value of the mass ratio would worsen our results, while a lower one would be in disagreement with the results of , which show that a typical merger with a mass ratio of {{formula:a72d2492-b947-4e6a-8ee7-d207072379e4}} does not induce starbursts. Moreover, according to , when {{formula:7272abe1-729d-4361-ba9c-193eca8a06f0}} the dynamical friction timescale is larger than the Hubble time, hence preventing the merging of satellite galaxies and, reasonably, making the accretion efficiency onto the central BH very low.We stress here that considering galaxy rather than halo mergers could lead to different predictions as it is not always true that a major",
        " it has been proposed that a significant contribution to the faint AGN mass accretion could come from the material liberated by the tidal disruption of stars by the central BHs , or by the mass of the stars captured by the BH disks and eventually dissolved . Other studies have considered the stochastic accretion of molecular clouds in quiescent systems , . Moreover, it was suggested that also disk instability could trigger mass accretion, contributing to increase even more the number density of faint AGNs , . Finally, a better treatment of mergers between haloes with low mass ratio may also contribute to solve these problems , .Whether including these alternative trigger mechanisms in our simple merger-driven scenario can help in reconciling model predictions with observations at {{formula:6e90bc42-32ec-4161-9832-acdb55c81f5b}} is a question that deserves further investigation.\nAcknowledgements: FM thanks the Institute of Astronomy, University of Cambridge, for the kind hospitality. This work has been partially supported by ASI and INAF. We would also like to thank the anonymous referee for very useful comments.",
        " halo hosts. In this framework, the merging history of dark haloes is described by the extended Press & Schechter formalism , , while phenomenological prescriptions are adopted to model the feeding of the central BH and the physical processes of the AGN activity.Like in M06, the models are based on the semi-analytic code developed by . AGN activity is assumed to be triggered when major mergers occur, i.e. when the two merging haloes have a mass ratio larger than {{formula:7169f5d8-fd42-4bed-a5fd-0a3e51367bf2}} . Due to the lack of an exhaustive study of the ultimate consequences of a galaxy merger in its whole parameter space, we are forced to make some simplifying assumptions to follow the merging events. Following , we can assume that all halo mergers, except the ones with mass ratio smaller than {{formula:f54745ef-0b02-4b46-8cc1-54d114b1fe4b}} , can destabilize the gas at the centre of the more massive halo, and consequently induce star formation and BH mass accretion. Notice that this threshold is lower"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the reduction of spherical symmetry in 4D theories impact the overall complexity and outcomes of the research, and what are the main findings related to this simplification?",
      "expected_output": "The reduction of spherical symmetry in 4D theories simplifies the system, allowing for exact solutions and a better understanding of the underlying dynamics. The main findings include the derivation of a reduced action in two dimensions, which can be presented as a dilaton gravity theory. This simplification enables the combination of different formalisms, such as Cartan, GHP, and metric formalisms, to investigate various sectors, including Einstein-Cartan theory, spinors, gauge fields, and scalar fields. The spherically reduced Standard Model of particle physics, including gravity, can be constructed and presented as a two-dimensional theory, providing valuable insights into the fundamental principles of the dynamical system.",
      "context": [
        " in a spherically symmetric context.This paper is organised as follows: in Section three different notions of spherical symmetry are discussed. Section fixes the notation and introduces the three formalisms (Cartan, GHP, metric) by means of relevant and rather explicit examples. A brief recapitulation of dilaton gravity with matter by means of a discussion of static perfect fluid solutions is given. Collecting them the spherically symmetric Standard Model plus gravity is constructed in Section . It is presented as an effective theory in two dimensions (Section ). The final Section contains some concluding remarks. The appendices provide supplementary material mostly related to the GHP formalism.\nThree ways of spherical symmetry/reduction: In the following we will define different notions of spherical symmetry.0) In order to be able to talk about spherical symmetry one needs an action of the rotation group {{formula:cbc1cb24-3ee2-4237-a94a-9b53cee4ed84}} on the spacetime manifold {{formula:a1e28e3e-2540-41db-9b06-f2d2cb9e5a33}} under consideration. For the geometry we require that the vector-fields",
        "# arXiv Paper: gr-qc/0412098\n\nThe Spherically Symmetric Standard Model with Gravity\nabstract: Spherical reduction of generic four-dimensional theories is revisited. Three different notions of \"spherical symmetry\" are defined. The following sectors are investigated: Einstein-Cartan theory, spinors, (non-)abelian gauge fields and scalar fields. In each sector a different formalism seems to be most convenient: the Cartan formulation of gravity works best in the purely gravitational sector, the Einstein formulation is convenient for the Yang-Mills sector and for reducing scalar fields, and the Newman-Penrose formalism seems to be the most transparent one in the fermionic sector. Combining them the spherically reduced Standard Model of particle physics together with the usually omitted gravity part can be presented as a two-dimensional (dilaton gravity) theory.\nIntroduction: Spherical symmetry plays a pivotal role in theoretical physics because the system simplifies such that an exact solution is often possible; this in turn allows for an understanding of some basic principles of the underlying dynamical system and thus can be of considerable pedagogical value.It is impossible to present a complete list of references regarding spherical symmetry",
        " coupling and the bracket denotes integration over the angular part – this system of averaged equations of motion can be deduced from an action in {{formula:12a9c6b3-105c-41c6-9668-4dc11fcb39e2}} , the geometric part of which is just the spherically reduced Einstein-Hilbert action. For the Einstein-massless-Klein-Gordon model the matter part (REF ) contains an infinite tower of scalar fields with dilaton dependent (and {{formula:ae0b1925-d7f6-46da-97dd-b1e3852c53a0}} -dependent) mass.Each of these notions is weaker than its predecessor: {{formula:48f91797-529f-4f3b-ae33-e3d8db1274be}}For the rest of this paper we assume spherical symmetry according to the second notion, unless stated otherwise.Having defined spherical symmetry we would like to focus on spherical reduction. By this procedure we mean the derivation of a reduced action in {{formula:1eb53db7-6ab0-4814-b217-eb0c209238"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the effect of collision on gas density in this research paper?",
      "expected_output": "The main findings regarding the effect of collision on gas density in this research paper are that a distinct and stable density peak forms at the point of collision, and a large density gradient is maintained at the edge of the collision zone. The simulation shows that the dissipative force is required to attenuate the large density gradients and avoid a 'gradient catastrophe'. The data are well described by a simple one-dimensional model based on dissipative nonlinear quantum hydrodynamics, suggesting the formation of a dissipative shock wave.",
      "context": [
        " collision zone is maintained throughout the rest of the experiment. For most of the data, we find relatively small deviations from the simulation. The largest deviation occurs at 4 ms, where the maximum density of the observed central peak exceeds that of the simulation by {{formula:1bb973b6-acd9-495b-9d06-4d9aa8494f21}} %.The steep density gradients observed in Fig. REF suggest shock wave formation. A deeper analysis of the simulation curves provides additional evidence for shock waves. Without any dissipation, the numerical integration of the quasi-1D theory breaks down due to a “gradient catastrophe.\" We find that the dissipative force in Eq. , which is described by the kinematic viscosity coefficient {{formula:7e898a3e-6f4b-43aa-861d-d4e933f8e98d}} , is required to attenuate the large density gradients and avoid gradient catastrophe. For the data shown in Fig. REF , we find that the best fits are obtained with the viscosity parameter {{formula:8d83dfed-5037-4cb6-9b9",
        " the viscosity parameter {{formula:8d83dfed-5037-4cb6-9b9c-278ad6934607}} . For smaller values of {{formula:41b60608-5f12-4763-8cb9-f8a52d59b7d3}} , the simulation produces qualitatively similar results to those shown in the figure, only with steeper density gradients at the edges of the collision zone. The dissipative term {{formula:45c8c717-1d76-484a-b046-134686bd6f06}} has a relatively small effect on the density profiles, unless we are in a shock wave regime, where the density gradients are large. Hence, the numerical model suggests that the large density gradient observed at the edge of the collision zone is the leading edge of a dissipative shock wave.Our data for a strongly interacting Fermi gas are very well described by a simple one-dimensional model based on dissipative nonlinear quantum hydrodynamics. In this sense, a unitary Fermi gas is different than a weakly interacting Bose-Einstein condensate, where dispersive terms play a major role",
        " a function of time after the repulsive potential is extinguished. Fig. REF shows the predictions and the data, which are in very good agreement. For the simulation curves shown in the figure we use a grid of 150 points. To check for numerical consistency, we also employ a smoothed-particle-hydrodynamics approach, where the fluid is described by discrete pseudo-particles. The results obtained indeed coincide with the discretized-grid approach described above. {{figure:b7a46a0b-d70b-4086-8fd0-ac768a979b04}}As shown in Fig. REF , we observe a dramatic evolution for the density of the gas. During the collision, a distinct and stable density peak forms at the point of collision in the center of the trap . The density gradient at the side of the central peak increases from its onset until {{formula:37863f34-5071-47ee-88d3-df494ecee676}} ms, at which point the gradient reaches its maximum value. A large gradient at the edge of the collision zone is maintained throughout the rest of the experiment. For most of the data, we find relatively small deviations from the simulation"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between blazars' spectral energy distributions and leptonic or hadronic processes?",
      "expected_output": "The research paper finds that blazars have a continuous Spectral Energy Distribution (SED) with two broad humps, one in the UV to soft X-ray and a second in the GeV-TeV range. Leptonic models, such as the Synchrotron-Self Compton (SSC) model, are successful in describing most SEDs and explaining fast variability. Hadronic models, like the Synchrotron Mirror Model (SMM) or Synchrotron Proton Blazar (SPB), also describe SED structure and explain 'orphan' gamma-ray flares, predicting high energy neutrino emission. However, the data is not enough to fully constrain theoretical models, leaving unanswered whether leptonic or hadronic acceleration processes are responsible for observed blazar behavior.",
      "context": [
        " oriented towards the Earth. They have a continuous Spectral Energy Distribution (SED) with no or weak emission lines and two broad humps (one in the UV to soft X-ray and a second in the GeV-TeV range). Moreover, their flux was found to be variable at all observed frequencies, but on different time scales ranging form years to minutes .In recent years numerous multiwavelength campaigns were performed with the aim of explaining the acceleration and emission mechanisms in blazars. In many campaigns the new generation of Imaging Atmospheric Cerenkov Telescopes (IACTs) like HESS , MAGIC and VERITAS also took part allowing us to have a deeper look at the highly variable VHE (E {{formula:c5621dcb-f709-4061-9ddb-037323a90a1a}} 100 GeV) {{formula:3ad06c78-e52d-4767-b444-c27942a17463}} -ray emission. Unfortunately the data collected so far is not yet enough to fully constrain the theoretical models and still one of the most important question remains unanswered: are the leptonic or hadronic acceleration processes responsible for the observed blazar behavior",
        "# arXiv Paper: 0907.0893\n\nMonitoring of bright blazars with MAGIC telescope\nabstract: Blazars, a class of Active Galactic Nuclei (AGN) characterized by a close orientation of their relativistic outflows (jets) towards the line of sight, are a well established extragalactic TeV $\\gamma$-ray emitters. Since 2006, three nearby and TeV bright blazars, Markarian (Mrk) 421, Mrk 501 and 1ES 1959+650, are regularly observed by the MAGIC telescope with single exposures of 30 to 60 minutes. The sensitivity of MAGIC allows to establish a flux level of 30% of the Crab flux for each such observation. In a case of Mrk 421 strong flux variability in different time scales and a high correlation between X-ray/TeV emissions have been observed. In addition, preliminary results on measured light curves from Mrk 501 and 1ES1959+650 in 2007/8 are shown.\nIntroduction: Blazars belong to the class of AGN and are characterized by relativistic jets oriented towards the Earth. They have a continuous Spectral Energy Distribution (SED) with no or weak emission lines and two",
        " of the most important question remains unanswered: are the leptonic or hadronic acceleration processes responsible for the observed blazar behavior?Leptonic models, like for example, the Synchrotron-Self Compton (SSC) are very successful in describing most of the existing SEDs and offer a reasonable explanation for the fast variability of blazars. Hadronic models, on the other hand, like the Synchrotron Mirror Model (SMM) or Synchrotron Proton Blazar (SPB) apart from a good description of the SED structure can also explain the “orphan” {{formula:bf8c7b7c-168d-426e-8e5e-ca2b2adac83b}} -ray flares (see e.g. ) and predict emission of high energy neutrinos.\nAGN monitoring: As mentioned before the new generation IACTs can give a valuable input for understanding of the acceleration mechanism in blazars. Not only by participation in multiwavelength observations, but also by performing a source state independent, long term monitoring of the most interesting brighter {{formula:b7ffe2a2-5f26-4dc1-8"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the larger receptive fields in CNNs affect the consistency of local features in image recognition tasks?",
      "expected_output": "The larger receptive fields in CNNs allow them to capture more context and generate more consistent local features, which improves the accuracy of image recognition tasks. For example, in the context of the research paper, CNNs are capable of generating more consistent local features due to their larger receptive field, making them suitable for modeling 2D distribution from keypoint prediction heatmap with a probabilistic explanation.",
      "context": [
        " techniques, CNNs have a larger receptive field and are capable of generating more consistent local features. Therefore, we model their 2D distribution from keypoint prediction heatmap with a probabilistic explanation. Considering the prediction as a mixture of multivariate Gaussian distributions, to approximate the single variate parameters, we have: {{formula:c581b54c-037c-4bcc-8e96-d459ddad30fe}} {{formula:2a6b017d-db59-4a21-b920-69c84714f31b}}For the simplicity of implementation, here we represent {{formula:ebec3e4e-665c-45a5-8524-1fb6cdeb61ae}} as a {{formula:9699d2b0-948c-4b04-99e9-708ad3efdbc2}} patch. This parameterization scheme is further verified by sec.evalparam.After associating landmarks with the local observations, the current camera pose {{formula:a089a619-dccf-4293-9abf-580f09675ed7}} is refined by minimizing the repro",
        "}} and {{formula:9924b2fc-236d-496d-9604-43e90ff73712}} . Previous works resolve this issue in two approaches. In the early stage, methods model local features as 2D Gaussian distributions with anisotropic covariance. Some interesting discussions can be found in , , where the authors generally conclude modelling such anisotropic covariance can improve the visual state estimation. On the contrary, modern SLAM systems , , generally assign an isotropic covariance for each feature. This gives {{formula:c725416e-36ba-4927-a9f1-068868df9fab}} , with {{formula:70ec703b-e682-448c-8374-54b64acdc498}} proportional to the pyramidal level of the corresponding feature. However, as described above, generating a pyramidal prediction is prohibitive, making this parameterization method not practical in our implementation.Following previous works, we seek to weigh the contribution of different local features to the overall estimation. Compared to traditional feature extraction techniques, CNNs have a larger receptive field and are capable of generating more consistent local features. Therefore, we model their 2",
        " Tracking: Assuming an initial sparse structure built from the backend, the frontend recovers the camera pose and associates 3D landmarks with 2D observations, which is illustrated in fig.tracking.Initially, we track the current camera pose in a direct manner. The direct method of the camera pose estimation advances in their correspondence-free formulation, where there is no need for heuristic design of matching strategies. To take both the advantage of direct method and the network predictions, we propose to hierarchically track the camera pose on the repeatability map. Our intuition behind is that 3D landmarks triangulated from 2D observations are consistently repeatable in different views. In other words, the reprojection location of a 3D landmark in {{formula:70f8372c-89cb-49cc-ba33-302c995b835c}} should be the local extremum. {{figure:398a86d6-5fc5-461f-a078-a7d256d12f91}}For the simplicity, here we denote the set of visible landmarks as {{formula:b295b0b3-758d-4897-9d83-f00bcda8b546}} . The current camera pose is den"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the combination of DUNE and T2HK experiments resolve the degeneracy between GRB and HG, and at what confidence level?",
      "expected_output": "The combination of DUNE and T2HK experiments can resolve the degeneracy between GRB and HG at more than 3σ confidence level, as shown in Figure REF. This means that if one of these forms is realised in Nature, the other can be distinguished with a high degree of confidence.",
      "context": [
        "b2a3-a1e9-4b77-b57d-b5e3c494b3cb}}Further, performing the more involved analysis described in Appendix , we obtain the results summarised in Fig. REF . {{figure:a0d3f948-0abc-48c7-9952-dc3a6b5432e2}}This figure allows us to see immediately at which C.L. a given pair of the symmetry forms can be distinguished, under the assumption that one form in the pair is realised in Nature. In particular, the numbers presented in Table REF get clear graphic representation. Indeed, we see that using the combination DUNE + T2HK, GRB and HG can be resolved at more than {{formula:313f6a4a-0af5-455a-bd6a-dbf067d74ee5}} C.L., while TBM and HG can be distinguished at almost {{formula:75290a91-edb0-4d86-b10b-37883a4aba75}} .As we see from Appendix , the external prior on {{formula:c0212cae-e949",
        "}} of interest. The points where it crosses the {{formula:4a6fd002-e930-40d5-94dc-7c9f3a95721b}} curves will provide the confidence levels at which the TBM, GRA, GRB, and HG forms are compatible with this {{formula:22121913-d2ce-4a24-9fae-1b36ed0dd54e}} . In particular, for {{formula:378500b0-0678-4a05-8dcb-27536de756b1}} , we find numbers which correspond to those extracted from Fig. REFNote that the numbers we read from Fig. REF are slightly smaller than those extracted from Fig. REF due to the fact that now we marginalise over the values of the mixing angles..\nHow well can DUNE and T2HK Separate between Various Symmetry Forms?: In this subsection, we will answer the question of how well DUNE and T2HK can distinguish the discussed symmetry forms under the assumption that one of them is realised in Nature. Given the fact that the BM form is not compatible with the current best fit values of the neutr",
        ", the GRB and TBM forms would be disfavoured at more than {{formula:308622b0-b927-4ebe-bb80-80a0de8d9fce}} , while the HG (GRA) symmetry form would be compatible with this hypothetical position of the star at {{formula:e4939782-792f-4508-aea7-58532fb60a1a}} ({{formula:b7f5af83-3a18-4ddb-81e2-60e39a9fe628}} ) confidence level.Finally, we would like to notice the compatibility between this figure and the numbers in Table REF . Let us consider an example, in which GRB is the true form and HG is tested against it. In this case, from Table REF , we read the C.L. at which these two symmetry forms can be distinguished by DUNE + T2HK, namely, {{formula:f92c2d60-8c1d-457b-9e83-5d13bccea3a8}} . We recall that the results in this table have been obtained assuming the current best"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in this research paper regarding the impact of a light stop's mass on SUSY models, and how do the methods used affect the key results?",
      "expected_output": "The research paper finds that a light stop's mass can significantly impact SUSY models, potentially allowing it to evade detection even if other squarks and gluinos have masses of several hundred GeV. The paper uses ISAJET 7 to improve on previous parton-level studies and reevaluate the stop signal, finding that a stop with a mass just beyond the LEP limit could have escaped detection. The methods used, such as analyzing 4.3 inverse femtobarns of data, lead to key results including published limits on the masses of squarks and gluinos of around 100 GeV. For example, the CDF Collaboration's analysis sets a limit of 100 GeV on the masses of squarks and gluinos, while the D0 and CDF experiments' accumulated integrated luminosity enables them to extend the direct search for charginos and neutralinos to regions of MSSM parameter space beyond LEP's range.",
      "context": [
        " matrix split the top squark masses, reducing the mass of the lighter stop ({{formula:bdf230ba-baa0-4e9d-8e48-67dfff9b3c48}} ) state even further. In fact, {{formula:2e1c0c02-a1e0-496a-b595-0f3623308773}} may be as light as 50 GeV even if the other squarks and gluinos have masses of several hundred GeV. This led a group of us to study the phenomenology of a light {{formula:89dd7b85-9d7e-48a6-bae2-a5fed4faa6c6}} and its effect on the CDF top quark search at the Tevatron. It had been concluded that a {{formula:f98b4214-b18c-4d26-8a7d-273241479794}} with a mass just beyond the LEP limit could well have escaped detection in the analysis of the 1991 data sample. In this study we improve on this parton level study and reevaluate the stop signal using ISAJET 7",
        ", compositeness, exotic quarks, leptoquarks and, of primary interest to us, supersymmetric particles. The CDF Collaboration , based on an analysis of 4.3 {{formula:8916e30b-dd94-455a-a679-4b996f644748}} of data have published limits of around 100 GeV (160 GeV if {{formula:c9dbeacf-68d1-4d7d-ae1d-41938b19af0e}} ) on the masses of squarks and gluinos of the minimal supersymmetric model (MSSM). By now, the D0 and CDF experiments at the Fermilab Tevatron have, between them, accumulated an integrated luminosity of almost 40 {{formula:6be22b39-01c6-4f1e-b2b8-d9e4f6f1eccf}} , and are expected to collect a data sample in excess of 100 {{formula:7b7fe1b7-591f-40d2-ad8b-dba77b3431f8}} by the end of the current",
        "ula:465cdff3-5fde-47fc-90ec-b249eeb055f6}} , the trilepton signal from the continuum {{formula:700c51ad-3098-4a1b-8872-7a902826c05d}} production would enable the CDF and D0 experiments to extend the direct search for charginos and neutralinos to regions of MSSM parameter space beyond the range of LEP experiments. Continuing our study of other SUSY signals that may be accessible at the Tevatron, we focus here on the strategies for detection of the {{formula:06d4d82d-32f2-4112-a6b0-3aaca2e667e4}} -squark which can be considerably lighter than all the other squarks even in supergravity models where all the sfermions have a common mass at an ultrahigh energy unification scale.2. Why is the top squark different?Supersymmetry must be a broken symmetry. Since the dynamics of SUSY breaking is as yet unknown, the breaking of supersymmetry is parametrized by a rather large number of soft SUSY-breaking parameters"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the cosmic microwave background optical depth play in the formation of Population III stars, and how do the findings of Planck's CMB research impact our understanding of this process?",
      "expected_output": "The cosmic microwave background optical depth plays a crucial role in understanding the formation of Population III stars. According to the research paper, the low value of the CMB optical depth measured by Planck, τ=0.066 ± 0.016, leaves little room for early partial reionisation by high-redshift Population III stars. This suggests that without Lyman-Werner feedback or prompt metal enrichment, the Pop III star formation efficiency cannot exceed a few × 10^{-4}, excluding massive Pop III star formation in typical minihaloes. The findings of Planck's CMB research impact our understanding by constraining the production of Pop III stars in minihaloes and implying that massive Pop III stars could form efficiently early on, but are suppressed at lower redshifts, reducing the optical depth to be consistent with Planck.",
      "context": [
        "cf747040-e5a6-4117-a55b-161c45d034ab}} (hereafter “atomic cooling haloes”) can form stars through atomic hydrogen cooling. Pop III stars could form in these haloes if they contain pristine gas, but once they become metal-enriched from previous generations of stars, Pop I/II star formation occurs.Recently Planck has reported an improved measurement of the CMB electron scattering optical depth, {{formula:306bbf5b-9742-44a0-8f7e-baac77d269f4}} . This is lower than previous measurements from WMAP and leaves less room for an early partial reionisation. In this paper, we examine the constraints this new measurement puts on the production of Pop III stars in minihaloes , . We utilise a semi-analytic model that includes the contribution from both atomic cooling haloes, calibrated with observations of {{formula:ae5e826e-6413-4df5-a213-4a1625b0a56c}} galaxies, and minihaloes with a self-consistent treatment of LW feedback. Using a simple treatment, we also",
        "# arXiv Paper: 1505.06359\n\nLimits on Population III star formation in minihaloes implied by Planck\nabstract: Recently, Planck measured a value of the cosmic microwave background (CMB) optical depth due to electron scattering of $\\tau=0.066 \\pm 0.016$. Here we show that this low value leaves essentially no room for an early partial reionisation of the intergalactic medium (IGM) by high-redshift Population III (Pop III) stars, expected to have formed in low-mass minihaloes. We perform semi-analytic calculations of reionisation which include the contribution from Pop II stars in atomic cooling haloes, calibrated with high-redshift galaxy observations, and Pop III stars in minihaloes with feedback due to Lyman-Werner (LW) radiation and metal enrichment. We find that without LW feedback or prompt metal enrichment (and assuming a minihalo escape fraction of 0.5) the Pop III star formation efficiency cannot exceed $\\sim{\\rm a~few}\\times 10^{-4}$, without violating the constraints set by Planck data. This excludes massive Pop III star formation in typical $10^6",
        " galaxies, and minihaloes with a self-consistent treatment of LW feedback. Using a simple treatment, we also include the possible effects of metal enrichment in minihaloes from Pop III supernovae, which causes a transition to Pop II star formation.We compute the ionisation history and corresponding CMB optical depth for a variety of model parameterisations with and without LW feedback and metal enrichment and find that without LW feedback or metal enrichment, massive Pop III stars cannot form efficiently in minihaloes without violating the Planck constraints. When LW feedback and metal enrichment are included, massive Pop III stars could form efficiently early on, but they are suppressed at lower redshifts, reducing the optical depth sufficiently to be consistent with Planck. We also find that, irrespective of the feedback prescription used, the total density of Pop III stars formed over all cosmic time cannot exceed {{formula:e06f6f46-3148-4f93-8a6d-d6cc09e051fe}} without violating the Planck optical depth constraints.Note that recently performed an analysis of reionisation and the Planck optical depth. Their study is empirical, focusing on the ionisation history implied by the observed UV luminosity function (L"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding how particles alter interface energy in this research paper?",
      "expected_output": "The research paper finds that particles alter interface energy in various ways, depending on their shape and properties. Smooth spheres obeying equilibrium wetting conditions become trapped at planar fluid interfaces without distorting the interface, while particles with pinned contact lines, patchy wetting, or non-spherical shapes distort the interface around them. The distortions can be observed at different distances from the particle, with quadrupolar distortions in the far field, and features like particle elongation, waviness, roughness, and sharp edges playing a role in the moderate to near-field. On curved interfaces, particles with pinned contact lines and associated quadrupolar modes can orient and migrate in the curvature field, as demonstrated by experiments using lithographic methods to mold fluid interfaces.",
      "context": [
        " corners, sharp edges, or sites of patchy wetting , , . We will discuss both, emphasizing when they give remarkably different outcomes.\nParticle with an equilibrium contact angle: First consider an isolated, spherical particle placed on an otherwise planar interface (see ref. for detail). We can evaluate the change in surface energy upon attachment of a sphere that is initially completely immersed in either of the sub-phase fluids (State 1) to an interface (State 2, in which the particle is partially immersed in each phase and there is a hole in the interface). The integration domain can be divided into {{formula:182bc644-dd80-4642-a439-162a13b550fe}} , external to {{formula:849c66bb-3c93-4fa1-bd4e-aa853255117a}} , the projection of the circular hole made by the particle in the {{formula:62a0dae5-5e62-4e62-9da3-91689ad38007}} - {{formula:6ed0172a-07c7-45ec-886f-8df0f4067e9a}} plane (see fig. RE",
        " undulations and will change the energy landscape around the particle.To conclude this section, particles become trapped at planar fluid interfaces. If they are perfectly smooth spheres obeying equilibrium wetting conditions, they become trapped while letting the interface around them remain unperturbed. If they have pinned contact lines, patchy wetting or non-spherical shapes, they distort the interface around them. Distortions due to various particle features are observed at different distances from the particle. All particles make quadrupolar distortions in the far field. In the moderate to near-field, features like particle elongation become apparent; closer still, waviness, roughness and sharp edges play a role .\nIsolated particles trapped on curved interfaces: What if the interface is curved? How does the energy trapping the particle in the interface change? Here we discuss this for a particle with pinned, undulated contact lines with associated quadrupolar modes of amplitude {{formula:c8626887-a858-409f-a29f-d553bc5d3199}} . We can locally expand any host interface near the particle center in terms of the mean curvature {{formula:38eb8fa2-cd54-4072-ab3b-",
        " field. When particles attach to this interface, they will orient and migrate in this curvature field. Our laboratory has developed the use of lithographic methods to mold fluid interfaces for use in experiment that can be compared with theory. We have studied the migration of particles with increasing complexity on curved fluid interfaces: Planar microdisks, microspheres, and microcylinders, trapped at oil-water interfaces. For each shape, we assume pinned contact lines. These methods are described briefly here.\nMolding of the fluid interface: We commonly form a curved oil-water interface around a circular micropost of height {{formula:5ca04c41-ef78-45f9-9e3c-12c24eeaea82}} and radius {{formula:218f0115-6d00-4214-a76a-6f07f04681c9}} . Far from the micropost is a confining ring, located several capillary lengths from the micropost center. This structure is filled with water so that the contact line is pinned at the top of the micropost and the slope of the interface is shallow ({{formula:b2dac4e4-444d-4209"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the article reference counts in WoS and Scopus compare in terms of accuracy and comprehensiveness?",
      "expected_output": "According to the research paper, when comparing the average total reference counts between WoS and Scopus, it initially appears that Scopus outperforms WoS. However, when considering records marked as articles, they perform more comparably. For example, in the Shared Corpus with references from 1996 to 2022, the difference between the source reference count and total reference count decreases as the restrictions are added, suggesting that Scopus still has a small disadvantage due to its initial indexing start.",
      "context": [
        " publication years 2015 to 2022 inclusive.As the Scopus database mainly contains items from 1996 onwards (although since 2015, pre-1996 cited references and backfiles of major publishers have been addedhttps://blog.scopus.com/posts/breaking-the-1996-barrier-scopus-adds-nearly-4-million-pre-1996-articles-and-more-than-83), and WoS and OpenAlex have had no such restriction, to avoid bias in the computation of source reference counts and internal coverage we further restrict references to those items published between 1996 and 2022.We include in Table REF a section on articles published 2015-2022 in all three databases, to illustrate the influence of the time restriction to the size of the Shared Corpus, and to give context to the DOI matching and deduplication work described in Section REF .In the Scopus and WoS databases, pre-computed total 'reference counts', are delivered by the data providers Elsevier and Clarivate, whereas 'source reference counts' are computed for each record by our data provider FIZ Karlsruhe. Both databases are expected to contain all references of a given publication, regardless of whether they refer to items contained within or not contained",
        " portrayed in Figure REF , similarly records which have a shared DOIs are counted once. The error margin for the number of records compared to DOIs can be inferred from the data in Table REF .Another reason for the exclusion of items in the DOI match, which at the same time restricts to publication years 2015-2022, is the fact that publication years are not always exactly the same between databases, possible due to differences in the handling of early access and print publication dates. We define the time restriction as applying to all three databases at the same time.\nMethodology: To evaluate the reference and source reference coverage of WoS and Scopus against OpenAlex, we first used the reported reference counts and pre-calculated source reference counts as described in Section REF . The average total reference count and source reference count was computed for: each database, for records marked as `article' (or comprising the document type `article' alongside other type markings, in the case of Scopus and WoS) and for the Shared Corpus resulting from the DOI match (publication years 2015-2022). These numbers were replicated by counting the total number of references and records in each database and taking the ratio (`references per record'), as reported in Table REF",
        " was not marked as closed. We have normalised the journal title to lowercase to aggregate the items.\nTotal and Source Reference Coverage: Table REF , in a naïve averaging of the source reference count, leaves OpenAlex looking comparatively poor at 7.6 references per record to the 16.9 or 18.7 of WoS and Scopus (and well behind the other databases' average total reference count). However, when restricting to the 2015-2022 corpus shared by the three databases, OpenAlex proves competitive with a higher average source reference count than both WoS and Scopus. The fact that results vary greatly depending on the underlying corpus definition could be explained by the fact that OpenAlex comprises of many–possibly less scientific–publications with comparatively short reference lists. When focusing on the comparison of the average total reference counts between WoS and Scopus, it initially appears that Scopus outperforms WoS, however when considering records marked as articles they perform more comparably. This trend continues when observing the Shared Corpus and the Shared Corpus with references from 1996 to 2022. Notably here the difference between the source reference count and total reference count decreases as the restrictions are added. The results suggest that Scopus still has a small disadvantage due to its initial indexing start in"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between zero modes and the commutation of modular Hamiltonians, and how do the main findings of this research paper explain this connection?",
      "expected_output": "The research paper explains that zero modes of the modular Hamiltonian commute with the modular Hamiltonian, meaning their commutator vanishes. This is shown by the equation where the commutator between the modular Hamiltonian and certain operators is standard, and thus the commutator vanishes if and only if a specific condition is met. The paper finds a complete set of zero modes for massless free scalar and lists them according to their degree, shedding light on the study of zero modes and their gravitational dual in the large limit.",
      "context": [
        "In modular fourier space, {{formula:4b09703c-d41e-460d-b670-c3c134c3ba42}}one finds the commutation relation {{formula:1a85b5f5-c6de-424a-8821-409789d9740c}}In the limit {{formula:41c650cb-c4f5-463c-89b6-2f696cdc433b}} , operator {{formula:8ae58973-e0ae-46e9-9600-b70ba1907806}} belongs to zero modes of modular Hamiltonian since it commutes with modular Hamiltonian. Zero mode is claimed to have a gravitional dual {{formula:432647f8-efa9-422f-ab44-3c82428d5968}}in the large {{formula:98e9034e-585b-45bd-a818-5fb711e6d05e}} limit. Roughly speaking, zero mode can be expressed as an integral over {{formula:45c23650-4e48-4ac7-b260-6",
        "-a5f7bf0a13cc}} could be any reasonable function of quantum number {{formula:535db6ba-9a07-41c5-bcae-e58c73c6ec31}} . This sheds light on the study of zero modes in this paper.\nZero modes of modular Hamiltonian: Modular Hamiltonian of region {{formula:5790508f-4265-4780-b206-ccf8d6ae7d9a}} will send operators {{formula:2e351da4-02e2-498f-9821-5cc77b7ca4dd}} of region {{formula:a245590f-6168-4e31-9583-0f1a949da8ca}} to another operator {{formula:4d2542ba-1f4e-427b-aa37-e71673892bd1}} by modular flow {{formula:f9ec1811-498b-4977-9aea-c9fa80b67aa6}}In modular fourier space, {{formula:4b09703c-d41e-460d-b670",
        "8d2da7393}}The commutator between modular Hamiltonian and {{formula:591d4ab6-4054-439f-8259-4aa38469cfcc}} or {{formula:05da385d-c9e0-42d8-8964-36e01d24243e}} is standard, {{formula:50e4dce6-5039-4c45-9563-fd6dff06cb1e}}Therefore the commutator {{formula:3d69a1b5-bf19-4e39-acf4-d57fd10c3667}}vanishes if and only if {{formula:05bf07f0-eeea-4343-84aa-d8cd514137ce}}Hence a complete set of zero modes is found for massless free scalar. We list first few zero modes according to the degree of {{formula:fe0d6248-d22c-4ff1-a59e-2363be055adc}} and {{formula:64a8ef77-16db-4815-9d25-de56a40875"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the critical magnitudes of PRBM and Anderson transitions, and how do they relate to the main findings of this research paper?",
      "expected_output": "The critical magnitudes of PRBM and Anderson transitions analyzed in the research paper include the correlation dimension, the scaling of the fluctuations, and the exponent characterizing the long tail of the distribution. The paper finds that these transitions are similar for all critical magnitudes, with specific values obtained for the Anderson model and the PRBM model. For example, the exponent characterizing the long tail of the distribution is 4.2 for one transition and 2.1 for another, and 1.6 for the Anderson model, which is in relative agreement with the PRBM model.",
      "context": [
        "4}} with {{formula:65faa83f-d9fe-4f78-b275-f6beae77b51b}} . We have checked that {{formula:e3b716f9-a181-4fea-b145-0d39af129266}} versus {{formula:8d93a1ee-dcfc-4d03-a584-ed09296b1d5c}} do not follow a linear behavior. We believe that the disagreement is due to two reasons. Firstly, the relatively small system sizes employed in Ref. , and secondly the use of different estimates of an effective IPR, instead of directly studying the width of the {{formula:d91f93d1-383b-49f7-b8cb-7c5ba30aa5e6}} distribution. On the other hand, Mirlin and Evers already obtained for the PRBM model similar results to ours.Eq. (REF ) may be also valid for other transitions, like the integer quantum Hall transition, where the irrelevant exponent {{formula:92526776-57b1-49ca-b567-04ae9c97",
        "c-abdf-03daff93f317}} , a result that may be extended to other transitions like the integer quantum Hall transition. The behavior of the distributions of {{formula:c76ec31f-714b-4e9c-b665-25553b42092d}} and of {{formula:aa021ef8-f0b2-4e9f-a326-2f9ea9f73168}} is different. We checked that the 3D Anderson transition and the PRBM transition for {{formula:cd6e7f4d-7b0c-4cef-b3ab-4cf86632a8a6}} are similar for all the critical magnitudes analyzed: the correlation dimension, the scaling of the fluctuations of {{formula:824380d1-baca-4d2f-b38a-bd1617b3fe64}} , the exponent characterizing the long {{formula:5c0d7251-b903-4e12-8c65-43ab2bb23abf}} tail of the distribution {{formula:c71b26b5-9cd2-49dd",
        "1c6a55ff44}} , where {{formula:b9264d12-8508-498c-9b92-8f98a504ce41}} depends on the transition considered. For example, {{formula:0e529134-edd9-454d-8ed2-245745669015}} is equal to 4.2 for {{formula:70d71553-21e2-40e2-8021-eae04a3c22fc}} and 2.1 for {{formula:3185af32-fbf6-4dcb-8604-467df740def9}} . We have checked that this prediction is also obeyed by the Anderson model, for which we obtained a value of {{formula:08c75bd0-92a1-4a14-8ef1-9dcdad4b9fd7}} equal to 1.6, in relative agreement with the PRBM model for {{formula:54218e94-6193-4d6f-87c0-2b167612092d}} .In summary, we found that the variance of the fluctuations of {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do PID controllers and camera feeds enable robot motion regulation in the context of this research paper, and what are the main findings regarding their effectiveness?",
      "expected_output": "In the context of this research paper, PID controllers and camera feeds enable robot motion regulation through a visual servoing controller. The controller uses uncalibrated camera feeds to detect a diver and generate a bounding box, which is then used to regulate the robot's motion. The PID controllers normalize the yaw and pitch commands based on the horizontal and vertical displacements of the observed bounding box-center from the image-center. The main findings regarding their effectiveness show that the system can achieve 6-7 positive detections per second on average, which is sufficient for successfully following a diver in real-time.",
      "context": [
        " directions. The task of the robot is to visually detect the diver using its camera feed and follow behind him/her with a smooth motion.\nVisual Servoing Controller: The Aqua robots have five degrees-of-freedom of control, i.e., three angular (yaw, pitch, and roll) and two linear (forward and vertical speed) controls. In our experiments for autonomous diver-following, we adopt a tracking-by-detection method where the visual servoing controller uses the uncalibrated camera feeds for navigation. The controller regulates the motion of the robot in order to bring the observed bounding box of the target diver to the center of the camera image. The distance of the diver is approximated by the size of the bounding box and forward velocity rates are generated accordingly. Additionally, the yaw and pitch commands are normalized based on the horizontal and vertical displacements of the observed bounding box-center from the image-center (see Figure REF ); these navigation commands are then regulated by separate PID controllers. On the other hand, the roll stabilization and hovering are handled by the robot's autopilot module . {{figure:2b00e49a-f55c-4b5e-9e69-b99",
        " open-water conditions (i.e., in pools and in oceans). An autonomous underwater robot of the Aqua family is used for testing the diver-following modules. During the experiments, a diver swims in front of the robot in arbitrary directions. The task of the robot is to visually detect the diver using its camera feed and follow behind him/her with a smooth motion. Visual Servoing Controller The Aqua robots have five degrees-of-freedom of control, i.e., three angular (yaw, pitch, and roll) and two linear (forward and vertical speed) controls. In our experiments for autonomous diver-following, we adopt a tracking-by-detection method where the visual servoing controller uses the uncalibrated camera feeds for navigation. The controller regulates the motion of the robot in order to bring the observed bounding box of the target diver to the center of the camera image. The distance of the diver is approximated by the size of the bounding box and forward velocity rates are generated accordingly. Additionally, the yaw and pitch commands are normalized based on the horizontal and vertical displacements of the observed bounding box-center from the image-center (see Figure REF ); these navigation commands are then regulated by separate PID",
        " bounding box-center from the image-center (see Figure REF ); these navigation commands are then regulated by separate PID controllers. On the other hand, the roll stabilization and hovering are handled by the robot's autopilot module . {{figure:2b00e49a-f55c-4b5e-9e69-b9937b7f589b}}{{figure:907a35d4-b341-423a-9b67-04dfe42b47a7}} Feasibility and General Applicability As mentioned, the diver-following module uses a monocular camera feed of the robot in order to detect a diver in the image-space and generate a bounding box. The visual servoing controller uses this bounding box and regulates robot motion commands in order to follow the diver. Therefore, correct detection of the diver is essential for overall success of the operation. We provided the detection performances of our proposed model over a variety of test scenarios in Table REF (few snapshots are illustrated in Figure REF ). During the field experiments, we have found 6-7 positive detections per second on an average, which is sufficient for successfully following a diver in real-time. In addition, the on"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings and key results of the research on Palais-Smale sequences, and what methods were used to obtain these results?",
      "expected_output": "The research paper discusses Palais-Smale sequences and their relation to critical points. The main findings include that a sequence is a Palais-Smale sequence if it satisfies certain conditions, and that critical points of a function can be found using these sequences. The methods used to obtain these results include analyzing the properties of Palais-Smale sequences and applying them to find critical points. For example, the paper states that if a sequence satisfies the Palais-Smale condition, then it has a critical point. Additionally, the paper provides conditions under which a critical point can be achieved, such as satisfying the (PS) condition or the (C) condition. Overall, the research provides new insights into the properties of Palais-Smale sequences and their applications in finding critical points.",
      "context": [
        "c47-39cc-4166-b6e2-5ea23cd1f6d5}} is a Palais-Smale sequence for {{formula:a2fc4368-77d7-4090-8334-2392512c8445}} in {{formula:b88d58b9-40b5-45ed-ae4b-a0a4e65539e0}} .f) {{formula:3dd4914c-3a88-4e57-81da-9776d17a9d5a}} is a critical point of {{formula:ea781c04-be7b-4e29-94df-d8cb7bbb27af}} if, and only if, {{formula:6fcc7d48-86a9-4225-b66d-f2cbedac2b7b}} is a critical point of {{formula:2fe46508-3647-48c4-8348-f29ee0467a56}} .g) If {{formula:3b453adc-40a8-472b-9a64-",
        " {{formula:717e4dcb-4fc3-4e43-9189-73226f55907d}} . b) If {{formula:3c2bd70d-be1c-4b2a-b9ae-a3fdd11553dc}} satisfies the {{formula:4fe8b39f-d2a7-40aa-9be6-6f13c677f31d}} -condition in {{formula:4e4b5ea9-25a1-4097-bb67-31b80d64c0ee}} then {{formula:13abd13a-bddb-4bd4-a02a-1241bc535072}} is achieved by a critical point of {{formula:fb63e5bd-fafd-432e-88f7-0a8392190adc}} . c) If {{formula:089571bb-f8b6-49c0-9dd2-952ccf732e65}} satisfies the {{formula:d7f9f9d4-b506-4ae8-a23",
        "b3e7-14e3349c692d}} .b) If {{formula:f61f5519-e51b-41a9-bf81-52f907c0c486}} satisfies the {{formula:59c69276-63b3-4eee-816f-695477f06abe}} -condition in {{formula:7c63b492-7e50-41e2-accc-42bd7eb3da5b}} then {{formula:b9b99246-2439-4891-a601-6b127a5305ca}} is achieved by a critical point of {{formula:740812fb-f616-4ec0-aebc-721f7124b53a}} .c) If {{formula:03a13e3a-9b87-41e5-b161-ee058c42dd3e}} satisfies (I8), the {{formula:32805e9e-aea1-41b6-9fbb-9cdc6f486b0a}} -condition in {{formula:01b92081"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between quark masses and binding energies in particle physics, and how do researchers measure this correlation?",
      "expected_output": "According to the research paper, the heavy quark masses can be presented in terms of binding energies, which are assumed to be small. The relationship is given by a formula that expresses the quark masses in terms of the binding energies. Researchers use a nonrelativistic approach and diagrams, such as those in Fig.1, to describe the decays and measure this correlation. For example, in the case of a meson with four-momentum, the quarks are assumed to move with equal velocity, allowing for the calculation of the binding energy and its relationship to the quark masses.",
      "context": [
        "86f9}} {{formula:1c5199f2-6138-42fe-967b-ded1ed6fe1be}}Assuming that quarkonium binding energies are small, the heavy quark masses may be presented in the terms of {{formula:86ee37e0-7de0-403b-b8f8-1e925c03885a}} and {{formula:33aa34a1-2869-4b43-9e76-c4a6ec83b868}} as follows {{formula:f2673122-92c1-4af9-8f5d-12f3ce3dec52}}The transition from the amplitude of free heavy quarks with equal four–velocity {{formula:8248c6da-ac09-4d57-984f-8f67c8ff6bfb}} production (or decay) to the amplitude of the binding state production (or decay) is done using following prescription: {{formula:f4470ca4-cb21-43be-87a7-78dbd264fa57}}where {{formula:04c168",
        "d665656a1}} decays is the fact that both quarks in the {{formula:171399b7-6463-4ea2-9211-c908bb571a10}} meson ({{formula:38d5e2e0-34e3-4cb9-abcc-1b6d110f654b}} and spectator {{formula:dfe9c609-cacb-435b-8c8b-3f418c88f89b}} –quark) are heavy. The necessary condition of {{formula:1528d9e3-b653-45de-b9db-0e6f7199ee3d}} bound state formation creation is large momentum transfer {{formula:b879ea33-49e5-4e8e-bb41-0fffcc6abe5e}} to the spectator quark and spectator picture is not valid in the decays under consideration. In fact, the gluon virtuality in the decays (REF ) is equal to {{formula:f2881d55-59af-4fdf-9468-ff38e22de7",
        " decays {{formula:24e2c303-df65-43ce-96ae-bad0392a50e8}} are described by diagrams in Fig.1. Using nonrelativistic approach, we assume that quarks in a heavy quarkonium move with equal velocity. So, in the case of {{formula:149df547-b516-4df7-a79f-5552e1ce27e2}} meson with four–momentum {{formula:7752ab69-8144-4d4c-9865-5ee8f9ae6eee}} one has {{formula:689b8b67-6faa-4922-8cda-e725be949ccd}}and in the case of {{formula:ffa88684-723f-4fe7-ba4c-66591ac195c9}} meson with four–momentum {{formula:f966cdb2-c776-473b-b2ae-949ce57086f9}} {{formula:1c5199f2-6138-42fe-967b-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the decay of active bond fraction in surviving runs over time, and how do the methods used in this research paper support these results?",
      "expected_output": "The main findings of the research paper indicate that the fraction of active bonds in surviving runs does not decay to zero over time, instead remaining at a finite and large fraction. This is supported by the methods used, which show that the decay of the number of surviving runs is the primary factor in the observed decay of active bonds. Specifically, the paper states that the fraction of active bonds as a function of time decays exponentially fast to zero for a certain parameter, but the survival probability and the fraction of active bonds restricted to surviving runs follow specific equations, demonstrating that surviving runs do not order. The research also highlights the difference between the behavior on regular lattices and BA networks, with the latter exhibiting a distinct scaling of time with the number of nodes.",
      "context": [
        " of active bonds in surviving runs {{formula:ef9d18fc-f86f-49bd-92dd-b4a5eda8b3d4}} does not go to zero when {{formula:7f8cbe03-fd89-4877-acf3-1076755ad722}} grows. This means that surviving runs do not order; they rather stay in configurations with, on average, a finite (and large) fraction of active bonds. Random fluctuations bring eventually all surviving runs to the fully ordered absorbing state; however, as long as the runs survive they do not order on average. The decay of {{formula:cbe55512-7fb8-4154-95f8-e4bbcd4b0360}} is just a consequence of the decay of {{formula:07ff005c-9d6f-47d3-afac-e680ae32c509}} , the number of such surviving runs.This is completely different from what occurs on regular lattices. In such a case {{formula:cd7788fc-1fc0-4e9a-8643-0b9695d545ef}} remains 1 up",
        " attachment).Also on the BA networks the fraction of active bonds {{formula:8853bb54-1ea9-4da4-ad59-1665c59a4c8f}} as a function of time decays, for {{formula:19357224-0318-4e4c-8a5c-35cba2876296}} , exponentially fast to zero. The survival probability {{formula:f92bdd02-369c-4b8d-a95a-0963530bd546}} and the fraction of active bonds restricted to surviving runs {{formula:63fbd1e6-bcf9-497c-9407-18e77eac8603}} (Fig. REF ) follow Eqs. (REF ) and (REF ), i.e. on finite systems the model always reaches the perfectly ordered state, but surviving runs do not order. {{figure:b130f7d3-50e1-456c-b637-ac5f39fed42d}}The only difference is the scaling of time with the number {{formula:18c5f5b3-5e3d-4c55",
        " i.e. the probability that a run has not reached the fully ordered state up to time {{formula:2842b2c1-d978-45c5-acb1-d4807eab1b16}} and {{formula:ddd3cfc9-5db5-4764-8ad9-333c05f0a511}} , the fraction of active bonds averaged only over surviving runs. Clearly the equality {{formula:594cdad0-61b8-4d90-94a5-036776ac4509}} holds.The survival probability is evaluated in Ref. and it reads, for {{formula:36f7dd82-1e3d-4aec-940d-fdd85803f3ec}} {{formula:54fc266e-c4d2-439e-8b90-34019741a285}}The quantity {{formula:fda4eef8-516a-411b-a459-5efc9574aedd}} is easily computed and turns out to be {{formula:0469ce33-e90f-41bf-94c6-55cf"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do dark matter and anisotropy stabilize neutron stars, and what are the main findings of this research?",
      "expected_output": "The research does not discuss dark matter and its effect on neutron stars. However, it does mention that anisotropy plays a crucial role in stabilizing Néel skyrmions in ultra-thin nanodots. The main findings of this research include the development of an analytical expression for the energy of a Néel skyrmion, which considers exchange, uniaxial anisotropy, Dzyaloshinskii-Moriya, and dipolar contributions. The study found that the dipolar contribution cannot be neglected and that both Dzyaloshinskii-Moriya interaction and anisotropy are important for stabilizing the skyrmion. The research also obtained a good agreement between analytical calculations and previously published micromagnetic simulations.",
      "context": [
        " have obtained an analytical expression for the energy {{formula:c9056851-657d-4933-8dc8-4320d2cf0fb1}} of a NS whose profile of magnetization is given by Eq. (1). The idea is that the radius of the skyrmion {{formula:54a4cfd4-d4c4-430f-a359-70d0dba16c2b}} is the minimizable parameter. In order to verify that the proposed magnetization profile is adequate and correctly describe a NS, as well as the analytical expressions obtained for the different energy contributions, we will obtain analytical results for ultra-thin Co/Pt nanodots, with the aim of comparing these results with numerical simulations previously reported . In this way, we have considered the geometric and magnetic parameters used in this reference, that is, {{formula:cd432ad5-f5fd-476f-acfb-9ee27898b9f8}} , {{formula:d7a9d059-6152-4590-a79f-247e43e077a0}} , {{formula:6d0563d0-a",
        "6844f1}} of a NS confined in an ultra-thin Co/Pt nanodot as a function of the radius of the skyrmion {{formula:456aadb9-f881-4894-9e78-e9ac3f1cc46a}} for different values of {{formula:830c433a-d7b1-44a6-be99-6da9fb7076a2}} . The left column (Figs. 4(a), (b) and (c)) shows the total energy for different values of {{formula:faf51356-c391-4283-b4b3-b6f126608687}} (with {{formula:c20e6e85-d49b-4a97-aed0-b66c59a61cce}} ), while the right column (Figs. 4(d), (e) and (f)) shows the total energy for different values of {{formula:a9924f1a-9d5d-4819-867d-4cc4b13509a6}} (with {{formula:364f249f-",
        "ization of the particular case {{formula:1f8b3935-38a1-466a-927a-4e6f439cef64}} previously used by Guslienko for the study of BS in a nanodot with perpendicular anisotropy. In particular, we have obtained an analytical expression for the energy of Néel skyrmions in ultra-thin nanodots considering exchange, uniaxial anisotropy, Dzyaloshinskii-Moriya, and dipolar contributions, the latter not considered in previous studies , .As proof of concept, we have calculated the energy of a Néel skyrmion in an ultra-thin Co/Pt cylinder, and we found that the dipolar contribution cannot be neglected and that both Dzyaloshinskii-Moriya interaction and anisotropy play an important role to stabilize the skyrmion. We have obtained a good agreement between our analytical calculations and previously published micromagnetic simulations for {{formula:5bbf7d09-2d64-4446-b54c-51950c9befc6}} . In particular, we have obtained that for a D"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role did Doppler tomography and transit models play in determining the inclination and impact in the research, and how did the findings contribute to the overall understanding of the subject?",
      "expected_output": "Doppler tomography played a crucial role in determining the inclination and impact in the research by providing additional constraints on the system parameters. The transit models, including those with and without gravity-darkening effects, were used to fit the transit light curves from various instruments such as MuSCAT, MuSCAT2, and TESS. The findings contributed to the overall understanding of the subject by allowing for a better measurement of the stellar inclination and transit impact parameter, which are essential for characterizing the system. For example, the research found that the results from the fits were largely insensitive to the details of the pulsation modelling used, as long as sufficient frequencies were considered. This suggests that the inclusion of Doppler tomography and transit models helped to improve the accuracy and robustness of the results.",
      "context": [
        "cdbd041e-84d5-47ff-88dc-44b820bc433a}} from Doppler tomography, as well as the transit measurements from the MuSCAT and MuSCAT2 instruments. We also include the TESS fits from and , for which we plot the result for their fit with {{formula:5b8a1fc9-c188-431f-b00c-d67fe64e2089}} fixed. We note that transit light curve fits are not able to distinguish between {{formula:e8de67b8-1199-48ed-8e76-785ed8fb7ec7}} and {{formula:73436b1f-c5a9-44a3-95ec-db9eebd815df}}i.e. between a transit occurring across the northern half of the stellar disc, and one occurring across the southern half., but we plot these points with negative {{formula:1c68d999-5256-47b8-b8c3-059646eafb5b}} , since that is what is expected at these epochs from the DT results. We note that one of",
        "b5b}} , since that is what is expected at these epochs from the DT results. We note that one of these TESS points appears to be a slight outlier ({{formula:199a8444-e51e-4b12-b01f-359a40addf7c}} ), and suggest that this may be the result of the well-known degeneracy between {{formula:bc4ed701-8484-43bc-9cd4-e981596f4e64}} and {{formula:d38f6851-16a4-4b2c-9acc-7be72dbba8e1}} (and perhaps also the gravity-darkening parameters). One of the DT points is similarly discrepant from the model, but with a much smaller error bar, it is less noticeable.Because the four CHEOPS transits are spread over more than 400 d, we also performed two additional fits, fitting only the first two transits (which are separated by only 13 d) and only the last two transits (separation: 61 d). The fits were otherwise identical to the {{formula:f0e13967-ee52-4945-",
        " from Doppler tomography by , {{formula:a5db83a1-9461-4ef0-84cb-3610f1d1d4e5}} . This allows us to measure better the stellar inclination (from the gravity-darkening effect), as well as the transit impact parameter. We also tried fitting using a transit model that does not include the gravity-darkening effect (i.e. the list of fitted parameters above does not contain {{formula:33a5c7ba-2083-4f8f-87b7-37953e326ec7}} or {{formula:071d8d80-bfac-4b94-8521-722ccbc8d66b}}, and beyond limb-darkening the stellar disc is assumed to be uniformly bright).We found the results of these fits to be largely insensitive to the details of the pulsation modelling used, as long as sufficient frequencies were considered. The results from the fits where the pulsations were modelled alongside residual systematics using the wavelet method (Section REF ) are consistent with those where Equation REF and a list of frequencies derived from the TESS data were used. When"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on Vlasov/N-body simulation and its impact on Universe structure formation, and how do the methods used in this study contribute to our understanding of key results in this field?",
      "expected_output": "The research on Vlasov/N-body simulation achieved excellent performance scaling with up to 147,456 nodes on Fugaku, with weak and strong scaling efficiencies of 82% - 96% and 82% - 93%, respectively. The study presented the world's first Vlasov simulation of cosmic relic neutrinos on a full six-dimensional phase space domain and the largest Vlasov simulation ever conducted. The methods used combined the advantages of particle-based N-body simulation and Vlasov simulation for a mixture of different kinds of matter components, contributing to our understanding of Universe structure formation. For example, the simulation of the CDM component used 147,456 computational nodes, which can be regarded as a full system run of the Fugaku supercomputer.",
      "context": [
        "2ac06570-936e-4552-acba-b5e468ec9532}} -body particles for the CDM component is proportional to that of Vlasov mesh grids as {{formula:829d9436-445c-4932-86e6-8b71d506c2a8}} , except for that of the largest run (U1024), in which {{formula:b952203f-26cf-444c-857a-c09b93c9be45}} is same as that in the H run group and set to {{formula:c5b465f2-46a5-446a-9ff4-e6e0625b9a35}} . Note that H1024 and U1024 employ 147,456 computational nodes out of Fugaku's entire system (158,976 computational nodes), and thus they can be effectively regarded as full system runs of Fugaku supercomputer. {{table:759614e4-850f-48b5-b7ac-ac418f9ad410}}\nScalability: In order to measure the weak and strong scalings of our hybrid Vlasov/N",
        " number of MPI processes along each axis of domain decomposition, and the number of MPI processes per node. We adopt a naming convention of these runs in which the first letters S, M, L, H and U denote the number of spatial mesh grids of the Vlasov simulation {{formula:2ff85495-4ce3-48a9-9dab-779ebeb549e3}} , {{formula:8b4f09ef-9b8d-4eed-8788-08e8df26607a}} , {{formula:82625597-21c0-4da1-b658-920be82714e1}} , {{formula:c644e9bc-f03b-49a6-b3e9-7b82c9512874}} and {{formula:a854b32f-d605-4b55-884b-72e9075480ad}} , respectively, followed by the number of computational nodes in units of 144 nodes. The number of {{formula:2ac06570-936e-4552-acba-b5e468ec9532}} -",
        " performance scales excellently with up to 147,456 nodes (7 million CPU cores) on Fugaku; the weak and strong scaling efficiencies are 82% - 96% and 82% - 93%, respectively.\nJustification for ACM Gordon Bell Prize: We present a series of hybrid Vlasov/{{formula:6bbd1d85-6e21-4a37-979c-c7bea743fd9a}} -body simulation of the large-scale structure formation in the Universe. This includes the world's first Vlasov simulation of cosmic relic neutrinos performed on a full six-dimensional phase space domain, and the largest Vlasov simulation ever conducted. This is also the first successful run in the world that combines the complementary advantages of the particle-based {{formula:7e6e4f89-39c0-4f61-98b6-2959a6175ebe}} -body simulation and the Vlasov simulation for a mixture of different kinds of matter components. Our simulations are performed on Fugaku supercomputer installed at RIKEN Center for Computational Sciences with up to 147,456 nodes (7,077,888 CPU cores). We achieve"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does k-perp-factorization play in the calculation of perturbative invariants, and how does it impact the overall results of the research?",
      "expected_output": "The k-perp-factorization approach plays a crucial role in the calculation of perturbative invariants by providing resummed perturbative calculations for many different processes, allowing for the introduction of quantities that are invariant with respect to factorization-scheme transformations. This enables the elimination of factorization-scheme dependence and emphasizes the perturbative QCD component, as discussed in the paper 'k-perp-Factorization and Perturbative Invariants at Small x' (hep-ph/9608310). For example, ratios of hadronic cross sections and properly defined factors can be used to cancel the dependence on parton densities, resulting in factorization-scheme independent quantities.",
      "context": [
        "ula:0f879749-1563-4dbf-ba07-3de1c49e35b3}} .A better theoretical control on perturbative physics can be achieved by exploiting the very physical content of the collinear-factorization theorem, that is, the universality (process independence) of the parton densities. This means that the same parton densities and the same perturbative approach have to be used to study the small-{{formula:5ca9a6bf-adf4-4b58-b10d-43ad42e2544b}} behaviour of different physical observables. Considering more observables, besides {{formula:808d02c3-cb6c-4262-84a6-59cca8a0306f}} , one can (over-)constrain the definition of the parton densities thus eliminating the factorization-scheme dependence and emphasizing the perturbative QCD component.The {{formula:0024becf-4bd1-4c93-8ecb-c05315993bd6}} -factorization approach shares these universality features and provides resummed perturbative calculations for",
        "05315993bd6}} -factorization approach shares these universality features and provides resummed perturbative calculations for many different processes. Thus, one can introduce quantities that are invariant with respect to factorization-scheme transformations. As discussed in Refs. , , one can consider ratios of hadronic cross sections and properly defined {{formula:ad9670a5-102a-4c40-9334-3bafd8a81849}} -factors (the functions {{formula:d8d491a8-03a0-4b36-bfca-b1c2f76b2ba2}} in Eq. (REF ) are an example for that) in which the dependence on the parton densities cancels. These {{formula:71b2da4f-ba5d-4d43-b3f0-73dc795b9bff}} -factors are factorization-scheme independent by definition. Analogously, by simply using two hadronic observables one can formulate the dynamics of scaling violation enterely in terms of perturbative quantities that play the role of physical anomalous dimensions .\nPhysical anomalous dimensions: Considering",
        "# arXiv Paper: hep-ph/9608310\n\n$k_{\\perp}$-Factorization and Perturbative Invariants at Small $x$\nabstract: I review the basic idea of $k_{\\perp}$-factorization and its relation to collinear factorization. Theoretical results in resummed perturbation theory are summarized and the example of the heavy-flavour structure functions is explicitly considered. Using these results one can investigate the small-$x$ behaviour of quantities that are independent of the non-perturbative parton densities. In particular, one can introduce physical anomalous dimensions that relate the scaling violations in different hadronic observables.\n{{formula:ac14d3e0-febb-4de6-beac-68bd7020f886}} -factorization in hadron collisions at high energy: Hadron collisions at large transferred momentum {{formula:63d877f1-e93c-453d-909c-e0464b0bb495}} ({{formula:2f1f556c-1b48-48d1-bd53-7dfc0ed2aa"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the superstring field equation of motion, and how do the methods used in this research relate to its key results?",
      "expected_output": "The main findings of the superstring field equation of motion include the derivation of the equation of motion for the original string field with a specific identification, which is a natural extension of a previous reference for the bosonic case. The methods used in this research involve explicit construction for a class of marginal deformations, following a strategy adopted from previous work, and using an action in terms of deformed structures to derive the equation of motion. For example, the equation of motion is given by a specific formula, where the superstring field of ghost number zero is denoted by a particular symbol, and the leading term in the expansion is given by another formula. The research also relates the deformed string field to the original string field through a specific relation, which is an extension of a previous reference.",
      "context": [
        "its . The equation of motion is {{formula:44b0355a-b381-4c57-91e2-271894441455}}where {{formula:c36c8579-3a6c-42da-8272-02f635e7feb4}} is the superstring field of ghost number zero. The leading term in the expansion of (REF ) in {{formula:d5bfc326-fa75-4e6b-b899-06e8020ecd1d}} is given by {{formula:936e5cf7-7657-49c8-ae57-8ba62f5c3777}}where we have used {{formula:e26e7bbd-1f80-481a-b63f-03aede498625}} . To leading order in the deformation parameter {{formula:ec6ffbac-f51a-4ff2-932f-fd661e4e207b}} , a solution {{formula:0fbad1c4-cd25-49eb-a847-0f7cea3af1c9}} associated",
        "}}where we denoted the real superstring solution (REF ) by {{formula:07379cdc-c817-4804-a0da-fc504e3869c0}} . The equation of motion can then be written as {{formula:9754b395-0f57-4155-93d7-929d45780ebd}}We recognize this as the equation of motion for the original string field {{formula:f95fe9c4-13ab-4a06-87bc-9ae15a52ee3f}} with the following identification: {{formula:5511815c-47c1-4df2-ad36-7a358e38c105}}This is the relation between {{formula:fa6b62fb-6ef2-47c5-ab38-bc38aebcf4f1}} and {{formula:ff08701b-722b-4622-99f0-25fd06f99772}} , which is a natural extension of (REF ) for the bosonic case.\nExplicit construction for a class of marginal deformations: We followed the strategy adopted in , and we",
        "8}} , {{formula:86d1e646-e6f8-4494-a557-31e01590a8a1}} , and {{formula:9996b65d-7260-4278-a49d-2c8f405f0c22}} are of {{formula:21757cfc-bb71-44d7-9553-0d762a390000}} . The equation of motion derived from the action in terms of the deformed structures is {{formula:a8bb507f-9f49-48b9-9808-dde5999a122f}}where {{formula:f1cb631f-6187-4402-8b0f-d15d088ebce1}} is related to the original string field {{formula:44e95812-caeb-48b1-b220-2cc7611f2141}} expanded around the real solution (REF ), which we now denote by {{formula:4a01105e-b68c-4dbe-8715-9e8f334d57e0}} , as follows: {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of angular momentum on boson behavior under perturbation in this research paper?",
      "expected_output": "The research paper explores the behavior of bosons under perturbation, specifically focusing on the impact of angular momentum. It assumes the 'bare' particles to be bosons, which means the angular momentum must be even. The paper discusses the difficulties that arise when treating the dependent terms as a perturbation, particularly for cases where the unperturbed wave functions do not vanish at the origin. The exact ground state energy is calculated, and perturbation theory is applied to understand the behavior of bosons. The findings highlight the importance of considering all orders in perturbation theory to accurately describe the behavior of bosons under certain conditions.",
      "context": [
        "2-6535-46db-bc6d-56108c572161}}{{formula:718396ec-e938-4992-98dd-7369cc018be0}} is the angular momentum, and we assume the “bare” particles to be bosons, whence {{formula:aefc2e9c-8fe9-4064-a2c6-09e38eead1d5}} must be even. We now wish to treat the {{formula:b1369ed5-f5bc-45b1-b2dc-99b6aec644f1}} dependent terms as a perturbation. The difficulties arise for {{formula:62c95f02-f81a-4220-a3ea-ea443e1a8698}} , when the unperturbed wave functions do not vanish at the origin. In fact, it is enough to consider the ground state. For {{formula:9de1f3e9-6784-4de9-abe9-c1b960660457}} , its wave function is {{formula:bbdc6a9b-54e",
        ", is {{formula:b5556476-17b0-4e8f-a2f0-874828e8defe}} for any {{formula:f1419846-30e0-41dc-aff1-06575de2b87f}} .)\nExact solution: To be able to check the perturbative results, let us first find the exact ground state energy of {{formula:58305801-d4dd-45a2-a06b-a0d6423fc42b}} . Symmetry with respect to a change {{formula:b3f6edea-f080-4237-b7fa-a840aafa95a7}} is present, so it is enough to consider {{formula:803dee7c-b7ef-44ed-868a-e059fa88af08}} , which is what we will assume, unless otherwise specified. For {{formula:781b1274-646a-4cb7-b481-48c97a261751}} , the wave function with the correct behavior at infinity is {{formula:0d44033f-",
        "3aea8adab23}} one gets {{formula:56c9bdaf-8877-4399-b893-15b9d8033c8f}} , i.e., {{formula:234ae60a-680e-4126-8dc4-483a36029db8}}For {{formula:a1ec25c4-30b2-4177-950a-3604cbe75967}} , this tends to {{formula:be0c38f0-ba78-45aa-aa67-084a1614411b}} , but to see this, one has to take into account all orders in {{formula:d29a089a-78b6-45d3-8cd5-1d82c9a30c0e}} .\nPerturbation theory: Let us now come back to perturbation theory. Following the idea of , , in order to get rid of the singularities we redefine the wave function as {{formula:a9873e7b-aba3-40af-9350-6b58341521df}}where {{formula:7213d4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding strong ergodicity and condition satisfaction, and how do the methods used relate to the key results?",
      "expected_output": "The research paper discusses strong ergodicity and condition satisfaction, where it's found that given the input, one gets a specific outcome since a particular condition is met. For instance, it's clear that a certain formula implies that a condition is satisfied on a specific set. If a certain value is the strong ergodic, then it satisfies a particular condition on another set. The methods used, such as denoting specific formulas and obtaining certain outcomes, relate to the key results by showing how strong ergodicity and condition satisfaction are achieved. The paper also mentions that taking any value and fixing arbitrary parameters leads to specific implications being true, demonstrating the relationship between the methods and the results.",
      "context": [
        " the {{formula:7a83e716-60ee-4c5f-a076-aa5b554ab11b}} -strong ergodicity one gets {{formula:91f90d15-48ca-4bac-b4ab-a195eca16116}} since {{formula:aa548b30-154b-4474-a376-941c1a466c47}} . Denote {{formula:687e2d3c-61d5-4e1c-9bbb-e88d132b060b}} From (REF ) we obtain {{formula:668c2363-e442-4e12-8637-fc389bb1fdf1}} It is clear that {{formula:b338f950-fea9-4e2b-adc7-7f462f68dafb}} This implies that condition {{formula:95026296-4b39-48af-bdc8-7107c8fd92fb}} is satisfied on {{formula:e04b5723-9c33-475e-9c0c-2324c",
        "bda-56ab-4e15-8180-279d2e5dd273}} . If {{formula:4a5daf4c-d7a9-4028-8ca8-c0b7bacf7c6d}} is the {{formula:23b777cc-6d46-4203-8475-33dc744b7cb1}} -strong ergodic, then it satisfies condition {{formula:1c980c81-696e-4fe5-a971-d4604ae46f32}} on {{formula:afbbc59b-939e-4b5c-8484-3b6378e972eb}} .Take any {{formula:f2d6d9fc-831c-4977-b568-f9bab829dace}} and fix arbitrary {{formula:f57a2701-b2d5-4f7e-8ef3-ce87e761dec7}} . Then from the {{formula:7a83e716-60ee-4c5f-a076-aa",
        "ula:088d06f4-f60b-4dd3-8df0-460fc1c9bb9f}} with {{formula:7d7e65b6-5e1e-40eb-a259-3c9793ad9c0c}} {{formula:c3aaa38f-0764-4025-bfb9-d3814a57dcca}} is the {{formula:443fcd28-56d1-48c6-885c-73819f4b8d58}} -weak ergodic; {{formula:b884e1bb-9a1b-4555-a5e6-8f635c0b942d}} satisfies condition {{formula:495b09db-16d7-4c0d-a702-06be41f6ab08}} on {{formula:db1b1e29-7cc9-4453-bd5d-680ed0c0ce17}} ;the implications are true: (i){{formula:494239b3-516d-48d3-b2ec-cf2e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between G_j, p_j and Carathéodory convergence in the context of this research paper?",
      "expected_output": "default",
      "context": [
        " that \\lbrace G_j\\rbrace _{j>1}=\\lbrace f_j (\\Delta )\\rbrace _{j>1} converges to \\Delta by, taking first the closure of the G_j's, taking the limit, which is the closed unit disc, and finally taking the interior. However, it is well-known that \\hat{f} (\\Delta ) is the upper half-disc \\Delta ^+:=\\lbrace x+iy \\in \\Delta : y > 0\\rbrace . In contrast, the convergence of \\mathcal {G}:=\\lbrace (G_j, \\frac{i}{2})\\rbrace _{j>1}=\\lbrace (f_j (\\Delta ), p_j)\\rbrace _{j>1} in the sense of Carathéodory finds exactly the correct limit, namely \\textup {\\textbf {Ker}}_{i/2}(\\mathcal {G})=(\\Delta ^+,\\frac{i}{2})=(\\hat{f}(\\Delta ),\\hat{p}).Definition 2.4 Given a sequence of pointed domains \\lbrace",
        "C}\\colon -1<x<1-\\tfrac{1}{j}, \\ y=0 \\big \\rbrace .Let \\hat{p} := p_j:=\\frac{\\sqrt{-1}}{2} =\\frac{i}{2}. By the Riemann mapping theorem there exists, for every j>1, a biholomorphism f_j mapping \\Delta onto G_j with f_j (0)=\\hat{p} and f_j ^{\\prime }(0)>0. The sequence of maps \\lbrace f_j\\rbrace _{j>1} contains a subsequence that converges uniformly on compact subsets (or alternatively, locally uniformly) by Montel's theorem. So we may assume, by taking a subsequence, that this sequence converges in the same manner to \\hat{f}\\colon \\Delta \\rightarrow \\mathbb {C}. Let us now find \\hat{f} (\\Delta ).In the sense of the Hausdorff convergence, it may be natural to expect that \\lbrace G_j\\rbrace _{j>1}=\\lbrace f_j (\\",
        " sequence \\lbrace (G_j, p_j)\\rbrace _{j\\ge 1} is tamed at \\hat{p} \\in \\mathbb {C}^n. (B) For each subsequence \\lbrace (G_{j_k}, p_{j_k})\\rbrace _{k\\ge 1} it holds that \\textup {\\textbf {Ker}}_{\\hat{p}} \\lbrace (G_j, p_j)\\rbrace _{j\\ge 1} = \\textup {\\textbf {Ker}}_{\\hat{p}} \\lbrace (G_{j_k}, p_{j_k})\\rbrace _{k\\ge 1}.Example 2.3 Let \\Delta = \\lbrace z \\in \\mathbb {C}\\colon |z|<1\\rbrace be the open unit disc in \\mathbb {C}. Then for each j \\ge 1 putG_j := \\Delta \\setminus \\big \\lbrace x+iy \\in \\mathbb {C}\\colon -1<x<1-\\tfrac{1}{j}, \\ y=0 \\big"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does galaxy mass play in the formation of r-process stars, and how do the main findings of this research paper support this relationship?",
      "expected_output": "According to the research paper, galaxy mass plays a significant role in the formation of r-process stars. The paper suggests that less-massive galaxies have a smaller star formation efficiency (SFE), leading to an increase in [Eu/Fe] at lower [Fe/H] levels. For example, the model predicts that highly r-process-enhanced stars, such as J1521-3538 and CS 31082-001, originate from ultra-faint dwarf (UFD)-sized building blocks. In contrast, r-process-deficient stars likely originate from more massive building-block galaxies. The paper supports this relationship by showing that the amount of gas available for mixing with Eu from neutron star mergers (NSMs) is smaller in less-massive galaxies, resulting in a higher [Eu/Fe] ratio. This is evident in the highly eccentric orbit of J1521-3538, which indicates that it originated from a building-block galaxy subsequently accreted by the Milky Way.",
      "context": [
        " might have been contaminated by the binary mass transfer from former asymptotic-giant-branch stars. The smaller SFE for a less-massive galaxy leads to an increase of [Eu/Fe] at a lower [Fe/H]. It is notable that our model predicts that almost all highly r-process-enhanced stars ([Eu/Fe] {{formula:6690dec6-357a-435c-9192-97c91343eb8d}} at [Fe/H] {{formula:84a9ff79-b2a6-4efd-80d2-78087b0e5c33}} ) originate from UFD-sized ({{formula:671aa83a-9c0d-47dc-a03c-547de193633c}} ) building blocks (but see a caution in § for the applicability of the mass-metallicity relation of to UFD-sized systems), as also suggested in , . For instance, our model suggests that J1521-3538 and CS 31082-001 were born in building-block galaxies of {{formula:1b784392-1c7f-4",
        "6a302d79-a6bf-4b37-be95-619c179723fe}} ) and r-process-deficient ([Eu/Fe] {{formula:846bfdb9-e4e4-4f11-8868-d15bf35a07f0}} ) stars likely originate from UFD-sized ({{formula:eca459b0-b5e8-43a6-8a7f-67bc7f9085c0}} ) and massive ({{formula:2779cb7e-b5a2-4d47-958c-232e948d8f4d}} ) building-block galaxies, respectively; the latter includes the in situ halo. Subsets of CCSNe, e.g., collapsars or MRSNe are not favored as candidates for the major r-process site in the MW halo but cannot be excluded as those in the satellite dwarfs or the disc. Finally, the natal kicks of NSMs appear to play a subdominant role for steepening [Eu/Fe] (and flattering [Eu/Mg]) at high metall",
        "-001 were born in building-block galaxies of {{formula:1b784392-1c7f-4db3-af2b-99159b1013f8}} and {{formula:50b82b93-2ac1-4d38-a49c-3ad3b621defd}} (in {{formula:7ca0c1be-52bd-4a45-92f3-61ddf536f0a9}} ), respectively. This is a consequence of the fact that a less-massive galaxy contains a smaller amount of gas to be mixed with Eu from a NSM. In fact, the highly eccentric orbit of the former indicates that J1521-3538 originates from a building-block galaxy subsequently accreted by the MW . In contrast, the r-process-deficient stars may have been born in the most-massive building-block galaxies of {{formula:d963198a-ad76-4a7b-857d-923f76e56bec}} {{formula:b5dbe7fe-81e7-481a-b0a3-4b2abcda4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does graph theory play in enhancing robust control, and how do its methods contribute to the main findings of this research?",
      "expected_output": "Graph theory plays a crucial role in enhancing robust control by facilitating the mathematical definition of periodic control patterns, known as simple cycles, which form a linear basis for locomotion gaits. The graph representation of discretized robot behaviors allows for easy visualization and introduces robustness into the control framework by making it fault-tolerant. For example, in the event of a fault, such as the loss of a limb, the graph structure can be modified at runtime by removing the corresponding nodes and arcs, and the optimal gaits can be re-computed. This is demonstrated in the research paper, where the use of graph theory enables the optimization problem to be formulated as an Integer Linear Programming (ILP) problem, which can be quickly solved using standard linear solvers.",
      "context": [
        " advantageous - 1) The graph representation of discretized robot behaviors facilitates easy visualization, 2) the unique mathematical representation of simple cycle vector (Equation REF ) is instrumental in defining the 3) circulation-locomotion gait analogy (Equation REF ). 4) The optimization problem is an ILP problem (Equations REF , REF ) which can be quickly solved using standard linear solvers for small to medium size graphs.Speed. The periodic control sequences (simple cycles) are independent of actuation and material variations. Hence, the speed of locomotion is solely dependent on speed at which a robot can transition from one state to another. As an example, given two same soft robots {{formula:9e2694fe-7d90-4b97-93b0-fa51323caac7}} actuated by actuators (e.g. motors) {{formula:4a92802a-6195-4c55-b67b-6fb1ac1e1c5e}} with power {{formula:cca92724-279a-4846-b1e5-6cf1af81f990}} such that {{formula:563b3275-",
        "environment interaction, 2) using graph theory to visualize relationships between discretized robot states, 3) learning the surface-dependent results of state transitions and 4) optimizing desired cost function to obtain a control sequence. Computationally, the optimization problem is an Integer Linear Programming (ILP) problem that can be solved using standard solvers. The use of graph theory facilitates mathematical definition of periodic control patterns (simple cycles) that form linear basis to locomotion gaits. Furthermore, the graph representation introduces robustness into the control framework by making it fault-tolerant.\nModel-free control framework: Controlling locomotion by soft robots is challenging due to the difficulty to accurately model most robot-environment interactions. This interaction is easier to model in fluids because there are good mathematical tools for describing force propagation in continuous media, however, modeling discontinuous terrestrial interactions is much more complex. Furthermore, the modeling of the soft robot may be restricted by shape (for analytical continuum solutions) , or involve simplification/discretization . The complexity is further increased by actuator-specific modeling. As an example, the properties of shape memory alloy actuators (SMAs) change over time because heat flux cannot be controlled very precisely in natural settings. The model-free control approach takes",
        " as the loss of a limb. In this case, one of the actuators/sub-systems of the robot becomes inoperable. A runtime modification of the graph structure (by removing the corresponding nodes and arcs), followed by re-computation of the optimal gaits can handle this situation.Furthermore, given the data on a certain class of actuators, the length constraint of maximum number of state transitions in equation REF can be replaced by a weighted constraint i.e. {{formula:ace1e043-32a0-4660-a3e8-d801f956a2cf}} , where {{formula:39b0f95a-ca25-4804-974f-924e4ed1c9c2}} is vector containing time taken for each state transition, and {{formula:85cf78d1-a37d-4462-9a93-926bc8964c64}} is the maximum allowable gait time.\nExperiment: The experimental soft robot is similar to the example soft robot, but has three limbs with gripper-like two state friction mechanisms at end of each limb as shown in Figure REF . The soft robot has a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how voxel-based methods' information loss and compression affect event-based vision, and how do these methods impact the overall quality of the vision?",
      "expected_output": "The main findings indicate that voxel-based methods suffer from information loss due to drastic compression, resulting in motion blur and accuracy drop for dynamic scenes. This compression, which can be as high as 30k times, reduces spatial-temporal 3D data to 2D, causing misalignment problems. The proposed Aligned Event Tensor (AET) aims to alleviate these issues by quantizing and voxelizing event data in high temporal resolution, minimizing information loss and improving overall vision quality.",
      "context": [
        "-based approaches such as and achieve the state-of-the-art (SOTA) performance on several event-based datasets, while enjoying higher efficiency than point-cloud-based methods, due to the good regularity for parallel computing on GPUs.Nonetheless, the voxel-based methods have to compress the data drastically to meet the memory and computational requirements, and hence suffer from information loss. According to the observation in our early-stage experiments, the reduced temporal resolution also causes problems such as motion blur and accuracy drop for dynamic scenes. Moreover, to fit the neural networks originally designed for image data, they usually stack the different frames along the channel axis , , which makes the temporal information tangle with channel information. Therefore, voxel-based methods are limited by information loss and flexibility under scenes at different speeds.To this end, we propose an efficient representation for event data, which can alleviate the information loss problem of voxel-based methods, while also being robust under both static and dynamic scenes. We name it Aligned Event Tensor (AET). In AET, the event data are quantized and voxelized in high temporal resolution to minimize information loss. The voxels are then compressed in local spatial-temporal regions",
        " approaches encounter motion blur problems due to the drastic compression in the compromise of memory cost (e.g., 30k times compression to 9 frames). The accuracy also drops because of information loss during the compression from spatial-temporal 3D to 2D. In addition, we argue that since events are triggered by the moving objects in the real world, the pixel-wise operation in , will mistakenly compress events from different objects (or parts) into one element, causing the misalignment problem.\nMethodology: In this section, we will introduce the proposed Aligned Event Tensor (AET) and the Event Frame Net (EFN). AET is an efficient voxel-based representation for event data, which is end-to-end trainable and alleviates problems of information loss, motion blur and misalignment, while EFN is a versatile network for handling both static and dynamic scenes for event-based vision. {{figure:2abe8cc6-58f2-4c66-89de-e4333838f35e}}\nAligned Event Tensor: AET comprises three parts, namely, Timestamp Quantization, Accumulative Voxelization and Aligned Compression, as shown in Fig. REF",
        "# arXiv Paper: 2103.11645\n\nAET-EFN: A Versatile Design for Static and Dynamic Event-Based Vision\nabstract: The neuromorphic event cameras, which capture the optical changes of a scene, have drawn increasing attention due to their high speed and low power consumption. However, the event data are noisy, sparse, and nonuniform in the spatial-temporal domain with an extremely high temporal resolution, making it challenging to design backend algorithms for event-based vision. Existing methods encode events into point-cloud-based or voxel-based representations, but suffer from noise and/or information loss. Additionally, there is little research that systematically studies how to handle static and dynamic scenes with one universal design for event-based vision. This work proposes the Aligned Event Tensor (AET) as a novel event data representation, and a neat framework called Event Frame Net (EFN), which enables our model for event-based vision under static and dynamic scenes. The proposed AET and EFN are evaluated on various datasets, and proved to surpass existing state-of-the-art methods by large margins. Our method is also efficient and achieves the fastest inference speed among others.\nIntroduction: Recently,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between D_L, D_M, and redshift z, and how do these parameters impact the main findings of the research paper?",
      "expected_output": "The relationship between D_L, D_M, and redshift z is given by D_L(z) = (1+z)D_M(z), where D_L is the luminosity distance, D_M is a function of the comoving distance, and z is the redshift. This relationship is crucial in understanding the expansion history of the universe. In the context of the research paper, the luminosity distance D_L is used to derive the distance modulus, which is then used to constrain the parameters of the f(Q) models. The redshift z is a key parameter in this analysis, as it provides information about the distance and time of observation. The authors use the Pantheon Sample of Type Ia supernovae, which covers a redshift range of 0.01 < z < 2.26, to derive the luminosity distance and constrain the models. The main findings of the research paper rely on the accurate determination of D_L, D_M, and z, which are used to test the validity of the f(Q) models and understand the evolution of the universe.",
      "context": [
        " })}\\right)\\,,and \\mathrm {sinn}(x) is defined as\\mathrm {sinn}(x)={\\left\\lbrace \\begin{array}{ll} \\sin (x), & \\Omega _{k}<0\\,,\\\\ x, & \\Omega _{k}=0\\,,\\\\ \\sinh (x), & \\Omega _{k}>0\\,, \\end{array}\\right.}where \\Omega _{k}=-k/a_{0}^{2}H_{0}^{2} with the scale factor a_0=1 for the current universe, c is the speed of light and H_0 is the Hubble parameter today. The luminosity distance can be obtained byD_{L}(z)=(1+z)D_{M}(z)\\,.In fact, considering the peculiar velocity of the observer, the luminosity distance of SNe is defined as D_{L}^{\\mathrm {SNe}}=(1+z_{\\mathrm {hel}})D_{M}(z_{\\mathrm {cmb}})\\,,where z_{\\mathrm {cmb}} is the CMB",
        "z_{\\mathrm {cmb}})\\,,where z_{\\mathrm {cmb}} is the CMB restframe redshifts of SNe and z_{\\mathrm {hel}} is the heliocentric redshifts. In the following I introduce the data sets and describe the main results on two different f(Q) models.\nSNe: The sample of Type Ia supernovae (SNe Ia) is called Pantheon Sample. This sample contains 1048 sources in the redshift range 0.01<z<2.26 . This sample covers the data records from the Pan-STARRS1 (PS1) Medium Deep Survey, Sloan Digital Sky Survey (SDSS), Supernova Legacy Survey (SNLS), and Hubble Space Telescope (HST) survey. The standard description provides numerical values of the distance modulus, which can be directly employed to derive the luminosity distance D_{L} (in Mpc) according to:\\mu (z)=5\\log _{10}\\left[\\frac{D_{L}^{\\mathrm {SNe}}}{\\mathrm {Mpc}}\\right]+25\\,.The apparent magnitude m of a supernova at redshift",
        "}{\\mathrm {Mpc}}\\right]+25\\,.The apparent magnitude m of a supernova at redshift z is given bym=\\mu +M_B\\,,where M_B is the absolute magnitude. The chi-square (\\chi ^2) of SNe Ia is written as follows\\chi _{\\mathrm {SNe}}^{2}=\\sum _{i=1}^{1048}\\frac{\\left(m_{th}(z_{i})-m_{obs}(z_{i})\\right)^{2}}{\\sigma _{m}^{2}(z_{i})}\\,.Here \\sigma _{m} is the observed error of the apparent magnitude m, while m_{th} and m_{obs} are the theoretical value and the observational value, respectively.\nCC: The Hubble parameter H(z) can be estimated at certain redshifts z byH(z)=\\frac{\\dot{a}}{a}=-\\frac{1}{1+z}\\frac{dz}{dt}\\simeq -\\frac{1}{1+z}\\frac{\\Delta z}{\\Delta t}\\,.Determining \\"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how attackers' skills affect the effectiveness of fingerprint presentation attack detection, and what methods were used to obtain these results?",
      "expected_output": "The main findings suggest that the attackers' skills significantly impact the effectiveness of fingerprint presentation attack detection. The 2017 edition of the LivDet competition focused on assessing how the composition and data acquisition methods influence PAD systems, including the presence of different operators creating and submitting spoofs. The results indicate that an average skilled attacker may have the technique needed to circumvent the matcher, but more skills are needed to spoof an integrated system. The methods used to obtain these results included analyzing the impact of the operator's skill level and the use of CNN-based methods, which are outnumbering local image descriptors methods.",
      "context": [
        " of attackers of the provided fakes, and how this ability impacts on the final performance. These two challenges together may set at which extent the fingerprint presentation attacks are an actual threat and how to exploit additional information to make the PAD more effective.\nIntroduction: In the development of biometric systems, the study of the techniques necessary to preserve the systems' integrity and thus to guarantee their security, is crucial. Fingerprint authentication systems are highly vulnerable to artificial reproductions of fingerprint made up of materials such as silicon, gelatine or latex, also called spoofs or fake fingerprints . To counteract this possibility, Fingerprint Liveness Detection (FLD), also known as Fingerprint Presentation Attaks Detection (FPAD), is a discipline aimed to design pattern recognition-based algorithms for distinguishing between live and fake fingerprints. Detecting presentation attacks is not trivial because we have an arms-race problem, where we potentially are unaware of the materials and methods adopted to attack the system .In the last decade, most of the previously proposed PAD methods have focused on using handcrafted image features, for example LBP, BSIF , WLD or HIG . Over the years, PAD methods have been refined and recently CNN-based methods are outnumbering local image descriptors",
        " . Over the years, PAD methods have been refined and recently CNN-based methods are outnumbering local image descriptors methods , .In order to assess the performance of FPAD algorithms by using a common experimental protocol and data sets, the international fingerprint liveness detection competition (LivDet) is organized since 2009 . Each edition is characterized by a different set of challenges that must be dealt with by the competitors. Working on the LivDet competitions since its birth, that is, developing a wide experience in providing large sets of data for assessing the state of the art on this topic, we realized that some data acquisition methods influence the FPAD systems performance. For this reason, the 2017 edition of the LivDet was focused on assessing how much the composition and the data acquisition methods influence the PAD systems. In particular, we faced here with the followings points:the presence of different operators for creating and submitting spoofs, that is, two levels of operators which provided spoofs and attacked the submitted algorithms - can spoofing be an actual threat independently of the ability of attackers, or an expert is necessary to break a fingerprint PAD?; when a PAD must be integrated into a fingerprint verification system, the templates of genuine users are available, and there is no good",
        "/fake classification can be exploited in real applications, where the enrolment phase allows to record the user fingerprint image previously. It is therefore possible to state that the “generic-user” approach is the main resource when the final user population is unknown (for example in forensic applications or border checks); the “user-specific\" approach can be used to improve security of an integrated fingerprint verification system (for example authentication on mobile or bank account), in which the attacks are finalize to break the verification step and not to simply being disguised for another person.On the other hand, common sense tells us that the system's performance is influenced by the attacker's ability to replicate the fingerprint. Although it is known that fakes can be produced with commonly used materials, even if these are of excellent quality, the attacker must know how to use them to spoof the system. In fact, we believe that an average skilled attacker may have the technique needed to circumvent the matcher, but more skills are needed to spoof an integrated system (both matcher and liveness detector). Although the impact of image quality has been used to classify fingerprints , , to the best of our knowledge, no one has ever analyzed the impact of the operator's skill level. {{table:89f75"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main characteristics of Calabi-Yau 3-folds according to Definition 1.1 and how do these characteristics relate to the main findings of the research paper?",
      "expected_output": "According to Definition 1.1, the main characteristics of Calabi-Yau 3-folds are that they are smooth, complex, projective 3-folds with specific properties. The research paper highlights that these characteristics are crucial in understanding the geometric transitions and deformations of Calabi-Yau 3-folds. For example, the standard example of a smooth quintic threefold in projective space is given, and the paper discusses how the dimension of the complex moduli space of Calabi-Yau 3-folds can be determined. The characteristics of Calabi-Yau 3-folds are also related to the concept of conifold transitions and the calculation of Betti numbers, as described in Proposition 1.9.",
      "context": [
        "a-4aa6-a69b-041fc5480f18}} (see for a complete description of equivalences and implications). In the present paper we will choose the following equivalent version of (3), which is, in the algebraic context, the analogue of smooth elliptic curves and smooth {{formula:5e5ef5cd-5a85-4dec-8128-6153f917bbbb}} surfaces.Definition 1.1 (Calabi–Yau 3–folds) A smooth, complex, projective 3–fold {{formula:5f5be0d4-5283-4b45-9eb7-d2c946cf2376}} is called Calabi–Yau if{{formula:298b32a6-e4a2-416e-b441-471f700d4854}} , {{formula:3798f00a-17bf-4d2a-b478-70350e813a56}} .The standard example is the smooth quintic threefold in {{formula:9a5045bc-af06-4238-a38b-11d9163ed438}} .",
        "ula:ebc79d8f-7ed3-46c4-b6e1-ccd02ca5e736}}. In particular {{formula:ee2e4614-4ebb-438c-92e1-7d289b882a7e}} turns out to be the dimension of the complex moduli space of {{formula:79ec64d8-98f9-4da1-a675-29e484d21e8a}}.\nGeometric transitions: Definition 1.5 (see and references therein) Let {{formula:f494b28a-cc75-44b5-85fd-c09b8838c9bb}} be a Calabi–Yau threefold and {{formula:32c83475-dd34-4dd0-b68c-f14cdbdaf017}} be a birational contraction onto a normal variety. Assume that there exists a Calabi–Yau smoothing {{formula:0eea7f67-d47e-4c37-a580-5630fc9deb14}} of {{formula:7bc383b9-5f",
        "ula:3725d85e-4717-4468-8a89-09d2133c7e93}} . Then Theorem REF and (REF ) allow to conclude that {{formula:2de576ae-b930-4419-92df-15be8f1b5434}} admits a universal effective family of Calabi–Yau deformations.Proposition 1.9 (Clemens' formulas ) Let {{formula:83ff094d-271a-4428-984e-cb7ed9392818}} be a conifold transition and say {{formula:bcd25998-2227-40ec-97d4-727ab645e790}} the {{formula:49d32d1b-fa3a-4279-a9f9-8050c5048327}} –th Betti number i.e. the rank of the {{formula:092a1b30-9960-4e45-a397-c455a3190dd0}} –th singular homology group. Then there exist two non negative integers {{formula:293301bb-2b92-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about vector space structure and its relation to modules, and how do the methods used impact the key results?",
      "expected_output": "The research paper discusses the structure of real vector spaces and their relation to modules. The main findings include the equivalence of certain conditions for a vector space to have a specific structure, and the use of the normal basis theorem to establish this equivalence. For example, the paper states that if certain conditions hold, then a specific structure exists by the normal basis theorem. The methods used, such as linear algebra and module theory, impact the key results by providing a framework for understanding the relationships between vector spaces and modules. Specifically, the paper uses a lemma from linear algebra to establish the equivalence of conditions for a vector space to have a certain structure, demonstrating the importance of this method in achieving the key results.",
      "context": [
        "f8ea1ac332}} is a {{formula:811687bc-3714-4817-af45-260b0f2bfade}} -structure of the real vector space {{formula:1903df97-05e9-48b2-961d-b5a80ef17c93}} . For any {{formula:4a15146d-7c30-4443-a121-d49d3dec2025}} , the space {{formula:eae79165-c107-4850-88ab-569dbc47ab22}} is a {{formula:186e10fa-e0aa-4453-b669-a9421c0f991b}} -structure of the {{formula:056921ee-e029-4b78-8f7f-9f3c01e8c331}} -module {{formula:31132e24-a5df-464b-9d38-a0a54a24abd4}} .Moreover, if these conditions hold, then {{formula:c3235907-298c-4abb-",
        "ac2589-da22-4955-b528-5e10b81c1832}} by the normal basis theorem.We will use the following lemma from linear algebra. Recall that if {{formula:45b72205-7582-41e9-bbde-3cad951b69ce}} is an {{formula:b6157e3a-abd3-41c4-a6d5-37729ed5647a}} -algebra and {{formula:c1fdc308-0052-42f9-8abc-0dc66574e999}} is a {{formula:21eebfe5-3562-4e16-9cd3-f8f2b2b82e07}} -module, an {{formula:10974468-8bda-4f24-b86d-2dbf582d127b}} -structure of {{formula:71cbb178-4fc3-44cf-9e42-c835350c4d24}} is an {{formula:da59e89e-8a4e-401c-8cd8-",
        "formula:9dfabaa5-242c-4cf1-8e00-89b7a0b00915}} -submodule of {{formula:a07ea984-04ba-47e3-abb8-9b9be1e28ef5}} . The following conditions are equivalent : {{formula:d8d6f8c3-c677-4f65-b703-e0f8ea1ac332}} is a {{formula:811687bc-3714-4817-af45-260b0f2bfade}} -structure of the real vector space {{formula:1903df97-05e9-48b2-961d-b5a80ef17c93}} . For any {{formula:4a15146d-7c30-4443-a121-d49d3dec2025}} , the space {{formula:eae79165-c107-4850-88ab-569dbc47ab22}} is a {{formula:186e10fa-e0aa-4453-b669-a9421c0f99"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the H(T) model in regards to electron dynamics, and how do the methods used in this research contribute to our understanding of this phenomenon?",
      "expected_output": "The H(T) model, also known as the Hubbard Hamiltonian, is used to study the dynamics of electrons in a lattice. The main findings of this model are that it can describe the behavior of electrons in a lattice, taking into account the hopping of electrons between sites and the on-site repulsion between electrons. The model shows that the ground state energy per atom in the thermodynamic limit depends on the hopping matrix elements and the on-site repulsion. The research methods used in this study, such as the theorem outlined, contribute to our understanding of electron dynamics by providing a framework for analyzing the behavior of electrons in a lattice and the effects of distortion energy on the system. For example, the study assumes that the distortion energy depends on the physical distance between lattice sites, which is a key aspect of understanding electron dynamics.",
      "context": [
        " carbon atoms, i.e., the conduction band is half-filled. The dynamics of the electrons is often modelled by the Hubbard Hamiltonian H(T)= - =, x,y txy ( cx, cy, + h.c. )+ U x (nx -12) (nx -12) + x,y F(txy) . The {{formula:2ba071c2-4512-4c4b-9813-20f26e2f2803}} are fermion annihilation operators and {{formula:67ad286a-4cb8-4d6e-805c-f2f6a479c116}} . For the purposes of this paper the on-site repulsion {{formula:d81ad1cf-3b83-410d-8ea1-ec7c9d17c698}} can have any sign as long as it is the same for all lattice sites. In particular, for {{formula:a3442ba8-5fd3-4db6-9870-10e46229ab5b}} we have the Hückel model.The notation emphasizes the dependence on the",
        " whereas for the hexagonal lattice it is zero, as assumed in {{formula:53d34f03-bad4-401f-830c-ede1581ae1ff}} above .Naturally, we cannot expect perfect periodicity in a finite lattice, because there will always be edge effects. This remark motivates the following theorem, whose proof we will outline here.Theorem.— For any triplet of hopping matrix elements {{formula:d5570d8c-c22c-4db8-8d9e-0c451b2bdf65}} let {{formula:f7dfc73a-3c85-440e-9efb-74fdc1a200b8}} denote the ground state energy per atom in the thermodynamic limit for the Hubbard model (REF ) with these {{formula:c3793c9a-4f3f-4768-a08b-c5fefa8d8caf}} 's arranged as in Fig. 1(a). Let {{formula:fea4b89d-3076-45ce-ac7e-8763779769ef}} be the minumum of",
        "5fb97}} is real, i.e., that there is no magnetic field.The interesting quantity, which does not usually appear in the Hubbard model, is the distortion energy {{formula:fe357869-484e-4290-aa67-35dee63122c4}} . What we are assuming is that {{formula:5c6a1144-e749-4ffd-8bc1-f9099b12c8d2}} depends on the physical distance {{formula:f4e071f1-d05d-48d8-bd49-b904852229d4}} between the lattice sites {{formula:fe104661-ac44-4c4d-a465-b3b84ffa61f3}} and {{formula:fbdc31f8-e3a7-445f-af2f-f7aa7e425b73}} . There is an equilibrium value {{formula:b4360fe4-b6c5-4f03-9d55-9064b013363f}} and deviations from this value cost a positive energy. After eliminating {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does preferential attachment play in the diffusion of infections, and how do the main findings of this research paper support this concept?",
      "expected_output": "Preferential attachment plays a significant role in the diffusion of infections, as it models the time evolution of the underlying degree distribution in social networks. The research paper supports this concept by showing that the preferential attachment model can be used to estimate the diffusion threshold in evolving networks. The paper's main findings indicate that the diffusion threshold decreases with the mean preserving spread, and that the addition probability in the preferential attachment model is related to the diffusion threshold in the SIS model. For example, the paper states that 'the primary motivation for choosing a preferential attachment graph is that it is the simplest graph whose steady state distribution obeys a power law', which is commonly observed in real-world networks. Overall, the paper's results suggest that preferential attachment is an important factor in understanding the diffusion of infections in evolving social networks.",
      "context": [
        "Analysis of infection diffusion in evolving social networks: So far in this paper, we have discussed estimating infection diffusion in a fixed network. In this section, we consider social networks that evolve with time, represented by changing degree distributions, and analyse their effect on the diffusion of infection over time. Since information diffusion occurs at a faster time scale compared to forming connections in social networks, we consider a two time scale formulation: where the degree distribution of the underlying network changes on a slow time scale and the infection diffuses over a faster time scale. There are various generative models for large real world networks in the literature, see , , and the references therein. In this paper, we consider the preferential attachment model discussed extensively in , to model the time evolution of the underlying degree distribution. The primary motivation for choosing a preferential attachment graph is that it is the simplest graph whose steady state distribution obeys a power law , which commonly arises in most real world networks, see , , .\nPreferential Attachment Model for Network Evolution: A network evolving according to the preferential attachment modelThe structural results we present here apply to any dynamical graph model that satisfies the stochastic dominance conditions given in Theorem REF . is characterized by two parameters - a probability {{formula:080ad330",
        " with smaller number of nodes of higher degree requiring a larger fraction of infected individuals to have a positive fraction of infected individuals when the dynamics converges. Remark: The key result above is the stochastic dominance structure for the preferential attachment graph - it is of interest in future work to give sufficient conditions on more general types of dynamic graph models. Filtering for Estimating the degree distribution So far in this paper, it was assumed that the degree distribution of the social network is known. In this section, we outline a Bayesian filter to estimate the degree distribution, when it evolves according to the preferential attachment modelRecall that Sec deals with filtering to track the infected degree distribution in a fixed network.. In Sec. REF , it was shown that the social network structure (degree distribution {{formula:d78a50d8-6b71-4a39-9e23-50f7e2c5f815}} ) plays a significant role in the asymptotic infected degree distribution (Lemma REF ). However, since the infection diffusion occurs at a faster time scale ({{formula:448ec5e0-6bb5-4058-8565-7ba086a9c46c}} ) compared",
        " we will give sufficient conditions to compare the evolution dynamics of two social networks when the underlying graphs are changing according to a preferential attachment scheme. On networks having fixed degree distribution (fixed networks), identified conditions under which a network is susceptible to an epidemic using a mean-field approach and provided a closed form solution for the diffusion threshold for infection diffusion{{formula:dc8fac3c-b178-4f87-91d0-e61d51af93ae}} . It was shown that under reasonable conditions on the infection probabilities, the diffusion threshold decreases with the mean preserving spread. In this paper, we extend the analysis of diffusion thresholds to evolving networks by providing sufficient conditions on the parameters that dictate the evolution. Effect on Diffusion Threshold in SIS model In this section, we establish a relation between the addition probability {{formula:40956cf5-78fb-4a45-a2c6-c5f59122a2ed}} in the preferential attachment model and the diffusion threshold {{formula:26c91d7a-5235-4aa2-bcd2-38aceb403a23}} in the SIS model. Definition 1 ( ) The Diffusion Threshold ({{formula:583"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research paper on simulation-based inference in health policy, and how do the methods used support the key results?",
      "expected_output": "The main findings of the research paper on simulation-based inference in health policy are the potential of machine learning, specifically simulation-based inference, in supporting the design and evaluation of public health interventions. The methods used, such as probabilistic programming and Bayesian inference, support the key results by enabling efficient and interpretable model calibration. For example, the paper discusses the development of software interfaces for COVID-sim and OpenMalaria models, which allows for probabilistic programming inference over public health scenarios. This approach can be applied to both communicable and non-communicable diseases, highlighting the value of machine learning in achieving universal health coverage and sustainable development goals.",
      "context": [
        "# arXiv Paper: 2005.07062\n\nSimulation-Based Inference for Global Health Decisions\nabstract: The COVID-19 pandemic has highlighted the importance of in-silico epidemiological modelling in predicting the dynamics of infectious diseases to inform health policy and decision makers about suitable prevention and containment strategies. Work in this setting involves solving challenging inference and control problems in individual-based models of ever increasing complexity. Here we discuss recent breakthroughs in machine learning, specifically in simulation-based inference, and explore its potential as a novel venue for model calibration to support the design and evaluation of public health interventions. To further stimulate research, we are developing software interfaces that turn two cornerstone COVID-19 and malaria epidemiology models COVID-sim, (https://github.com/mrc-ide/covid-sim/) and OpenMalaria (https://github.com/SwissTPH/openmalaria) into probabilistic programs, enabling efficient interpretable Bayesian inference within those simulators.\nIntroduction: Machine learning has a growing role in increasing health service access and efficiency, particularly in resource-constrained settings, making it a valuable tool for the global health community , . Moreover, the COVID-19",
        " unified interface specification afforded by probabilistic programming could allow simulators to also become amenable to other techniques from simulation-based inference and control, including dynamic programming and reinforcement learning , , . To foster the development of a new standardised approach to model calibration and evidence-based decision-making in public health, we are working on instrumenting the existing CovidSim and OpenMalaria simulators with a probabilistic programming interface through the PyProb library.https://github.com/pyprob/pyprob We will publicly release our code to provide out-of-the-box probabilistic programming inference over public health scenarios of interest in these two domains. We expect the mentioned techniques to play a role in dealing with communicable (infectious) diseases, which already entailed a significant burden for health systems in developing countries before the worldwide impact of COVID-19. However, they can also be applicable to non-communicable diseases, such as diabetes and cancer, which are recognised major causes of morbidity and mortality worldwide , . This will add to the already identified potential of machine learning in health policy and improving health access, emphasising its value for global health in the efforts to achieve universal health coverage and sustainable development goals , .\nEpidem",
        " and reinforcement learning , , .To foster the development of a new standardised approach to model calibration and evidence-based decision-making in public health, we are working on instrumenting the existing CovidSim and OpenMalaria simulators with a probabilistic programming interface through the PyProb library.https://github.com/pyprob/pyprob We will publicly release our code to provide out-of-the-box probabilistic programming inference over public health scenarios of interest in these two domains.We expect the mentioned techniques to play a role in dealing with communicable (infectious) diseases, which already entailed a significant burden for health systems in developing countries before the worldwide impact of COVID-19. However, they can also be applicable to non-communicable diseases, such as diabetes and cancer, which are recognised major causes of morbidity and mortality worldwide , . This will add to the already identified potential of machine learning in health policy and improving health access, emphasising its value for global health in the efforts to achieve universal health coverage and sustainable development goals , ."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of data scarcity and appearance changes on age progression in this research paper?",
      "expected_output": "The main findings regarding the impact of data scarcity and appearance changes on age progression in this research paper are that the lack of labeled age data of the same subject covering a large time span and the great change in appearance over a long time interval collectively make age progression and regression a difficult problem. This is because traditional face aging methods, which utilize prototype images or parametric anatomical models, struggle with preserving personalized facial texture information and modeling complex age-related changes. However, the use of Generative Adversarial Networks (GANs) has shown promise in addressing these challenges by generating high-fidelity images and preserving personalized features.",
      "context": [
        " is computed as the prototype for each age group. After that, transition patterns between prototypes are learned and then applied to render effects of age changing. In , the compositional model represented faces in each age group by a hierarchical and or graph. Then a Markov process on the graph representation was used to model face aging process. presented an illumination-aware method for automatic face aging of a single photo by leveraging thousands Internet photos across age groups. The main problem of prototype-based methods is that the personalized facial texture information are lost when computing the average faces.In recent years, with the success of GANs in generating high fidelity images, they have been used in many face aging methods to solve the above problems. In , a face image is first mapped to a latent vector, and then the latent vector is projected to the face manifold conditioned on the age information. Besides, two adversarial networks are imposed on the encoder and generator in order to generate photo-realistic face images. To generate more facial details, a GAN-based model with pyramid architecture for face aging was proposed in . Song et al integrated the target age condition into the discriminator to supervise the age in the generated images, and used residual blocks instead of latent vectors at the bottleneck of the generator",
        " of a face, which is defined as its distance from the average face in a 3D face space, is related to facial age. Then Tiddeman et al presented a wavelet based methods for prototyping facial textures and then transforming it for different age groups. A craniofacial growth model that characterizes face aging effects is proposed in . In the following, they further proposed a two step approach towards face aging in adults , which comprises a shape variation model and a texture variation model. {{figure:c899f95b-16b4-4a59-b50f-eaa3dd0666a1}}With the success of Generative Adversarial Networks (GANs) in generating visually appealing images, many efforts have been made to solve age progression and regression using GAN-based frameworks. There have been two key research directions in age progression for GAN-based methods: single model synthesis and multiple model synthesis. Single model synthesis refers to using only one single conditional image translation model to describe all of the face aging process. Different age translations are achieved by a single framework with the target age as a prior condition , , . While multiple model synthesis uses multiple image to image translation models to describe the face aging process. Deep convolutional",
        " demonstrate the ability of our model in synthesizing lifelike face images at desired ages with personalized features well preserved, and keeping age-irrelevant regions unchanged.\nIntroduction: The target of age progression and regression is predicting the appearance of a given face at different ages. An increased research activity in the area of facial aging simulation is recorded in the last two decades. Its applications range from social security to digital entertainment, such as cross-age identification and face age editing. Despite the appealing practical value, the lack of labeled age data of the same subject covering a large time span and the great change in appearance over a long time interval collectively make age progression and regression a difficult problem.Many approaches have been proposed to tackle this issue. They can be roughly divided into two types: traditional face aging methods and deep learning based face aging methods. Traditional face aging methods utilize prototype images or parametric anatomical models to describe face aging procedure. Burt et al studied visual cues to age by using facial composites which blend shape and color information from multiple faces. It is one of the early works for face aging. O'Toole et al suggested that the distinctiveness of a face, which is defined as its distance from the average face in a 3D face space, is related to facial"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of neutrino experiments and how do they affect the validity of the MSW scenario?",
      "expected_output": "The main findings of neutrino experiments, such as BOREXINO and HELLAZ, are crucial for checking the MSW scenario and the production through ASFP. These experiments will be sensitive to different slopes of energy spectrum profiles for various scenarios, which would be a crucial test in favor of the mechanism providing the solution to the SNP. The experiments will also help determine the bound on neutrino mixing angles in the RSFP like scenario, and confirm the moderate mixing angle region for which the RSFP solution to the SNP is still acceptable for the lowest allowed strengths of the magnetic field.",
      "context": [
        " level {{formula:8d056a73-c2f8-4cd3-a060-019dd538db2c}} in low energy {{formula:672eba3a-75b1-4c9a-8e7f-bb0c381a312f}} -scattering, currently planning in laboratory experiments , will be crucial for the model considered here.We would like to emphasize the importance of future low-energy neutrino experiments (BOREXINO, HELLAZ) which will be sensitive both to check the MSW scenario and the {{formula:379b2ed3-6884-40f1-b659-876103e6c514}} -production through ASFP. As it was shown in a recent work a different slope of energy spectrum profiles for different scenarios would be a crucial test in favour of the very mechanism providing the solution to SNP.",
        "ffde8cc6776}} for {{formula:58634eba-a818-4d62-8aa3-bdf019bae79f}} , ) puts the bound on neutrino mixing angles in the RSFP like {{formula:f2808a31-e6bb-4d00-950b-81cfb11fae30}} . We checked this statement from our numerical code using different regular magnetic field profiles and this bound almost coincides with the limit {{formula:cfd63dae-8414-468d-868c-e7228d13c250}} obtained earlier in . Moreover, we confirm another result in that the moderate mixing angle region {{formula:701b33d2-19d6-4002-8faf-46a987d4df75}} for which {{formula:601aa252-99a7-48f0-9372-e70f2c7131e1}} maybe detectable is still acceptable for the RSFP solution to the SNP for the lowest allowed strengths of the magnetic field, {{formula:880a549d-e79b-42e6-bae3-45c",
        "esary to repeat that the above estimate Eq. (REF ) indicates possible danger when treating numerically the neutrino propagation in noisy media. Indeed, usually adopted one-dimensional (i.e. along one ray only) approximation for the {{formula:ce3d27d6-bb8d-4c1b-8177-d3c06a1a8870}} master equation or {{formula:4f279c67-4e7f-46be-a76e-bc353e40958e}} Eq. (REF ) can suffer from large dispersion errors and one should make certain precautions when averaging these equations over the random noise before numerical simulations. Otherwise, the resulting error might be even unpredictable.\nDISCUSSION and CONCLUSIONS: In order to find the regions in the {{formula:4d9191b0-c1ec-40b3-9e0c-cb70436e7365}} , {{formula:ff09783f-6410-4554-a67b-83eec802b6b7}} plane excluded from nonobservation of {{formula:20c67d19-5c2e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does associativity play in the algebra's product, and how does it impact the research findings?",
      "expected_output": "According to Proposition 2.1, associativity plays a crucial role in the algebra's product, ensuring that the order in which elements are multiplied does not affect the result. This property is essential for the research findings, as it allows for the manipulation of algebraic expressions and the derivation of equalities, such as the one implied by the equation {{formula:b467ee58-25ad-4d90-81de-80698ef777a8}}. The associativity of the product also enables the use of inductive proofs, as seen in Theorem 2.2, which provides further insights into the algebra's properties.",
      "context": [
        "877d-61a213b7de56}} .Proposition 2.1 The algebra {{formula:05779478-a3c8-42a3-a64a-03aaa2b248f2}} with the {{formula:21f1cd30-aefb-4db8-a4be-3bd9347b34a2}} -product is associative.Let {{formula:81fccfe0-adeb-47e6-991f-19ed2c5c6044}} , {{formula:9640c473-1263-424f-a77f-4cc7f58e8ee9}} and {{formula:10896ae1-0f56-4583-b7f6-7865ffa49ec4}} . Then {{formula:227ecb70-1850-4ebd-9919-448b7d9a8e0d}} {{formula:39b1f226-c530-4f95-b7f4-d66e4d9d39d5}} {{formula:c29c6620-ae99-",
        "-d66e4d9d39d5}} {{formula:c29c6620-ae99-4104-958a-76a54ae6fb58}} Performing analogous manipulations, one can show that {{formula:836b5c3f-1e98-4494-985e-0ba8588e40ee}} The above implies the equality {{formula:b467ee58-25ad-4d90-81de-80698ef777a8}} .We will need a few more properties of the algebra {{formula:d4484a25-0479-493b-b48d-a23061f95104}} , summarized below.Theorem 2.2 For generic {{formula:6077a554-fb58-4536-a724-43d7db4c10a3}} (such that {{formula:c04c31e5-e0c0-40df-ae8c-4f43a8af7c38}} for any {{formula:70d1c62a-6ff8-4a7b-8aa7-cc071cc44",
        "d5c6}} This can be shown inductively. If there is only one term, the assertion is trivial. Otherwise, suppose there is {{formula:15d0c11c-7e35-4816-8291-08546fcbdf53}} , s.t. {{formula:972cef08-ed57-4cc1-8d90-083949cba254}} . We treat the cases {{formula:865273f6-5042-4736-92ad-47b6123b60ed}} and {{formula:6931650a-66dc-4d30-a0a3-774546321527}} separately. For the first one, using relation (REF ), we write {{formula:2ecfccca-24ff-42ec-9d06-f07b44f537f5}} {{formula:8624412e-3686-49f0-9784-b99314ad2d77}} When {{formula:613ba949-f72e-415c-a3e1-b0d455e41f59}} , this simplifies to"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in optimization efficiency between the Newton-Raphson and Conjugate Gradient methods, and how do their findings relate to the research paper's key results?",
      "expected_output": "The main differences in optimization efficiency between the Newton-Raphson and Conjugate Gradient methods are that the Conjugate Gradient method takes less time to optimize the cost function, but there is a tradeoff among these algorithms. According to the research paper, the Conjugate Gradient method has relatively lesser number of iterations compared to Newton-Raphson. The paper's key results show that the Steepest Descent method gives the minimum cost, and it takes the least number of iterations. The Conjugate Gradient method has almost similar performance to Steepest Descent in terms of average optimized cost.",
      "context": [
        " cost function has also been optimized by using Newton-Raphson and Conjugate Gradient method and it will be verified that the Conjugate Gradient method will take less time to optimize the cost function but there also exists tradeoff among these algorithms which has been shown later in simulation part. For performance evaluation, the results given by these algorithms has been compared based on optimized cost, number of iterations and elapsed time i.e. total time taken to execute the algorithm.\nNewton-Raphson method : In general for the real valued function {{formula:25f891fd-a9ae-4eb2-aa4d-5092d976810c}} if all second order partial derivatives of {{formula:f8bca1b5-79be-4a83-8faf-5d01d40313ff}} exist and are continuous over the domain of the function, then the Hessian matrix of {{formula:9e549279-75af-4f9c-92f2-328176c4ea2a}} is given by {{formula:c8674ec4-6bdb-46a8-a4de-e8f91011",
        "5-9151-41ea-b06c-ceae597830fc}} has taken. Thus by Newton's method, iterative scheme is given as {{formula:635aa535-b5a4-49cd-b500-c76a8d33ea1f}}\nConjugate-Gradient Method: In this method, in place of descent direction, set of conjugate vectors are generated which are known as conjugate direction along which optimization of cost function will take place. First conjugate direction {{formula:8cb9ca5e-9feb-4472-b7ee-19577dda8f51}} will be steepest descent direction at the initial point {{formula:e85ad498-79ff-478b-a550-2a279b602e7a}} . After this successive one-dimensional minimizations will take place along each of the conjugate directions {{formula:37d387f3-68a4-4c52-8181-fc79fa2a0985}} until convergence is achieved. For details one can refer , .The iterative scheme is given by {{formula:a9fd28",
        "b70}} presents the comparison of different optimization algorithms in terms of average optimized cost (average over 20 independent networks). From this figure it can be seen that the initial cost of the network is approximately 74294 and the cost of optimized network is in the range of {{formula:e5ee9f33-cf50-4397-8703-a7f44052064a}} . Among the different optimization algorithm, the Steepest Descent gives minimum cost. It can be also observe from the figure that, Steepest Descent and Conjugate Gradient have almost similar performance in terms of average optimized cost.Figure {{formula:c91ae568-7d28-400a-b70d-c11cb771c6d8}} shows the comparision of average number of iterations (average over 20 independent networks) taken by each algorithm. From this figure it can be observed that, Steepest Descent method takes least number of iterations. Between Newton-Raphson and Conjugate Gradient method, Conjugate Gradient has relatively lesser number of iterations, therefore it may be concluded that Steepest Descent is best among all optimization methods in terms of number of iterations.Although the Steepest"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between surface gravity and gravitational charge in black hole dynamics?",
      "expected_output": "The research paper finds that the third law of black hole dynamics, which states that the surface gravity of a black hole cannot be reduced to zero in a finite sequence of physical interactions, also applies when surface gravity is replaced by gravitational charge. The paper demonstrates that as extremality is approached, the window for allowed ranges of parameters of infalling energy and radiation pinches off, making it impossible to reduce the gravitational charge to zero in a finite number of physical processes.",
      "context": [
        " shall argue that the law can also be stated by replacing surface gravity by gravitational charge enclosed by the horizon. Gravitational charge of a hole can be defined by the flux of red-shifted proper acceleration across the closed 2-surface defined by the horizon , and value of the acceleration at the horizon defines the surface gravity. Thus the two quantities are intimately related. By considering variation in gravitational charge and applying the area non-decrease theorem, we would like to exhibit how it cannot be reduced to zero in finite sequence of physical processes. It turns out that as extremality ({{formula:593b26ec-43a3-4fee-b31e-9ea4c14ce77a}} ) is approached, the window for allowed range of parameters of infalling energy and radiation pinches off.The third law of thermodynamics has two essentially equivalent statements; (a) isothermal reversible processes turn isentropic as temperature of a system approaches zero, and (b) temperature cannot be reduced to zero in finite number of physical operations . There is yet another stronger version due to Planck which states that the entropy of any system tends to an absolute constant, which can be taken as zero, as temperature tends to zero.In the case of",
        " third law of black hole dynamics states that the surface gravity (temperature) of black hole cannot be reduced to zero in finite sequence of physical interactions. We argue that the same is true when surface gravity is replaced by gravitational charge. We demonstrate that the prescribed window for infalling energy and radiation pinches off as extremality ({{formula:6a47fee1-ce0f-4294-a0b2-cede0d3e4a08}} ) is approached.PACS numbers : 04.20.Cv, 97.60.LfIUCAA-31/97Parallel to the laws of thermodynamics, the laws of black hole dynamics (BHD) were formulated comprehensively by Bardeen, Carter and Hawking . The identification of temperature with surface gravity of the hole was clinched by the Hawking radiation which followed from application of quantum theory in general relativity. Subsequently Israel has given the precise formulation and proof of the third law . Although the law has been proved in an elegant way using sophisticated global analysis, a clear and straight forward demonstration of its working has its own merit and usefulness . We shall argue that the law can also be stated by replacing surface gravity by gravitational charge enclosed by the horizon. Gravitational charge of",
        "# arXiv Paper: gr-qc/9704070\n\nOn the third law of black hole dynamics\nabstract: The third law of black hole dynamics states that the surface gravity (temperature) of black hole cannot be reduced to zero in finite sequence of physical interactions. We argue that the same is true when surface gravity is replaced by gravitational charge. We demonstrate that the prescribed window for infalling energy and radiation pinches off as extremality ($M^2 = a^2 + Q^2 $) is approached.\n: elsartON THE THIRD LAW OF BLACKHOLE DYNAMICS Naresh Dadhich emal E-mail : naresh@iucaa.ernet.in; Fax : 91-212-350760 Inter-University Centre for Astronomy & Astrophysics, Post Bag 4, Ganeshkhind, Pune - 411 007. K. Narayan emal1 E-mail : narayan@niharika.phy.iitb.ernet.in Department of Physics, IIT Powai, Bombay - 400 076The third law of black hole dynamics states that the surface gravity (temperature) of black hole cannot be reduced to zero in finite"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Algorithm REF's complexity when updating scaled forms, and how do the methods used impact the key results?",
      "expected_output": "The main findings of Algorithm REF's complexity when updating scaled forms are that if the scaled forms are updated, then corresponding multiplications are not necessary. This means that calculating certain values only requires some addition operations, resulting in an overall computational complexity of roughly a specific formula in each iteration. The methods used, such as updating scaled forms, impact the key results by reducing the need for multiplication operations and thus decreasing the computational complexity.",
      "context": [
        ":d38a21f3-a2a3-4c21-925a-56e74b443168}} in (REF ) (REF ) (REF ), one can find that if their scaled forms {{formula:9348bb2c-8c99-4258-bc42-59c8fe00f018}} and {{formula:b21b4071-3bc6-4039-8bb0-08439e6d0051}} are updated, then corresponding multiplications are not necessary, i.e., calculating {{formula:c2ea3ced-ef5a-40bb-987d-44f0de0671e8}} and {{formula:fbc2f11e-3003-4495-a8e8-cae54bf71daa}} only requires some addition operations. From the above analysis, we can see that the overall computational complexity of Algorithm REF in each iteration is roughly {{formula:44ddc7ef-47c7-4846-854e-8bb93def6ed3}} . Furthermore, based on {{formula:d14cc2dc-a3f8",
        "d8}} and {{formula:d38a21f3-a2a3-4c21-925a-56e74b443168}} in (REF ) (REF ) (REF ), one can find that if their scaled forms {{formula:9348bb2c-8c99-4258-bc42-59c8fe00f018}} and {{formula:b21b4071-3bc6-4039-8bb0-08439e6d0051}} are updated, then corresponding multiplications are not necessary, i.e., calculating {{formula:c2ea3ced-ef5a-40bb-987d-44f0de0671e8}} and {{formula:fbc2f11e-3003-4495-a8e8-cae54bf71daa}} only requires some addition operations. From the above analysis, we can see that the overall computational complexity of Algorithm REF in each iteration is roughly {{formula:44ddc7ef-47c7-4846-854e-8bb93def6ed3}} . Furthermore, based on {{formula:d14cc",
        "4f-0a9a-48c9-b44b-e5aef07a46d8}} and {{formula:d38a21f3-a2a3-4c21-925a-56e74b443168}} in (REF ) (REF ) (REF ), one can find that if their scaled forms {{formula:9348bb2c-8c99-4258-bc42-59c8fe00f018}} and {{formula:b21b4071-3bc6-4039-8bb0-08439e6d0051}} are updated, then corresponding multiplications are not necessary, i.e., calculating {{formula:c2ea3ced-ef5a-40bb-987d-44f0de0671e8}} and {{formula:fbc2f11e-3003-4495-a8e8-cae54bf71daa}} only requires some addition operations. From the above analysis, we can see that the overall computational complexity of Algorithm REF in each iteration is roughly {{formula:44ddc7ef-47c7-4846-8"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the variables s, t, and u relate to the parameters M, rho, q_T, p_T, and z in the context of this research paper?",
      "expected_output": "The variables s, t, and u are related to the parameters M, rho, q_T, p_T, and z through the following equations: s = M^2*rho - q_T^2, t = -[M^2*(rho*z-1)-p_T^2]/z, and u = -[M^2*(1-z)+p_T^2]/z, where rho is defined as rho = ((p_T-q_T)^2)/(M^2*(1-z)) + (M^2+p_T^2)/(M^2*z). These equations show how the Mandelstam variables s, t, and u depend on the masses M, the transverse momenta q_T and p_T, and the parameter z.",
      "context": [
        ",u).where t_1={\\bf q}_T^2, Mandelstam variables s = M^2\\rho -{\\bf q}_{T}^2, t = -[ M^2(\\rho z-1)-{\\bf p}_{T}^2 ]/z, u = -[ M^2(1-z)+{\\bf p}_{T}^2 ]/z, \\rho =\\frac{({\\bf p}_T-{\\bf q}_T)^2}{M^2(1-z)} + \\frac{M^2+{\\bf p}_T^2}{M^2 z}, and coefficients:&& \\hspace{-14.22636pt} w_{-1}= s t t_1 u \\Bigl ( t + t_1 + u\\Bigr ) \\Bigl ( s + 2 t_1 + u\\Bigr ) \\ \\Bigl ( s^2 [ t_1 - u] [ t + t_1 + u] + t [ t + t_1] [ t_1 + u] [ 2 t_1 + u] \\\\ &&",
        ". :{\\cal J}^{\\text{(sub.)}}_{Rg}({\\bf q}_T,z,{\\bf p}_T,r) = \\frac{2C_A}{{\\bf k}_T^2} \\left[ \\frac{1-z}{(1-z)^2 + r\\frac{{\\bf k}_T^2}{q_-^2}} + \\Delta p_{gg}({\\bf q}_T,z,{\\bf p}_T) \\right],where \\Delta p_{gg}({\\bf q}_T,z,{\\bf p}_T)=z(1-z) + 2[{\\bf k}_T^2 {\\bf q}_T^2 - ({\\bf k}_T {\\bf q}_T)^2]/(z {\\bf k}_T^2 {\\bf q}_T^2) - [3{\\bf k}_T^2 {\\bf q}_T^2 - 2({\\bf k}_T {\\bf q}_T)^2]/({\\bf k}_T^2 {\\bf q}",
        "mu _F^2}{{\\bf p}_T^2} P_{gg}(z) + 2C_A \\frac{1-z}{z} \\right. \\\\ &&\\left. +\\delta (1-z) \\left[ -\\frac{\\pi ^2}{2}C_A +\\frac{8}{3}C_A - \\frac{5}{3}\\beta _0 - 2C_F \\left( 2 + \\frac{3}{2} \\ln \\frac{4{\\bf p}_T^2}{M^2} \\right) +\\beta _0 \\ln \\frac{\\mu _R^2}{{\\bf p}_T^2} + F_{{}^1S_0^{[1]}}({\\bf p}_T^2/M^2) \\right] \\right\\rbrace ,where K_{\\rm BFKL}=\\frac{2 C_A}{{\\bf k}_T^2}\\left[\\delta ^{(2)}({\\bf q}_T-{\\bf k}_T-{\\bf p}_T) - \\frac{{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used in image segmentation to partition pixels into distinct objects, and what are the key findings of this research in terms of object recognition?",
      "expected_output": "The methods used in image segmentation to partition pixels into distinct objects include graph partitioning techniques, such as weighted correlation clustering, which sums up the edges cut by a proposed partitioning of the graph. The key findings of this research are that the proposed algorithm, PlanarCC, outperforms existing global optimization techniques in minimizing the objective and is competitive with the state of the art in producing high-quality segmentations. Additionally, the research suggests that learning an optimal cue combination via structured prediction may improve performance, and that the algorithm is particularly effective in applications where the local boundary signal is noisier, such as biological imaging.",
      "context": [
        "# arXiv Paper: 1208.0378\n\nFast Planar Correlation Clustering for Image Segmentation\nabstract: We describe a new optimization scheme for finding high-quality correlation clusterings in planar graphs that uses weighted perfect matching as a subroutine. Our method provides lower-bounds on the energy of the optimal correlation clustering that are typically fast to compute and tight in practice. We demonstrate our algorithm on the problem of image segmentation where this approach outperforms existing global optimization techniques in minimizing the objective and is competitive with the state of the art in producing high-quality segmentations.\nIntroduction: We tackle the problem of generic image segmentation where the goal is to partition the pixels of an image into sets corresponding to objects and surfaces in a scene. Cues for this task can come from both bottom-up (e.g., local edge contrast) and top-down (e.g., recognition of familiar objects). For closed domains where top-down information is available, this problem can be phrased in terms of labeling each pixel with one of several category labels or perhaps “background”. There is a rapidly developing body of research in this area that integrates multiple cues such as the output of a bank of",
        " is that the greedy merging is truly successful in optimizing the correlation clustering objective. Figure REF shows that this is not the case – while there is usually some UCM threshold that provides a segmentation with a fairly low-cost clustering, it is still suboptimal compared to the solutions returned by PlanarCC. This suggests learning an optimal cue combination via structured prediction may improve performance.Finally, it is worth noting that the boundary detection benchmark does not provide strong penalties for small leaks between two segments when the total number of boundary pixels involved is small. We found that on the region based benchmarks, PlanarCC did outperform UCM slightly when the optimal segmentation threshold was chosen on a per-image basis (GT Covering OIS 0.65 versus 0.64 for UCM). We expect these differences may become more apparent in an application where the local boundary signal is noisier (e.g., biological imaging) or when there is a greater cost for under-segmentation.\nConclusion: We have presented a novel, fast algorithm for finding high quality correlation clusterings in planar graphs. Our algorithm appears to outperform existing approaches on a variety of real problem instances. Our method exploits decomposition into subproblems that lack efficient combinator",
        "�. There is a rapidly developing body of research in this area that integrates multiple cues such as the output of a bank of object detectors into a single model, typically formulated as a Markov random field over the pixel labels and some additional hidden variables , , , , , , .When top-down information is not available, it may still be quite valuable to estimate image segments. Bottom-up segmentations provide candidate support for novel objects and can simplify the processing of the scene to the problem of understanding a small number of salient regions. Without a predefined set of labels, it is natural to describe the segmentation task as a graph partitioning problem in which pixels or superpixels have pairwise or higher order similarities and the number of parts must be estimated. There is a rich history of applying graph partitioning techniques to image segmentation (e.g., , , , , ).Here we consider the weighted correlation clustering objective which sums up the edges cut by a proposed partitioning of the graph. Edges may have both positive and negative weights. Correlation clustering is appealing since the optimal number of segments emerges naturally as a function of the edge weights rather than requiring an additional search over some model order parameter. Further, because the objective is linear in the edge weights"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the significance of LMC's PM range in relation to 1.5-2.0 mas yr, and how does it impact the research findings?",
      "expected_output": "The LMC's PM range of 1.5-2.0 mas yr is significant because it is compatible with theoretical models, such as those by Gardiner et al. (1994). This compatibility suggests that the research findings are consistent with predicted values, providing validation for the study's results. For example, the paper states that the total PM of the LMC is +1.5 mas yr, which falls within the predicted range. This agreement between observed and predicted values supports the accuracy of the research findings and contributes to a better understanding of the LMC's motion.",
      "context": [
        "-3fbcd4c4e4a3}} 0.1) mas yr{{formula:503e10f7-d9d4-453e-87a8-e0c633fd82ae}} , with a position angle of {{formula:e1aedd23-2c4e-4766-9db5-3984b29908e8}} = (62.4{{formula:8134d0dc-07a0-452a-b4e0-8c38968a648e}} 3.1){{formula:6fa62b5a-ee9c-418f-8896-5b3a1529645f}} , measured eastward from the meridian joining the center of the LMC to the north celestial pole. This result is compatible (particularly the PM's absolute value) with theoretical models (Gardiner et al. 1994), which predict a PM for the LMC in the range 1.5{{formula:87ca7091-6d79-4e70-a365-7b394cb4ad74}} 2.0 mas yr{{formula:72a20d9c",
        " rows 5-8 we give calculated PM values relative to the GRF, both in equatorial and galactic coordinates. These values correspond to the LMC's PM as seen by an observer located at the Sun, with the contributions to the PM, from the peculiar solar motion and from the LSR's motion, removed. In rows 9-11 we give the {{formula:9114275b-8e23-49fc-9ad5-fc0841b68163}} , {{formula:3b8f6b9e-3502-4a73-ace3-f56f04b00cdd}} and {{formula:0a7540c1-15ea-4524-95f1-fb869fcda43a}} components of the space velocity in a rectangular cartesian coordinate system centered on the LMC (as defined by Schweitzer et al., 1995, for the Sculptor dSph). The {{formula:0de7b56a-3fa4-458a-bb5d-6ec8a2e53520}} component is parallel to the projection onto the Galactic plane of the radius vector from the center of the Galaxy to the center of the",
        "-23b7-4ee3-9924-4e7bab12ffb4}} ] = [+1.5,0] mas yr{{formula:f1654178-f0f5-4b73-afb9-5c11da5d9ef2}} , giving a total PM of {{formula:6315382a-25ac-4de4-acaf-67fc288f6c08}} = +1.5 mas yr{{formula:3d7fa20f-674e-4b4c-82d8-032bf6f18be6}} , with a position angle of {{formula:40fec49e-6ca1-48eb-9780-414734443634}} = 90{{formula:326bbcc1-c749-4474-8fcf-cf261a97d8d0}} . A comparison of this prediction with our result [{{formula:f435fcd4-ba32-4737-af17-b5a8f1f4d54e}} = ({{formula:7851f15c-cc86-4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the key optimizations used in the push-relabel algorithm to improve flow, and how do these optimizations impact the overall performance of the method?",
      "expected_output": "The key optimizations used in the push-relabel algorithm include employing a level graph and blocking flows, which reduce the complexity to O(V^2E) and O(V^{3/2}E) on general and unit-capacity graphs, respectively. Additionally, the push-relabel algorithm itself focuses on local operations to adjust flows, introducing the excess flow concept and active vertices. The algorithm's performance is further improved by using a thread-centric approach, assigning threads to scan vertices, and utilizing a two-level parallelism approach. These optimizations lead to significant runtime speedups, such as up to 7.31x and 2.29x on real-world graphs in maximum flow and bipartite matching tasks, respectively.",
      "context": [
        " al. further improved upon the basic augmenting path approach by employing a level graph and blocking flows, which significantly reduced the number of augmentations needed; hence, reducing the complexity to O(V^2E) and O(V^{3/2}E) on general and unit-capacity graph respectively. Goldberg et al. proposed the push-relabel algorithm (also known as the preflow-push algorithm), which focuses on local operations to adjust the flows. Some optimizations are based on it to improve the complexity to O(V^2E).\nGeneric Parallel Push-relabel Algorithm: The basic idea of the push-relabel algorithm is to generate as much flow as possible at the source, and gradually push it to the sink. It introduces the excess flow concept that allows a vertex to have multiple flows coming into it; namely, it allows a vertex to have more flow coming in than passing out during the push-relabel procedure, this vertex is called active. We denote e(v) as the excess flow value on vertex v. The procedure of the push-relabel algorithm is to find active vertices and apply push and relabel operations on them until there are no remaining active vertices. To find the initial active vertices,",
        " + E). Moreover, we can achieve up to 7.31x and 2.29x runtime speedup compared to the state-of-the-art on real-world graphs in maximum flow and bipartite matching tasks, respectively. Our code will be open-sourced for further research on accelerating the push-relabel algorithm.\nIntroduction: The study and application of maximum flow algorithms have long been central to a myriad of computational problems across various disciplines within computer science, including VLSI design , , optimization , and computer vision . The push-relabel (preflow-push) algorithm , in particular, represents a cornerstone in solving the maximum flow problem due to its efficiency and versatility. This algorithm iteratively improves the flow within a network by locally pushing excess flow at vertices until the algorithm achieves an optimal flow, leveraging relabel operations to dynamically adjust the heights of nodes to maintain a valid flow.As graphs grow to encompass billions of nodes and edges, traditional CPU-based solutions need help with the computational requirements necessary to process such immense datasets efficiently. This limitation is particularly pronounced in the applications handling large-scale social networks, web graphs, and biological networks, where the ability to compute efficiently maximum flow or minimum cut is fundamental but essential for analyzing or understanding",
        ".\nOverview: Figure REF (b) shows the architecture for our workload-balanced push-relabel implementation. As mentioned in the Section , we aim to alleviate the two workload imbalances using the thread-centric approach. Fundamentally, we first assign all threads to scan all vertices to find the active ones and add the active vertices to the active vertex queue (AVQ). With the AVQ, each thread has an equal workload when finding active vertices. Besides, the active vertices can be coalesced in the AVQ; hence, we can assign a tile (a group of threads) to find the minimum-height neighbor vertex of an active vertex, which reduces the time of the searching time (Algorithm line: 11-13) from O(d(v)) to O(log_2{d(v)}). Since we can process multiple vertices and use multiple threads in an active vertex simultaneously, we call this approach two-level parallelism. Furthermore, we designed two enhanced CSRs, reversed CSR and bidirectional CSR, to reduce significant time in scanning all neighbors or finding backward edges on the residual graph.\nEnhanced Compressed Sparse Representation: The prior approach used an adjacency matrix"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Ptolemy's complex in relation to ideal triangulations, and how do the methods used impact the key results?",
      "expected_output": "The main findings of Ptolemy's complex in relation to ideal triangulations are that it provides a 2-dimensional CW-complex that describes ideal triangulations on a surface. The complex has vertices representing ideal triangulations, edges representing flips between them, and 2-cells representing certain combinations of flips. The methods used, such as the definition of flips and the construction of the Ptolemy complex, impact the key results by allowing for a combinatorial treatment of the surface and enabling the study of ideal triangulations in a more structured way. For example, the Ptolemy complex helps to understand how ideal triangulations can be transformed into each other through a sequence of flips, analogous to the 2-2-Pachner move for triangulations.",
      "context": [
        " triangulations, there is an analog of the 2-2-Pachner move for ideal triangulations.Definition 0.9 Let f\\in E(\\mathsf {T}) be such that f is adjacent to two different 2-faces of \\mathsf {T} and f is not a boundary edge. A flip along f is the move shown in figure REF . {{figure:beff7d1d-f0cb-4bc9-bce2-5d1bb6f63536}}In contrast to triangulations, there is no 3-1 move for ideal triangulation, since we work with a fixed set of vertices. To be more precise, there exists a 2-dimensional CW-complex \\mathcal {P}(\\Sigma ,\\delta ,M), the Ptolemy-complex, describing ideal triangulations on (\\Sigma ,\\delta ,M). Like the Lego-Teichmüller complex, the Ptolemy-complex is additional structure om \\Sigma , which enables us to treat the surface \\Sigma in a combinatorial way.Definition 0.10 The Ptolemy complex \\mathcal {P}(\\",
        " triangles.The dual graph to an ideal triangulation is a uni-trivalent fat graph, whose cyclic order at vertices is induced by the orientation of the boundaries of dual triangles. The orientation of the boundary of a triangle is, of course, induced by the orientation of \\Sigma .The 1-skeleton \\mathsf {T}^{[1]} of \\mathsf {T} is an isotopy class of an embedded graph and when speaking of vertices, edges and faces of an ideal triangulation, we always mean vertices, edges and faces of \\mathsf {T}^{[1]}. Ideal triangulations aren't triangulations in general, e.g. we may encounter bubble graphs like in figure REF . {{figure:c1294f00-0b6e-4bf6-8080-c62f18311cad}}It is well known that any two triangulations of a surface can be transformed into each other by a finite sequence of 2-2 and 3-1 Pachner moves. Though ideal triangulations aren't triangulations, there is an analog of the 2-2-Pachner move for ideal triangulations.Definition",
        "igma in a combinatorial way.Definition 0.10 The Ptolemy complex \\mathcal {P}(\\Sigma ,\\delta , M) is the 2-dimensional CW-complex with0-cells: Vertices of \\mathcal {P}(\\Sigma ,\\delta , M) are ideal triangulations. 1-cells: There is an edge between two vertices for any flip F_e:\\mathsf {T}\\rightarrow \\mathsf {T}^\\prime as in figure REF . 2-cells: There are three types of 2-cells: P1: For f\\in E(\\mathsf {T}) such that the flip F_f exists, there is the 2-cell [label=above:\\mathsf {T}] (a) at (0,0); [label=below:\\mathsf {T}^\\prime ] (b) at (0,-2); (a) circle (2pt); (b) circle (2pt); [->-] (a) to [bend right] node [midway, left]F_f (b); [->-] (b) to [bend right] node [mid"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of future CMB observations regarding neutrino mass and how do the methods used in this research impact the key results?",
      "expected_output": "The main findings of future CMB observations regarding neutrino mass are that they have the potential to measure absolute neutrino masses to about 0.04 eV. The methods used in this research, such as gravitational lensing of the CMB and all-sky polarized CMB missions, impact the key results by providing a promising probe of the growth of structure and the fundamental physics that affects it, allowing for an absolute determination of neutrino mass.",
      "context": [
        "# arXiv Paper: astro-ph/0307538\n\nApplications of High Resolution High Sensitivity Observations of the CMB\nabstract: With WMAP putting the phenomenological standard model of cosmology on a strong footing, one can look forward to mining the cosmic microwave background (CMB) for fundamental physics with higher sensitivity and on smaller scales. Future CMB observations have the potential to measure absolute neutrino masses, test for cosmic acceleration independent of supernova Ia observations, probe for the presence of dark energy at redshifts of 2 and larger, illuminate the end of the dark ages, measure the scale--dependence of the primordial power spectrum and detect gravitational waves generated by inflation.\n: Applications of High Resolution High Sensitivity Observations of the CMB Manoj Kaplinghat Department of Physics, One Shields AvenueUniversity of California, Davis, California 95616, USAWith WMAP putting the phenomenological standard model of cosmology on a strong footing, one can look forward to mining the cosmic microwave background (CMB) for fundamental physics with higher sensitivity and on smaller scales. Future CMB observations have the potential to measure absolute neutrino masses, test for cosmic acceleration independent of supernova Ia observations, probe for",
        " due to reionization. For all entries in Table II we have assumed this as prior knowledge and not included the low multipoles in the analysis.Gravitational lensing of the CMB is clearly a promising probe of the growth of structure and the fundamental physics that affects it. CMBpol can provide evidence for the acceleration of the universe, independent of SNIa observations. It can throw light on inflationary dynamics by testing for consistency with the relation {{formula:4003d38c-1d33-4dd5-a998-631ebd986115}} . The most striking result in Table II is the promise that cosmology can provide us with an absolute determination of neutrino mass to about 0.04 eV. We can be optimistic that a future all-sky polarized CMB mission aimed at detecting gravitational waves is likely to succeed in determining neutrino mass as well.Acknowledgments. This article is based on work done in collaboration with Lloyd Knox and Yong-Seon Song, and I would like to thank both my collaborators. I would also like to thank Wayne Hu for helpful discussions.",
        " have the potential to measure absolute neutrino masses, test for cosmic acceleration independent of supernova Ia observations, probe for the presence of dark energy at {{formula:fdf06c82-1c9f-4e76-bc5e-1ab8eebe3938}} , illuminate the end of the dark ages, measure the scale–dependence of the primordial power spectrum and detect gravitational waves generated by inflation.Introduction. The WMAP experiment conclusively showed that the standard cosmological model is a good phenomenological description of the observed universe . In combination with other observations (supernova Ia and large scale structure), the consistent picture that emerges has dark matter contributing about 30% and dark energy about 70% to the energy density of the universe. Curvature could contribute a few % to the above energy budget but for the rest of this article we will assume a flat universe.Given that the basic phenomenological structure is in place, one can look forward to the future with some confidence. The CMB has much more to offer, and in many ways far more spectacular discoveries are waiting to happen. The aim here will be to lay out a list of things that are possible (with only a brief description of each topic) with"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What portion of the aggregate problem does this research paper address, and how does it contribute to the overall understanding of the issue?",
      "expected_output": "This research paper addresses the portion of the aggregate problem solved by the agents at iteration, namely the computation of each agent's portion. It contributes to the overall understanding of the issue by applying perturbation theory to perturbed constraints, allowing agents to compute their local allocation estimates and Lagrange multipliers, thus enabling the solution of large-scale instances of the problem.",
      "context": [
        " larger than the number of coupling constraints. Therefore, in this paper we let {{formula:1d00f22b-abf8-4751-a3c6-e2b185bcce5e}} , leading to large-scale instances of problem (REF ).We assume each agent {{formula:0441217d-5bfc-4a32-b8de-cfb8bcde085d}} has a partial knowledge of problem (REF ), i.e., it knows only its local data {{formula:b1202ec7-4b30-4056-aba3-b3a73d23e831}} , {{formula:0a287b7a-5783-468b-9766-ff38f5af53ec}} , {{formula:f4e75a05-706e-4512-9768-315f0d463d54}} and {{formula:9361b790-7a64-4368-8e17-a21145968ff7}} . The goal for each agent is to compute its portion {{formula:0f57522a-30fa-4b56-",
        "1f2b15be}} , namely the aggregate problem solved by the agents at iteration {{formula:83a3073e-1e2e-40d6-9e68-1e380f490b70}} , i.e., {{formula:1a68d53b-c4cc-489e-a7f1-d5983ec153ff}}In particular, the constraints {{formula:07617ee2-a850-4238-8eae-b80a069b17dd}} are perturbed by {{formula:37a114ec-163d-4b76-9c5e-5eb8551e2fc9}} to obtain {{formula:6bdc4bac-a6bf-4a86-a870-cdfc76251886}} . By applying perturbation theory , we have for all {{formula:8190f2ee-93d0-4165-bc16-6333ae7e3a7f}} {{formula:31ea2227-c8d3-465e-b650-2c93bbb577cf}}[ Supplement to",
        "1-a72a8392d351}} maintains a local allocation estimate {{formula:8cd0cbfb-2eaf-420f-927f-13e9d67bf382}} . At each iteration {{formula:77ebe504-33c5-458f-a83f-6da8d7336d04}} , agents compute {{formula:87b18277-5311-4003-bcc8-1a5680102386}} as a Lagrange multiplier of {{formula:79d9926a-9b4d-42f6-8c0e-a65928eea836}}where {{formula:ac72461f-c5c9-47f0-8ee8-ba6a7b911c37}} and 1 is the vector of ones. Then, each agent {{formula:fa0b7bfa-b868-422b-b56a-6034092d3fd8}} receives {{formula:4cf47649-6104-4405-8158-89961c6c58ee}} from its neighbors {{formula:3349"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between ladder operators and spectrum-generating operators in the context of this research paper?",
      "expected_output": "The ladder operators can be substituted by other ladder operators in the set of spectrum-generating operators. For example, the ladder operators {{formula:78027fc2-4c15-499f-8071-5861995ef479}} can be substituted by the ladder operators {{formula:9f4a978b-2da3-4b31-b757-e046d7859de0}}. These operators are constructed by Darboux-dressing of the ladder operators and commute for a certain polynomial in the Hamiltonian.",
      "context": [
        " , {{formula:e94161e3-6721-471b-837e-543d3b0c9221}} , or {{formula:4fc3d5a6-84e8-46a8-b8fd-6f83237e4654}} they will form the set of the spectrum-generating operators. So, let us consider the differential operators of order twelve, {{formula:00ff4f96-419b-4aeb-b762-2abe5fddb71b}}They are independent from the ladder operators constructed via the Darboux-dressing procedure, and their commutator {{formula:2d65608f-b114-4985-8109-e7aecb810fdd}} is a certain polynomal of order 11 in the Hamiltonian {{formula:fba59f95-e3d8-45f2-9637-872f481028e8}} . The operators {{formula:e5fae160-3810-4d20-826d-94d7833901ef}} divide the space of the states of the system into six",
        " {{formula:d81f1f71-bae3-4923-9cb5-fb9712a64075}} form the set of spectrum-generating operators. In these sets of spectrum-generating operators, the ladder operators {{formula:78027fc2-4c15-499f-8071-5861995ef479}} can be substituted by the ladder operators {{formula:9f4a978b-2da3-4b31-b757-e046d7859de0}} . The operators {{formula:10550fa9-a432-41ca-8364-6598235f479c}} are constructed by Darboux-dressing of the ladder operators {{formula:0713fb5e-5d9a-4415-ba75-9593846a457e}} in the way how we constructed such operators in the considered examples. Here we assume that the corresponding Hamiltonians {{formula:0ad44ef3-3bf4-41ef-b023-9826a1ae77c6}} and {{formula:ffc14ed6-a2a0-4c55-ba89-",
        "0f06-4978-bb7c-b798f7c90211}} act irreducibly. These ladder operators commute for certain polynomial of order nine in the Hamiltonian {{formula:dbdd0da2-2b73-4e1e-b1a7-39c1b3018a97}} .Like in the isospectral case considered in the previous section, here we have two ways to realize Darboux-dressing of the ladder operators {{formula:e972e637-89a5-40e1-96f7-fe5ea2a8f5e9}} of the half-harmonic oscillator system {{formula:32aea26f-657c-45ca-91be-ed89120674ec}} . Using for this purpose the intertwining operators {{formula:3d3000c3-0e08-41db-8396-c2ab6bacba52}} , we obtain differential operators of order twelve: {{formula:a0c791b8-5b5c-4bde-b325-499e44a7fde4}}With the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the significance of density in the context of Lebesgue measure and how does it impact the main findings of this research paper?",
      "expected_output": "The density in the context of Lebesgue measure is crucial as it defines the distribution of a measure with respect to another. In this research paper, the density is given by the formula {{formula:0be284ca-62c6-47fa-aeae-23962dcd1188}} and is used to derive important findings. For instance, it is used to establish the relationship {{formula:af8f785b-d6ee-467d-8383-bb9bd11e9679}}, which holds for all {{formula:79bffef5-d29b-410e-8813-a8316208d1c3}} and {{formula:81bb6fa7-a8bf-464c-94db-6c01336f2412}}. The significance of density is further emphasized when the measure is symmetric, leading to the conclusion {{formula:edc266b3-fadc-49c9-a50c-294c5c288130}} under certain conditions.",
      "context": [
        "vy measure with density {{formula:0be284ca-62c6-47fa-aeae-23962dcd1188}} with respect to the Lebesgue measure and {{formula:d9e9f504-aa5a-46da-b709-ee8b4027109f}} for some {{formula:7103dc1c-dab7-4753-8a6a-e3bd088ddf21}} and {{formula:d4e6be32-6464-4e11-b8dd-9585b282d774}} . Then, for all {{formula:79bffef5-d29b-410e-8813-a8316208d1c3}} , {{formula:81bb6fa7-a8bf-464c-94db-6c01336f2412}} , it holds: {{formula:af8f785b-d6ee-467d-8383-bb9bd11e9679}}where {{formula:32c060e9-36cf-406d-a392-8df11495335c}} and {{form",
        "cab1d2222e41}} with respect to the Lebesgue measure and {{formula:9b3f56b0-ce2b-4145-8b11-b76e48b18512}} for some {{formula:536e55f5-3eb7-412c-ae81-edfa1941123c}} and {{formula:4bd0cdaf-3ee1-408f-8304-78c14b57f509}} . Let {{formula:a169f00e-15ac-4208-9a2e-a95db1ebaade}} and set {{formula:74d119b8-e0b0-4724-8107-c8c2779eb934}} . Then, for all {{formula:c5979d30-b7cf-4d57-878f-b5193e89e823}} it holds: {{formula:a54f410f-96d0-4d83-8ede-b2976097aba4}}where {{formula:a5d09046-7e3f-4a77",
        " in (REF ).If in addition we suppose that {{formula:b768344f-6d9e-461a-b1e8-6d5158c95f8f}} is a symmetric measure, thenIf {{formula:56652791-6fe0-40c8-9171-03a267d0bf2b}} and {{formula:bb5e7806-6c43-488d-97c1-a2fba8e590af}} for some {{formula:8492c728-2bea-47e7-a32c-96f42b1e695e}} and {{formula:04dfa32e-aed6-49b3-8fec-6f6eb8f18b61}} , it holds {{formula:edc266b3-fadc-49c9-a50c-294c5c288130}} where {{formula:b8c7bef5-c82d-4b57-b168-09932ccf0547}} is defined as in (REF ). If {{formula:3ee93750"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between d81 and 6101bac7 in the context of this research paper?",
      "expected_output": "In the context of the research paper, d81 refers to the components of the 3-momentum parallel and orthogonal to 6101bac7, which represents the direction of the electric field. This relationship indicates that the momentum components are described in relation to the electric field direction.",
      "context": [
        "d81}} are the components of the 3-momentum {{formula:2b9239b8-c803-4f30-b23f-d96259b2da31}} parallel and orthogonal to {{formula:6101bac7-5f2a-44b9-a3b1-6c0f6a93f37f}} . We assume that the pairs are produced at rest in the direction parallel to the electric field , . We also have, in Eqs. (REF ), () and (), {{formula:aebf639b-1f4c-41be-b0e7-5e35a1aa2d71}}which describe probability rates for pair creation by photons and pair annihilation into photons, {{formula:83857924-8c7d-4c0a-935c-66d5de8207ac}} being the matrix element for the process {{formula:392d5e57-541b-4e8e-97c8-c45738aa67f8}} . Note that the collisional terms (REF ) and () are either unapplicable or negligible",
        "3a3-5c37-4adb-8163-b63768ae86a5}} , {{formula:ac41bf02-813c-4c60-8d10-ba6e64517488}} , where {{formula:41a349b0-24fb-4101-a970-1ec31c6eb083}} and {{formula:a443a0b6-bea5-4d11-8e43-1b381be3e8a4}} are number densities of positrons (electrons) and photons, respectively. The term {{formula:d7cc5803-7784-42ea-9c96-a3311f4e5ce2}}is the Schwinger source for pair creation (see , ): {{formula:985882e9-a5d0-4b1d-ad19-f9434823762e}} and {{formula:7900d125-bc58-46e3-8665-d57e49673d81}} are the components of the 3-momentum {{formula:2b9239b8-c",
        " is encoded in the terms describing pair creation and scattering probabilities. Eqs. (REF ) are consistent with energy density conservation: {{formula:e6ca3b93-a367-490a-9f96-2169af83f978}}The initial conditions for Eqs. (REF ) are {{formula:4208266a-f511-4a93-b396-14e0cb9e1567}} . In Fig. REF the results of the numerical integration for {{formula:c52e5cdc-679c-4173-aa3c-3d8f1e42e8a2}} is showed. The integration stops at {{formula:ceefc5e3-cd74-47d1-8b08-4ffd6d204ed0}} (where {{formula:249041c0-5c6b-49bd-bdc0-1ef5c9ac8724}} ). Each variable is represented in units of {{formula:d005ebd5-03e7-45d4-8565-a6f157d5663b}} and {{formula:5fc4c1"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between C and quark mass, and how do the research findings support this connection?",
      "expected_output": "The relationship between C and quark mass is that C, a dimensionless constant, represents the one-gluon exchange strength, which contributes to a repulsive pressure that counterbalances the attractive pressure in the quark system. According to the research findings, this connection is supported by the equation m_i = m_{0i} + D/rho_B^{1/3} + C*rho_B^{1/3}, where the last term, C*rho_B^{1/3}, contributes to the repulsive pressure, preventing the star from collapsing as its mass increases. The findings also show that modifying the model by expanding the single gluon exchange term with an extra term in the order of rho_B^{2/3} can achieve a more compact quark star with a larger maximum mass.",
      "context": [
        " is a dimensionless constant representing the deconfinement strength. In this relation, z=1, therefore, Eq. (REF ) becomesm_i = m_{0i} + \\dfrac{D}{\\rho _B^{1/3}} + C\\rho _B^{1/3},here, C is a dimensionless constant representing the one-gluon exchange strength. In this relation, the last term contributes to a repulsive pressure that counterbalances the attractive pressure and prevents the star from abruptly collapsing as its mass increases. This cubic-root mass scaling law adopted here has been widely used in investigating QSs at zero temperature , , and has been extended to the finite temperature case , , .\nThe Modified DDQM: The main motivation for this modification is to achieve an enhanced maximum stellar mass with smaller radii and, consequently a more compact QS than the one that can be achieved from Eq. (REF ). In the authors compared QSs built from Eq. (REF ) to others built from vector MIT bag model , and they observed that the DDQM produces less compact QSs with masses far lower than that of the vector MIT bag model. Using Bayesian inference to fix",
        "i \\sim \\rho _B^2\\partial m_i(\\rho _B)/\\partial \\rho _B. The confining part of the expression above contributes to an attractive pressure that reduces the maximum gravitational mass of the QSs. However, the single gluon exchange term contributes to a repulsive pressure which increases the star's maximum mass. Therefore, we modify the model by expanding the single gluon exchange term by an extra term in the order of \\rho _B^{2/3}, therefore Eq. (REF ) assumes the formm_i = m_{i0} + \\dfrac{D}{\\rho _B^{1/3}} + C\\Big (1+\\kappa \\rho _B^{1/3}\\Big )\\rho _B^{1/3}.Thus, \\kappa is a dimensionful constant with a positive or negative sign depending on the stellar mass required. We noticed that \\kappa is generally small, having magnitude \\kappa \\ll \\rho _0^{-1}, with \\kappa >0 for \\rm M\\ge 2M_\\odot and \\kappa <0 for \\rm M <",
        " \\beta -function are among some of the quantities whose characteristics are used to distinguish matter states. The numerical values of these properties determined through five different model frameworks at different \\rho _B have been presented in Tab. REF for comparison.In this work, we use QSs to investigate the behavior of strongly interacting matter. Firstly, we modified the widely used DDQM:m_i = m_{i0} + \\dfrac{D}{\\rho _B^{1/3}} + C\\rho _B^{1/3},where m_i is the equivalent quark mass, m_{i0} is the current quark mass, D is a dimensionful constant representing the confinement strength and C is a dimensionless constant representing the single gluon exchange strength , , , , , , , , , such that larger maximum masses and more compact stable stars can be reached. Secondly, we focus on the behavior of QM in QSs in relation to the size of the stars. Considering that the pressure of the quark system in Eq. (REF ) is given by P_i \\sim \\rho _B^2\\partial m_i(\\rho _B)/\\partial \\r"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the role of Stirling numbers in the research paper and how do they relate to the main findings?",
      "expected_output": "The Stirling numbers of the first and second kind play a significant role in the research paper, as they are used to derive important formulas and equations. Specifically, they are defined for all integer numbers and are used to express certain constants independent of other variables. For example, the paper mentions that the Stirling numbers are used in the definition of the Beta function, which is a crucial component of the main findings. The Stirling numbers help to establish relationships between different variables and constants, ultimately contributing to the derivation of key equations and results in the paper.",
      "context": [
        "3e434b01-1a80-40f4-a4c3-ea1a9e67d755}}Hence, by telescoping we derive {{formula:cfd7ee63-612d-4b59-85e9-2cb138dfc381}}Notice that {{formula:f8c6f55f-9c21-4389-93c2-0a1e83f58aa5}}where {{formula:ceef2637-6d9c-421c-8bec-fe701d553b3b}} are some constans independent on {{formula:a689b550-414b-48e3-bd1e-aad3fb74033a}} (see ).Let {{formula:5bcf7d8e-20f2-4b23-adc8-9e36b3a2bed5}} {{formula:59332074-cc7e-498d-b2b7-0a57727bb6a9}} be the Stirling numbers of the first and second kind respectively, which are defined for all integer numbers such that {{formula:",
        "adc8c32-c9a8-48c8-b771-7c392706b88b}}Observe that {{formula:a62f2e2a-5d02-4b7b-aec3-6e354b900165}}Let {{formula:c32afae9-9f69-4c13-93d8-02132ae07f42}} {{formula:b3062ced-3d67-4da5-98f9-80af59685b48}}where {{formula:f65127ba-d59f-4c6f-bfb4-5ed29cad2579}} {{formula:b4f7214b-6aa2-4e8a-a7d0-49a31812f960}}Applying Lemma REF we deduce that {{formula:f4408815-d84f-4e98-a55b-11546fd6053f}}From Lemma REF for {{formula:d9bc8b3a-4cdd-4cfd-b120-c0bb9f311",
        " {{formula:344002b9-9d19-4632-a717-91b822a73267}}Using the definition of the Beta function (see (REF ) for {{formula:f14cb2d4-7721-4728-9788-c2250742cf22}} {{formula:db5cadea-03d8-49a4-afd0-a19774461969}} ) we have {{formula:c09a8217-fe79-4be3-b9a0-8033991a3ae1}}Observe that {{formula:c2f0b12b-661a-4a86-bf8d-cc0a79a943f1}}where {{formula:d1d290c5-6ac1-465d-adfd-43e2ac337709}} are some constans independent on {{formula:df7e480a-8514-4226-83d1-548d19595d9b}}Therefore, from Equation (REF ) for {{formula:b20c2d8a-2897-4871-8126"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the significance of orthogonal vectors in the context of the Minkowski metric, and how do they impact the main findings of this research paper?",
      "expected_output": "In the context of the Minkowski metric, orthogonal vectors play a crucial role in identifying first-class and second-class constraints. The unit spacelike vector is an example of an orthogonal vector that helps express the matrix in a specific form, which agrees with a given equation. By contracting equations along a particular direction and considering other equations, the research finds the second-class constraints. The orthogonal vectors help expand these constraints in terms of an orthogonal basis, allowing for the identification of equivalent sets of constraints. For instance, the vectors span a basis for the first-class constraint surfaces in the phase space. The significance of orthogonal vectors lies in their ability to provide a geometric viewpoint for understanding the constraints, enabling the research to find the whole set of orthogonal vectors and contract relevant equations to derive important findings.",
      "context": [
        " and () we have another first-class constraint {{formula:5c9d4432-4364-4184-95d6-7536470a8daa}}An orthogonal vector to {{formula:8838e2ca-52cf-409b-b66a-fbadde0ef0e9}} , under a Minkowski metric, is provided by the unit spacelike vector {{formula:18f728ec-780e-4838-bd4b-8b8600bf029f}} . In consequence, the matrix {{formula:0ed173a4-d503-4c1d-92ec-7f7f698d6531}} in (REF ) may be expressed as {{formula:e08b3b2b-630d-46dc-87cb-fefd8c4ceda2}}which is in agreement with equation (REF ).Hence, by contracting (REF ) and (REF ) along {{formula:08b779ea-d965-4529-b89e-5bc889e9b87d}} , and considering equations (REF ) to (), we find the second",
        " the metric in order to expand these constraints in terms of the orthogonal basis, i.e., {{formula:548378fb-7971-4393-bc38-7c2fd70c6dbf}} . From here, by linear independence, we can identify an equivalent set of constraints to {{formula:d20d0907-02e5-48a4-a445-98ab07b8e91b}} and {{formula:97a080c3-d1af-4f56-a766-363b7151ed7f}} . We can check by straightforward computation that the resulting expressions {{formula:cab0c28b-7876-4554-8613-b41b8adace9c}} and {{formula:7e9acddb-ee68-4cf5-9680-465f917aedad}} are second-class constraints. Hence, the vectors {{formula:c2fec0d4-de1e-428e-87c9-94e28ef8427a}} span a basis for the first-class constraint surfaces in the phase space whereas {{formula:05",
        "ab-e1ac6f7b3279}} .With respect to the second-class constraints, for this specific case and adopting a geometric viewpoint, we need to find the whole set of orthogonal vectors to {{formula:b4f1cf37-c141-44a1-a73f-488880e2a200}} , say {{formula:8011502f-15eb-4345-a4a4-68e7f6770b4e}} , and then contract {{formula:334def56-dadd-4e67-a000-b4114e5fa826}} and {{formula:70d74af8-66a5-4919-ade2-f87f1ef26079}} with {{formula:745cf7e2-5792-4e6c-8424-a4c239434d9d}} , where {{formula:899f5902-8031-4e69-93b5-6177724f1dbe}} keeps track of the number of orthogonal vectors to {{formula:ece2e080-bb0b-4d"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main coefficients of theta series and how are they used in the research paper?",
      "expected_output": "The main coefficients of the theta series are denoted as 'a's and 'b's. In the research paper, these coefficients are used to construct the ring of integers, where 'a's are the coefficients of a specific series and 'b's are the coefficients of the theta series associated with a particular elliptic curve. For example, the paper mentions that the generalized Heegner cycles correspond to elements in a specific group for elliptic curves that are isogenous to another curve in a particular field.",
      "context": [
        "ula:fc041c70-6680-45e9-90c3-2737186ab293}} be the ring of integers of {{formula:99fe5ba5-cec6-4f11-8031-e47497290c69}}where the {{formula:9cbf79a7-23e5-43cb-b57c-8014fbb7aabc}} 's are the coefficients of {{formula:7f61dba2-1d82-43cc-8b27-91e46e0473ca}} and the {{formula:7104f5a9-9a5d-4cff-bbd1-4c2bffec2b48}} 's are the coefficients of the theta series {{formula:1220f8d5-843f-4458-a492-0b546755a3d9}}associated to {{formula:dfd125de-b4d0-46af-af0e-d23bd1292066}} . Then {{formula:764c18e5-7ff5-49f0-8fed-",
        "6-490c-b359-fce47018e611}} . Then {{formula:950a7c96-3a30-44b5-a804-3cbdc6e2e31c}}By , {{formula:f369fada-caf3-41b2-a4a3-f96e9c0ecbb4}}where the generalized Heegner cycles {{formula:02aba5dc-5542-4453-bbed-be21de89e6ef}} correspond to elements {{formula:4db6f8b5-dce1-4e00-b58d-e5202df174a0}}in {{formula:4bdbf8cd-5678-48a6-94ee-302d9c2e8507}} for elliptic curves {{formula:336a6050-4512-4896-bd8d-a0df471bee93}} that are {{formula:ee5a13b1-3252-4635-b2cb-3eac91c7161a}} -isogenous to {{formula:594b7",
        "ea527c0e-f98d-4cbb-b927-c1f847fc31ce}} in {{formula:aea5dc1c-7d0f-4a39-be45-8e75b18ea0fb}} , we have that {{formula:71fa0ecd-8d18-447c-87f7-fd6eb1f0f294}}For {{formula:588fb4b8-c4bc-438e-ab13-00dd93cd89b9}} and {{formula:636605c1-9805-49de-b0be-5666cf51d9a4}} in {{formula:2debe79f-6697-4c41-bf75-e1202ae6aa65}} , we have that {{formula:21123029-a596-4ba1-980e-c6c461a64b33}}Hence {{formula:4a092a38-59f4-4998-b9f9-1373da888c8b}}The identity {{formula:2b0e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does Lemma 3.4 contribute to the existence of unique solutions in the research paper?",
      "expected_output": "Lemma 3.4 contributes to the existence of unique solutions by assuming that a given formula satisfies a certain condition, and then proving that under specific assumptions, a unique probabilistically strong solution exists for a particular equation. The proof involves a Galerkin approximation argument and uses techniques such as Itô's formula and Jakubowski-Skorokhod's representation theorem to show the convergence in distribution of a sequence of solutions.",
      "context": [
        "5d-e00a8effc0da}}Lemma 3.4 Assume that {{formula:b0abde39-5ca7-44b4-bb66-559780ef5341}} satisfies (REF ). Then under the assumptions (A0), (A1), (A2), equation (REF ) has a unique probabilistically strong solution {{formula:3a01612f-7dd6-43b3-8d10-b7c0d4f5f9c7}}The proof follows a very similar Galerkin approximation argument as in , we show some key steps here.Let {{formula:2ee2473f-2c54-4773-a441-9de76d2b1a49}} be an orthonormal basis of {{formula:efa2347b-b7bb-46e3-be1f-cc78e14318c7}} whose elements belong to {{formula:4787d591-20ef-4f2a-a979-38ea70e73feb}} and orthogonal in {{formula:b08ce293-951d-4",
        "54121b4dad}} .Fix {{formula:e949624a-a121-4f22-ad61-f426341068ba}} and for {{formula:55c7fc1f-5742-4ee2-9acf-fa64291f589e}} consider the following equation on {{formula:2f48d7ad-15c8-437d-b072-33bc48a648b0}} : {{formula:70f6eb8d-8786-47c4-9182-c826f53e1ee5}}Then by there exists unique global strong solution {{formula:45509344-b797-45c6-b5da-64a320a8e754}} to (REF ). Moreover, {{formula:aa64ac20-fe95-4ca1-8988-d528176124fe}} .We first prove a priori estimates. Applying Itô's formula to {{formula:f080f6aa-3b9a-4acc-9bc3-2c5212f97dd6}} , we have {{formula:6736",
        "-4fec-b44c-f539681bd78f}} . Our goal is to show that {{formula:429741d7-a3f3-4fd3-a816-3aea372752bc}} has the same law as {{formula:71da81bd-d831-49e5-82fc-2db86b5148cc}} and {{formula:d1fff2f5-c55e-410e-85e7-bd856e70e3fa}} convergence in distribution to {{formula:660ea5b4-2825-470f-81aa-12585138025b}} in the space {{formula:d3fcd124-e85d-455f-b54b-218c2a917707}} .By Jakubowski-Skorokhod's representation theorem (see or ), there exists a stochastic basis {{formula:83e74a89-3179-4452-8632-6f60dda0ba59}} and, on this basis, {{formula:f5d05038-1e90-494b-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between Penrose diagrams and spacetime's global structure, and how do the methods used support these findings?",
      "expected_output": "The research paper explores the relationship between Penrose diagrams and spacetime's global structure, finding that a globally naked singularity cannot form once a marginally trapped surface has developed. The methods used, including the construction of a global double null chart and the analysis of the maximal globally hyperbolic development, support these findings. Specifically, the paper uses a general characterization of the boundary of spacetime in the context of asymptotically flat, spherically symmetric Cauchy data for the EMCSF system, allowing for the rigorous association of a global Penrose diagram and the identification of a nonempty null boundary component, future null infinity.",
      "context": [
        " component {{formula:00bbba5b-6774-4b2e-9810-bb9f8ac87bc5}} emanating from a (possibly absent) “locally naked” singularity at the center. The Penrose diagram {{formula:87cc639b-6159-4f3c-a022-18e0f2515d59}} can be viewed as a global double null chart for the spacetime, with {{formula:e10816fd-d175-416f-a9f2-6a70816aa6c0}} the “outgoing” null coordinate and {{formula:ceadb7aa-0c52-4ea7-a29b-065d817fbae6}} the “ingoing” coordinate. See Kommemi.Note that the above general boundary decomposition in particular proves that one cannot form a globally naked singularity once a marginally trapped surface has developed in the spacetime, which already rules out naked singularity formation by supercharging a black hole in spherical symmetry, see . It is thus not at all surprising that ongoing numerical searches for these continue to be futile.For use",
        "rapped spheres. At this point, all attention is restricted to this future domain of dependence. A posteriori, by the existence and uniqueness theory for the maximal globally hyperbolic development, the spacetime will then be contained in the maximal development of the induced data on the Cauchy hypersurface {{formula:a8aae5e6-1d84-4963-9a5b-f3a9063e0867}} . {{figure:d686a323-7337-48fa-bcd9-b6997154ab2b}}Since our examples are maximal globally hyperbolic developments of asymptotically flat, spherically symmetric Cauchy data for the EMCSF system with no antitrapped spheres of symmetry, we can make use of a general characterization of the boundary of spacetime in this context appearing in . In particular one can rigorously associate a global Penrose diagram, and unambiguously identify a nonempty null boundary component future null infinity {{formula:f671d14d-bcc2-46cd-aae6-aafff05c5b22}} , domain of outer communication {{formula:92cad8a1",
        "b4-4637-bbf7-78b0eded0d02}} curve near {{formula:630a308b-666e-4a06-b57b-8dc27da47fc9}} in standard coordinates. Furthermore, by having {{formula:7312188a-8707-48fc-afde-07f14795647b}} hug the gluing region closely enough, we are guaranteed to have no spherically symmetric antitrapped surfaces on {{formula:59c988ce-95dc-40c5-aa3a-7f8288d31fbe}} .Completeness of null infinity {{formula:f2a15e60-68f3-4cec-9867-46dd0d700f1f}} is inherited from the exact Reissner–Nordström solution. By inspecting Penrose-diagram-proof, we see that the null hypersurface {{formula:036e3142-c5d9-4209-a2f2-9449c1462d2c}} is the event horizon {{formula:f82fd270-1101-41c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between isoscaling and the nuclear equation of state, and how do the methods used impact the key results?",
      "expected_output": "The research paper explores the relationship between isoscaling and the nuclear equation of state, particularly the symmetry term. The main findings indicate that the symmetry term coefficients, deduced from isotopic scaling data, are near 25 MeV for peripheral and 15 MeV for central collisions. The methods used, including the statistical description of isotopic scaling and exclusive data from heavier projectiles, impact the key results by allowing for the extraction of scaling parameters that decrease strongly with increasing centrality. The paper also highlights the importance of the symmetry term in the nuclear equation of state for astrophysical applications, such as supernova simulations and neutron star models.",
      "context": [
        "}} Sn and {{formula:61b53683-b7af-4145-bf13-bfacf55ba408}} Sn targets at 300 and 600 MeV per nucleon incident energy. Here, according to the established systematics , maximum fragment production occurs at central impact parameters.Isotopic scaling, also termed isoscaling, has been shown to be a phenomenon common to many different types of heavy ion reactions , , , . It is observed by comparing product yields from otherwise identical reactions with isotopically different projectiles or targets, and it is constituted by an exponential dependence of the measured yield ratios {{formula:4b786c5a-c47c-4334-923b-bc7974b1d88e}} on the neutron number {{formula:6c720259-1451-4bac-8487-73013e57591f}} and proton number {{formula:8b1722ad-8fb2-47c1-87fb-53e256375ad7}} of the considered product. The scaling expression {{formula:461a853f-9c15-4f5f-a77f-ecb30cf86e",
        " the density is only low on average. An experimental value for {{formula:4884eaf0-2bf4-4713-8a59-e57e00863f7b}} of about standard magnitude has recently been obtained within a statistical description of isotopic scaling in light-ion (p, d, {{formula:2f95735f-cac1-4ed1-9f1b-5b6e29800ca7}} ) induced reactions at relativistic energies of up to 15 GeV . This result, however, may not be representative for multi-fragment decays because the data were inclusive and the mean multiplicities of intermediate-mass fragments correspondingly small . In the present work, we apply the same method to exclusive data obtained with heavier projectiles, {{formula:02527490-b049-4cb1-b673-088e2b3eb5f6}} C on {{formula:c15a1051-6a86-46e9-938c-a62ffb723855}} Sn and {{formula:61b53683-b7af-4145-bf13-bfacf",
        "a3}} detector. The measured yield ratios for light particles and fragments with atomic number {{formula:20057ff8-275f-4ee0-b907-bf761a75ab71}} 5 obey the exponential law of isotopic scaling. The deduced scaling parameters decrease strongly with increasing centrality to values smaller than 50% of those obtained for the peripheral event groups. Symmetry term coefficients, deduced from these data within the statistical description of isotopic scaling, are near {{formula:52cffb22-5247-4234-af58-783c31234c5e}} 25 MeV for peripheral and {{formula:63a8d45d-e553-417b-9878-bfb3ab0af1e3}} 15 MeV for central collisions.25.70.Mn, 25.70.Pq, 24.10.PaThe growing interest in isospin effects in nuclear reactions is motivated by an increasing awareness of the importance of the symmetry term in the nuclear equation of state, in particular for astrophysical applications. Supernova simulations or neutron star models require inputs for the nuclear equation of state at extreme values of density and asymmetry , , ."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does CKM quark mixing play in causing CP violation, and how is this phenomenon observed in the research findings?",
      "expected_output": "The CKM quark mixing matrix plays a crucial role in causing CP violation in the Standard Model. According to the research, CP asymmetries in certain decays are defined by specific formulas, and the origin of CP violation can be tested through decays such as B->Kπ. The CKM matrix is responsible for the weak CP violating phase, which is necessary for CP violation to occur. The research findings show that the present world average for certain observables is consistent with the SM prediction, indicating that the CKM quark mixing matrix is a key component in understanding CP violation.",
      "context": [
        " or CP-odd observable, needs both weak CP violating and strong phases, while the latter one of the time dependent CPA contains not only a non-zero CP-odd phase but also the {{formula:ca7c120a-a829-4ab4-9a4a-41766a5aad0e}} mixing. We note that the present world average for {{formula:8cb6ba8d-b83a-46ae-8b28-930065decd95}} is {{formula:2a285cc4-9091-425f-968a-975d34913ec9}} , comparing with the SM prediction of {{formula:2e922923-2ee1-44e9-85cb-8584da81d207}} .To study CPV, one can also define some other useful observables by the momentum correlations. For example, in a three-body decay, the simplest ones are the triple correlations of {{formula:10ad3e1c-3c37-458f-9d30-3c7dc49c30e8}} , where {{formula:f54fab64-6a53",
        "-c432-4f51-b433-892cf83b67d2}} are also within the theoretical predicted ranges .Via {{formula:97775515-af39-4b55-8a5d-245061325802}} decays such as {{formula:9dbe1fc7-948e-490a-adea-8598a2b198bb}} , we can test the origin of CP violation (CPV) in the SM which is the consequence of the CKM quark mixing matrix . CP asymmetries (CPAs) in {{formula:a28348cc-8eeb-4bb1-98d8-4603776016be}} decays are usually defined by {{formula:9c020f89-496d-4baf-937a-1feb4e4937ef}} and {{formula:a8b452f9-eb87-4f43-9dae-b60b76f49b24}} . The former case, called direct CPA or CP-odd observable, needs both weak CP violating and strong phases, while the latter one of the time dependent CPA",
        " the origin of CPV. We note that the T violating observables such as the above triple correlations do not require strong phases. In the decays of {{formula:b180820b-862e-451d-8d6c-2e4bad340787}} ({{formula:dd6d436e-b894-44a6-bbda-0383084f0a70}} , and {{formula:e7de45f2-1525-4718-a1f5-af1515acdbca}} ), the spin {{formula:c038b581-a8e7-41d6-afe2-b14f19bc41cd}} can be the polarized lepton, {{formula:76992b29-1616-4760-8c9e-edd8a7bec91e}} or the {{formula:bfc01cc0-1118-4f95-bd84-b91adb8ffe89}} meson, {{formula:9f66c804-bd28-4b10-b4e6-6c110d6a97de}} . Considering"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about Ricci limit spaces and how do boundaries relate to them?",
      "expected_output": "The research paper discusses Ricci limit spaces, which are limits of complete manifolds with uniform Ricci curvature lower bound. The main findings include that a non-collapsed Ricci limit space has empty boundary. The paper also introduces the concept of Ricci limit spaces with boundary, which is a subclass of spaces that includes manifolds with boundary. The boundary of a Ricci limit space is said to be convex if the second fundamental form of the boundary is non-negative definite. For example, Corollary 1.4 states that a complete, non-compact, manifold with smooth convex boundary and non-negative Ricci curvature is a Ricci limit space with boundary.",
      "context": [
        "1e17-48c5-a7ec-9b9ae565d5ba}}We also have a version of this theorem when {{formula:1ee88339-bf22-4313-8821-5d4413eade27}} has boundary, based on the boundary regularity theory of {{formula:a0febc3b-3ee8-4d00-8a2f-102d57b5d2bc}} spaces, which contains the class of Ricci limit spaces with boundary. See section REF for relevant definitions.Corollary 1.4 Let {{formula:23512826-ab8c-46dc-8c6f-a8770a4d4ae5}} be a complete, non-compact, {{formula:444f9bf9-f3d7-4f83-9217-31be1a1bd478}} -dimensional manifold with smooth convex boundary {{formula:4e646431-177d-49be-9e5a-b1adac324e7e}} , non-negative Ricci curvature {{formula:3d4b9a9",
        "ci limit space, then {{formula:0a2039c2-a271-489c-9333-90e373cf9a51}} , it means a non-collpased Ricci limit space has empty boundary. Under the notion of boundary of a {{formula:b655c72b-894a-412b-86c5-3b66634f3b57}} space, we can also include manifolds with boundary into our study that gives rise to the class of Ricci limit spaces with boundary. This is a subclass of {{formula:6d645e52-49c9-48fe-890d-f1bac977ee8f}} spaces.Definition 2.6 A Ricci limit space with non-empty boundary is a {{formula:d8d7998a-25c8-4c04-9c0a-22518a515f8a}} limit of (pointed) complete manifolds of the same dimension, with smooth convex boundary and uniform Ricci curvature lower bound in the interior.Here, the boundary is said to be convex if the second fundamental form of the boundary is non-negative definite. We",
        "001f2d3}} has second fundamental form {{formula:74e5ac51-8008-4b5b-8fcb-892ca42f4e40}} .Remark 2.7 From the computation carried out by Han, we can infer that if a manifold with boundary is an {{formula:57a75456-9214-4d46-9d50-3dc18d2bf480}} space (hence also a {{formula:a6e345bf-9bc6-4bd3-9950-75e1d13a59a4}} space), then the boundary is necessarily convex, see *Corollary 2.5.It is worth of pointing out that the class of {{formula:70d9fe42-b9c4-4150-b8bf-2599c8789216}} spaces without boundary is strictly larger than the class of non-collapsed Ricci limit spaces. For example {{formula:86086bf1-13fc-4a50-a8fd-979a0c12a3fd}} is a {{formula:ba3ebaf6-9657-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the role of ellipses in spectrum enclosure via Gershgorin's theorem and how does it relate to the main findings of this research paper?",
      "expected_output": "In the context of this research paper, ellipses play a crucial role in spectrum enclosure via Gershgorin's theorem. The paper discusses how a two-parameter family of ellipses can be used to enclose the spectrum of a given matrix. Specifically, the ellipse's focal interval is equal to the interpolation interval, and its convex hull encloses the spectrum. The parameters of the ellipse are the right endpoint of the interpolation interval and the capacity. The use of ellipses in this way allows for a more precise enclosure of the spectrum, as demonstrated by the paper's findings. For example, the paper shows how Gershgorin's disk theorem can be used to enclose the spectrum in a rectangle, which can then be further refined using the ellipse-based approach.",
      "context": [
        "-4009-9eba-171624cbf5da}} and {{formula:9ef1b6d4-aa83-4a7a-bd31-3924e6149c89}} ), otherwise we shift the matrix accordingly. In order to keep the notation simple we consider a single ellipse {{formula:ce0c426e-4ffc-430d-ac39-ec83c59e86cb}} with focal interval {{formula:e4889cfa-4f9a-4cbd-9ba9-ed181f50ae51}} and capacity {{formula:2cc33178-6797-4aa6-92df-2c0f9042fe72}} for which (REF ) is satisfied. As before we denote the convex hull of {{formula:d73e85ba-6553-4836-b40b-f2314c4df55d}} by {{formula:0cb0246e-b4c9-4737-bfc4-3f5f5979a4b0}} . Let {{formula:ddb94d2d-ce",
        "37385}} . Furthermore, we assume that {{formula:fdaa1d4c-dce6-433f-bde9-b6cc82cd51cc}} is an ellipse, with focal interval equal to the interpolation interval {{formula:1f1a730f-9331-4de4-a99b-ba57aa99814e}} , and its convex hull {{formula:da2e46c8-6c6e-46e4-8127-0e04ba8db508}} encloses {{formula:11ea6ba3-99d6-4d59-9b59-7e2203ec8690}} . As a result of these assumptions, there is not only one ellipse but rather a two parameter family of ellipses {{formula:4db2ed9b-416e-4ef6-8fa8-e44261c59638}} . The parameters are the right endpoint {{formula:eb49eb59-8d88-4538-afae-d5b61fa08c7e}} of the interpolation (focal) interval and the capacity",
        "77063-d2b6-4314-b317-7202513ff69b}} for a given matrix {{formula:22bc707b-5fee-42ec-8227-985eca0bcc78}} and an ellipse {{formula:30bace1f-ab80-4134-9951-bcb6ff61fc88}} . We recall that, with the help of Gershgorin's disk theorem, we can enclose the spectrum of {{formula:e5ef5097-eb4d-4e09-9b97-9c1661aab629}} in a rectangle {{formula:13d252f5-5f79-4853-a0dd-51a48a609e37}} with vertices {{formula:f6354d13-10b5-4fc0-88d5-36fa79673cae}} , see (REF ). Furthermore, we assume that this rectangle is centered in zero ({{formula:7812f31e-8ca2-4009-9eba-171624cbf5da}} and {{formula:9ef1b"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between quark mass and the range of gluon condensate, and how do the research findings support this connection?",
      "expected_output": "The research paper states that the range of gluon condensate, between 301 MeV and 327 MeV, is related to the constituent light quark mass, which varies between 230 MeV and 270 MeV. This connection is supported by the findings, where the calculated gluon condensate range corresponds to the given quark mass range, indicating a direct relationship between the two. For example, the paper mentions that the gluon condensate range is obtained by calculating the matrix element of the colour magnetic operator between heavy states, and this calculation depends on the constituent light quark mass.",
      "context": [
        "formula:f80b5544-48c2-4ae3-93f6-f286c8811d28}} states via the matrix element of the colour magnetic operator between heavy states . Calculating this matrix element in our model gives a gluon condensate range (301 MeV){{formula:2271bad5-8992-466a-b329-1efdf0553459}} (327 MeV{{formula:b3aab540-b8bb-4cba-81f3-30d29676ba6d}} for a constituent light quark mass between 230 and 270 MeV. Then we find that also numerically, the simple expression (REF ) is close to eq. (REF ) because the second term ({{formula:43c05e08-cb15-47f6-8c4d-a04311139ee6}} ) and third term ({{formula:eabed1a7-3828-4fe5-a721-b183dfcf821c}} ) in the denominator of eq. (REF ) tend to cancel.When calculating the diagram for {{formula:e3da",
        " the numerical value for the LEET form factor {{formula:521f8278-8508-4524-b67f-e1eee3cc406b}} from lattice and light-cone sum-rule results , from which we obtain the value {{formula:8e51e92f-ffa0-43b8-90f7-2cffe184a39c}}The only model-dependent parameters entering eq. (REF ) are the gluon condensate and the constituent light quark mass {{formula:330fc185-cf74-4a56-9d33-71c149129b26}} . As mentioned above, the range {{formula:a075b900-6de7-4f71-8548-ce9102c371c1}} MeV for the constituent quark mass gives the range (301 MeV){{formula:085b3313-2f71-4eec-b2eb-79fb068b3afe}} (327 MeV{{formula:fb7dbf5d-bf55-485a-a72c-dc0653f65a51}} for the",
        "dbf5d-bf55-485a-a72c-dc0653f65a51}} for the gluon condensate.Inserting all these values in (REF ), and including the uncertainty in {{formula:84c623c3-0bb7-4a7d-b9ff-4c443c1b197a}} , we obtain the decay rate : {{formula:1952834b-b3ac-4412-9b36-d79f66f8dfd4}}which gives a branching ratio {{formula:bc68680e-47c1-42ff-842b-d5e617df782e}}The ranges in (REF ) and (REF ) come from the variation of the constituent light quark mass {{formula:3f25a324-f22f-4382-b970-a82dbddf5892}} in the range 230 – 270 MeV, and the uncertainty in {{formula:9ed2e45d-2e83-4faa-9dc3-a9401b473faf}} , i.e. the uncertainty within the model. In addition, there"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of this research paper on defining a tuple of terms via induction, and how do the methods used contribute to the key results?",
      "expected_output": "The research paper presents a method for defining a tuple of terms via induction, specifically for intuitionistic formulas. The main finding is the Monotonicity lemma for these formulas, which states that under certain assumptions, a specific property holds for each intuitionistic formula. The method used involves induction on the complexity of the formula, and the key results are derived from assumptions (A6) and (C). For example, the paper defines the tuple of terms by induction on the formula, using recursive definitions for predicate symbols. This approach contributes to the key results by providing a systematic way to analyze the properties of intuitionistic formulas.",
      "context": [
        "dbe}} .Definition 2.11 For each intuitionistic formula {{formula:475b7011-41c7-4317-a570-62f37b832172}} , define the tuple of terms {{formula:b9291788-2e4a-4842-bcad-2bd47ddf80c3}}by induction on {{formula:f96f0171-83b9-4bc9-95d7-6710ee0e4655}} , as follows: {{formula:2ee68776-7b02-4b4c-8d48-b1b3762d2330}}where, for predicate symbols {{formula:4ba14823-cb4e-40e4-8ae5-cd7453180a8d}} we let {{formula:d4f4a595-d23b-429d-ba78-200f03ddc4b7}} be the term assumed to exist in (A7), and {{formula:750b0097-3a71-4211-b754-a7042b8645f3}} is defined recursively as",
        "formula:8d0aa753-e100-44e8-ba6c-c3f6ca4fb01f}} , define the tuple of terms {{formula:f327d9d4-df61-45df-951c-c47d14334244}}by induction on {{formula:999b5bb7-eab4-4d4f-bdfa-1af20ea2fdb5}} , as follows: {{formula:c9f19640-6b08-41ef-94ea-909ec239a1c8}}where, for predicate symbols {{formula:aa12486c-6859-4fe3-b536-3b4376fa6fed}} , we let {{formula:f264d123-b89d-46d3-a1a1-5a972724f270}} be the term assumed to exist in (A6).Lemma 2.9 (Monotonicity lemma for {{formula:e04fc68c-3812-4da9-8560-a23410ba9d60}} ) Under assumption (A6), for each",
        "-4da9-8560-a23410ba9d60}} ) Under assumption (A6), for each intuitionistic formula {{formula:e157513c-b0ad-43f6-9b5b-4d8b309538bc}} , we have:{{formula:81cb697e-de4f-4bcc-ae61-36fec86b089f}} {{formula:a393c5a5-9109-4941-8947-99ea8cc63e66}}By induction on the complexity of the formula {{formula:96bf8a39-e5c3-48a1-b21c-5db4a91a39dd}} . {{formula:de3f84c1-4a6b-44b9-a16a-244c580f2dbe}} follows directly from assumptions (A6) {{formula:39bad1b2-143f-44ae-8885-edc66d2b3805}} and (C{{formula:10e7f7bb-beb3-4d19-8dcf-e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the batch normalization layer evaluate the importance of each channel in the neural network, and what methods are used to determine this importance?",
      "expected_output": "The batch normalization layer evaluates the importance of each channel in the neural network by using learnable parameters, specifically the scaling factor and bias parameter, to affine the normalized features. According to the paper, the BN layer parameters are updated through standard forward-backward training, while the other parameters of the supernet are fixed after initialization. This allows the BN layer to learn the importance of each channel, which can then be used to determine the optimal subnet architecture.",
      "context": [
        " need external parameters and can ensure fairness among all candidates during training.\nPreliminary: Since our approach is based on the One-shot NAS method , and the Batch Normalization Layer . A brief introduction of them is provided in this subsection.\nOne Shot NAS : In One-Shot (e.g. SPOS) methods, a supernet {{formula:a204d06b-5617-4970-912f-d5601f7e936e}} with weights {{formula:1c09fad2-754b-4df6-96e8-567d99049af7}} is constructed by all candidate operations forms the search space {{formula:f8a12874-9278-4e47-8c96-56abe0799f16}} . The whole pipeline of these methods can be divided into three stages, supernet Training, Subnet Searching, and Subnet Retraining.Search Space. The supernet architecture is constructed by a series of candidate operations as shown in Fig. REF . A layer contains multiple ({{formula:811e5d75-bde3-49f3-a42f-e5b5",
        "98ca4}} and bias parameter {{formula:1a0bac01-93e5-4723-beea-ef8d56d9b3c5}} are learnable parameters in BN layers to affine the normalized features {{formula:9b0d00d1-55f0-4fa4-9153-854838deadf5}} .\nAlgorithm Overview: The pipeline of our proposed NAS method is shown as Fig. REF . We follow the three stages in one-shot methods, supernet training, subnet searching, and subnet retraining. In supernet training stage, the supernet containing all candidate operations are randomly initialized. Only BN layer parameters are updated through standard forward-backward training, while the other parameters of the supernet are fixed after initialization (Section REF ). In subnet searching stage, subnets are sampled and evaluated based on our BN indicator (Section REF ). In the subnet retraining stage, the best subnet chosen in the subnet searching stage is retrained.In the following, we start from the second stage (Subnet Searching). The order of the following description is consistent with the order of our exploration in this",
        " with weights inherited from the supernet should be highly predictive on the validation set, the supernet training often requires hundreds of epochs.Subnet Searching After training the supernet, the next step is to find the optimal architecture with the best performance. In SPOS, the accuracy on validation set is used for evaluating the subnet performance. The optimal subnet is selected according to the subnet accuracy on the validation set. To get a reliable searching result, thousands of subnets are needed to be evaluated.Subnet Retraining In the retraining stage, the {{formula:0b25b2b3-2a37-4f44-8463-1fe42eb3df8f}} subnets found at the subnet searching stage with the highest accuracy are retrained. They are then evaluated on the validation set and the subnet with the highest accuracy is chosen as the final optimal subnet.\nBatch Normalization Layer : Batch Normalization (BN) layer has been used in network pruning , , , which is a good evaluation of channel importance. Given the input {{formula:db0e3af5-b9de-4c40-a442-258fa8498958}} of B"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main implications of Corollary 4.21 on the formulas presented in the research paper?",
      "expected_output": "Corollary 4.21 does not imply certain formulas over specific structures, as shown by Csima & Mileti's proof. For example, it does not imply formula 7fbd4668-e26d-4cef-b6c0-4620efa4fdcd over structure f1628925-7d97-4673-9a38-3fdee40908ff. The research paper explores the implications of Corollary 4.21 on various formulas, including whether formula fdaf86e3-f137-4fa0-b5ab-3aaff4c74be2 implies formula c8825016-538b-4120-8827-9df9241204e5 over structure 256a31e7-e0cf-4f5c-bedd-2aa6e5c81991e4a4dc0.",
      "context": [
        "-d091-4be8-85cb-d563d9d347cc}} .Corollary 4.21 {{formula:c20d97c1-5786-46ca-bb26-0362aacffb9e}} does not imply {{formula:7fbd4668-e26d-4cef-b6c0-4620efa4fdcd}} over {{formula:f1628925-7d97-4673-9a38-3fdee40908ff}} .Csima & Mileti proved that {{formula:0f31014e-2508-41b2-8649-f8ee755f1aad}} . Apply Corollary REF .Question 4.22 Does {{formula:fdfa86e3-f137-4fa0-b5ab-3aaff4c74be2}} imply {{formula:c8825016-538b-4120-8827-9df9241204e5}} over {{formula:256a31e7-e0cf-4f5c-bedd-2aa6e5c",
        "81991e4a4dc0}} from {{formula:f3987a66-9047-4a5b-9638-d54d3d582742}} as well (see ).Question 4.1 Does {{formula:824b33d6-16d4-46aa-beb2-4175ffeeb4c1}} imply {{formula:6dbbfdf8-ff97-4cc6-a78e-6ca1b0bc9134}} (or even {{formula:17198c68-103d-457c-b99a-3fff5915da1a}} ) over {{formula:0c23b7a5-0d15-41eb-a7ab-ebac32df44f9}} ?The following question is still open:Question 4.2 Is there any {{formula:0b00ff14-47f9-4323-b99f-04ba62f4ac05}} such that {{formula:13a15872-c1ef-4f45-a864-8d4292698ff8}} ?Cholak et al. conject",
        "4369-a1f8-700f69b2f8d4}} -model of {{formula:265920f5-11a5-4f85-a68f-b6a552c5013f}} not model of {{formula:9908889e-d09f-4cb4-9745-932218cd0ea7}} . We prove that that {{formula:2f150854-6533-43e1-b86e-26a649a24cd1}} implies {{formula:8c0ecfa1-0a0d-430d-be2f-d91782c4fde8}} over {{formula:afe44b1b-67a9-4e87-a02e-7c96a52cffe0}} using both a direct proof and the equivalence between {{formula:0e3b0a00-b257-4e48-99b0-d5f80468a4f2}} and {{formula:0cdb5c92-ade6-4257-bee3-f5cc1fcc35bb}}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between automorphisms and universal coverings, and how do the authors' methods contribute to these results?",
      "expected_output": "The research paper explores the relationship between automorphisms and universal coverings, specifically stating that there does not exist any automorphism of a given space such that a certain condition is met. The authors use duality to prove their assertion, focusing on the compact type, and introduce the universal covering of a space to derive their results. For example, they let the universal covering of a space be denoted by a specific formula and use this to show that an automorphism must send certain elements to others in a particular way, contributing to the understanding of the relationship between automorphisms and universal coverings.",
      "context": [
        "102c08-dab8-450c-9730-016594b03230}} . Equivalently {{formula:6607aba4-6932-46d9-bb02-96acac855e0a}} , in other words, there does not exist any automorphism {{formula:42005f7b-1d8c-4144-8cfe-efb559bd5d90}} of {{formula:95b47dfa-0d83-4977-a1c4-7012dcd84993}} such that {{formula:5d8d0231-2ab8-45b7-9947-3f204d7fb114}} .Proof. By duality, it is enough to prove the assertion for the compact type. In this case let {{formula:59b047a0-c6f2-45df-af5f-b00b2247d8fa}} be the universal covering of {{formula:a57aaed7-197f-43c8-90c8-9f9503794ff5}} , we have {{formula:ca5f5",
        "d102c08-dab8-450c-9730-016594b03230}} . Equivalently {{formula:6607aba4-6932-46d9-bb02-96acac855e0a}} , in other words, there does not exist any automorphism {{formula:42005f7b-1d8c-4144-8cfe-efb559bd5d90}} of {{formula:95b47dfa-0d83-4977-a1c4-7012dcd84993}} such that {{formula:5d8d0231-2ab8-45b7-9947-3f204d7fb114}} .Proof. By duality, it is enough to prove the assertion for the compact type. In this case let {{formula:59b047a0-c6f2-45df-af5f-b00b2247d8fa}} be the universal covering of {{formula:a57aaed7-197f-43c8-90c8-9f9503794ff5}} , we have {{formula:ca5f",
        "c4-7012dcd84993}} such that {{formula:5d8d0231-2ab8-45b7-9947-3f204d7fb114}} .Proof. By duality, it is enough to prove the assertion for the compact type. In this case let {{formula:59b047a0-c6f2-45df-af5f-b00b2247d8fa}} be the universal covering of {{formula:a57aaed7-197f-43c8-90c8-9f9503794ff5}} , we have {{formula:ca5f5135-2f0d-4ffa-88a9-b3ebd458eefd}} and {{formula:7432125e-31c8-4882-bb4f-bac6bede1117}} . Then an automorphism {{formula:84ce3403-ee8b-4cc9-b5a9-5c5e40ce4143}} must send {{formula:d72ffe82-17b1-49a0-ad75-99e2"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.6
      }
    },
    {
      "input": "What are the main findings of the Hutchinson-Skilling estimator and how does it simplify computing Delta v_phi?",
      "expected_output": "The Hutchinson-Skilling estimator is used to approximate the computation of Delta v_phi, which is burdensome. The main finding is that this estimator simplifies the computation by providing an efficient way to estimate the trace of a matrix. According to the context, the implementation of the trace estimator follows the likelihood estimation, allowing for a more manageable calculation of Delta v_phi.",
      "context": [
        " data (target) distribution \\nu . Moreover, computing \\Delta v_\\phi in line 6 is burdensome, hence, we approximate it through Hutchinson-Skilling trace estimator , . Our implementation of the trace estimator follows the likelihood estimation implemented in .\nData Description: We obtain each data as follows:8-Gaussian: For m_i = 12 \\left(\\cos {\\frac{i}{4}\\pi },\\sin {\\frac{i}{4}\\pi }\\right) for i=0, 1, \\dots , 7 and \\sigma =0.04, the target distribution is defined as the mixture of \\mathcal {N}(m_i,\\sigma ^2) with an equal probability. Moon to Spiral: We follow . blackGaussian Experiments: We follow , note that the data dimension is 50.\nNetwork Architectures: black Let y be concatenated variable of source data x and auxiliary variable z\\sim \\mathcal {N}(0,I). The dimension of the auxiliary variable is set to be same as the dimension of data. We parametrize the generator by T_\\theta (x, z) = x + t_\\theta (",
        " of 20, \\alpha =1, and uniform distribution for \\mathcal {T}. Moreover, we set \\lambda _G = 0.1, \\ \\lambda _D = 1, p=1 and \\sigma = 0.8.\nDiscrete OT Solver: We used the POT library to obtain an accurate transport plan \\pi _{pot}. We used 1024 training samples for each dataset in estimating \\pi _{pot} to sufficiently reduce the gap between the true continuous measure and the empirical measure.\nCIFAR-10 Generation: We follow the generator T_\\theta architecture and the implementation of . Moreover, we follow the discriminator architecture in . We trained for 300K iterations with the batch size of 256. We use Adam optimizer with (\\beta _1, \\beta _2 )=(0, 0.9), learning rate of 10^{-4}. We use a cosine scheduler to gradually decrease the learning rate from 10^{-4} to 5\\times 10^{-5}. We used a gradient clip of 1.0 for both the generator and discriminator. We use the number of timesteps of 20, \\alpha =1, p=2, \\sigma =0."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the Anderson model regarding disorder impact on localization in tree graphs, and how do the methods used affect the key results?",
      "expected_output": "The Anderson model's main findings regarding disorder impact on localization in tree graphs show that for bounded random potentials, there is no transition to a spectral regime of Anderson localization unless the disorder is strong enough. The methods used, such as ergodicity arguments, lead to the key result that the spectrum of random operators on tree graphs is the set sum of the spectrum of the operator without disorder and the spectrum of the random potential. This implies that even under circumstances that locally seem to imply localization, extended states are found. For example, on the Bethe lattice, a regular tree graph, the absence of a mobility edge is observed, challenging the generally shared picture of the mobility edge in such systems.",
      "context": [
        " `Anderson random potential' with {{formula:464d4bb3-06d7-40e6-86b9-0d80963656e1}} constant over the interval {{formula:ca29f3b9-1ab4-49ab-aca4-9e26d233ae6d}} and zero elsewhere.On homogenous graphs, such as the regular lattice {{formula:472884b1-b216-41cc-ba6c-4a8961e3ee49}} or the homogenous trees discussed here, the spectrum itself is easy to determine for random operators such as {{formula:6bbe7484-75ca-48e2-a973-dd9b64ad9441}} : by ergodicity arguments , , it is the set sum of the spectrum of {{formula:3d3bbfb5-3242-4d08-b661-f430cd855e48}} and of {{formula:f3658506-74d5-41a7-a1f1-2fb011cdb0de}} . More explicitly, for tree graphs with branching number {{formula:6",
        "# arXiv Paper: 1109.2210\n\nAbsence of mobility edge for the Anderson random potential on tree graphs at weak disorder\nabstract: Our recently established criterion for the formation of extended states on tree graphs in the presence of disorder is shown to have the surprising implication that for bounded random potentials, as in the Anderson model, there is no transition to a spectral regime of Anderson localization, in the form usually envisioned, unless the disorder is strong enough.\nIntroduction: Since the early studies of Anderson localization, random operators on tree graphs have provided a testing ground for insights on the effect of disorder on quantum spectra and dynamics , , . The basic phenomenon to which this refers is the transition in the spectra of Schrödinger-type operators with random potential from regimes of energies with only localized states, to energies with extended states which facilitate conduction, e.g. in the sense depicted in Figure REF .Our purpose here is to report on rigorous results which challenge some of the generally shared picture of the mobility edge in such systems on the Bethe lattice, i.e. within a regular tree graph (whose degree here is {{formula:c36b28c4-4842-4c23-9eb1",
        "7-9939-793b6b9ff172}}As a consequence, even under circumstances which locally seem to imply localization, e.g. as in the Lifshitz tail regime of low density of states, we find extended states. Some basic rigorous results related to this mechanism were presented the recent work . Our focus here is on the somewhat surprising implication for bounded random potentials, as featured in a class of basic models of the Anderson transition.\nThe basic model: A specific example of a one particle Hamiltonian to which our discussion relates is {{formula:58dc49cb-a9fe-4324-8a96-0ac6fd8fe1ea}}where the first sum is over pairs of neighboring sites of a regular tree graph {{formula:46e6b72d-5c2f-4b30-968d-a965ce0a6dd9}} , i.e., the Bethe lattice, and {{formula:4579fa1e-9c69-4b45-a869-ddacfb830572}} are fermionic creation and annihilation operators. The disorder is expressed in the random `onsite energies' {{formula:c9"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What evidence do annihilations provide for the existence of specific particles, and how do researchers interpret these findings?",
      "expected_output": "Annihilations provide evidence for the existence of specific particles, such as the formula:7fdbae0f-e801-4f4e-99a2-60cbe761be65, by producing them in reactions like a1 annihilations and formula:6773556b-6325-4812-aece-d7f522229b50 annihilation. Researchers interpret these findings by analyzing the decay modes and final states of the reactions, such as the formula:9a41fa04-662c-4dd8-b007-8022feb694e9 decay mode of the formula:040325ca-9739-4218-9ace-5ad9972f94e5. For example, the Crystal Barrel Collaboration and E760 at Fermilab observed evidence for the formula:7fdbae0f-e801-4f4e-99a2-60cbe761be65 in a1 annihilations, and the NA12 experiment claimed a formula:b5984005-8eec-48cf-b52c-795575674c2b in formula:c3b6ac8b-e35f-44d4-98ae-74ccb09ac91b interactions.",
      "context": [
        "a1}} annihilations by the Crystal Barrel Collaboration and E760 at Fermilab . In all these reactions evidence for the {{formula:7fdbae0f-e801-4f4e-99a2-60cbe761be65}} has emerged. In addition, in {{formula:6773556b-6325-4812-aece-d7f522229b50}} annihilation evidence is claimed for a {{formula:39d82113-c878-4568-8ec3-f24a1d630923}} , and an {{formula:9a41fa04-662c-4dd8-b007-8022feb694e9}} decay mode of the {{formula:040325ca-9739-4218-9ace-5ad9972f94e5}} . In {{formula:c3b6ac8b-e35f-44d4-98ae-74ccb09ac91b}} interactions the NA12 experiment claim a {{formula:b5984005-8eec-48cf-b52c-795575674c2b}} .",
        "b5984005-8eec-48cf-b52c-795575674c2b}} . In radiative {{formula:dc122e41-8208-4eaa-a54f-31548109c164}} decays evidence was reported for the {{formula:9a609cdc-2650-4a7a-b0c9-d58e9ae1d091}} . In central production the NA12/2 experiment claimed evidence for a {{formula:034a017b-1e17-452c-9bfe-c8fb47414cb1}} .In this paper a study is presented of the {{formula:1f9b578e-dce3-4604-a0ba-d31a7a651039}} final state formed in the reaction {{formula:2b35fd7f-1ff8-4f57-84c5-9cf4d9867d1c}}at 450 GeV/c. It represents more than a factor of 6 increase in statistics over the only other data on the centrally produced {{formula:f1ad01c8-f5",
        "47}} final state of experiment WA102 , an upper limit for its decay to {{formula:aa38f910-1732-44ec-a9ca-c92b92488436}} has been calculated and gives {{formula:83f1d601-a387-4e14-bb27-acb57b14616f}}Hence this would imply that if the observation in the {{formula:f94cd2e3-3294-42eb-b528-f244fc1982b8}} final state of {{formula:dc522456-8258-433d-92c6-a880e1509197}} annihilations is correct, then a very large signal for the {{formula:be74e558-dc63-4094-875e-4491a70f3740}} should be seen in the {{formula:292ea6d6-89e8-4635-8bcd-ea008cc242c8}} final state of the same experiment.The {{formula:4f99b44d-7ec1-40a8-9460-0bc7c2b06b5c}} has"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main connections between polylogarithm and hypergeometric functions in the context of this research paper?",
      "expected_output": "The research paper highlights several connections between polylogarithm and hypergeometric functions. Firstly, the polylogarithm function can be expressed in terms of a generalized hypergeometric function, which can further be represented using modified Bessel functions. Additionally, the paper shows that specific choices of parameters in the hypergeometric function can yield representations of the four largest Heegner numbers. The connections are established through the application of the second Gauss theorem and Bailey's theorem, demonstrating the relationships between these functions in the context of the paper.",
      "context": [
        "formula:1084b65c-d491-413b-8312-e30540d97eb9}} is the polylogarithm function; {{formula:61de0901-e57c-4796-a827-da562b343584}}where {{formula:24b4efff-913c-497b-a4bc-4732f90d4a49}} is a generalised hypergeometric function that can be expressed in terms of modified {{formula:5476900d-5346-41c6-a046-5c45c659ffca}} -Bessel functions of order {{formula:49cdeee2-5999-42e0-90d3-186904abbc4a}} ; and finally {{formula:54419447-4e42-4e71-8efa-4d407a4cc9fc}}where {{formula:fcb36758-ee9e-4a8b-a2d1-ae67e783fc6f}} is the well-known Gauss hypergeometric function . The result (REF ) can be easily",
        "bf9-4d46-8183-02f159120e10}}In addition, if we choose {{formula:93c6c295-9b70-4b6c-ae5d-6a1244c86da1}} , {{formula:33d50fa1-b39e-4a21-baad-8f699df5190d}} , {{formula:5e48721e-fe22-4ff7-996d-c2bc03923657}} and {{formula:e7e3eaf4-6e27-403a-8e35-2c6dd78a3b39}} in (REF ), we obtain hypergeometric function representations of the four largest Heegner numbers, viz. {{formula:33608af3-8862-4769-bb2c-f802c4223a06}} {{formula:d3ef07bd-d53d-4abb-b213-b7e921410004}} {{formula:d59d6e77-585e-4142-bfde-7fc6ac71cfc6}} {{form",
        " by the second Gauss theorem and the second hypergeometric function by Bailey's theorem viz. {{formula:e3d1aca8-9eba-4fd9-b322-bee42aa1214f}} {{formula:53cbedd7-be3a-41f2-b749-117ff7aefba0}}We now derive the analogue of Theorem 1 by making use of the extension of the second Gauss and Bailey's theorems applied to {{formula:c667de6b-6efa-427e-a38c-3b469ba09383}} series. These are given by : {{formula:9001d564-0cfa-4984-a822-d113961f7572}} {{formula:a1c12008-c309-4d42-a92f-9dc281a445d6}}and {{formula:a7bbb1bf-2f61-4d33-b9da-4ef8999fa4de}} {{formula:b6d7f4ca-204a-4e4c-a5a9-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the database use to fill gaps in stellar models and what are the main findings of this research?",
      "expected_output": "The database uses two methods to fill gaps in stellar models: scaled solar models and isochrones with and without convective core overshooting during the H-burning phase. The main findings of this research are the presentation of a large and updated stellar evolution database for low-, intermediate-, and high-mass stars in a wide metallicity range, and the provision of a homogeneous set of stellar models suitable for population synthesis analysis and investigating evolutionary properties of field and cluster stars.",
      "context": [
        "# arXiv Paper: astro-ph/0405193\n\nA large stellar evolution database for population synthesis studies. I. Scaled solar models and isochrones\nabstract: We present a large and updated stellar evolution database for low-, intermediate- and high-mass stars in a wide metallicity range, suitable for studying Galactic and extragalactic simple and composite stellar populations using population synthesis techniques. The stellar mass range is between \\sim0.5Mo and 10Mo with a fine mass spacing. The metallicity [Fe/H] comprises 10 values ranging from -2.27 to 0.40, with a scaled solar metal distribution. The initial He mass fraction ranges from Y=0.245, for the more metal-poor composition, up to 0.303 for the more metal-rich one, with Delta Y/Delta Z\\sim 1.4. For each adopted chemical composition, the evolutionary models have been computed without and with overshooting from the Schwarzschild boundary of the convective cores during the central H-burning phase. The whole set of evolutionary models can be used to compute isochrones in a wide age range, from \\sim30 Myr to \\sim15Gyr. Both evolutionary tracks and is",
        " obtained estimates of Galactic globular cluster ages (e.g., Salaris, Degl'Innocenti & Weiss 1997; Salaris & Weiss 1998, 1999, 2002), initial He abundance of globular cluster stars (Cassisi, Salaris & Irwin 2003; Salaris et al. 2004), pulsational properties of variable stars (De Santis & Cassisi 1999; Bono et al. 1997). This notwithstanding, we have never published an extended and homogeneous set of stellar models covering a large range of stellar masses, chemical compositions and evolutionary phases, suitable for population synthesis analysis as well as for any other kind of investigation making extensive use of stellar evolution models.The main purpose of this work is to fill this gap, by providing an up-to-date and complete set of stellar models for both low-and intermediate mass, and high mass stars up to 10 {{formula:02de45dc-24d0-461f-b59b-058296e5e308}} , spanning a large metallicity regime from metal-poor star systems to super-metal-rich populations. Our updated theoretical models are then coupled to a new set of color-transformation and bolometric corrections. In this paper, all theoretical results",
        " provides the closest possible match to both components, that is however much worse than the case of canonical isochrones), even though, with our prescription, the overshooting region is of about 0.08{{formula:c473cbe1-2b05-4cd8-b324-9cf153dde776}} in this mass range. This result agrees with the case of M 67, where we have shown (in agreement with Sandquist (2004) results obtained by employing different isochrones) that masses of {{formula:fc95d2c8-a4d7-49d3-a791-f4420b5e0700}} have an overshooting region already reduced to virtually zero for metallicities around solar.\nSummary: We have presented an homogeneous and updated database of stellar evolution models and isochrones for old-, intermediate-, and young stellar populations for a wide range of chemical compositions. Two large grids, with and without the inclusion of convective core overshooting during the H-burning phase are provided. This set of theoretical models is suitable for population synthesis analysis as well as for investigating evolutionary properties of field and cluster stars in a wide range of masses and chemical compositions. The main"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between RTA's coefficient value and those reported in previous studies, and how do these differences impact the research findings?",
      "expected_output": "The main difference between RTA's coefficient value and those reported in previous studies is a factor of five. RTA implies a value of 64d71b35-ba6f-4bd7-af65-3984b5ec2b90 for the coefficient 8c72541e-5362-4484-856e-160f28701e56, whereas prior studies obtained 830f3de0-cd4b-465d-992b-1e1f23f34bcf. This discrepancy affects the research findings, as using the correct value for 2102fc90-7ff1-49e0-84a4-bbd901bb16db results in the DNMR RTA equation, which agrees with Eq. (9) of Ref.",
      "context": [
        "}} , Eq. (REF ) agrees with previously obtained RTA second-order viscous hydrodynamics results , , , , . However, at order {{formula:bf6889b5-8f68-47d1-aa86-412cae4275cd}} , the value of {{formula:8c72541e-5362-4484-856e-160f28701e56}} implied is {{formula:64d71b35-ba6f-4bd7-af65-3984b5ec2b90}} which is different by a factor of five compared with prior reported values , which obtained instead {{formula:830f3de0-cd4b-465d-992b-1e1f23f34bcf}} .The coefficient {{formula:cec896ac-499e-4e41-8425-687f1ed8970f}} emerges in the literature because the {{formula:724510a1-1de3-4550-8a7a-5c8a97a09692}} term appearing on the RHS is traditionally written in the form {{formula:1ba",
        " value {{formula:bda66275-bc28-42c4-9523-f7adef04c49e}} one obtains {{formula:8808829d-b272-422b-a5de-345939468d02}}which agrees precisely with Eq. (9) of Ref. ; however, for RTA this value for {{formula:e91c6c93-ee1a-4cd9-bdac-8eb148b6193b}} is incorrect. Using the correct value for {{formula:2102fc90-7ff1-49e0-84a4-bbd901bb16db}} one obtains the DNMR RTA equation (again neglecting quadratic terms in {{formula:8af8f244-5939-4e4f-9af8-403ef09a4c52}} ) {{formula:d4a0587f-f700-4d06-a19c-cc744565c343}}Also, we note that, as demonstrated in Eq. (REF ), aHydro naturally reproduces this equation when truncated at leading order in {{formula",
        "-9e7d-cbd71a74a803}} being the inverse function of {{formula:9aebf532-f598-4f8c-a264-20421ccb73fc}} (shown in the right panel of figure REF ). Written in this form, we can see explicitly that the aHydro second-moment equation sums an infinite number of terms in the expansion in the inverse Reynolds number (REF ). In the next section we will expand this equation in powers of the inverse Reynolds number through second order in order to compare it to standard viscous hydrodynamics.\nSmall {{formula:0230087d-1bb3-478b-aea4-7766f5f1ac32}} expansion: In order to make the final connection to standard viscous hydrodynamics, one can expand Eq. (REF ) in {{formula:5766d933-0276-4d79-b961-c65a3bf63dcc}} around {{formula:d6e0cdb5-8975-4c47-a637-a7aed9bf92a0}} .The Taylor series around {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the formulas in this research paper imply disjoint sets for almost all variables, and what are the key findings related to this concept?",
      "expected_output": "The research paper implies disjoint sets for almost all variables through a series of formulas. For instance, it states that if one formula is true, then another formula is also true, leading to a specific conclusion. The paper also mentions that for almost all variables, a certain condition is met, which further supports the idea of disjoint sets. A key finding is that for each variable, a specific choice can be made such that a certain property holds, and this property is true for almost all variables. The paper concludes that two sets are disjoint, meaning they have no elements in common, for all variables. This concept is crucial in understanding the relationships between different variables and sets in the research paper.",
      "context": [
        ":cb67e343-cc44-42dc-9d08-2988ae939882}} . It is easy to see that if {{formula:d8ab32bd-37ae-437b-b820-4e859ed634b4}} then {{formula:56411dcd-0d99-4d10-b9b9-5f1adbef4756}} . Hence {{formula:200a25d2-14ab-4154-aaf6-356ac10257a4}} . Since {{formula:a3f3500e-3acc-496c-b063-69d40ad85244}}for almost all {{formula:b37362dc-55f1-4926-994b-5e4a6b6dbb34}} , we deduce that {{formula:626d1d74-e07a-4a2a-910e-f916f5cc7b0e}} , i.e., {{formula:0fb11c30-2376-453f-b736-55b94ad15a5d}} .",
        "-fbf8-473e-94f6-0b0429def033}} by {{formula:96ad176c-ecac-45ab-b7e3-d967ca10016a}}For each {{formula:126f5947-1d65-46c2-b110-6f9e7d9404aa}} , we choose {{formula:52993e4c-9215-4b71-b258-a95735ecc02b}} such that {{formula:b355933f-ca14-4a93-a9da-925a786102cf}} . Then {{formula:8027bd6e-0e7c-4e57-a329-b227a48a39a1}} for {{formula:961f13f8-dbed-4abe-a1e2-776c36ce1a07}} -almost all {{formula:8b1394e1-5a3b-4b48-9970-5077afed1dc5}} . As {{formula:613c1206-27a6-4d",
        "-049e-42ad-ba27-e57000fe207c}} . As {{formula:44b1b4ac-59fa-4450-93af-708ed9f89d14}} , we see that {{formula:1930f807-834f-42af-bd34-5819be490130}}As {{formula:6f78e55f-4ef8-424f-8e82-bbdba498045d}} and {{formula:d200b86a-037a-4882-a0db-08086d4a9aff}} , it follows that {{formula:9530b141-cd7f-437c-a490-6ccb67b2b091}} for all {{formula:958a73a9-88ec-40ab-8041-01f416a2091a}} . Hence {{formula:4fd79ccd-249d-47a7-a31b-15f97ea398e9}} is disjoint from {{formula:db5a7271-e662-4d56-bd"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of JT gravity in describing 2D space, and what methods were used to achieve these results?",
      "expected_output": "The main findings of JT gravity in describing 2D space include the description of 2D de Sitter (dS) or anti-de Sitter (AdS) space, depending on the sign of a specific formula. To achieve these results, methods such as setting a parameter to a specific value and introducing a new field to put the action into a certain form were used. For example, setting the parameter to zero and introducing a new field allowed the action to be put into the form of Eq. (REF), which helped in describing the 2D spacetime metric. Additionally, the consideration of torsionless gravity and the use of algebraic equations of motion to eliminate auxiliary fields were also crucial in obtaining these results.",
      "context": [
        "88ce9dc-148b-442d-a115-711574f4f085}} , {{formula:f8822f24-9e7b-4b21-a3f0-182baf9a6c68}} . In this case, {{formula:1ebe5c0b-1242-49b3-8a86-f7dc0288204b}} is an auxiliary field and can be eliminated by its algebraic equation of motion, giving {{formula:cfd7aef4-1c21-4796-b3ec-2a498cfbf747}} . Depending on the sign of {{formula:7f3555d5-67bf-4911-b25a-a1bdccfb551f}} , JT gravity thus describes a 2D de Sitter (dS) or anti-de Sitter (AdS) space. For convenience, one usually takes {{formula:7dffdae2-1e28-4a42-97dc-54cd0926ee62}} so that {{formula:b53315f6-acf9-4aae",
        "-e43883122bab}} in the first term in the square brackets can be a general function {{formula:25c224cb-f1c9-429e-955a-f040a14ade2c}} . But for physical reasons, {{formula:f72e4b98-20d4-4bdc-a863-7527b9588e4b}} is non-zero everywhere and we can introduce a new field to put the action into the form of Eq. (REF ). Moreover, here we only consider torsionless gravity. For reviews of dilaton gravity, see Refs. , , , , and recent studies, see Refs. , . {{formula:c9fc1ecc-7cce-46f0-8631-3ef930eea5fe}}where {{formula:43b8ce53-2dba-4efa-ade5-22d3e8a8227b}} is the 2D spacetime metric. JT gravity corresponds to {{formula:e88ce9dc-148b-442d-a115-711574f4f085}} , {{",
        " the action with respect to {{formula:f011d2d2-9860-4ce9-b75d-028f4bc84912}} , one has {{formula:9097730c-8f0c-463c-8bc0-73fdcfbcc3e1}}In this paper, we consider a special model with {{formula:9935123c-0c80-4ea6-a5e7-15c76aeae0c6}}where {{formula:fc62edef-aebc-4f80-b22f-190fb2e97550}} is a parameter of dimension mass. As done for JT gravity, we will also set {{formula:cb4c4316-2ff5-449b-9ce1-0a78a6840d97}} and one should be able to restore it in physical quantities. As we shall see shortly, this theory describes asymptotically flat spacetimes. We will show that it permits a very interesting solution in which there is an event horizon but no singularity. The structure is similar to the domain wall of a scalar field theory. Therefore we"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the scheme enhance the accuracy of ISRF model predictions, and what methods are used to achieve this improvement?",
      "expected_output": "The scheme enhances the accuracy of ISRF model predictions by using an improved numerical method to operate the Maxwell model of the ISRF. This method involves generating a finite set of point-like stars with specific probability laws to ensure an approximately axisymmetric distribution. The model is adjusted to match measured local values of the SED, and then used to make predictions about the spatial variation of the SED in the Galaxy. Asymptotic expansions are also used to interpret the findings and improve the accuracy of the model. For example, the model predicts extremely high values of the SED on the axis of the Galaxy, which is confirmed by the improved numerical scheme.",
      "context": [
        " the SED.\nShort presentation of the model: This model has been presented in detail in Ref. . An axisymmetric galaxy is modelled as a finite set of point-like “stars\", the azimuthal distribution of which is uniform. Those points {\\bf x}_i \\ (i=1,...,i_\\mathrm {max}) are obtained by pseudo-random generation of their cylindrical coordinates \\rho ,\\phi ,z with specific probability laws, ensuring that the distribution of \\rho and z is approximately that valid for the star distribution in the galaxy considered, and that the set \\lbrace {\\bf x}_i \\rbrace is approximately invariant under azimuthal rotations of any angle \\phi . In the present work, as in Refs. , , 16\\times 16 \\times 36 triplets (\\rho ,z,\\phi ) were thus generated, so that i_\\mathrm {max}=9216, and the distribution of \\rho and z is approximately that valid for the star distribution in the Milky Way.The ISRF is also assumed axisymmetric, and thus depends only on \\rho and z. Since we want to",
        " by our model: (i) is extremely high on the axis of the Galaxy — i.e., on the axis of the axial symmetry that is assumed for the model of the Galaxy; (ii) has rather marked oscillations as function of the wavelength; and (iii) seems to decrease more slowly when the altitude z increases (or rather when \\left|z\\right| increases), as compared with the radiation transfer model.The aim of this paper is to present an improved numerical scheme to operate that “Maxwell model of the ISRF\", and to apply this improved scheme to check the findings (i)–(iii) above. Section provides a summary of the model. Section describes the improvement of the numerical scheme. In Sect. , we check whether the model really predicts extremely high values of the SED on the axis of the Galaxy. Section studies the spatial variation of the SED and compares it with results of the literature. In Sect. , asymptotic expansions are used to interpret the findings of the foregoing section. The Conclusion section is followed by Appendix , which discusses the relation between the discrete and continuous descriptions of the SED.\nShort presentation of the model: This model has been presented in detail in Ref. . An axisy",
        "omega _j, as given by Eq. (REF ), coincide with the measured values, as determined from spatial missions. We take the measured local values f_{\\bf x_\\mathrm {loc}}(\\lambda _j) as plotted in Ref. (see Appendix ), and we take \\rho _\\mathrm {loc} = 8 kpc and z_\\mathrm {loc} = 0.02 kpc, see e.g. Ref. . The model thus adjusted then allows us to make predictions: in particular, predictions of the spatial variation of the SED in the Galaxy. Such predictions may then be compared with predictions of the mainstream models of the ISRF, which models are very different from the present model.\nResults: maximum energy density: In the foregoing work , the same adjustment just described was used in the framework of the “grouped fitting\" (i.e. the least-squares problem (REF )). A surprising result was that found for the values of the maximum of the energy density u_j({\\bf x}) in the Galaxy — thus, owing to the axial symmetry (REF ), for the values ofu_{j \\mathrm {max}} = \\mathrm {Max"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main properties of a dyadic tree and how are they utilized in research papers?",
      "expected_output": "The main properties of a dyadic tree include independence and closeness. According to the research paper, independence means that for all elements, there exists some subset such that a certain condition is met. Closeness refers to the property where for all elements, a specific relationship holds. These properties are utilized in research papers to achieve certain utilities with respect to a given function, as seen in the example of the dyadic tree data structure.",
      "context": [
        "-aa62-6f4c766d8c55}} outputs {{formula:dc36478c-f469-4e4a-8df6-88809ac9b0d9}} . Then, {{formula:60032909-27ca-4398-8e61-2d3050b2ea04}} achieves {{formula:e0c73ac9-3e37-41d7-911a-f5771d594929}} -utility with respect to a function {{formula:917c63d5-8307-49c4-b1f0-94cae5cc8efc}} , if for all {{formula:99daa530-1312-4fb4-840f-36bc7a8d6a7a}} , {{formula:f0115150-076f-409b-89c5-6c83de82e15b}} . {{figure:54b76e2b-1fef-41ac-9cc4-88d6c5394ee7}}Dyadic Tree Datastructure We repeatedly use the following dyadic",
        "6bd015e3}} , each of length {{formula:97539b0f-2b33-477b-b2c9-b2669fca273e}} , that satisfy the following properties:{{formula:8a57c8d1-1dd2-44b3-b1f5-1a710f3af48a}} -independence: for all {{formula:40fb912a-012f-40f7-9094-79ec030a3a9f}} , there exists some {{formula:6e76f2d3-d2ee-4fc0-ae3b-64decacd354d}} such that {{formula:8acc182b-0bf1-4114-92ba-37fd0894b53b}} . {{formula:53a19a8e-05e1-4bed-8a17-0f8ef138f67e}} -closeness: for all {{formula:7d361fe5-c228-45ed-8dd8-35f03b9ea0b9}} , we have {{formula:a",
        "ddcad-8d78-40df-8ff8-e8b9697c5486}} , {{formula:2950a855-f426-40f6-a99c-ac17b06255b7}} , we have {{formula:0b2b9a11-ae96-4736-a7c3-e5817aed45c4}} If for some {{formula:01546da7-503f-458f-b778-d4e8f7177c5e}} there exists a set {{formula:88462d78-ea2a-4758-9c70-1ea9d225ba2e}} that satisfies {{formula:82d4d901-8cf0-4ba3-97f4-e414161f6ff9}} -independence and {{formula:7b2807c0-3ac6-4c60-af1d-538a23ca9d7b}} -closeness with respect to {{formula:1f125228-5416-44f9-bc06-d82608acdef1}} , then {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the key similarities and differences between very well-covered graphs and unmixed bipartite graphs, and how do the main findings of this research relate to their structures and properties?",
      "expected_output": "The key similarities between very well-covered graphs and unmixed bipartite graphs are that both have specific properties related to their vertex covers and cycles. Unmixed bipartite graphs are a subclass of very well-covered graphs, as every unmixed bipartite graph without isolated vertices is very well-covered. A main difference is that unmixed bipartite graphs have no odd cycles, which distinguishes them from other very well-covered graphs. The research extends the result that the equality holds for unmixed bipartite graphs to very well-covered graphs without odd cycles of certain lengths, highlighting the relationship between cycle lengths and graph properties.",
      "context": [
        "arslan, and Selvaraja extend this result to any unicyclic graph which is not a cycle.Let {{formula:c21d43d3-4a40-423a-a15d-5fb4b2072fef}} be an integer. Jayanthan, Narayanan and Selvaraja prove that the equality {{formula:96673c80-8785-46f3-8d20-7df355facf30}} is true for several subclasses of bipartite graphs. The most interesting part of states that the equality {{formula:3d4ce11c-79ad-4d2e-9c36-e9f8cef5fa54}} holds for every unmixed bipartite graph. The aim of the current paper is to extend this result as follows. It is obvious that every unmixed bipartite graph (without isolated vertices) is a very well-covered graph. On the other hand, by , for every very well-covered graph {{formula:1cb06dd1-74c5-466e-9df3-cf7c445c2f08}} we have {{formula:67",
        " then {{formula:65cc950d-6a6a-4eae-b35d-fe4660952b5d}} , for every integer {{formula:9f61c9f6-2af0-465e-8b49-b14936ea74ff}} . As bipartite graphs have no odd cycles, our result is an extension of for unmixed bipartite graphs.\nPreliminaries: In this section, we recall the definitions and basic facts which are needed in the next section.Let {{formula:66cc58f9-24d1-4d03-aac2-6857b19872d6}} be a finite simple graph with vertex set {{formula:45d6b08b-f682-4841-9f0d-5b101eda30da}} and edge set {{formula:e9344be3-fbd5-437a-a0f9-f102ea74c874}} . We assume that our graphs have no isolated vertices. By abusing the notation, we sometimes identify the edges of {{formula:0a881d56-0244-4593",
        "5-466e-9df3-cf7c445c2f08}} we have {{formula:67ebe6dc-d831-4ca4-99b9-3d3d11806484}} . Therefore, it is natural to ask whether the equality {{formula:ec7adf2c-776e-4b71-932a-277eced366ea}} is true for every very well-covered graph. Unfortunately, we are not able to give a general answer to this question. However, we prove in Theorem REF that if {{formula:168bce08-ae49-4de1-8a92-9f036b66bc75}} is a very well-covered graph which has no odd cycle of length at most {{formula:3f305720-29be-41ce-b8d5-2841f245bbd6}} ({{formula:060d43f6-e6ab-492d-9458-f94f48e092c1}} ), then {{formula:65cc950d-6a6a-4eae-b35d-fe46"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the parton shower affect the transverse momentum of the 4-lepton system in this research paper?",
      "expected_output": "The parton shower has a larger impact on the transverse momentum of the four-lepton system. At small transverse momenta, it undershoots the LHE-level prediction by roughly 20%, but above 30 GeV, it becomes larger than the LHE-level results, reaching a plateau around 150 GeV. This discrepancy can be explained by the fact that the shower increases the transverse momentum of the color-neutral four-lepton system by adding further radiation.",
      "context": [
        "45e3-baf8-c71d5cbb4bdf}}By contrast, the parton shower has a larger impact on the transverse momentum of the four lepton system, Figure REF . At small transverse momenta it undershoots the LHE-level prediction by roughly {{formula:3dd1e1ce-b615-40c9-9098-5418cd5eeecc}} , but above 30 {{formula:02fe1c30-2568-4bfe-80ea-8af61ff22bed}} it becomes larger than the LHE-level results, reaching a plateau around 150 {{formula:5af2e0c6-19f5-4b7e-b491-d63b8dd410d6}} , where the ratio between the two predictions is between 1.5 and 2. The large discrepancy between the showered results and the fixed order (or the LHE-level) ones in the tail of the distribution, which are still however roughly compatible given the correspondingly large LO scale variations, can be explained by the fact that by adding further radiation the shower increases the transverse momentum of the color-neutral four le",
        " define the two {{formula:8c7ca64c-f9c2-477e-b065-fe3ae7d63f07}} bosons also for final states with equal pairs of leptons. {{figure:a21660cd-c706-4f85-b04c-2d790775fb7b}}The transverse momentum of the {{formula:520cd81d-80ae-4a7f-b68f-c7f9ecae9a9f}} boson, displayed in Figure REF , is almost unaffected by parton shower corrections for values of the transverse momentum smaller than 150 {{formula:b6c8d074-aaa7-418c-a3b0-daceeba0e15f}} . For harder values of {{formula:07548f10-4e64-4f38-a322-51f7ea05280d}} the shower increases the cross section. This effect is related to the crossing of the kinematic threshold already observed in the comparison of LO and NLO predictions for the electron transverse momentum in Figure REF .\nATLAS fiducial",
        "-lepton system, which at NLO is constrained by momentum conservation to recoil against the emitted parton. Other situations where the parton shower provides large corrections appear in the presence of multiple shower emissions, which allow observables to evade kinematical bounds that would be otherwise present at fixed-order. This is e.g. observed in the transverse momentum of hardest {{formula:ac2fb570-993c-402c-8c67-a61e370ff8a2}} -boson, above the kinematical bound set by the generation cuts on {{formula:b52b4bc7-a82f-4e47-af86-ef03814e2978}} . We have found similar effects also when applying realistic experimental cuts, modeled on those used by the ATLAS collaboration in a previous analysis of the {{formula:288089cd-5a4f-4a57-8269-cbe03bc81a9f}} four-lepton final state. We provided predictions in this fiducial region for the 13 TeV LHC Run II.There are several interesting further developments we didn't investigate in this first study: for example the inclusion of quark-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between constants of motion and Bessel equation solutions, and how do the methods used impact the key results?",
      "expected_output": "The research paper presents findings on the relationship between constants of motion and Bessel equation solutions. The constants of motion are obtained from the Killing equations and energy conservation, leading to equations such as Eq. (REF) and the modified Bessel differential equation of imaginary order. The solutions to this equation are given in terms of constants of integration, with specific examples including the functions represented by formulas 1e669c90-0116-4cf1-befe-57d5b09af208. The methods used, including the ansatz e0b0e243-650a-43eb-b06c-ebb567bdd813, impact the key results by providing a framework for deriving the constants of motion and solving the Bessel equation. For instance, the energy parameter 2f93bb6b-a482-4e45-8f40-fcd733d93b5e plays a crucial role in determining the particular form of the relation between constants of motion.",
      "context": [
        "-14e1-42a0-a7cc-6e8391ccca54}} will be considered, so that {{formula:880de66b-666b-411c-a9a8-195a59a339ba}} . A third constant of motion is obtained from energy conservation. The line element gives: {{formula:0340cda1-c898-45d1-bbc1-a1b75471d005}}Note that Eq. (REF ) can also be deduced from the eikonal equation {{formula:4c2b2949-e7b2-49d1-94e2-7feb44c699a9}} . Substituting (REF )-() gives {{formula:cfea096a-662d-4b0e-9424-896e9e22622e}}It must be remarked that the particular form of that relation requires that the energy parameter {{formula:2f93bb6b-a482-4e45-8f40-fcd733d93b5e}} . The resolution of that equation leads to: {{formula:e7e",
        " obey the Killing equations, which give the constants of motion: {{formula:5d5e29cc-22e3-4929-bc61-1bb09cb6df0c}}Here {{formula:10ab7046-674b-4c69-b28d-57e8cdffc416}} is an affine parameter such that {{formula:199c4e39-fa7a-4272-a1f6-9dd26bfb911c}} with {{formula:02cfc7bc-c850-462e-b7db-c4c841fc504e}} in order to preserve the causal regions in terms of the two variables. Denoting by {{formula:85070546-743c-4dd7-91cd-d4be3ee5f657}} , one gets {{formula:3ec05466-61ba-4eb1-9996-62300156bb67}}In the remainder, propagation along increasing values of {{formula:4a2f88d3-14e1-42a0-a7cc-6e8391ccca54}} will be considered,",
        "498-4bef-88bd-7b4e36cb9182}} is the angular frequency of the light. Using the ansatz {{formula:e0b0e243-650a-43eb-b06c-ebb567bdd813}} leads to the equation {{formula:b01e2f1c-b007-4173-9783-f49eef92cec2}}Eq. (REF ) is the modified Bessel differential equation of imaginary order {{formula:82063328-ec2e-4617-af25-66f2c0ea13f9}} , with solutions , {{formula:1e669c90-0116-4cf1-befe-57d5b09af208}}where {{formula:9423b635-21b4-4f57-a6c0-f922d3e44b7c}} and {{formula:7a54fda6-f3e6-46ce-8c1e-a8facf5ff7a5}} are constants of integration. The functions {{formula:76f3729c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does contextuality play in enabling quantum computation and what are the main findings of the research on this topic?",
      "expected_output": "Contextuality plays a critical role in enabling quantum computation, particularly in universal quantum computation via 'magic state' distillation and measurement-based quantum computation. The research findings indicate that contextuality is a fundamental property of quantum theory, and its simplest form has been experimentally observed. This observation connects the quantum violation of the simplest noncontextuality inequality with the Kochen-Specker theorem, providing a link between two fundamental results in quantum theory. The experiment demonstrates the independence of correlations between sequential measurements from the order of measurements, confirming the small quantum violation of the relevant noncontextuality inequality.",
      "context": [
        " ) fails. Instead of Eq. (REF ), QT predicts {{formula:a70cabcf-6006-4c28-86cf-e410018f7632}}Why Hardy-like experiments are difficult.—As pointed out by Mermin, although Hardy-like proofs “reign supreme in the gedanken realm,” they “provide a rather weak basis for a laboratory violation of the experimentally relevant inequality” . Each of these proofs is equivalent to a violation of an inequality: the Clauser-Horne-Shimony-Holt Bell inequality in the case of Hardy's proof of nonlocality , and the Klyachko-Can-Binicioğlu-Shumovsky (KCBS) noncontextuality (NC) inequality in the case of the Hardy-like proof of contextuality . The difficulty comes from the fact that the violation is small compared to the violation that can be achieved when the constraints of a Hardy-like proof are removed. Consequently, the experimental observation of Hardy-like nonlocality or contextuality requires very precise state preparation and measurements and is much more difficult than observing a violation of a Bell or NC inequality.Despite these difficulties, several experiments have",
        "� (i.e., of the set of other compatible measurements that may be carried out). Consequently, some quantum predictions of QT cannot be reproduced by any noncontextual hidden variable (NCHV) theory. In this sense, it is said that QT exhibits contextuality.Recently, contextuality has been identified as a critical resource for universal quantum computation via “magic state” distillation , and for measurement-based quantum computation . Contextuality is also the underlying property behind nonlocality and its applications, e.g., cryptography , reduction of communication complexity , and randomness expansion .All this makes the following question of fundamental importance: What is the simplest form of contextuality and how can it be observed? It has been recently pointed out that there is a form of contextuality that is analogous to “the simplest and cleanest” form of nonlocality found by Hardy , . In this Letter we present the first experimental observation of this form of contextuality.Contextuality made simple.—The result in Ref. can be summarized as follows. Consider five imaginary boxes, numbered from 1 to 5, which can be either full or empty. {{formula:db162db4-2dda-4d5a-826c",
        "1021-4cc0-9cf8-748561f92d3c}} for the ten experimental configurations, since in this case the bound is shifted from 2 to {{formula:66a843d9-8f4d-476f-bd87-8114f1deecfc}} .Conclusion and further applications.—By implementing a new method for performing two sequential measurements on the same photon, we have presented the first experimental observation of Hardy-like contextuality, which is arguably the conceptually cleanest form of contextuality in physics and connects the quantum violation of the simplest NC inequality with the Kochen-Specker theorem , thus providing the link between two fundamental results in QT .In this experiment, we have observed for the first time with photons that the correlations between the outcomes of sequential measurements represented by commuting operators are independent of the order in which the measurements are performed. In addition, the experiment is precise enough to confirm the small quantum violation of the relevant NC inequality. This shows that this method can be used for a variety of pending fundamental experiments demanding high precision sequential measurements, e.g., contextuality-based nonlocality , , almost-state-independent contextuality , , and contextuality-nonlocality monog"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the metallicity of LMC, measured in dex, impact its overall composition and what are the key findings related to this in the research paper?",
      "expected_output": "The metallicity of the LMC, measured as 0.008 dex, impacts its overall composition by influencing the formation of stars. According to the research paper, this metallicity corresponds to a specific isochrone, Z=0.008, which is used to determine the age of the stars. The key findings related to metallicity include the presence of young field populations, intermediate-age field populations, and distinct supergiant populations, such as blue and red supergiants, which are supported by corresponding isochrones, including 16 Myr, 25 Myr, and 32 Myr isochrones.",
      "context": [
        " was adopted. The metallicity of the young field population of the LMC was found to be {{formula:54478873-3240-4e82-a386-ebaa6404b934}} dex by various authors (Russell & Bessell 1989, Luck & Lambert 1992, Russell & Dopita 1992, Thévenin & Jasniewicz 1992). We therefore adopted Geneva isochrones with Z=0.008 which corresponds to {{formula:c5f9c9de-7a46-4a1c-b2a1-25c3e1436a9f}} dex.Galactic field stars contaminate our observed area. Ratnatunga & Bahcall (1985) estimate the number of foreground stars towards the LMC, and in Table REF we present their counts scaled to our total field of view ({{formula:f3ed6a58-60c0-4864-90ff-b0e505a518e0}} ). {{table:d8cdacc4-b587-43db-9553-99c51d101452}}SL 538: We see two red and one blue supergiant in the {{form",
        " LMC shows up through red giants and the pronounced red horizontal branch clump. We are not able to distinguish between distinct young populations, but the plotted isochrones represent ages which are supported by corresponding supergiants.The brightest blue supergiants and some of the brightest red supergiants are represented by the 16 Myr isochrone (solid line). Also the 25 Myr (dotted) and 32 Myr (dashed) isochrones are supported by bright blue, yellow and red supergiants.Several supergiants are traced by the 80 Myr isochrone. Note that the redder main sequence stars at {{formula:70c11485-f07f-43fd-b358-a411d3f8bd4e}} mag are candidate Be stars (see Sect. 5). One could easily mistake them for more evolved stars marking an additional field population with an age of approximately 100 Myr. The stellar density seems to be lower between 80 Myr and 200 Myr which indicates a possible decrease in the field star formation rate.Along the 200 Myr isochrone and below, the star density is increased along the subgiant branch, again indicating enhanced star formation.{{formula:d319aa9d-9fbd-44",
        " the smallest one with the lowest number of stars. Few data points are located in the red clump and the red giant branch (RGB) region, and it is very likely that these stars belong to an intermediate-age field star population, while the supergiants, on which our age determination mainly relies, are located in the cluster centre and thus we assume that they belong to the star cluster. The main sequence is sparse, especially in the upper part brighter than {{formula:d941cc4d-ffa9-47e7-bedc-96b19603d909}} mag. The best fitting isochrones in both CMDs result in an age of 16 Myr (Fig. REF ). The reddening of {{formula:fa01ef06-9766-4d8c-8045-8f1ada7fd21c}} mag is the same for all fits.The surrounding field: The field population comprises a mixture of ages. Apart from a blue main sequence and blue and red supergiants, which represent the young field populations, the intermediate-age field population of the LMC shows up through red giants and the pronounced red horizontal branch clump. We are not able to distinguish between distinct young"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between dL and dA via DDR, and how do the research findings support this connection?",
      "expected_output": "The distance duality relation (DDR) states that dL(z) = (1+z)^2 dA(z). According to the research paper, this relation holds under the conditions that the number of photons is conserved and they travel on null geodesics in a pseudo-Riemannian spacetime. The paper also provides expressions for luminosity distance (dL) and angular diameter distance (dA) in terms of comoving distance (r), which are dL(z) = (1+z)r(z) and dA(z) = r(z)/ (1+z). These expressions support the DDR connection between dL and dA.",
      "context": [
        " the observations of standard candles, which probe the luminosity distance {{formula:b126bcb7-4607-46b6-86e1-5b1cc40d2f92}} , and standard rulers, through which we can measure the angular diameter distance {{formula:46746371-dbe4-4e8a-8934-567dffca6b8a}} . The general relation between these quantities, which holds under the two conditions that the number of photons is conserved and that they travel on null geodesics in a pseudo-Riemannian spacetime , , is given by {{formula:e2895b19-5aeb-4ba5-b554-922f9764cb80}}which is called the distance duality relation (DDR). Both the luminosity and angular diameter distance can be obtained in terms of the comoving distance {{formula:5c5bd642-6a8b-44ec-9ad1-b242dae6dd9a}} as dL(z)=(1+z)r(z) ,dA(z)=r(z)1+z . Even though these relations hold",
        " conformal Hubble parameter ({{formula:6b5c5922-5aad-4751-b480-49cf9a45062f}} ) and the index {{formula:54b01a45-91a8-4039-85fb-a6f94fc1e9c8}} running over the two polarisations.It can be shown that {{formula:e2b74512-6d2e-43bc-bca0-154e2c1dd0a8}} scales with the luminosity distance as {{formula:d88e5689-ffbf-405c-8f67-33e3e9aa066a}}and therefore distance measurements can be obtained by observing gravitational waves from merger events. If the redshift of the event is measured by observing an electromagnetic counterpart, we can construct a Hubble diagram using these as standard sirens.The photon–axion coupling we consider in this work does not affect the luminosity distance measured through GW observations. These therefore probe the bare luminosity distance, assuming that no other physical mechanism is leading to deviations from the GW propagation predicted by GR, and in eq:GWdlrelation {{formula:489485",
        "c5db3663757e}} , , {{formula:125f58b4-632a-4487-a88b-37f75ba8809b}}which reduces to the standard luminosity distance for {{formula:65cfd82b-a87e-4def-bc7b-500b0445380f}} and {{formula:a974efbb-a934-4eae-b767-593ef0edf7b9}} . In what follows we assume that the bound on {{formula:448972e6-4a02-4db4-93e5-280e05433f2d}} provided by GW170817 holds at all redshifts, and therefore, setting {{formula:992e4c6f-c03b-477f-80a1-abde275724dc}} , the previous equation reduces to {{formula:07af7253-c3c7-4c29-aa3c-beaf8feb4113}}To connect this expression to non-standard theories of gravity, we can use the relation between {{formula:08c68d2"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between HP Regge trajectory and eikonal, and how do the main findings of this research paper explain their impact on each other?",
      "expected_output": "The HP Regge trajectory has a weak dependence on the energy, which is an important feature that allows for unambiguous conclusions. The eikonal approximation includes the soft-Pomeron-exchange and the HP exchange contribution. The research paper explains that the smallness of the HP Regge residue is the reason for its subdominance at accessible energies. For example, the paper states that assuming a certain value for one of the parameters, we obtain a specific value for another parameter, which is natural given the presumed glueball nature of the Pomerons. The extremely weak energy dependence of the HP Regge trajectory is related to the smallness of its residue at low negative energies.",
      "context": [
        "a0}} , it is quite reasonable to consider {{formula:534396a7-d212-45fd-a2b5-a566fdd6f568}} at{{formula:afa03215-17dc-4394-9789-cac01c36c467}} GeV{{formula:830c4d65-4e77-466a-afd9-5070b9f47b31}} .Such a weak {{formula:f951f759-3127-4cf5-bdfb-a2b3de4ce737}} -dependence is a very important feature of the HP Regge trajectory, which allows to make unambiguous conclusions on the basis of further analysis.\nThe HP exchange contribution into the eikonal: The soft-Pomeron-exchange eikonal approximation has the following structure : {{formula:d2a86fc0-b533-45d4-ad1a-e1af6e817bcf}} {{formula:42caee9e-2954-4cf9-9e90-c9a3d7ebe821}} {{form",
        "-65d8-46d6-972a-be1e26f092f3}} is the Regge trajectory of the soft Pomeron, {{formula:02f43859-7b73-4a34-aba2-6c37f8411636}} is the SP coupling to proton. At {{formula:3ffb0831-e049-4a3b-9104-8f0001f67aaf}} , {{formula:0c9de915-82bd-4159-b3f0-8ea538f444b8}} and {{formula:d4243177-b1ff-4e25-8c33-cd2460ce74f5}} can be approximated by simple test functions {{formula:3d8a4513-922f-419b-89aa-99b91d5fd52d}}where the free parameters take on the values presented in Table REF .Inclusion of the HP exchanges into consideration requires a replacement{{formula:f07901fe-47b5-4cef-a2a8-5bb77ec2d650}} ,",
        " conclusions: The HP subdominance at accessible energies is, certainly, determined by the smallness of its Regge residue: {{formula:f1b397da-dc7f-46e0-939c-14fff9fdfba0}}Assuming that {{formula:3b29e1d7-7830-40fa-b6a9-67ad02d1ccf0}} , we obtain {{formula:4bb2a95f-8d37-4e9a-bee6-48f7cb5fe311}} , what is quite natural in view of the presumed glueball nature of both the Pomerons. The smallness of {{formula:c953dd54-78b1-49d5-83af-fc99540e35d3}} at low negative {{formula:1146861e-2639-4060-8540-9911389dcd5c}} is, thus, related to the extremely weak {{formula:9414ce7f-6256-4d5f-a644-931584cc48a0}} -behavior of {{formula:15aef7d8-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding nilpotency in Fitting classes and how do the methods used in this research contribute to the overall understanding of this concept?",
      "expected_output": "The research paper explores the concept of nilpotency in Fitting classes, finding that if a group belongs to a Fitting class and has a nilpotent subgroup, then the group itself is also nilpotent under certain conditions. For example, if a group is supersoluble, it coincides with the product of the class of all abelian-groups and the class of all nilpotent groups. The methods used in this research contribute to the overall understanding of nilpotency by providing specific conditions under which a group can be considered nilpotent, such as when a subgroup is nilpotent or when a group is supersoluble.",
      "context": [
        "6cbe2c464}} belongs to a Fitting class {{formula:b76f9f46-e1a8-4d08-ac13-fc411750ed21}} , then {{formula:0cb67453-7270-473a-94fb-aaca5dd1c416}} . In particular,{{formula:1a3d5f2d-976f-4061-9ce8-2237fad1d513}} if {{formula:aa6850cf-e549-4a51-b2f4-d5074b1452ea}} is nilpotent, then {{formula:92675c8c-fec1-4d89-9826-7173b38b3358}} is also nilpotent;{{formula:9f3829ad-f910-484c-9c50-04e7612eba55}} if {{formula:e817cfa7-ff48-427d-b614-cbb71c6e1d9c}} is {{formula:ec740022-4ddb-43cd-9",
        "-46d1-baac-30f6d56c3aef}} {{formula:17a1b536-77c4-4295-866b-d0f885e465eb}} ;{{formula:7bcee36f-1651-4e9a-995d-d6422c519195}} {{formula:e7768dd1-61ed-48e1-89f3-4d621f7d054e}} ;{{formula:f949e02f-2c42-4b1b-b6c0-a4aea571923c}} if {{formula:3f5f503f-f09b-4ad7-b3ac-07f4a5592ab0}} , then {{formula:026ac928-cabd-447f-8401-13edd5780acc}} .If {{formula:2a49e8a4-7839-400f-9d0c-82ee71fb7765}} is a subgroup of a group {{formula:f2427045-dfb3-4253-",
        "ac-ca33f060bafb}} is supersoluble. {{formula:e46401bc-a8f4-4bd4-8ede-4dcd144178bf}}The class of all {{formula:fc4d2cf7-9d44-49d6-a488-b248c776a878}} -nilpotent groups coincides with the product {{formula:87c8e802-9654-4945-95b4-0bbde3a7f58a}} , where {{formula:d86f87b8-6806-42df-930b-3521b942e20e}} is the class of all {{formula:659839db-a8eb-413e-80ab-ae97df4e9102}} -groups and {{formula:79c89334-6100-4bb7-ae07-0cfa5e9be46f}} is the class of all {{formula:d2fcfb4d-9431-4f98-89d2-f4b78528bc4a}} -groups. A group {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research paper on PZW representation in relation to light-matter interaction, and how do the methods used contribute to the key results?",
      "expected_output": "The research paper explores the Power-Zienau-Wooley (PZW) representation of Quantum Electrodynamics to study light-matter interaction. The main findings include the expression of the light-matter interaction Hamiltonian, which describes the energy of the electromagnetic field and its interaction with phonon and magnon excitations. The methods used, such as the PZW representation and the definition of a finite volume of space, contribute to the key results, including the derivation of the dielectric function and the expression of the electromagnetic field. For example, the paper shows that the dielectric function can be expressed as a piecewise constant function, with values of ε∞(z) = εCBO∞ for the CBO slab, εNiO∞ for the NiO layer, and 1 for elsewhere. Additionally, the paper discusses the light-phonon coupling in the CBO slab and provides a general expression for the phonon Hamiltonian and polarization.",
      "context": [
        "arepsilon (z) \\varepsilon _0}d^3\\mathbf {r} \\\\ -\\int _V \\mu _0 \\hat{\\mathbf {H}}(\\mathbf {r})\\hat{\\mathbf {M}}(\\mathbf {r}) d^3\\mathbf {r}The light-matter interaction Hamiltonian is expressed here in the Power-Zienau-Wooley representation of Quantum Electrodynamics . The first line contains the Hamiltonians of the phonon and magnon excitations that will be detailed further. The second line of Eq. (REF ) describes the energy of the electromagnetic field. The third line describes the light-matter interaction with the dipolar excitations of interest, which in the present case are the phonons of the CBO slab. Respectively, the field \\hat{\\mathbf {P}}(\\mathbf {r}) is the polarization field of the CBO phonons. In that case the dielectric function \\varepsilon _{\\infty }(z) is a piecewise constant function that does not contain the phonon contribution and is expressed as:\\varepsilon _{\\infty }(z) = \\vare",
        " the phonon contribution and is expressed as:\\varepsilon _{\\infty }(z) = \\varepsilon ^{CBO}_\\infty , \\phantom{Q} z\\in CBO, \\\\ = \\varepsilon ^{NiO}_\\infty ,\\phantom{Q} z\\in NiO \\\\ =1, \\phantom{Q} z\\in \\mathrm {Elsewhere}The last line of Eq. (REF ) describes the light-matter interaction with the \\alpha magnon of NiO , with \\hat{\\mathbf {M}}(\\mathbf {r}) the corresponding magnetization that is provided explicitly further.We will consider the case where the matter systems interact with an electromagnetic wave that is polarized along the dielectric layers, along the unit vector \\mathbf {u}_x, and propagates in the z-direction. We can then write a general expression of the electromagnetic field:\\hat{\\mathbf {H}}(\\mathbf {r})=\\mathbf {u}_y \\sqrt{\\frac{\\hbar \\omega _m \\varepsilon _0 c^2}{2SI}} h(z",
        " that we can define a finite volume of space containing the system and that contains the majority of the electromagnetic energy. The corresponding length along the z direction will be noted L. This assumption is questionable for the type of waveguide modes that are used in the experiment, however, we will see further how this formalism can still be applied by introducing effective overlap factors and linking the polariton dispersion to the effective dielectric and magnetic susceptibilities of the layers. Finally, in accordance with Maxwell's equation the function h(z) must obey the identity:\\int _L \\frac{(\\partial _z h(z))^2}{\\varepsilon _\\infty (z)} dz = \\frac{\\omega _m^2}{c^2} \\int _L h(z)^2 dzThis identity states that the total magnetic energy is equal to the total electric energy in the system, as expected for guided modes.\nLight-phonon coupling in the CBO slab: A very general expression for the phonon Hamiltonian and polarization is discussed in the Ref . In particular, it can be shown for the optical phonon the following results hold, in the long wavelength limit:\\mathcal {\\hat{H}}_{ph"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do unitarily equivalent systems preserve frame properties, and what are the key findings related to this concept in the research paper?",
      "expected_output": "Unitarily equivalent systems preserve frame properties, meaning that if two systems are unitarily equivalent, they have the same frame bounds and one is a frame or Riesz basis if and only if the other is. The research paper shows that this property allows for the derivation of new results on Gabor systems and other related systems, such as the Balian-Low type theorem. For example, if a Gabor system is unitarily equivalent to a Riesz basis, then the original system is also a Riesz basis. This concept is crucial in understanding the properties of frames in Hilbert spaces and has significant implications for signal processing and other applications.",
      "context": [
        "4206-89b0-95ed4bf7f8dd}} for all {{formula:04373b4d-6b47-49fb-86a9-daad940fa2ed}} .Basis and frame theoretical properties are preserved by unitarily equivalence, e.g., if {{formula:9177a586-1144-4e6e-9cad-f52211c42e3b}} and {{formula:17c7e60d-4014-472a-84fd-e33a1828f709}} are unitarily equivalent, then {{formula:d86df145-8696-446a-af7d-6e7f0487da58}} is a frame (or Riesz basis) precisely when {{formula:4f494186-97b1-4c9c-bd4b-4bca1828a64c}} is a frame (or a Riesz basis), and the optimal frame bounds are the identical. Hence, from a frame theoretical point-of-view two unitarily equivalent systems are identical objects.We write {{formula:71e70d17-e77",
        "e1a4ec7}} , the unitarily equivalent Gabor system in Theorem REF (ii) becomes {{formula:fc2f5a8d-309e-45ba-88b1-0c166581c4fd}}Hence, from well-known results in Gabor analysis on critically sampled Gabor system, it follows that the frame property of {{formula:730267d7-a596-4281-b9d6-169171a6ab3a}} automatically implies that the {{formula:b98d940e-b6ac-45fa-a2d1-1def275f2d32}} -system is, in fact, a Riesz basis; see , for the direct proof in case of {{formula:f1f15537-55ac-471f-87d4-d4a1c19a8461}} -systems.Theorem REF can also be used to obtain new results on {{formula:52b772cb-1f26-406a-beb7-e1acc617507d}} -systems. E.g., a Balian-Low type-theorem",
        "a8620cbfdacb}} -systems and Gabor systems: A frame in a Hilbert space {{formula:fb39d7a1-f29f-4989-b9b0-8fd578339ff2}} is a countable family of vectors {{formula:b74c2457-4cc3-41b6-a371-9bd373a753a2}} for which there exist constants {{formula:bf86ad84-4a84-4aa4-8506-8715ec003028}} , called frame bounds, so that {{formula:c4f799c1-d622-4682-b43e-b313b7cbec48}}The largest such constant {{formula:a288d686-39fa-41bb-ada5-9e7a2837fe9e}} and smallest such constant {{formula:6da05e7d-fb45-41f2-ab8a-992630bd0a29}} are called optimal frame bounds.Definition 4.1 Two countable families {{formula:ee92e79c-865e-4ed8-84"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does P2LDGAN use to generate line drawings and what are its key findings in terms of image quality and realism?",
      "expected_output": "P2LDGAN uses a joint geometric-semantic-driven generator to generate line drawings, which densely concatenates feature maps with different scales and information flows from the encoding stage into the decoder using cross skips. This approach fuses geometric and semantic features for fine-grained drawings generation. The key findings show that P2LDGAN achieves superior image quality and realism compared to state-of-the-art methods, as demonstrated by quantitative and qualitative comparisons on the newly collected dataset of 1,532 pairs of character photos and line drawings.",
      "context": [
        " supervised image-to-image translation problem which heavily relies on significant training samples, therefore, it is necessary to construct a dataset to help our model learn an accurate mapping to bridge the real character photo/image and freehand character line drawing domains.Specifically, we first collect high-resolution character images/photos from the internet mainly covering five categories, namely male, female, manga/cartoon male, manga/cartoon female, and others, to enrich or diversify the data. Then, to meet the demands of different artistic styles, we invite experienced artists and many skilled students majored in visual communication design to manually draw the character line drawings at the same scale as the given source images/photos using specific application and digital devices. Finally, to standardize the gathered images/photos and hand-drawn line arts, we preprocess them to the size of 1024 {{formula:f2cd6868-81fc-4bf4-acd0-1716dda70604}} 1024 pixels, and fine tune the structural lines manually to form the strictly aligned image/photo-line drawing pair (describing the same character) with the help of professional artists and image processing software.We carefully select 1,532 pairs of high-quality character line drawings",
        " the help of professional artists and image processing software.We carefully select 1,532 pairs of high-quality character line drawings paired with real images/photos to construct our final dataset. Table 1 illustrates the distributions of our collected dataset. Figure REF shows representative character image/photo-line drawing pairs. In order to facilitate the learning and evaluation, we choose to split the dataset into two disjoint subsets, i.e. 70% for training and the remaining 30% as testing set. The data will be released to the public for research related to line drawing. {{table:37af7448-af9b-49fb-bd29-6b15a031203d}}\nOverview: We attempt to generalize the application of generative adversarial framework to the photo-to-line drawing style conversion problem, i.e. automatic artistic character line drawings generation from real images/photos. Given the well-aligned source-reference pair {{formula:d7620c33-09d2-4083-aad8-cf7498cf0cad}} , where {{formula:8bbc678e-b1b8-4fb0-94e7-ba5c63eb75",
        " images/photos and hand-drawn line drawings. The starting point of our P2LDGAN is an input real character photo, outputting a realistic hand-drawn character line drawing. To improve the generation quality with more details and clear lines, we design a joint geometric-semantic-driven generator, in which the feature maps with different scales and information flows from encoding stage are densely concatenated into corresponding layers of decoder using cross skips to fuse geometric and semantic features for fine-grained drawings generation. For discriminator, we adopt patch discriminator to improve the discriminative ability.In order to train and evaluate our proposed model, we introduce a new dataset, which consists of more than one thousand of character images/photos and line drawings pairs, where the line drawings are manually created by skilled artists we invite. We quantitatively and qualitatively compare our framework against the state-of-the-arts on this newly collected dataset, and experimental results show the superiority of proposed P2LDGAN. Finally, we perform the ablation studies to further validate the effectiveness of our key components.Summarily, the main contributions of our study include,We contribute a new photo-to-line drawing dataset including 1,532 pairs of character photos/images"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main research findings of A.V. Kotwal, M. Kreps, and J. Kroll, and which universities are they affiliated with?",
      "expected_output": "The main research findings of A.V. Kotwal, M. Kreps, and J. Kroll are not specified in the provided context, but their university affiliations are: A.V. Kotwal is affiliated with Duke University, M. Kreps is affiliated with Institut für Experimentelle Kernphysik, Universität Karlsruhe, and J. Kroll is affiliated with University of Pennsylvania.",
      "context": [
        "orytov University of Florida, Gainesville, Florida 32611 A.V. Kotwal Duke University, Durham, North Carolina 27708 M. Kreps Institut für Experimentelle Kernphysik, Universität Karlsruhe, 76128 Karlsruhe, Germany J. Kroll University of Pennsylvania, Philadelphia, Pennsylvania 19104 D. Krop Enrico Fermi Institute, University of Chicago, Chicago, Illinois 60637 N. Krumnack Baylor University, Waco, Texas 76798 M. Kruse Duke University, Durham, North Carolina 27708 V. Krutelyov University of California, Santa Barbara, Santa Barbara, California 93106 T. Kubo University of Tsukuba, Tsukuba, Ibaraki 305, Japan T. Kuhr Institut für Experimentelle Kernphysik, Universität Karlsruhe, 76128 Karlsruhe, Germany N.P. Kulkarni Wayne State University, Detroit, Michigan 48201 M. Kurata University of Tsukuba, Tsukuba, Ibaraki 305, Japan S. Kwang Enrico Fermi Institute, University of Chicago, Chicago, Illinois 60637 A.T. Laasanen Purdue",
        "88c-4037-aec0-e30c1f6afa2b}} University of California, Santa Barbara, Santa Barbara, California 93106 D. Hirschbuehl Institut für Experimentelle Kernphysik, Universität Karlsruhe, 76128 Karlsruhe, Germany A. Hocker Fermi National Accelerator Laboratory, Batavia, Illinois 60510 S. Hou Institute of Physics, Academia Sinica, Taipei, Taiwan 11529, Republic of China M. Houlden University of Liverpool, Liverpool L69 7ZE, United Kingdom S.-C. Hsu Ernest Orlando Lawrence Berkeley National Laboratory, Berkeley, California 94720 B.T. Huffman University of Oxford, Oxford OX1 3RH, United Kingdom R.E. Hughes The Ohio State University, Columbus, Ohio 43210 U. Husemann Yale University, New Haven, Connecticut 06520 M. Hussein Michigan State University, East Lansing, Michigan 48824 J. Huston Michigan State University, East Lansing, Michigan 48824 J. Incandela University of California, Santa Barbara, Santa Barbara, California 93106 G. Introzzi Istituto Nazionale di Fisica N",
        " of Tsukuba, Tsukuba, Ibaraki 305, Japan K. Nakamura University of Tsukuba, Tsukuba, Ibaraki 305, Japan I. Nakano Okayama University, Okayama 700-8530, Japan A. Napier Tufts University, Medford, Massachusetts 02155 V. Necula Duke University, Durham, North Carolina 27708 J. Nett University of Wisconsin, Madison, Wisconsin 53706 C. Neu{{formula:350f09e8-b165-4b11-8b77-770c16e98699}} University of Pennsylvania, Philadelphia, Pennsylvania 19104 M.S. Neubauer University of Illinois, Urbana, Illinois 61801 S. Neubauer Institut für Experimentelle Kernphysik, Universität Karlsruhe, 76128 Karlsruhe, Germany J. Nielsen{{formula:b9eacdc5-42bb-4daf-bf82-4da3a25ab9ff}} Ernest Orlando Lawrence Berkeley National Laboratory, Berkeley, California 94720 L. Nodulman Argonne National Laboratory, Argonne, Illinois 60439 M. Norman University of California, San Diego, La Jolla"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What were the main findings of Goddard's study on alpha-agostic interactions and how did they impact our understanding of the subject?",
      "expected_output": "The main findings of Goddard, Hoffmann, and Jemmis' study on alpha-agostic interactions, conducted over 20 years ago, radically altered the view on these interactions. They found that the direct interaction between the occupied bonding orbitals of the carbon-hydrogen bond and vacant orbitals in an electron-deficient metal atom is not the primary cause of agostic interactions. Instead, the study showed that tilting the CH2 towards the metal switches on two new interactions: the sigma CH bonding orbital interacts with the metal's LUMO+1 orbital, and the metal's LUMO+1 interacts with the HOMO of the CH2 ligand in its singlet state, an sp2 lone pair orbital of sigma symmetry. These findings changed our understanding of alpha-agostic interactions, suggesting that the structural change is not due to a direct interaction between the metal atom and the carbon-hydrogen bond, but rather a consequence of a change in the metal-carbon interaction.",
      "context": [
        " \\alpha -agostic interactions was done more than 20 years ago by Goddard, Hoffmann and Jemmis . The importance of this work stems from the fact that this study radically altered the view on \\alpha -agostic interactions. Until then, it was thought that the direct interaction of the occupied bonding orbitals of the carbon-hydrogen bond with vacant orbitals in an electron deficient metal atom facilitate the formation of agostic interactions. Hoffmann and coworkers could show that this is indeed not the case.The model system employed in clearly demonstrated that by tilting the \\rm CH_2 towards the metal two new interactions are switched on. Firstly, as previously thought, the \\sigma _{\\rm CH} bonding orbital starts to interact with the metal's LUMO+1 orbital. However, because the energy difference between the two interacting orbitals is large, the \\sigma _{\\rm CH} bonding orbital is very low in energy, the stabilization through this interaction is very small. Another interaction that is favored by the new, tilted structure is the interaction of the metal's LUMO+1 and the HOMO of the \\rm CH_2 ligand in its singlet state, an \\rm sp^2 lone pair orbital of \\sigma",
        " more thoroughly from the comparison of QM/MM and DFT calculations , Such studies are carried out with essentially the same results but different analytical tools and more sophisticated computational models to this date. Some authors go as far as arguing for a replacement of the term agostic interaction with agostic geometry to emphasize the fact that the structural change in those compounds does not stem from a direct interaction of the metal atom and the carbon-hydrogen bond , .Not surprisingly many of the conclusions drawn for \\alpha -agostic interactions are also valid for \\beta -agostic interactions , , . Here too the main contribution to the stabilization comes from the weakening of the \\rm C_\\alpha -metal bond and not from the direct donation of \\rm C_\\beta H-bond density to vacant metal orbitals. {{figure:740baf40-4fac-4eed-ae2f-3cdef6630abe}}The orbital picture is, however, somewhat different (See fig. REF ). In this case the main driving force for the stabilization of the agostic geometry is the delocalization of the two electrons in the metal-\\rm C_\\alpha -\\rm C_\\beta system through negative hyperconjugation .True \\rm",
        "rm CH_2 ligand in its singlet state, an \\rm sp^2 lone pair orbital of \\sigma symmetry. Because these orbitals are significantly closer to each other in energy and overlap significantly in the tilted geometry, they deliver the main energetic contribution to the agostic interaction. Because the metal's LUMO is not as favorably hybridized as the metal's LUMO+1 the loss of overlap and hence stabilization, by tilting the \\rm CH_2 ligand's \\sigma HOMO away in the primary bonding metal-carbon interaction is more than compensated by this new stabilizing interaction (See fig. REF ). {{figure:3b95401c-9fa9-43c0-9c50-d24a21d10948}}So, even though the previously proposed direct interaction between the metal atom and the carbon-hydrogen bond is present and even accompanied by the usual tell-tale sign of a decreased NMR \\rm ^1J_{C-H} coupling constant, it is too small to cause the geometric change by itself. It should rather be seen as a consequence of a change in the metal-carbon interaction.In a subsequent study it has been shown that this concept can be extended"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do matched pairs of groups play in shaping tensor categories, and how do these interactions influence the main findings of the research paper?",
      "expected_output": "Matched pairs of groups play a crucial role in shaping tensor categories by introducing the notion of (G, Γ)-crossed action, where (G, Γ) is a matched pair of finite groups. This interaction gives rise to a tensor category C^(G, Γ) that fits into an exact sequence of tensor categories Rep G → C^(G, Γ) → C. The research paper shows that if C is a (G, Γ)-crossed tensor category equipped with a (G, Γ)-braiding, then the tensor category C^(G, Γ) is a braided tensor category in a canonical way, extending the notion of G-crossed braided tensor category due to Turaev.",
      "context": [
        "# arXiv Paper: 1405.6970\n\nCrossed actions of matched pairs of groups on tensor categories\nabstract: We introduce the notion of $(G, \\Gamma)$-crossed action on a tensor category, where $(G, \\Gamma)$ is a matched pair of finite groups. A tensor category is called a $(G, \\Gamma)$-crossed tensor category if it is endowed with a $(G, \\Gamma)$-crossed action. We show that every $(G, \\Gamma)$-crossed tensor category $\\mathcal C$ gives rise to a tensor category $\\mathcal C^{(G, \\Gamma)}$ that fits into an exact sequence of tensor categories $\\operatorname{Rep G} \\to \\mathcal C^{(G, \\Gamma)} \\to \\mathcal C$. We also define the notion of a $(G, \\Gamma)$-braiding in a $(G, \\Gamma)$-crossed tensor category, which is connected with certain set-theoretical solutions of the QYBE. This extends the notion of $G$-crossed bra",
        " certain set-theoretical solutions of the QYBE. This extends the notion of $G$-crossed braided tensor category due to Turaev. We show that if $\\mathcal C$ is a $(G, \\Gamma)$-crossed tensor category equipped with a $(G, \\Gamma)$-braiding, then the tensor category $\\mathcal C^{(G, \\Gamma)}$ is a braided tensor category in a canonical way.\n: We introduce the notion of {{formula:1657d7ba-587a-41eb-9f8a-57651458f2d9}} -crossed action on a tensor category, where {{formula:aab8a272-a999-4d8c-a065-bda32584bd40}} is a matched pair of finite groups. A tensor category is called a {{formula:534d8a67-34a2-4dc0-849c-5e427c5fc276}} -crossed tensor category if it is endowed with a {{formula:7a54a03e-64fc-48c0",
        "or category if it is endowed with a {{formula:7a54a03e-64fc-48c0-a0f2-1d4c67769b71}} -crossed action. We show that every {{formula:fad48827-f0be-4629-8965-8e46f1455acc}} -crossed tensor category {{formula:1a00e935-dba5-4578-8f81-725ab2ef4965}} (G, ){{formula:a2bb177a-b23b-4a0a-97d1-23370a635199}} RepG (G, ) . We also define the notion of a {{formula:102f6d4e-9def-43ac-9ced-16c9581d19c4}} -braiding in a {{formula:e5c80778-1988-4960-9fdf-9d839427f6f3}} -crossed tensor category, which is connected with certain set-theoretical solutions of the QYBE. This extends the notion of {{formula:28a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.5
      }
    },
    {
      "input": "What are the main findings of the ab initio approach in describing nuclear phenomena, and how do the methods used contribute to the key results?",
      "expected_output": "The ab initio approach has successfully described nuclear phenomena by considering interactions between nucleons via two- and three-body interactions. The methods used, such as Quantum Monte Carlo calculations with AV18 two-body and IL7 three-body interactions, have contributed to key results including explaining available experimental data in a broad energy range. For example, corrections from two-body electromagnetic currents are as large as 40% in the calculated magnetic moment of certain nuclei, demonstrating the importance of many-body correlations in nuclear wave functions and electroweak currents.",
      "context": [
        ", if time permits, present a novel framework to calculate neutrino-nucleus cross sections for $A>12$ nuclei.\nnull: The nuclear ab initio approach aims at describing the widest range of nuclear phenomena through interactions occurring between nucleons inside the nucleus. In this microscopic picture, nucleons interact with each other via two- and three-body interactions, and with external electroweak probes via couplings to individual nucleons and to nucleon-pairs (a contribution described by two-nucleon currents). Albeit limited to light nuclei ({{formula:50c31689-1326-4ce1-af12-5a0065f29240}} ), Quantum Monte Carlo calculations based on the AV18 two-body and IL7 three-body interactions successfully explain available experimental data in a broad energy range, from the keV regime relevant to astrophysics studies to the GeV regime where short-range correlations become predominant , , . These studies yield a rather complex picture of the nucleus with many-body correlations in both the nuclear wave functions and electroweak currents playing an important role in reaching agreement with the data. For example, corrections from two-body electromagnetic currents are as large as 40% in the calculated magnetic",
        "# arXiv Paper: 1710.06870\n\nFundamental Physics with Electroweak Probes of Nuclei\nabstract: The past decade has witnessed tremendous progress in the theoretical and computational tools that produce our understanding of nuclei. A number of microscopic calculations of nuclear electroweak structure and reactions have successfully explained the available experimental data, yielding a complex picture of the way nuclei interact with electroweak probes. This achievement is of great interest from the pure nuclear-physics point of view. But it is of much broader interest too, because the level of accuracy and confidence reached by these calculations opens up the concrete possibility of using nuclei to address open questions in other sub-fields of physics, such as, understanding the fundamental properties of neutrinos, or the particle nature of dark matter. In this talk, I will review recent progress in microscopic calculations of electroweak properties of light nuclei, including electromagnetic moments, form factors and transitions in between low-lying nuclear states along with preliminary studies for single- and double-beta decay rates. I will illustrate the key dynamical features required to explain the available experimental data, and, if time permits, present a novel framework to calculate neutrino-nucleus cross sections for $A>12",
        " agreement with the data. For example, corrections from two-body electromagnetic currents are as large as 40% in the calculated magnetic moment of {{formula:eef6dda9-018c-460a-b6b1-9db5f228077b}} C , while electron scattering experiments have demonstrated the requirement of two-body currents in quasi-elastic scattering from nuclei, where they enhance the transverse response by up to {{formula:be4a6ca5-e41d-430f-aa81-bd3e7e8d3cc9}} , .The success of the microscopic picture in explaining the data both qualitatively and quantitatively is an important achievement from the nuclear physics point of view. But it is of much broader interest too, because the level of accuracy and confidence reached by these ab inito Quantum Monte Carlo calculations opens up the concrete possibility of using nuclei to address prominent and pressing open questions in nuclear physics and their connection to fundamental physics quests.Recently, we addressed the “{{formula:320bd150-44b5-420f-82c1-e06b4806ddf8}} problem”, that is the systematic overprediction ({{"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of PIC simulations regarding non-thermal emission, and how do the methods used in these simulations contribute to the key results?",
      "expected_output": "The main findings of PIC simulations are that the spectrum from relativistic collisionless shocks is entirely consistent with synchrotron radiation in the magnetic fields generated by Weibel instability. The methods used in these simulations, such as the extraction of synthetic photon spectra and the computation of the photon spectrum via Fourier transform of the electric far-field, contribute to the key results by self-consistently providing both the strength and the spatial structure of electromagnetic fields, as well as the particle distribution. For example, the simulations show that the magnetic power generated by Weibel instability is concentrated on scales as small as a few plasma skin depths, which is important for understanding the emission mechanism in unmagnetized collisionless shocks.",
      "context": [
        " of the magnetic turbulence, respectively). These findings may place constraints on the origin of non-thermal emission in astrophysics, especially for the interpretation of the hard (harder than synchrotron) low-frequency spectrum of Gamma-Ray Bursts.\nIntroduction: Non-thermal photon spectra from Pulsar Wind Nebulae, jets from Active Galactic Nuclei, Gamma-Ray Bursts and Supernova Remnants are usually explained as synchrotron radiation from a power-law population of particles, presumably accelerated in collisionless shocks. The microphysical details of shock acceleration are still poorly known, however, and are the subject of active research.Particle-in-cell (PIC) simulations of colliding plasma shells have shown that Weibel instability , , converts the free energy of counter-streaming flows into small scale (skin-depth) magnetic fields , , , , . The fields grow to sub-equipartition levels and deflect and randomize the bulk flow, thus creating a shock , , , . A few percent of the incoming particles repeatedly scatter off the magnetic turbulence created by Weibel instability, and eventually populate a power-law high-energy tail in the particle spectrum behind the shock , , .Since most of",
        ", and eventually populate a power-law high-energy tail in the particle spectrum behind the shock , , .Since most of the magnetic power generated by Weibel instability is concentrated on scales as small as a few plasma skin depths, it has been speculated that the emission mechanism in unmagnetized collisionless shocks may be the so-called “jitter” radiation. Whereas the standard synchrotron emission applies to large-scale fields, the jitter regime is realized if the scale {{formula:b5f73846-7854-4679-ac16-d68568db0dc8}} of the turbulence is such that the wiggler parameter {{formula:aeb65a90-d2f1-4f47-83eb-684c3a3c97de}} , , , . Jitter radiation has been proposed as a solution for the so-called “line of death” puzzle in Gamma-Ray Burst (GRB) spectra, which below the peak frequency are sometimes harder than expected from synchrotron radiation . Since PIC simulations self-consistently provide both the strength and the spatial structure of electromagnetic fields, as well as the particle distribution, it is possible",
        "# arXiv Paper: 0908.3193\n\nSynthetic Spectra from PIC Simulations of Relativistic Collisionless Shocks\nabstract: We extract synthetic photon spectra from first-principles particle-in-cell simulations of relativistic shocks propagating in unmagnetized pair plasmas. The two basic ingredients for the radiation, namely accelerated particles and magnetic fields, are produced self-consistently as part of the shock evolution. We use the method of Hededal & Nordlund (2005) and compute the photon spectrum via Fourier transform of the electric far-field from a large number of particles, sampled directly from the simulation. We find that the spectrum from relativistic collisionless shocks is entirely consistent with synchrotron radiation in the magnetic fields generated by Weibel instability. We can recover the so-called \"jitter'' regime only if we artificially reduce the strength of the electromagnetic fields, such that the wiggler parameter K = qB lambda/mc^2 becomes much smaller than unity (\"B\" and \"lambda\" are the strength and scale of the magnetic turbulence, respectively). These findings may place constraints on the origin of non-thermal emission in astrophysics,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the significance of eigenvalues in understanding the gaps in Theorem 5.1, and how do they impact the overall findings of the research paper?",
      "expected_output": "The significance of eigenvalues in Theorem 5.1 lies in their presence or absence within specific gaps, which determines the operator's behavior. According to the theorem, the operator has no eigenvalues except in a finite number of even or odd gaps, where it can have one eigenvalue per gap. For instance, if the operator has an eigenvalue in the second gap, it implies a specific condition for the parameter. The distribution of eigenvalues within these gaps provides crucial insights into the operator's properties and the underlying system's behavior, as demonstrated by Theorem 4.5, which counts eigenvalues in the first spectral gap.",
      "context": [
        "11ca80}} allow us to state the following result describing eigenvalues in the gaps.Theorem 5.1 Assume that {{formula:327be8af-114e-4575-b520-519acb6ffeef}} . If {{formula:cf0aeab5-082e-48e6-95a0-02e99ed7c7d0}} , the operator {{formula:87f96ebd-4669-4af6-a9ea-0b9d08ac097d}} has no eigenvalues as {{formula:a3116fd3-cc06-41f6-bd50-182bcb3fe650}} except in a finite number of even gaps, where it can have one eigenvalue per gap. Similarly, for {{formula:916bcff8-b868-434a-aa6f-3d977889a8e6}} and {{formula:4cd62427-36e3-45ab-9664-a05c3a71ce70}} it has no eigenvalues except in a finite number of odd gaps, where it can have one",
        " {{formula:cc3e6461-87f0-4a35-9384-3467b5601475}} the function counting eigenvalues of {{formula:d3f35f98-5251-4416-93fe-15d0bd507786}} in the first spectral gap of its continuous spectrum.Theorem 4.5 For {{formula:f0389c1a-0085-44a5-ac02-8e9e46f2f817}} and {{formula:ac8901d7-52ab-4751-b189-4f7a79cdd6bb}} , we have {{formula:853c41f8-09ce-41e9-92d7-4212ab29e949}} . On the other hand, if {{formula:c4de5f89-261a-4350-8d09-05e1b4039d51}} and {{formula:e67262af-367d-44bf-9dc2-21a63de0e186}} , then {{formula:d086314e-a58b-48b",
        "a71ce70}} it has no eigenvalues except in a finite number of odd gaps, where it can have one eigenvalue per gap. In particular, for a sufficiently large {{formula:8c9282e1-b1db-403f-b771-95300db9cb17}} , the operator has no eigenvalues, while for a sufficiently small {{formula:bb1faf37-d7ca-4220-b076-f6779ec8898a}} and {{formula:6ce52734-7017-4fab-a780-ec32850ee121}} (respectively, {{formula:3eca6997-de86-4ae8-98d9-55d947fda88d}} ) it has an eigenvalue in the second (respectively, the first) gap.On the other hand, let {{formula:87cf17c9-5998-407b-8cae-86436c11be0a}} . If {{formula:7298a2ba-36be-4acc-b2d3-d46f33c49a2f}} , the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between MAP models and PAC-Bayesian bounds, and how do the main findings of this research paper apply to these concepts?",
      "expected_output": "The research paper suggests a relationship between MAP perturbation models and PAC-Bayesian generalization bounds. The main findings of this paper can be used to derive PAC-Bayesian bounds for unbounded loss functions, which may be useful in applications such as computer vision. Specifically, the paper's results on measure concentration for Gumbel random variables and Poincaré inequalities can be applied to develop new stochastic gradient methods that control variance via controlling gradients, potentially leading to new PAC-Bayesian bounds.",
      "context": [
        "7-3647-4349-b6e0-45d7ed02f13f}} under the perturbations. We derived high-probability estimates of these expectations that allow estimation with arbitrary precision. To do so we proved more general results on measure concentration for functions of Gumbel random variables and a Poincaré inequality for non-strongly log-concave distributions. These results hold in generality and may be of use in other applications.The results here can be taken in a number of different directions. MAP perturbation models are related PAC-Bayesian generalization bounds, so it may be possible to derive PAC-Bayesian bounds for unbounded loss functions using our tools. Such loss functions may exclude certain configurations and are already used implicitly in computer vision applications such as interactive segmentations. More generally, Poincaré inequalities relate the variance of a function and its derivatives. Our result may suggest new stochastic gradient methods that control variance via controlling gradients. This connection between variance and gradients may be useful in the analysis of other learning algorithms and applications.",
        " the distribution of the MAP perturbation, which can be estimated by sample averages. This paper derives new measure concentration results that bound the error of this estimate in terms of the number of samples, making Algorithm REF practical.Measure concentration has appeared in many machine learning analyses, most commonly to bound the rate of convergence for risk minimization, either via empirical risk minimization (ERM) (e.g., ) or in PAC-Bayesian approaches (e.g., ). In these applications the function for which we want to show concentration is “well-behaved” in the sense that the underlying random variables are bounded or the function satisfies some bounded-difference or self-bounded conditions conditions, so measure concentration follows from inequalities such as , Azuma-Hoeffding , , , or . However, in our setting, the Gumbel random variables are not bounded, and random perturbations may result in unbounded changes of the perturbed MAP value.There are several results on measure concentration for Lipschitz functions of Gaussian random variables (c.f. Maurey and ). In this work we use logarithmic Sobolev inequalities and prove a new measure concentration result for Gumbel random variables.",
        " solvers on low dimensional perturbations which are only linear in the dimension of the structures.The samplers based on low dimensional perturbations involve calculating expectations of the value of the MAP solution after perturbations. In this paper we give a statistical characterization of this value. In particular, we prove new measure concentration inequalities that show the expected perturbed MAP value can be estimated with high probability using only a few random samples. This is an important ingredient to construct an alternative to MCMC in the data-knowledge domain that relies on MAP solvers. The key technical challenge comes from the fact that the perturbations are Gumbel random variables. Since the Gumbel distribution is continuous, the MAP value of the perturbed potential function is unbounded and standard approaches such as McDiarmid's inequality do not apply. Instead, we derive a new Poincaré inequality for the Gumbel distribution, as well as a modified logarithmic Sobolev inequality using the approach suggested by , as described in the monograph of . These results, which are of general interest, also guarantee that the deviation of the sampled mean of random MAP perturbations from their expectation has an exponential decay.\nNotation:: Boldface will denote tuples or"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do magnetic fields play in guiding electrons within the MAC-E filter, and how do the main findings of this research relate to its methods and key results?",
      "expected_output": "Magnetic fields play a crucial role in guiding electrons within the MAC-E filter by confining them in cyclotron motion along the magnetic field lines and transforming their transverse momentum into longitudinal momentum. The main findings of this research relate to the development of an analytical description of the response function of the KATRIN experiment, which includes the transmission function of the MAC-E filter and considers energy loss in the source. The research uses the adiabatic approximation to describe the electron motion and derives a detection rate formula that incorporates the response function and spectrometer potential.",
      "context": [
        "2-5277-4447-8080-5338b25c1c17}}\nMAC-E filter principle: The electrons emitted isotropically from tritium -decay in the gaseous source are guided adiabatically by magnetic fields. In the forward direction the -electrons are confined in cyclotron motion along the magnetic field lines towards the MAC-E filter. Along their path to the analyzing plane (central plane) of the spectrometer, the magnetic field strength decreases by several orders of magnitudeThe KATRIN main spectrometer employs a set of air coils to allow fine-shaping of the weak guiding field in the analyzing plane, and to compensate for influences by the earth's magnetic field and solenoid fringe fields , .. Due to the conservation of magnetic moment in a slowly varying field, most of the electrons' transverse momentum is adiabatically transformed into longitudinal momentum. With a high negative potential ({{formula:e2495c3b-4099-4ccf-a472-5a5a7d9b8ce0}} , corresponding to the endpoint energy of tritium) at its center and most of the electron momentum being parallel to the magnetic field",
        "rons through the MAC-E filter is an important characteristic of the measurement and a significant part of the response function. In the simplest case, one can assume that electrons enter the MAC-E filter with an isotropic angular distribution and propagate adiabatically towards the detector. In the discussion here we apply the adiabatic approximation (see eq:response:adiabaticapprox below), which is fulfilled in the case of KATRIN.In general, an electron from the source will reach the detector if the momentum {{formula:ab05dc05-28c3-4d1a-95f2-337cd72c1b78}} parallel to the magnetic field lines (or the corresponding fraction {{formula:ea43c674-5bcb-491f-9e8f-c5951dffc7c1}} of the kinetic energy) is always positive. The transformation of transverse to parallel momentum and back in a slowly varying magnetic field {{formula:989f72f5-5275-4e9d-8b67-cf9a13fcecef}} is governed by the following adiabatic invariant (which corresponds to the conserved orbital momentum",
        "6T, the polarization of the tritium nuclei can be neglected.. The emitted electrons are guided by magnetic fields through the spectrometer. The detection rate at the detector for a given spectrometer potential {{formula:1c7d6547-c705-496c-87e3-7e1f17ca78da}} can be expressed as: {{formula:804f7fa3-7748-4187-a8f8-408ea6e92934}}where the factor of {{formula:05414167-9782-4f8e-868e-40668e901b07}} incorporates the fact that the response function {{formula:272a399b-c087-4e83-b565-324938cc7d9e}} only considers electrons emitted in the forward direction.In the following, an analytical description of the response function of the KATRIN experiment will be laid out. At first, we derive the transmission function of the MAC-E filter that is implemented by the main spectrometer (sec:transmission). In sec:eloss we consider energy loss in the source and develop a first description of the response function"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between Psi_CL, phi, and psi_i in the context of this research paper?",
      "expected_output": "The resultant HE state of the coin-lattice system, denoted as Psi_CL, is a superposition of two states, 0*psi_0 and e^(i*phi)*1*psi_1, where phi is a relative phase factor and psi_0 and psi_1 are the conditional states of the lattice when the state of the coin is 0 and 1 respectively. The relative phase factor phi is mainly decided by the value of delta chosen for the initial coin state and the number of time steps taken in the quantum walk.",
      "context": [
        " The resultant HE state of the coin-lattice system can be written in the form{\\Psi }_{CL} = \\frac{1}{\\sqrt{2}}\\left({0}{\\psi _0} + e^{i \\phi } {1}{\\psi _1}\\right),where \\phi \\in \\lbrace 0, \\frac{\\pi }{2}, \\pi \\rbrace is a relative phase factor and {\\psi _0} and {\\psi _1} are the conditional states of the lattice when the state of the coin is {0} and {1} respectively. The relative phase factor \\phi is mainly decided by the value of \\delta chosen for the initial coin state and the number of time steps taken in the quantum walk. Moroever, the states {\\psi _0} and {\\psi _1} are either completely real or completely imaginary. It should be noted that the only reason we use {\\psi _i} to denote the conditional states of the lattice and not {\\alpha _i} is because these states are not exactly coherent states.The state {\\psi _0} can be evaluated by applying the projector {0} \\otimes",
        " REF -REF , we show the behaviour of states of the lattice when \\delta = \\frac{\\pi }{2} for the initial state of the coin. In Fig. REF , we see that after t = 1 time step, both the states of the lattice are in phase with each other. However, both of the states have complex coefficients. In this case, the HE state can be written as,{\\Psi }_{CL} = -\\frac{1}{\\sqrt{2}} \\left({0}{\\psi _0} + {1}{\\psi _1}\\right),where the state now picks up an overall negative global phase factor and the states {\\psi _i} are now complex. This is an example of behaviour Be4.In Fig. REF , we see that after t = 2 time steps, both the states of the lattice are still in phase with each other and have negative coefficients. The state can be written as{\\Psi }_{CL} = -\\frac{1}{\\sqrt{2}} \\left({0}{\\psi _0} + i{1}{\\psi _1}\\right),where the states {\\psi",
        " exactly coherent states.The state {\\psi _0} can be evaluated by applying the projector {0} \\otimes {1}_L to the state {\\psi }_{CL} and then taking a partial trace over the state of the coin. In a similar fashion, we can evaluate {\\psi _1} by applying the projector {1} \\otimes {1}_L.In Fig. REF , we plot the evolution of the conditional states of the lattice ({\\psi _0} and {\\psi _1}), hereafter denoted simply as states of the lattice (lattice states), for different values of time steps t. We observe that the initial coherent state of the lattice (Fig. REF ) splits into two parts, each one corresponding to the two orthogonal coin states. Also, the resultant lattice states {\\psi _0} and {\\psi _1} after 20 time steps retain the form of a coherent state with some additional deformation as the quantum walk evolves, which is evident from Fig. REF . Since the resultant lattice states are very close to coherent states, we define and identify them with coherent amplitudes {\\alpha }_1 and {\\alpha }"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does spatialization contribute to pre-attentive perception in the context of this research, and what methods were used to study its effects?",
      "expected_output": "According to the research paper, spatialization contributes to pre-attentive perception by mapping data to the spatial domain, allowing it to be perceived pre-attentively through inherent visual/spatial entities. The paper proposes a taxonomy that focuses on the spatialization process and pre-attentive stimuli employed by visualization techniques. Methods such as Dimensional Stacking, Worlds-within-Worlds, and Circle Segments were used to study the effects of spatialization, demonstrating improved space utilization and more complex visualization techniques.",
      "context": [
        "a77c-a48a207e0108}}\nProposed Taxonomy: Visualizing data demands a maximization of just noticeable differences. To satisfy this need, visualizations rely on pre-attentive stimuli - characteristics inherent to visual/spatial entities. Therefore, the data must first be mapped to the spatial domain (spatialized) in order to be pre-attentively perceived. Our taxonomy thus focuses on the spatialization process and on the pre-attentive stimuli that are employed by visualization techniques.\nInteraction Techniques: Interaction is an important component for visualization techniques but, differently from former works, we do not incorporate interaction to our taxonomy. In fact, we chose to handle visualization and interaction as disjoint concepts. However, interaction and visual applications present a notable synergy. Therefore, we must clarify the role of interaction techniques in the visualization scene. We define two conditions for identifying an interaction technique:An interaction technique must enable a user to define/redefine the visualization by modifying the characteristics of pre-attentive stimuli; An interaction technique, with appropriate adaptations, must be applicable to any visualization technique.The first condition arises from the direct assumption that interaction techniques alter the state of a computational application. In the",
        "th item. {{figure:df1809e9-4d18-4643-b0c8-42f7075b698b}}A similar approach is applied in techniques such as Dimensional Stacking , Worlds-within-Worlds , Circle Segments , Pixel Bar Charts and the so-called iconic techniques in general. Multiple spatialization cycles allow improved space utilization and result in more complex visualization techniques. Moreover, they define hybrid approaches for composing visualizations that allow for the vast number of techniques found in visualization literature. In such compositions, pre-attention occurs as a function of the visualization focus. Such understanding, coupled with our taxonomical system, can provide additional guidance on new thoughts for data visualization. {{table:3ba693a4-75c1-450e-a04a-bf865d84ce13}} Interaction Techniques Interaction is an important component for visualization techniques but, differently from former works, we do not incorporate interaction to our taxonomy. In fact, we chose to handle visualization and interaction as disjoint concepts. However, interaction and visual applications present a notable synergy. Therefore, we must clarify the role of interaction techniques in the visualization scene. We define two conditions for identifying an interaction",
        " technique does not satisfy our conditions. It is an application dependent automation that can be implemented only when brushing and multiple visualization techniques share a visualization environment. Conclusions We illustrate the proposed taxonomy in Table REF , which shows the categorization of some well-known visualization techniques. In proposing this taxonomy we focused on generalizing the rationale of how visualization scenes are engendered and how they are presented to our cognitive system. Such a general characterization results in a taxonomy that does not rely on specific details on how techniques operate. Rather, it considers their fundamental constituent parts: how they perform spatialization and how they employ pre-attentive stimuli to convey meaning. Our claim is that such an approach is required to gain a general understanding of the visualization process. Existing taxonomies categorize techniques based on diverse and detailed information on how techniques perform a visual mapping. This diversity and detailing (refer to Section ) include, e.g., axes arrangement (“stacked techniques\"), specific representational patterns (“iconic and pixel-oriented techniques\"), predisposition of representativeness (“network and tree techniques\"), dimensionality (“2D/3D techniques\") and interaction (“static/dynamic techniques\"). Although such approaches can suit"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding jet formation when bubbles collapse near porous boundaries, and how do the methods used in this research contribute to the understanding of this phenomenon?",
      "expected_output": "The main findings regarding jet formation when bubbles collapse near porous boundaries show that strong jets can propagate through holes in the porous material, producing long regions of entrained vapor. The methods used in this research, including the boundary element method and high-speed recordings, contribute to the understanding of this phenomenon by allowing for the quantification of asymmetry in bubble collapse and the prediction of jet direction and behavior in complex geometries.",
      "context": [
        "695-4b13-81b1-73182a9cd13b}} is the area of the equivalent solid element. This method assumes that the shape of the holes doesn't matter, and that the holes are small enough that any difference in horizontal position is negligible.After scaling the areas, the boundary element method solution can proceed as usual, using the reduced {{formula:d65495c2-a1c6-4e27-983d-48fda7461b85}} for both the boundary conditions and pressure integration. We have previously presented the core boundary element method procedure and anisotropy computations in detail.\nBubbles close to porous boundaries: Bubbles collapsing in close proximity to porous boundaries show a broad range of interesting dynamics. In this section, frames from high-speed recordings of five bubble collapses are presented, demonstrating some of these dynamics.When bubbles collapse close to solid boundaries, they produce strong jets that often impinge on the boundaries. When the bubble is nucleated above a hole (positioned above the orange circles in figure REF ), these jets can propagate through the hole, producing very long regions of entrained vapour. One such example is shown in figure REF . In this figure,",
        "c997fa646}}{{figure:6d4f403d-ef32-4afb-8adc-ec3530da8e2a}}{{table:cc774149-ebea-4702-a972-b77f81003aa0}}\nNumerical model: Bubbles collapsing in complex geometries experience varied degrees of asymmetry. This asymmetry can be quantified with the `anisotropy parameter' {{formula:720df633-7dc8-41c2-95b6-a94e9e439668}} , a dimensionless equivalent of the Kelvin impulse, which can predict several bubble collapse properties , , .We have previously presented a numerical model, based on the boundary element method, capable of predicting the anisotropy parameter for arbitrary rigid geometries . This model assumes that the bubble can be treated as a fixed three-dimensional point sink in potential flow with a strength depending on the bubble radius and radial velocity of the bubble surface which are calculated by numerically solving the Rayleigh-Plesset equation. Nearby boundaries are modelled by a distribution of point sink elements and the no-through-flow boundary condition is imposed at their",
        " found in numerous physical systems. Typically this involves bubbles collapsing in proximity to various boundaries. The high surface pressures and shear stresses generated by collapsing bubbles can damage, or clean, the boundary. This cleaning effect can be harnessed through processes such as ultrasonic cleaning which can involve complex geometries .Single bubbles collapsing near simple geometries, such as flat rigid boundaries, have been widely investigated. Much of this research has focused on understanding bubble morphology and jetting , , nearby surface shear stress , surface pressure , , and surface damage .There has been a recent effort to characterise single bubble collapse in a range of complex geometries. For example, characterising jet direction in a selection of geometries including in concave corners , inside rectangular and triangular prisms , above slots , and in the corner of a wall and a free surface . Bubble morphology, flow properties, and jetting behaviours have also been investigated in combinations of concave corners and free surfaces , , between two parallel rigid boundaries , , inside a slot , on a convex corner , on a crevice , and on ridge-patterned structures , . Nevertheless, there remain fundamental complex geometries that have yet to be explored.Porous materials are a large family of complex geometries"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used to find 3D knot embeddings that match target images, and what are the key results of this research?",
      "expected_output": "The methods used to find 3D knot embeddings that match target images include a differentiable rendering algorithm and an invertible neural network to represent knot embeddings. The key results of this research show that the approach can effectively learn knot embeddings whose projected images closely resemble target silhouette images, and the 3D knot embeddings satisfy physical constraints such as being free of self-intersection and lying within a predefined region in space. For example, the research demonstrates the effectiveness of the approach through scenarios like a person dancing, where the animation of the knot embedding creates a perception of movement.",
      "context": [
        " time instant, the rendering of the knot embedding is similar to the target image at the corresponding time instant. In this case, additional regularization is added to ensure smooth spatio-temporal deformation of the knot. In Fig. REF (b), all the knots are constrained to be smooth temporal deformations of each other. The animation depicting this temporal deformation of the knots creates the perception of a person dancing. Thus, in this case, the perceptual art also has a temporal aspect. The animation is available in the supplementary material.Our approach learns knot embeddings whose projected images closely resemble the target silhouette images in all the scenarios, proving the effectiveness of our approach. The knot-based tube embeddings when rendered in 3D, were also found to satisfy the physical constraints specified. More results along with the time evolution of the 3D knot embeddings during the optimization process are available in the supplementary material. {{figure:754fe1f5-ca39-4dd8-bd0e-b6670e276d0d}}\nAblation Study of Loss Functions: We conduct experiments to investigate the importance of each loss function proposed in our approach. Since image loss is the primary objective, keeping it weight fixed at w",
        "pling layers) and \\text{width}=512 (size of the hidden layer in S and T inside each Affine Coupling layer). The optimization was done using the Adam optimizer with a learning rate of 10^{-5}.\nNetwork Ablation: Fig. REF shows the 3D embeddings and renderings for network ablations on the considered target image. The effect of varying the network depth and width on the expressivity of the network is observed.\nVideo Animations: Videos have been included along with the supplementary which depict the results on the following two scenarios:Dancer Video. Given a video of a person dancing as the target input, the spatio-temporal embedding of the knot produced by our approach is displayed in the video from both side view and front view. Evolution of Knot Configuratioan during training. The evolution of the knot embedding during the optimization process is shown for two target images - Ironman and Bunny.",
        "# arXiv Paper: 2307.08652\n\nSearch Me Knot, Render Me Knot: Embedding Search and Differentiable Rendering of Knots in 3D\nabstract: We introduce the problem of knot-based inverse perceptual art. Given multiple target images and their corresponding viewing configurations, the objective is to find a 3D knot-based tubular structure whose appearance resembles the target images when viewed from the specified viewing configurations. To solve this problem, we first design a differentiable rendering algorithm for rendering tubular knots embedded in 3D for arbitrary perspective camera configurations. Utilizing this differentiable rendering algorithm, we search over the space of knot configurations to find the ideal knot embedding. We represent the knot embeddings via homeomorphisms of the desired template knot, where the homeomorphisms are parametrized by the weights of an invertible neural network. Our approach is fully differentiable, making it possible to find the ideal 3D tubular structure for the desired perceptual art using gradient-based optimization. We propose several loss functions that impose additional physical constraints, enforcing that the tube is free of self-intersection, lies within a predefined region in space, satisfies the physical bending limits of the tube material and the material cost"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on EOSs and their impact on relativistic star equilibrium, and how do the methods used in the study contribute to the key results?",
      "expected_output": "The main findings of the research on equations of state (EOSs) and their impact on relativistic star equilibrium are that the dependence of the mass-shedding angular velocity on the EOSs is determined from that of the non-magnetized case. The study used four kinds of realistic EOSs, namely SLy, FPS, Shen, and LS, and found that the stars with Shen (FPS) EOS reach the mass-shedding limit at the smallest (largest) angular velocity. The methods used in the study, such as solving master equations numerically using the Kiuchi-Yoshida scheme, contribute to the key results by allowing the researchers to investigate equilibrium sequences of relativistic stars containing purely toroidal magnetic fields and to determine the effects of the local change of the pressure on the important global properties of the strongly magnetized equilibrium star.",
      "context": [
        " to showing numerical results. Summary and discussion follow in Sec. . In this paper, we use geometrical units with {{formula:35bcaca0-c071-4cce-a9bf-0c2d0ddd2cd3}} .\nSummary of basic equations: Master equations for the rotating relativistic stars containing purely toroidal magnetic fields have been derived in . Hence, we only give a brief summary for later convenience.Assumptions to obtain the equilibrium models are summarized as follows ; (1) Equilibrium models are stationary and axisymmetric. (2) The matter source is approximated by a perfect fluid with infinite conductivity. (3) There is no meridional flow of the matter. (4) The equation of state for the matter is barotropic. Although we employ realistic EOSs in this paper, this barotropic condition can be maintained as explained in subsection REF . (5) The magnetic axis and rotation axis are aligned.Because the circularity condition (see, e.g. ) holds under these assumptions, the metric can be written, following and , in the form, {{formula:3749b13b-a329-46a3-9e30",
        "# arXiv Paper: 0904.2044\n\nRelativistic stars with purely toroidal magnetic fields with realistic equations of state\nabstract: We investigate equilibrium sequences of relativistic stars containing purely toroidal magnetic fields with four kinds of realistic equations of state (EOSs) of SLy (Douchin et al.), FPS (Pandharipande et al.), Shen (Shen et al.), and LS (Lattimer & Swesty). We numerically construct thousands of equilibrium configurations. Particularly we pay attention to the equilibrium sequences of constant baryon mass and/or constant magnetic flux, which model evolutions of an isolated neutron star. Important properties obtained in this study are summarized as follows ; (1) The dependence of the mass-shedding angular velocity on the EOSs is determined from that of the non-magnetized case. The stars with Shen(FPS) EOS reach the mass-shedding limit at the smallest(largest) angular velocity, while the stars with SLy or Lattimer-Swesty EOSs take the moderate values. (2) For the supramassive sequences, the equilibrium configurations are found to be generally oblate for the realistic EOSs in sharp",
        "OS. For the super-strong magnetic fields of {{formula:8a62df86-aa18-44af-9cea-e88ffca7b5ab}} , there appear two counter effects in the EOSs, namely the stiffening and softening due to the anomalous magnetic moments of the nucleons and to the Landau quantization, respectively . Since the equilibrium configurations are determined by the subtle local balance of the pressure gradient, vs. predominantly the gravitational force (plus the Lorentz force and the centrifugal force), it is by no means a trivial problem to see the effects of the local change of the pressure on the important global properties of the strongly magnetized equilibrium star, such as the mass and the radius. To answer this important problem, it is indispensable to incorporate the magnetic corrections to the EOSs, however beyond scope of this paper.\nSummary: In this study, we have investigated equilibrium sequences of relativistic stars containing purely toroidal magnetic fields with four kinds of realistic EOSs of SLy, FPS, Shen, and LS, which have been often employed in recent MHD studies relevant for magnetized neutron stars. Solving master equations numerically using the Kiuchi-Yoshida scheme,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What effect does Pb substitution have on the structure and properties of Bi-O pockets in Bi2223, and how does this impact the overall performance of the material?",
      "expected_output": "Pb substitution in Bi2223 significantly affects the structure and properties of Bi-O pockets. With 20-25% Pb substitution, the interaction between Cu-O and Bi-O4 planes is reduced, but Bi/Pb-O states still contribute around point Γ. At 26% Pb or more, the interaction between Cu-O and Bi-O4 planes is almost null, resulting in minimal contribution of Bi-O states and no Bi-O pockets on the Fermi surface. This impacts the material's electronic properties, as seen in the band structures of Pb-free and Pb-doped Bi2223, which show differences in the presence and absence of Bi-O pockets.",
      "context": [
        " reported , , , , . The difference with experiment is clear. {{figure:2b3f7307-4d4a-4318-8e56-51fab49ead2f}}In Figs. REF and REF two regions are distinguished, the gray region represent the presence of Bi-O pockets in the FS and the white area indicates the absence of them. In these regions the behaviour of the structural properties clearly shows notable differences associated to the presence (or absence) of the Bi-O pockets.\nElectronic properties: The electronic properties of Pb-free and Pb-doped Bi2223 are different in three regions. In the first, Pb-free Bi2223 shows the presence of Bi-O pockets in the FS due to the interaction between Cu-O and Bi-O4 planes by the mediation of O3, in total disagreement with the experimental reports . In the second, the Pb substitution (20-25 %) on the Bi sites in Bi2223 produces a significant reduction in the interaction between Cu-O and Bi-O4 planes but still the contribution of the Bi/Pb-O states at {{formula:02dc05f8-8307-4de3",
        " the Bi/Pb-O states at {{formula:02dc05f8-8307-4de3-94e1-6ed6fbace2cf}} around point {{formula:0521921a-fcb8-4518-a074-06d484b5797d}} remains. The third one, containing 26% Pb or more, shows an almost null interaction between the Cu-O and Bi-O4 planes and, consequently, the contribution of the Bi-O states at {{formula:328e5b58-87db-4dc8-bea1-9caf8ee02aca}} is minimal and the Bi-O pockets are not observed on the FS. The band structures of Pb-free and Pb-doped Bi2223 with Pb concentration of {{formula:12ceab0c-6bae-461a-9154-13dccb1245bd}} 0.0 (Undoped), 0.25 (25% Pb) and 0.26 (26% Pb) are shown in Fig. REF . {{figure:6f211bc3-c713-471e-8f",
        " However, the nature of Bi-O planes character for Bi-2223 needs to be tested with more experiments.We reproduce the calculations reported by H. Lin et al. and effectively it was found that for Pb doping concentration equal to or more than 22% the Bi-O bands move toward higher energies avoiding its contribution at {{formula:7be01d0d-ef8e-4627-9121-eed48fc2773c}} on the point {{formula:78bfc004-cfd5-46e1-a86b-0cc6fb666767}} (no Bi-O pockets in FS). Although it was also found in this case that, as in Bi2223, a slightly metallic character in the Bi-O bonds due to contribution of Bi-O states at {{formula:1b85c26f-8770-4d9e-9236-92bb56f53c73}} on {{formula:23b07e21-7609-437d-9d92-afc3656adab1}} -X direction is observed, in disagreement with the Bi2212 experimental reports. Thus, Pb doping in Bi2212 and Bi"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on Born-Infeld Lagrangian in the context of non-linear electrodynamics, and how do the methods used relate to the key results?",
      "expected_output": "The research on Born-Infeld Lagrangian in the context of non-linear electrodynamics reveals that it is a Lorentz-invariant Lagrangian that can be expressed in terms of two invariants of Maxwell's tensor. The methods used involve expressing the Lagrangian explicitly and analyzing its properties, such as the propagation properties of various non-linear generalizations of electromagnetism. A key result is that the Born-Infeld Lagrangian can be obtained from a certain choice of parameters, and it has been found to be related to string and brane theories. For example, the string Lagrangian in higher dimensions is a generalization of the geodesic equation for a point-like particle, which is connected to the Born-Infeld electrodynamics.",
      "context": [
        " published by now celebrated version of non-linear electrodynamics, in which they proposed the following Lorentz-invariant Lagrangian: {{formula:7b03d550-ec50-442f-88f5-e08dc658c68a}}The constant {{formula:d4d95382-25da-44cb-acda-304dd7b10d92}} appears for dimensional reasons, and plays the same rôle here as the limiting value of the electric field in G. Mie's non-linear electrodynamics.When expressed in terms of two invariants of Maxwell's tensor, {{formula:5d4dc58e-7736-44a2-b06f-81769b27b022}}this Lagrangian can be written explicitly as {{formula:b56ab4ca-d44f-4647-b20f-13903ca8adc1}} {{formula:f786be91-8f9f-41d1-8e4b-522a308dae5d}}With the advent of Quantum Mechanics and Dirac's equation for the electron, the interest in classical models",
        "9145-a31e18c76f8a}}with {{formula:806334e2-0ed2-492a-b588-8f3e8e5102a1}} being a certain dimensional parameter characterizing the strength of the non-linearity. When expressed in four dimensions in terms of the Maxwell tensor, it becomes {{formula:a68c298f-0cb2-472f-a3f3-6565ee869d2e}}In terms of the invariants {{formula:02a812cd-af50-4794-8218-40c4316235fc}} and {{formula:99b728f9-ae7e-4e54-93bf-f8d38a7c61b6}} this Lagrangian is given by {{formula:50355d4f-571b-48c5-a758-f953bb94fd23}} , which for the choice {{formula:58c3b4a6-5043-4e28-bf4c-ced6a175a2fb}} yields essentially the square of the Born-Infeld Lagrangian. The",
        "2b364d00-628c-43de-b13f-28af74351aa9}} .The same is true for the Born-Infeld non-linear electrodynamics. A more exhaustive discussion of the propagation properties of various non-linear generalizations of the electromagnetism can be found in recent papers ( , ).An entirely new and unexpected impulse for the revival of interest in the Born-Infeld electrodynamics, and in its non-abelian generalizations, came from recent developments of the string and brane theories. The string Lagrangian in (4 + D) dimensions, which defines a minimal surface in a (4+D)-dimensional Minkowskian space-time, is in fact a generalization of geodesic equation for a point-like particle.Consider a two-dimensional surface with cylindrical topology, parametrized with one time-like and one space-like parameter, {{formula:79d982c8-9bfd-44c3-8708-8ba3654afe82}} and {{formula:b6e94d2b-fc30-40fd-9403-5cad4af92c"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the research paper use to filter out low frequency noise in SDI, and what are the key findings regarding its effectiveness?",
      "expected_output": "The research paper uses two methods to filter out low frequency noise in SDI: applying a negative second derivative to the data, which is equivalent to weighting the Fourier components by a factor, and using a discrete SDI with a lattice spacing. The key findings are that the first method tends to result in unusable spectra due to high frequency noise, while the second method allows for high-quality images by choosing an appropriate value for the weighting factor, but may introduce noise from periodic peaks at higher frequencies.",
      "context": [
        "This weighting factor {{formula:bacafaab-2501-4532-8b8b-5903f298483d}} is illustrated by the dashed green curve in Fig. 1(b). It is easy to see why filtering with {{formula:85166749-29f2-4354-8a94-a6c678cf813e}} is appealing: it has the obvious advantages of suppressing the low frequency background, and sharpening spectral features by giving extra weight to their high frequency components. However, weighting the high frequency noise-dominated region in this way tends to result in completely unusable spectra, as will be shown below in Fig. 2(b,f).In practice, it is therefore more common to perform a discrete SDI. If the `dx' infinitesimals in the derivative operator are discretized on a lattice with characteristic spacing {{formula:e82044e5-73c4-4646-8f6b-e3f351fbd502}} , the negative second derivative can be described as: {{formula:9d24ff43-50e8-4e08-9c65-50f296106",
        " The graphene Dirac band is shown as a line with constant slope in the log-linear plot, representing the Fourier transform of a Lorentzian with the experimentally observed momentum-axis contour (width and amplitude). A similar linear trend is expected for any band with non-zero slope, though non-Lorentzian line shape factors such as resolution broadening will create deviations from a perfect linear trend.\nII.B The SDI filter in Fourier space: Applying a negative second derivative to the data is equivalent to weighting the Fourier components by a factor of {{formula:cd4011c5-e46f-4148-ae8b-2d5f82eae594}} . That is to say, given the Fourier decomposition: {{formula:1d6442ca-6a05-418e-8bcb-0e5bc5f3c131}}the SDI function is simply {{formula:3ad8dd1a-32f5-4844-b14e-1cd823863795}}This weighting factor {{formula:bacafaab-2501-4532-8b8b-59",
        "formula:9d24ff43-50e8-4e08-9c65-50f29610612e}}Considering this in Fourier space, we have: {{formula:87656c56-9cda-42c2-be59-78b3e0fb25f3}}which has the same form as Eq. REF , but with a new weighting factor {{formula:01f22f46-5a20-42b4-8601-4a04293d4051}} . This filter does not explode at high frequencies, and allows high quality images to be achieved by choosing an appropriate value of {{formula:2e61be35-3179-4130-8137-e700c3e81a12}} , such that the first peak of the function overlaps with the good statistical region of the data (see magenta curve in Fig. 1(b)). However, the periodic nature of the {{formula:8d9ae261-1875-45d3-9f41-9f9bbd3bbe03}} function results in peaks at higher frequencies, which can be expected to contribute noise to the SDI"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the SF formalism in the research paper preserve symmetry when using domain-wall fermions, and what are the key findings related to this method?",
      "expected_output": "The SF formalism with domain-wall fermions preserves symmetry in the same way as the overlap Dirac operator, as the symmetry is exactly the same. The key findings include the construction of a Hermitian massive Dirac operator for two flavors, where the phase is absorbed into the fermion field, although this breaks flavor symmetry. The research also proposes a new procedure to introduce SF Dirichlet boundary conditions for general fermion fields, focusing on the explicit breaking of chiral symmetry by the boundary condition.",
      "context": [
        " a massive Dirac operator which has the same phase as the massless one for a single flavor case. For two flavors we can construct a Hermitian massive Dirac operator, where the phase is absorbed into the fermion field. However we should notice that the flavor symmetry is broken in this two flavors formulation. The SF formalism with the domain-wall fermion can be formulated in the same way since the symmetry is exactly the same as the overlap Dirac operator .\nAcknowledgement: I would like to thank M. Lüscher for his valuable suggestions and discussions. Without his suggestions this work would not have completed. I also thank to R. Sommer, S. Aoki, O. Bär, T. Izubuchi, Y. Kikukawa and Y. Kuramashi for valuable discussions.",
        "3-8660-67c245ed47f8}}(ii) The massless SF Dirac operator {{formula:bae2b6a8-cb83-4890-a457-d7316236ebea}} breaks “chiral symmetry” under (REF ) explicitly by the projection {{formula:e4cd9110-c321-4306-af8d-46fe902ab479}} . (iii) We have a Hermiticity relation for this Dirac operator {{formula:b2e8e284-65ab-4f9b-86b4-3265524de403}}(iv) This Dirac operator connects the same Hilbert sub-space {{formula:65b3cf97-6250-456f-ab16-41abcd7de4e1}}\nConclusion: In this paper we propose a new procedure to introduce the SF Dirichlet boundary condition for general fermion fields. Instead of cutting the Dirac operator at the boundary we focus on a fact that the chiral symmetry is broken explicitly in the SF formalism by the boundary condition and adopt it as a criterion of the procedure. We also notice that an",
        "dd1682e}} is an anti-symmetric step function (REF ) on lattice.However we encounter a problem with this naive definition of mass term, since the massive Dirac operator does not satisfy the “Hermiticity” relation (REF ). The phase of the Dirac determinant becomes mass dependent although it is still irrelevant {{formula:7bf1cacd-49a1-4796-ae00-2b981c505383}} term. In order to avoid this unpleasant situation we may need even numbers of flavors.For two flavors case we define the two by tow Dirac operator as {{formula:79ee1818-465f-48b1-b222-f396f792dcb6}}where {{formula:de59515b-16e7-4c9b-8066-8f4a313b20e9}}A “Hermitian” relation can be found for this two flavors Dirac operator as {{formula:e0eca214-b0b0-44d0-ab58-1eb87d435901}}where {{formula:0ff6cad3-281d-4a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of small positive perturbations on the ground state of a Hamiltonian system, and how do the methods used in this research paper support these findings?",
      "expected_output": "The research paper finds that small positive perturbations in the Hamiltonian system can be approximated well using perturbation theory, allowing for the use of a specific ansatz state in a successful MB-VQE. However, if the perturbations are negative or include other operators, a superposition state with specific parameters is needed. The paper supports these findings by describing a generic rotation that can be implemented in a MBQC fashion, enabling the algorithm to find the best ansatz state within a degenerate subsector.",
      "context": [
        " perturbation {{formula:5c6771c8-64e6-40e6-8f50-a5f76210684c}} in Eq. (1) are small and positive, {{formula:f8522aa5-5927-46b9-a6ce-276a708d55c3}} approximates well the ground state of the perturbed Hamiltonian {{formula:70d4e850-4387-4b95-9898-3d28a540884f}} (this can be proven with perturbation theory). As such, {{formula:cfbaa6a8-52f5-4448-b09b-874b3c252d7d}} can be used as ansatz state in a successful MB-VQE. However, if some of the weights {{formula:c9e5cf9f-1ac0-458d-b84b-de13fe80504b}} are negative and/or the perturbation {{formula:eb67cf40-15ee-44f2-a580-d11141367cf5}} is changed to include other operators",
        "-15ee-44f2-a580-d11141367cf5}} is changed to include other operators – like {{formula:e357f246-95dc-4f86-b8e2-15cd7198e27a}} – then {{formula:4ebfe10d-0971-4c3a-949c-7991b45450c1}} is not a good ansatz state anymore. Generally, the best ansatz state becomes a superposition {{formula:acc24f17-47bb-4216-b983-d894ec417971}} , with {{formula:0452eddc-7ea4-41f5-b9d2-5458b16758c4}} ({{formula:afb3e559-fe3c-4d8d-be19-85d31a7fb747}} ). To create this superposition, it is possible to use the generic {{formula:789a7ac6-335c-49b4-abcc-1d4e470e7578}} rotation described in Ref. , with the logical-{{formula:8503",
        "1d4e470e7578}} rotation described in Ref. , with the logical-{{formula:8503f078-4a13-49d2-b116-6b26b5b4f97f}} and logical-{{formula:ad901c57-e95e-444c-9af8-a1be9e46054c}} operators as generators. Importantly, this {{formula:054c2b1f-e534-4a94-9f1a-46ae6a54d3d0}} rotation can be implemented in a MBQC fashion using the tools described in Ref. . The seven parameters describing this {{formula:3105f474-e19c-4f43-91c4-420b1ad13fec}} rotation can then be used as input parameters in the MB-VQE, such that the algorithm itself becomes capable of finding the best ansatz state within the degenerate subsector containing the four logical states.\nExamples of Measurement Patterns: In this section, we summarize the results of Ref. which are relevant to the proposals put forward in the main text. In particular, we review"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do polynomial coordinates contribute to achieving cage-aware deformations in the research paper, and what are the key findings related to this method?",
      "expected_output": "The polynomial coordinates contribute to achieving cage-aware deformations by enabling the transformation of input polynomial curves into polynomial curves of any order. The key findings related to this method include the extension of classical 2D Green coordinates to define conformal polynomial coordinates for 2D closed high-order cages, allowing for cage-aware conformal harmonic deformations. For example, when n_s=2, the formulation of alpha and beta values, such as alpha_0 and beta_0, are defined as alpha_0=(c_0-eta)c_1^perp and beta_0=(c_0-eta)c_1, respectively. These findings enable users to manipulate Bezier control points to easily generate the desired deformation.",
      "context": [
        "# arXiv Paper: 2408.06831\n\nPolynomial 2D Green Coordinates for High-order Cages\nabstract: We propose conformal polynomial coordinates for 2D closed high-order cages, which consist of polynomial curves of any order. The coordinates enable the transformation of the input polynomial curves into polynomial curves of any order. We extend the classical 2D Green coordinates to define our coordinates, thereby leading to cage-aware conformal harmonic deformations. We extensively test our method on various 2D deformations, allowing users to manipulate the \\Bezier control points to easily generate the desired deformation.\nThe Formulation of : When n_s=2, we have:\\alpha _0=(c_0-\\eta )c_1^{\\perp }, \\alpha _1=2(c_0-\\eta )c_2^{\\perp }, \\alpha _2=c_1 \\cdot c_2^{\\perp }.\\begin{aligned}&\\beta _0=(c_0-\\eta )c_1, \\beta _1=\\Vert c_1\\Vert ^2+2 (c_0",
        " )c_1, \\beta _1=\\Vert c_1\\Vert ^2+2 (c_0-\\eta )\\cdot c_2,\\\\ &\\beta _2=3 c_1\\cdot c_2, \\beta _3=2 \\Vert c_2\\Vert ^2. \\end{aligned}When n_s=3, we have:\\begin{aligned}&\\alpha _0=(c_0-\\eta )c_1^{\\perp }, \\alpha _1=2(c_0-\\eta )c_2^{\\perp },\\\\ &\\alpha _2=3(c_0-\\eta )\\cdot c_3^{\\perp }+\\frac{4}{3}c_1 c_2^{\\perp }+\\frac{1}{3}c_2 c_3^{\\perp },\\\\ &\\alpha _3=2c_1 c_3^{\\perp }, \\alpha _4=c_2 c_3^{\\perp }. \\end{aligned}\\begin{aligned}&\\beta _0=(c_0-\\eta )c",
        " }. \\end{aligned}\\begin{aligned}&\\beta _0=(c_0-\\eta )c_1, \\beta _1=\\Vert c_1\\Vert ^2+2 (c_0-\\eta )\\cdot c_2,\\\\ &\\beta _2=3 c_1\\cdot c_2+3(c_0-\\eta )c_3, \\beta _3=2 \\Vert c_2\\Vert ^2+4 c_1 c_3,\\\\ &\\beta _4=5 c_2 c_3, \\beta _5=3 \\Vert c_3\\Vert ^2. \\end{aligned}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of this research paper regarding unobservable category representations and how do they impact the research methodology and key results?",
      "expected_output": "The research paper introduces a method to estimate the structure of human categories by combining ideas from cognitive science and machine learning. The main findings include the development of a method that blends human-based algorithms with state-of-the-art deep image generators, allowing for the estimation of unobservable category representations. This approach enables the capture of human category representations by sampling in deep feature spaces, providing qualitative and quantitative results that demonstrate the feasibility of the method. The key results show that samples drawn from human distributions rival those from state-of-the-art generative models in quality and outperform alternative methods for estimating the structure of human categories. For example, the paper uses classification images and Markov Chain Monte Carlo with People (MCMCP) to estimate the decision boundary and capture mental categories, demonstrating the effectiveness of the proposed method.",
      "context": [
        " into the learned decoder. In addition, because of its simple form, the resulting latent space {{formula:31a74511-6131-4220-a886-462b97e0f552}} tends to be easy to traverse meaningfully (i.e., an intrinsic linear manifold) and can be readily visualized via the decoder, a property we exploit presently. {{figure:366d3c3b-6bec-475e-8b1e-8d0f14b8c068}}\nEstimating the structure of human categories: Methods for estimating human category templates have existed for some time. In psychophysics, the most popular and well-understood method is known as classification images (CI; ).In the classification images experimental procedure, a human participant is presented with images from two categories, A and B, each with white noise overlaid, and asked to select the stimulus that corresponds to the category in question. On most trials, the participant will obviously select the exemplar generated from the category in question. However, if the added white noise significantly perturbs features of the image that are important to making the distinction, they may fail. Exploiting this, we can estimate the decision boundary",
        "# arXiv Paper: 1805.07644\n\nCapturing human category representations by sampling in deep feature spaces\nabstract: Understanding how people represent categories is a core problem in cognitive science. Decades of research have yielded a variety of formal theories of categories, but validating them with naturalistic stimuli is difficult. The challenge is that human category representations cannot be directly observed and running informative experiments with naturalistic stimuli such as images requires a workable representation of these stimuli. Deep neural networks have recently been successful in solving a range of computer vision tasks and provide a way to compactly represent image features. Here, we introduce a method to estimate the structure of human categories that combines ideas from cognitive science and machine learning, blending human-based algorithms with state-of-the-art deep image generators. We provide qualitative and quantitative results as a proof-of-concept for the method's feasibility. Samples drawn from human distributions rival those from state-of-the-art generative models in quality and outperform alternative methods for estimating the structure of human categories.\nIntroduction: Categorization is a central problem in cognitive science and concerns why and how we divide the world into discrete units at various levels of abstraction. The biggest challenge for studying human categorization is",
        " the proposed state. The process then repeats until the chain converges to the target category distribution {{formula:25141a4d-ecbc-41f8-8eca-69a22ffdc5d4}} . In practice, convergence is assessed heuristically, or limited by the number of human trials that can be practically obtained.MCMCP has been successfully employed to capture a number of different mental categories , , and though these spaces are higher-dimensional than those in previous laboratory experiments, they are still relatively small and artificial compared to real images. Unlike classification images, this method makes no assumptions about the structure of the category distributions and thus can estimate means, variances, and higher order moments. Therefore, we take it as a starting point for the current method.\nMCMCP in deep feature spaces: The typical MCMCP experiment is effective so long as noise can be added to dimensions in the stimulus parameter space to create meaningful changes in content. In the case of natural images, noise in the space of all pixel intensities is very unlikely to modify the stimulus in meaningful ways. Instead, we propose perturbing images in a deep feature space that captures only essential variation. Since trials in an MCMCP experiment are not independent,"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how quantum synchronisation affects the efficiency of nanoscale heat engines, and what methods were used to obtain these results?",
      "expected_output": "The main findings of the paper show that quantum synchronization places an upper bound on the achievable steady-state power of nanoscale heat engines, and the efficiency of the engine determines the point at which synchronization vanishes. The researchers used mathematical models and quantum thermodynamics principles to obtain these results, specifically by analyzing the relationship between the power of the heat engine and its phase-locking properties.",
      "context": [
        "# arXiv Paper: 1812.10082\n\nQuantum Synchronisation in Nanoscale Heat Engines\nabstract: Owing to the ubiquity of synchronization in the classical world, it is interesting to study its behavior in quantum systems. Though quantum synchronisation has been investigated in many systems, a clear connection to quantum technology applications is lacking. We bridge this gap and show that nanoscale heat engines are a natural platform to study quantum synchronization and always possess a stable limit cycle. Furthermore, we demonstrate an intimate relationship between the power of a heat engine and its phase-locking properties by proving that synchronization places an upper bound on the achievable steady-state power of the engine. Finally, we show that the efficiency of the engine sets a point in terms of the bath temperatures where synchronization vanishes. We link the physical phenomenon of synchronization with the emerging field of quantum thermodynamics by establishing quantum synchronization as a mechanism of stable phase coherence.\nacknowledgements: Centre for Quantum Technologies is a Research Centre of Excellence funded by the Ministry of Education and the National Research Foundation of Singapore. This research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its Competitive Research Programme (CRP Award No. NRF-CRP14",
        "df498e8c80}} or when two of the three coherences vanish. In the main text, {{formula:c4e2bd8a-fa03-4c2a-a9ae-9bf2b52deed9}} and {{formula:793e5148-ddd3-42dd-9742-7f23ef39d094}} because two of the coherences vanish in the steady state, namely {{formula:4821df07-8eb1-453c-8e5e-d98041a9137f}} .Heat and Work for Quantum Engines.— Let {{formula:a2617156-e8ef-4b57-b097-5cbd8a0c5ddb}} be the Hamiltonian in an arbitrary frame obtained by the unitary {{formula:c2c2c17f-dffa-426d-874f-ff27ec66619e}} , ie. {{formula:89fe46e2-3849-44a5-803c-04353775a594}} , where {{formula:659b3c83-27"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the main role of the SEL method in galaxy surveys and how does it contribute to the research findings?",
      "expected_output": "The main role of the Strong Emission Line (SEL) method in galaxy surveys is to determine metallicities, specifically oxygen abundances, in large galaxy surveys by measuring strong emission lines in H ii region spectra. It contributes to research findings by providing a way to estimate oxygen abundances in distant galaxies where only strong lines are visible against the stellar continuum, although it has its limitations and discrepancies compared to other methods.",
      "context": [
        "{{formula:7aa59df2-6cd7-4f8e-8377-cc424f6a12bc}} times fainter than H{{formula:eb0c8067-b6a6-4f11-9c30-a66b5b494d88}} ) to measure. By measurement of the strong emission lines in the H ii region spectrum calibrated using photoionization modeling , , , , , , or H ii regions for which the oxygen abundance is known from the {{formula:457dc67e-0f16-4243-9e07-4ece30ca2c05}} method , , , , , , , . We will call this the Strong Emission Line (SEL) method, and it is nowadays extensively used to determine metallicities in large galaxy surveys.Each of these methods has its particular problems, and range of application. More importantly, where all methods can be used, the derived O/H abundances often show systematic disagreements between the various methods. These may amount to factors of two or more. In this paper, we seek to resolve some of the causes of these discrepancies by re-deriving the chemical abundances of model H ii",
        "9c-8569-4e37c26293d4}} ). Since then many other such ratios have been proposed. The most widely used of these are the calibrations of and , based on detailed photoionization modelling, and the empirical relations provided by , , and . Both kinds of calibrations strive to improve the accuracy by making use of the [O iii]/[O ii] ratio as ionization parameter, which accounts for the large scatter found in the {{formula:9810861b-b602-453e-ad53-e23e02bd783a}} versus oxygen abundance calibration, which is larger than observational errors Hence, the SEL method is ideally suited to global abundance determinations in distant galaxies, since only the strong lines are visible against the stellar continuum. The abundance scale for ensembles of H ii regions in galaxies has recently been calibrated by . The basic problem with all ratios of optical forbidden lines to hydrogen recombination lines is that they are two-valued as a function of chemical abundance. This abundance ambiguity can only be raised by the simultaneous use of several such ratios. An additional problem is that not all SEL method give similar oxygen abundances: the SEL methods based on calibrations using photoionization",
        " abundances in galaxies whose spectrum is lacking of the [O ii] {{formula:63d80a55-508b-4aad-9aa5-56bb71beefa6}} 3726,3729 emission lines.\nComparison of the SEL Techniques: We have applied all of these commonly-used SEL techniques to our model spectra to derive oxygen abundances, again via a double-blind procedure. The different SEL techniques fall into various classes, depending on which combination of strong line ratios are used. Table REF lists the values of all these parameters derived for each model. This table also includes the value derived for the {{formula:5ebe4e37-42e9-4bdb-89f6-2fdb2e1b9e97}} parameter obtained from the optimal calibration provided by and . Table REF compiles the oxygen abundances derived for each model, indicating which branch (high or low metallicity) the model spectra would fall into when using the {{formula:31831bea-ed5e-44d6-a9b4-aa9fb2076249}} parameter. We considered the values of the oxygen abundances derived"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the invariance principle and Central Limit Theorem apply to the analysis of spiked eigenvalues in this research paper, and what are the key findings related to their application?",
      "expected_output": "The invariance principle and Central Limit Theorem (CLT) are applied to the analysis of spiked eigenvalues in this research paper to derive asymptotical distributions without Gaussian assumption and restrictive assumptions on covariance matrices. The key findings include the establishment of an invariance principle theorem for generalized spiked Fisher matrices, which leads to a CLT for local spiked eigenvalues under mild assumptions on population distribution. This allows for the derivation of power functions of Roy's maximum root test for linear hypothesis in large-dimensional linear models and signal detection tests. The results extend existing works by relaxing the bounded fourth-moment condition to a tail probability condition, making the setting more realistic.",
      "context": [
        "# arXiv Paper: 2203.14248\n\nInvariance principle and CLT for the spiked eigenvalues of large-dimensional Fisher matrices and applications\nabstract: This paper aims to derive asymptotical distributions of the spiked eigenvalues of the large-dimensional spiked Fisher matrices without Gaussian assumption and the restrictive assumptions on covariance matrices. We first establish invariance principle for the spiked eigenvalues of the Fisher matrix. That is, we show that the limiting distributions of the spiked eigenvalues are invariant over a large class of population distributions satisfying certain conditions. Using the invariance principle, we further established a central limit theorem (CLT) for the spiked eigenvalues. As some interesting applications, we use the CLT to derive the power functions of Roy Maximum root test for linear hypothesis in linear models and the test in signal detection. We conduct some Monte Carlo simulation to compare the proposed test with existing ones.\nIntroduction: Motivated by several applications of hypothesis on two-sample covariance matrices and linear hypothesis on regression coefficient matrix in linear models, we consider the following spiked model. Let {{formula:510abf32-25d1-419e-a9ce-ca86",
        " for the generalized spiked Fisher matrix by using a similar but more complicated technique in . With the invariance principle of generalized spiked Fisher matrix, we establish a CLT for the local spiked eigenvalues of a generalized spiked Fisher matrix. As applications, we use the CLT to derive the power functions of Roy maximum root test on linear hypothesis in large-dimensional linear models and the signal detection test.Compared with existing works on spiked Fisher matrices, this work relaxes the bounded fourth-moment condition on population to a tail probability condition, which is a regular and necessary condition in the weak convergence of the largest eigenvalue. Under the new setting, we establish an invariance principle theorem for the generalized spiked Fisher matrix. With the aid of the invariance principle theorem, we further establish the CLT for the local spiked eigenvalues of large-dimensional generalized spiked Fisher matrices under mild assumptions on the population distribution. As a by-product, our results naturally extend the result of to a general case under which we can successfully remove the diagonal or block-wise diagonal assumption on the matrix {{formula:8b808a82-2c1a-4c72-adf1-3a6810166cbc}} . Our setting allows that the",
        "1a-4c72-adf1-3a6810166cbc}} . Our setting allows that the spiked eigenvalues may be generated from the variables partially dependent on the ones corresponding to the non-spiked eigenvalues. Our setting also allows a few pairs of thresholds for bulks of spiked eigenvalues. In summary, our setting is more realistic than the ones in the existing works.The rest of this paper is organized as follows. We establish the invariance principle and the CLT of generalized spiked Fisher matrix in Sections 2 and 3, respectively. We use the CLT to derive the local power functions of Roy maximum root test for linear hypothesis in large-dimensional linear models and the test in signal detection in Section 4. We present some numerical study in Section 5. Some technical proofs are given in Section 6. Additional technical proofs are presented in the supplementary material.\nPhase transition for the spiked eigenvalues: Suppose that {{formula:51fe2e99-cd94-46dc-a66d-19b1757b012b}} , {{formula:023d426a-1859-4447-932c-a7d3d7fa1794}} and {{formula:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do encodings play in improving optimization outcomes in discrete systems, and how do the methods used in this research contribute to these enhancements?",
      "expected_output": "Encodings play a crucial role in improving optimization outcomes in discrete systems by enriching the density of low-energy states. According to the research paper, redundant non-invertible encodings can enhance optimization by increasing the density of low-energy states, making it easier to find the minimum cost solutions. The methods used in this research, such as landscape encoding, contribute to these enhancements by transforming state variables and establishing smooth landscapes to guide local search dynamics towards the ground state. For example, the paper shows that the density of states is enriched by several orders of magnitude in the encoded landscape for the three landscapes and encodings considered.",
      "context": [
        "-cc5c-40bc-9990-0aa4a91af269}} than in {{formula:3b93e1d8-f115-41d8-8966-d2f22cf742c8}} . Here we demonstrate this enrichment of low energy states for three established combinatorial optimization problems and suitably chosen encodings, which we describe after giving a few mathematical prerequsites. We formalize and measure enrichment in terms of densities of states on {{formula:7693ba14-00b0-48e3-980a-f11ee92608eb}} and {{formula:84350516-b343-404a-a5c4-af1f591d9a1d}} . We illustrate the effects of encoding by comparing performance of optimization heuristics on the direct and encoded landscapes.A finite discrete energy landscape {{formula:0320482f-0bbe-4820-8d96-24e7d43040f7}} consists of a finite set of configurations {{formula:1244637d-ad0f-4103-a8d0-6fb0fab4512e}}",
        "# arXiv Paper: 1104.5024\n\nLandscape encodings enhance optimization\nabstract: Hard combinatorial optimization problems deal with the search for the minimum cost solutions (ground states) of discrete systems under strong constraints. A transformation of state variables may enhance computational tractability. It has been argued that these state encodings are to be chosen invertible to retain the original size of the state space. Here we show how redundant non-invertible encodings enhance optimization by enriching the density of low-energy states. In addition, smooth landscapes may be established on encoded state spaces to guide local search dynamics towards the ground state.\n: Landscape encoding: faster optimization in larger spacesKonstantin Klemm Bioinformatics Group, Department of Computer Science, and Interdisciplinary Center for Bioinformatics, University of Leipzig, D-04107 Leipzig, GermanyAnita Mehta S N Bose National Centre for Basic Sciences, Calcutta 700 098, IndiaPeter F. Stadler Bioinformatics Group, Department of Computer Science, and Interdisciplinary Center for Bioinformatics, University of Leipzig, D-04107 Leipzig, Germany Max Plan",
        " function for argument {{formula:274413b3-65ce-4f6b-bbd7-b2277aa2131f}} because {{formula:fc90a9aa-0cde-4dae-98d5-65ce14cb928c}} only changes value where {{formula:c0ae900e-5cb8-4550-a6a7-3791db615431}} also does.The encoding enriches low energy states if {{formula:02446c03-3cb5-41db-a0c3-ae1a32fe687a}} for small {{formula:b769ce67-d4bb-463f-80b4-123d88501861}} . Figure REF shows that this is the case for the three landscapes and encodings considered in this Letter. We find in fact that the density of states {{formula:7d531228-fba9-4571-99c9-df02bf7ee126}} is enriched by several orders of magnitude in the encoded landscape, for all the cases considered. {{figure:1cbc58ee-021"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between girth and 2-distance chromatic number in graphs, and how do the main findings of this research paper support this relationship?",
      "expected_output": "The research paper explores the relationship between girth and 2-distance chromatic number in planar graphs. The girth of a graph refers to the length of its shortest cycle. According to the paper, for planar graphs with girth six, the 2-distance chromatic number is improved by reducing one, as stated in Theorem 1.2. This suggests that as the girth of a planar graph increases, the 2-distance chromatic number may decrease. The paper also mentions that for planar graphs without short cycles, the 2-distance chromatic number is lower, supporting this relationship. For example, Dong and Lin proved that if a planar graph has girth at least 6, its 2-distance chromatic number is at most 4. Overall, the paper's findings indicate that there is a correlation between the girth of a planar graph and its 2-distance chromatic number, with higher girth leading to lower 2-distance chromatic number.",
      "context": [
        "4aa8bb}} if {{formula:17da725e-5676-4dfe-9f3b-e64d7465edcd}} is a planar graph with {{formula:ac5b2086-f711-4651-9810-5b35fe79f35b}} , which confirms the Conjecture REF for the planar graphs with girth six.Our focus will be on planar graphs with {{formula:a9855399-6eb2-4413-8eb8-958689bda601}} . In such a case, we improve the bound of Bu and Zhu by reducing one, and the main result of the paper is the following.Theorem 1.2 If {{formula:88d7f711-6bb9-4b46-bd35-6b99a2a8c056}} is a planar graph with {{formula:ef2e6810-2e9e-4b96-aa86-a3a34ef2943d}} and {{formula:aad7118c-f6ca-46f1-95d5-4d7d",
        "03e61860-0ee7-4d4c-b3ca-aca54ade5a61}} . In general, there are some upper bounds for 2-distance chromatic number of planar graphs. For instance, van den Heuvel and McGuinness showed that {{formula:7bc4a829-6003-43eb-84a3-e750f5fdf74d}} , while the bound {{formula:cee2c688-cce2-4f4f-8b3b-24ea126d516a}} For planar graphs without short cycles, Dong and Lin proved that {{formula:cb2ac836-8ad7-4a51-8a70-21492db2e5f1}} if {{formula:8f433aeb-5742-4156-9517-dee0b04878a7}} . In , Bu and Zhu showed that {{formula:1a3465fc-9c4e-4c72-a86a-a235004aa8bb}} if {{formula:17da725e-5676-4dfe-9f3",
        " colors, and the smallest number of colors for which {{formula:8301ecbe-c987-429c-b315-bef0290f6e47}} admits a 2-distance coloring is known as the 2-distance chromatic number {{formula:1773ec09-f99b-4bfd-9719-95b12bdffe7c}} of {{formula:99dd533f-8710-4ea2-9389-dff14d0dc16f}} .In 1977, Wegner posed the following conjecture.Conjecture 1.1 For every planar graph {{formula:4b222126-dba3-4e2b-b31d-1b0c2be6ce80}} , {{formula:b8b07fdf-ac1f-426f-9c44-8f3ca3812e4c}}The conjecture is still widely open. Thomassen (independently by Hartke et al. ) proved the conjecture for planar graphs with {{formula:03e61860-0ee7-4d4c-b3ca-aca54ade5a61}}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does Theorem 6.13 determine the order of contact and what are its implications for the research findings?",
      "expected_output": "Theorem 6.13 determines the order of contact of the boundaries of the rational tongues in admissible and guided families by assuming certain conditions, such as the family being guided by a holomorphic family and having specific properties near a certain point. The implications of this theorem are that it provides a new approach to understanding the order of contact and has further dynamical insight, connecting the order of contact to the parabolic multiplicity of the guiding family for the first time.",
      "context": [
        " family of cubic rational maps which are antipode preserving.Now we are ready to discuss the main theorem of this section where we derive the order of contact of the boundaries of the rational tongues in admissible and guided families under some assumptions.Theorem 6.13 Let {{formula:37f9c0d5-7d30-42a4-8ad2-2288c4f820cb}} be an admissible family of maps guided by a holomorphic family {{formula:02375280-e0a3-4d70-a407-eab4207e432b}} . We assume that {{formula:00cf2706-130c-463b-8adc-e08e57a2c56d}} with {{formula:e033b52c-a55e-4e56-a530-558472e6465d}} , {{formula:4828f4d2-7803-480f-993b-15368127472a}} and {{formula:6f2a97f6-e006-47fd-bf88-f38f9804e535}} with {{formula:b4db9b70",
        "-95045e25db8d}} and {{formula:8f5afc32-6ead-49a6-a8a1-9674236041eb}} near {{formula:0304a032-2e1a-4137-8982-5bb34a331903}} . This gives the angle of opening for the boundaries of the rational tongues at {{formula:41baf2bc-78d5-448e-a2b8-e13737fd58e2}} and this Theorem REF is discussed in section . In the case of standard family the boundaries of the rational tongues {{formula:5aded4d5-bc09-4cdc-a63f-506cfe39ea52}} have the same tangency and the order of contact of these boundaries is obtained in section using the technique of guided and admissible family. This is completely a new approach to prove the order of contact and this has further dynamical insight. The order of contact of the boundaries of the rational tongues in the standard family is connected with the parabolic multiplicity (see section 3 for definition) of the guiding family for the first time (see Theorem RE",
        "-98c8-296ff53ab37a}} . {{formula:7f27c2c6-19d1-43ae-9d4d-ebd3b56de61f}}In the following discussions we assume that {{formula:3abbf466-ef95-49a1-add2-653315714e92}} is an admissible family guided by a holomorphic family {{formula:4ad7e8c8-ffbf-4756-af81-baecc9f1e900}} . We would try to understand the properties of these families and characterise them according to some properties. First we show that for all {{formula:e664d610-532a-4b65-9967-327108ffccac}} , the map {{formula:fc1d67aa-56cf-450e-baa7-37b427a19104}} has one indifferent fixed point at {{formula:0787baac-e11a-491a-8605-67f0ac7085c5}} .Lemma 6.5 For all {{formula:c92b1e15"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how dwarf galaxy dynamics influence metal-rich star formation, and what methods were used to obtain these results?",
      "expected_output": "The main findings suggest that dwarf galaxy dynamics can lead to metal-rich star formation, as the Galactic tidal force induces radial inflow in the center of the dwarf, triggering nuclear starbursts. This is supported by the research of Bekki and Freeman (2003), who used simulations to model the interaction between a dwarf galaxy and the Milky Way, including effects of star formation in the dwarf dynamics. Their results indicate that efficient dwarf nuclear chemical enrichment can be associated with the origin of observed metal-rich stars in ω Cen.",
      "context": [
        " metallicity.A plausible explanation is that {{formula:b5b2f40c-e0c4-4649-93df-c73f8720b7ae}} Cen is the nucleus of a dwarf galaxy captured and disrupted by the the Milky Way (e.g., Freeman 1993). In this scenario, a dwarf sinks to the center of the Galaxy due to dynamical friction, simultaneously losing mass in the Galactic tidal field. All of the dwarf's mass is eventually stripped but the nucleus, with a mass of {{formula:59271d95-6554-49c7-be52-ec78d6fe281f}} {{formula:4b5dc188-c4d8-4707-924e-39aefff6d480}} , is left on the current orbit of {{formula:c06a233d-649f-4afb-a85c-aec3515b4af2}} Cen. The current angular momentum of {{formula:ddf27482-f8e6-4e47-967f-ee87a815cdea}} Cen is thus inherited from the dwarf galaxy, on a retro",
        " of a nucleated dwarf galaxy with an ancient bulgeless Galaxy whose thin disk has twenty percent of today's thin-disk mass, and included effects of star formation in the dwarf dynamics. They find that the Galactic tidal force induces radial inflow in the center of the dwarf that triggers nuclear starbursts. Bekki and Freeman (2003) conclude therefore that the effecient dwarf nuclear chemical enrichment can be associated with the origin of the observed metal-rich stars in {{formula:6f374945-15a5-43d3-b47f-9d4fccf42a8c}} Cen.\nConclusions: We have studied the dynamical evolution of a falling dwarf galaxy, which in turn is influenced by the strong gravitational shocks from the bulge and disc of the host galaxy. We focused our study on the mass-loss history, and on the orbital migration of the dwarf. The dwarf galaxy sinks toward the center of the host galaxy due to dynamical friction, which depends on the dwarf's mass. Simultaneously, the dwarf is losing mass due to strong bulge and disc gravitational shocks. As a result, the mass-loss history and orbital decay are closely connected and depend in turn on the internal density",
        " dwarf that may lead to the current position of {{formula:17095d40-71b1-41e7-b06a-ee0676b9ca05}} Cen by launching a dwarf on an orbit that started 50 kpc away from the Galactic center. Using a semi-analytical model, he concluded that a progenitor of {{formula:928aacd9-66c8-4470-af6d-b5431bc543a8}} Cen can not decay to its present orbit. He found that strong tidal shocks quickly reduce the mass of the dwarf so that dynamical friction becomes too weak to drag the remnant to the inner regions of the Galaxy. Zhao (2002) concluded that the only possibility to explain {{formula:a09ae031-9147-4f7c-95e3-db3adaba670c}} Cen - phenomenon is that its progenitor was born only 15 kpc away from the Galactic center.Zhao's analysis, however, was simplified in a number of ways. He assumed that the mass outside the tidal radius is instantaneously stripped, and the contribution from the bulge and disc components to the dynamical drag was not"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between BCS condensate amplitude and molecular density, and how do they impact the main findings of this research paper?",
      "expected_output": "The research paper shows that the BCS condensate amplitude and molecular density are related through the BCS-BEC coherence factor. The evolution of the molecular density is determined by the equations of motion, which depend on the initial values of the density of molecules, the BSC condensate density, and the BEC-BCS correlator density. For example, when there are no molecules in the initial state, the number of molecules cannot exceed a certain value at any time, as given by the formula: {{formula:a5d8a130-72c2-401f-ae9c-c2afc07f24ec}}. This relationship impacts the main findings of the paper, as it suggests that one condensate can be extended only at the expense of another, even at a finite sweep rate.",
      "context": [
        " that one of condensates can be extended only at the expense of another even at a finite sweep rate.Below we consider two experimentally most relevant situations: i) only fermions and no molecules; ii) only molecules and no fermions in the initial state. In both these cases the initial value {{formula:cb85f475-c3b2-423f-8038-08aea1bf46b7}} . In the case of no molecules in the initial state, so {{formula:e842b4e1-bb97-4045-92b0-0eb992369e9a}} , the general equations (REF ,) simplify to {{formula:16839586-1d4a-47d8-9613-a8b87ea66faf}}The evolution of {{formula:7bc18aa6-06ac-4fdc-860b-d63bba0d6534}} in this case is determined by (REF ) and (REF ). Note that the coherence factor {{formula:101d80d1-5544-4164-99e8-521477905856}} does not remain zero",
        "f66d63af5d}} – the initial values of the density of molecules, the BSC condensate density, and the BEC-BCS correlator density. Using (REF ) and summing eqs. (REF ) and (), we find: {{formula:dbd9c108-fc15-42f2-9806-807c9ee6f38f}}which is a consequence of the conservation laws. Since for any state {{formula:b22c0900-1956-4e24-a80e-8324263e5fdc}} , if there are no molecules in the initial state, their number {{formula:c8667db8-d894-4a0e-b75e-268e6f53191a}} can not exceed the value {{formula:a5d8a130-72c2-401f-ae9c-c2afc07f24ec}} at any time. Although equation (REF ) has the same meaning as equation (REF ) it is more general as it shows that one of condensates can be extended only at the expense of another even at a finite sweep rate.Below we consider",
        "f2248fd9-8d10-4131-a26d-953ce86140d7}} . These solutions have the following properties: {{formula:b165b55f-a069-4ea0-9047-4056162d99b7}}The solution (REF ,) completely determines the evolution of the number of molecules {{formula:ee769daf-a11d-4032-bf37-c235278b7f37}} , the BCS condensate amplitude {{formula:843c35b2-0264-4bbd-beee-bcedef137dec}} defined by equation {{formula:795060e9-18aa-4c7e-bd37-c292f789b112}} , and the BCS-BEC coherence factor {{formula:4859d4d1-cc3f-4e44-a785-772f9826e392}} . It is convenient to introduce the intensive values: the molecular (or BEC) density {{formula:7336158c-62bb-48e1-858e-19375f92fde9}}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on the interaction between viscous, inertial, and newly found regimes in this research paper?",
      "expected_output": "The research paper reveals that the conventional understanding of drop coalescence having just two dynamical regimes, viscous and inertial, is incomplete. The study finds a third regime that describes the initial dynamics of coalescence for all drop viscosities, which was previously missed. This new regime is universal and is controlled by the Laplace pressure, which diverges when the curvature of the liquid interface is infinite at the point where the drops first touch. The paper constructs a new coalescence phase diagram based on force balance, providing a more comprehensive understanding of the interaction between viscous, inertial, and the newly found regimes in drop coalescence.",
      "context": [
        ", two drops first touch and then merge as the liquid neck connecting them grows from initially microscopic scales to a size comparable to the drop diameters. The curvature of the interface is infinite at the point where the drops first make contact, and the flows that ensue as the two drops coalesce are intimately coupled to this singularity in the dynamics. Conventionally, this process has been thought to have just two dynamical regimes: a viscous and an inertial regime with a crossover region between them. We use experiments and simulations to reveal that a third regime, one that describes the initial dynamics of coalescence for all drop viscosities, has been missed. An argument based on force balance allows the construction of a new coalescence phase diagram.The collision and coalescence of water drops, so essential to raindrop growth and the development of thunderstorms, have captivated the attention of the atmospheric science community since the early studies by Benjamin Franklin and Lord Rayleigh . Coalescence also plays a central role in industrial processes involving emulsions or dispersions , . For example, in the petroleum industry, coalescence occurs during dispersed water removal and during oil desalting . It is a dominant process in determining the shelf life of emulsion-based products such as salad dressing",
        "# arXiv Paper: 1204.4174\n\nThe inexorable resistance of inertia determines the initial regime of drop coalescence\nabstract: Drop coalescence is central to diverse processes involving dispersions of drops in industrial, engineering and scientific realms. During coalescence, two drops first touch and then merge as the liquid neck connecting them grows from initially microscopic scales to a size comparable to the drop diameters. The curvature of the interface is infinite at the point where the drops first make contact, and the flows that ensue as the two drops coalesce are intimately coupled to this singularity in the dynamics. Conventionally, this process has been thought to have just two dynamical regimes: a viscous and an inertial regime with a crossover region between them. We use experiments and simulations to reveal that a third regime, one that describes the initial dynamics of coalescence for all drop viscosities, has been missed. An argument based on force balance allows the construction of a new coalescence phase diagram.\n: Drop coalescence is central to diverse processes involving dispersions of drops in industrial, engineering and scientific realms. During coalescence, two drops first touch and then merge as the liquid neck connecting them grows from initially microscopic scales to a size comparable to the",
        " oil desalting . It is a dominant process in determining the shelf life of emulsion-based products such as salad dressing and mayonnaise , and it occurs in dense spray systems and combustion . Also, sintering of two spherical particles closely resembles the coalescence of two dispersion drops in an emulsion . Moreover, the controlled coalescence of drops in microfluidic devices promises a host of potential applications in chemistry, biochemistry, and materials science .The initial dynamics of coalescence are expected to be universal. The expansion of the liquid neck connecting two drops is controlled by the Laplace pressure, which diverges when the curvature of the liquid interface is infinite at the point where the drops first touch. Thus the change in topology, as two drops become one, is inextricably linked to a singularity in the dynamics. Different regimes of coalescence have been studied , , , , , , , , , , , , , , , , . The understanding that has emerged is that coalescence has just two dynamical regimes with a crossover region between them: a viscous regime, which always dominates at sufficiently early times when the neck radius is microscopically small, and an inertial regime that occurs at late times if viscous effects become"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between nucleus radius and constrained energy, and how does it impact the overall findings of the research paper?",
      "expected_output": "The research paper shows that the constrained energy is a function of the nucleus radius, allowing for the examination of the isoscalar monopole motion of a nucleus as harmonic vibrations. The relationship between the nucleus radius and constrained energy enables the calculation of the frequency of the isoscalar monopole vibration, which is a key finding of the paper. For example, Figure 1 shows the constrained incompressibility for a few nuclei obtained using various RMF parameter sets, and Figure 2 shows the frequency of the monopole mode obtained using equation 17. This relationship is crucial in understanding the breathing-mode monopole vibrations of spherical nuclei.",
      "context": [
        "49a7906}} radius {{formula:4b74c4c9-7547-45e2-b49c-d573ae48fea5}} .This behaviour of the constrained energy (8) as a function of {{formula:7709adce-05ac-4247-b970-fe772a310207}} (or R) allows us to examine the isoscalar monopole motion of a nucleus as harmonic (breathing) vibrations changing the {{formula:a86ac545-9f61-48a1-9ada-614a7346eca0}} radius R around its ground-state value {{formula:dc108ce7-4a08-408d-8a83-e99c87d85059}} . Considering {{formula:d1806b53-dabb-4a05-a3e6-8d3e21cbb1f2}} as a dynamical collective variable and expanding (8) around the ground-state point s=0 (or {{formula:5d2cc930-07b8-4e01-bb49-155f14f21c6a}} ) we",
        "6c08d35b-8d49-4dc1-884a-10ad824fc887}} is the Hamiltonian density.We have obtained {{formula:08f91a94-e57e-4ac8-bfd3-4e88e7ecb0ee}} from eq.(10), {{formula:eadd87ce-b6b0-4acd-9de0-b279a9e02785}} from eq.(16) and the frequency of the isoscalar monopole vibration as {{formula:8075c91c-6f1a-40da-93a7-2d14d622d2fc}}for a number of spherical nuclei. Various parameter sets such as NL1 (Reinhard 1989), NL-SH (Sharma et al 1993), NL2 (Lee et al 1986), HS (Horowitz and Serot 1981) and L1 (Lee et al 1986) with values of the nuclear matter incompressibility {{formula:3cf37adf-5e91-479d-924c-877e19e573a2}} 211.7, 354.9, 399.",
        "ll J 1981 Nucl. Phys. A371 253.FIGURE CAPTIONSFigure 1. (a) The constrained incompressibility {{formula:d25ea793-9972-438a-ad9c-0cd9a8e8b57a}} in Eq. (10) for a few nuclei obtained using various RMF parameter sets. (b) The collective mass {{formula:76aa2caa-bcc7-46d9-bc92-c8a4c66a7c11}} for the breathing-mode monopole vibrations obtained from Eq. (16) within the RMF theory.Figure 2. The frequency {{formula:b6354348-e757-420c-a951-52ac300a03f5}} of the monopole mode obtained using eq. (17).Figure 3. The incompressibility {{formula:c932cfd6-34ee-4e0d-850d-30c4bfb6d4fc}} (Eq. 20) obtained from 'scaling' of the nuclear density in the semi-infinite nuclear matter using the Thomas-Fermi approximation."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main implications of Theorem 1.3 for Stein manifolds, and how does it relate to the overall findings of the research paper?",
      "expected_output": "Theorem 1.3 has significant implications for Stein manifolds, as it establishes that a locally biholomorphic map over a Stein manifold lifts to a biholomorphism, which is equivariant with respect to a certain automorphism. This result is crucial in understanding the properties of Stein manifolds and their biholomorphic mappings. According to the paper, Theorem 1.3 states that if we have a Stein manifold, then our original map locally lifts to biholomorphisms of this manifold to another manifold. This finding is closely related to Theorem 1.1, which provides conditions for a biholomorphism of Stein manifolds to be equivariant. Overall, Theorem 1.3 contributes to the paper's overall findings by shedding light on the behavior of biholomorphic maps on Stein manifolds and their relationship with automorphisms.",
      "context": [
        " locally {{formula:4d25ae7f-a46d-4451-bea3-8f40776f6e07}} -biholomorphic over {{formula:9db05854-6dbd-45ac-9b6b-e7833dac558c}} , hence {{formula:f191b63e-2089-46cd-b7f2-6b818ebb7852}} lifts to a {{formula:e1870104-ad78-40a3-837d-132a499a760c}} -equivariant biholomorphism {{formula:1044f1d3-6b41-4797-9c55-bac2caadfab5}} .In we establish the following:Theorem 1.3 Let {{formula:3681d936-b7bc-40b0-997d-47f097dbc76e}} be a Stein {{formula:1710d57d-1ebc-4370-8cc7-b45fc83828f7}} -manifold, {{formula:7d4732",
        " Equivalently, our original {{formula:e24ccc7c-9524-45c7-a3f6-f78365f4264d}} locally lifts to {{formula:771ff935-3813-4a78-bec0-43f6e9d5c51a}} -biholomorphisms of {{formula:3fabf94d-bef2-4391-87d2-3ea0618e48d0}} to {{formula:b90b9c5a-5b7b-4e66-998a-30474f54722e}} . From we have:Theorem 1.1 Suppose that {{formula:206a4fbf-bb49-42cd-9f6d-23132281a54b}} is a Stein {{formula:7deefc15-1a92-44ea-885c-049d4e1f7652}} -manifold, {{formula:b3393890-310e-4727-8af9-3924ec87968e}} is a {{formula:2acca9dc-",
        "618145-e950-4503-b32e-28a3818e0e71}} . Let {{formula:72ce4a78-7f9b-4f57-9a5d-87771f65f268}} be a biholomorphism of Stein {{formula:12a604fa-d401-4b16-8874-893ea1f39592}} -manifolds. We say that {{formula:01586740-1f7c-4b56-a1bb-f2949b34cdb8}} is {{formula:1edbde96-f366-42f5-ac65-8d81b3de9dd1}} -equivariant if there is an automorphism {{formula:d4c21eed-6342-4ece-bf40-cc06b26a162a}} such that {{formula:95aaf7b3-1087-4580-a049-3361104fa9c1}} for all {{formula:85a0b79f-ba91-40a4-a38c-f87663"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do frequency coefficients play in understanding spin imbalance dynamics, and how do the research findings relate to this phenomenon?",
      "expected_output": "According to the research paper, frequency coefficients play a crucial role in understanding spin imbalance dynamics. The coefficient of the higher frequency is substantially smaller than that of the lower frequency, and with increasing values, the lower frequency and amplitude of the higher frequency increase. For example, the spin imbalance is characterized by only two frequencies, and the coefficients of these frequencies are important in determining the dynamics of the spin imbalance. The research findings show that the poles of the matrix element contribute to the spin imbalance, and the time-dependent behavior is determined by these frequencies. Specifically, the coefficient of the higher frequency is smaller, and the constant term vanishes due to the numerator being zero for certain values.",
      "context": [
        "ae84-cfb4d47050da}} . The coefficient of the higher frequency is substantially smaller than that of the lower frequency. Moreover, with increasing {{formula:02c2ff5e-96b8-44e4-9f12-3db2b2fe85cb}} the lower frequency as well as the amplitude of the higher frequency increases. This behavior is very similar to the experimental observation of the spin imbalance by Trotzky et al. .The return probability is calculated from the matrix element {{formula:f05ed77b-d4e2-4950-bb89-e5f4a2d6b820}}which again has the poles {{formula:30156c41-1ac5-4642-ab37-87570d2c22e1}} of Eq. (REF ) like the spin imbalance. This leads to the time-dependent behavior {{formula:4f534166-c542-4387-88cf-6e542eefe640}}where {{formula:cebe8654-a08e-4270-83f2-934cf924c5e0}} and {{formula",
        " we get for the spin imbalance the expression {{formula:2090bab6-03a2-48e4-9042-5666ba24246e}} {{formula:35ae07c0-16a2-494c-8821-82ad8935d5df}}It should be noticed here that the poles {{formula:27970c34-fc6f-4f0e-b85d-0017dc7894cc}} have been canceled by the factor {{formula:5e3dd046-8f33-4d60-928b-24a98d5eec74}} in the numerator of Eq. (REF ). Thus, only the poles {{formula:10e6f708-758d-4d05-9d73-5a69afb67050}} contribute to the spin imbalance: {{formula:8a185139-a75e-4996-afb4-8ee33bc0d236}}After transforming back to the time-dependent behavior, the dynamics of the spin imbalance is characterized by only the two frequencies {{formula:b472d016-0bdf-4241",
        " the spin imbalance is characterized by only the two frequencies {{formula:b472d016-0bdf-4241-856a-601ccc3e0db5}} : {{formula:922c063b-7391-4f08-a9a6-4dabfb821cd1}}The constant term vanishes because the numerator in Eq. (REF ) is zero for {{formula:f1b2e577-42c8-4181-9ebf-8224c5ac3f43}} . Moreover, the coefficients are {{formula:b73c2eb9-5f16-449b-989f-4553316c5c7b}}The spin imbalance as a function of time and the corresponding energy levels are plotted in Fig. REF for {{formula:7c66c18d-b66f-433a-af4b-bca8d4714083}} and two different values of {{formula:87babd21-bd98-41b0-ae84-cfb4d47050da}} . The coefficient of the higher frequency is substantially smaller than that of the"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does local connectivity affect population heterogeneity when global resources are varied, and what are the main findings of this research in relation to these dynamics?",
      "expected_output": "According to the research paper, local connectivity affects population heterogeneity when global resources are varied. With modest global resources, adding small amounts of local connectivity yields an increasingly heterogeneous population. In contrast, with substantial global resources, high-performing yet reasonably homogenous collective states emerge instead. The main findings of this research highlight the dramatic interplay between local connectivity and global competition in a networked population, suggesting that internal connectivity and global resources can be engineered to enhance performance.",
      "context": [
        "-7129-4832-a117-f40376dc117f}} and 100, this yields {{formula:a31f65c5-7621-4e27-ac13-2b3b7d46d3dd}} and {{formula:c6b940f0-6c93-454a-84d9-1aa6710a1eb5}} respectively, which are also all in excellent agreement with the numerical results. A full analysis of the dynamics of, and switching between, these regimes will be given elsewhere.In conclusion, we have reported a rich dynamical interplay between local connectivity and global competition in a generic networked population. Apart from the intrinsic interest regarding functionality in complex system networks, our results suggest that the internal connectivity and global resources in such systems can be engineered in order to enhance performance.PMH acknowledges support from the Research Grants Council of the Hong Kong SAR Government (grant CUHK4241/01P).",
        "a-b232-9b42af563050}} , and N.F. Johnson{{formula:f6daa2a7-29c6-49f1-9ad1-a4a0625888df}} {{formula:879ae2d5-d5bb-4cbb-8071-8a6dc17ced46}} Clarendon Laboratory, Physics Department, Oxford University, Oxford OX1 3NP, United Kingdom,{{formula:6ef77cd6-6062-436c-b1e9-b0e5b6413d2a}} Department of Physics, The Chinese University of Hong Kong, Shatin, New Territories, Hong KongWe show, both numerically and analytically, that the consequences of `wiring up' a competitive population depend quite dramatically on the interplay between the local connectivity and the global resources. With modest global resources, adding small amounts of local connectivity yields an increasingly heterogeneous population. With substantial global resources, high-performing yet reasonably homogenous collective states emerge instead.PACS numbers: 02.50.Le, 87.23.Kg, 89.65.Ef, 05.40.",
        "# arXiv Paper: cond-mat/0401527\n\nDynamical interplay between local connectivity and global competition in a networked population\nabstract: We show, both numerically and analytically, that the consequences of 'wiring up' a competitive population depend quite dramatically on the interplay between the local connectivity and the global resources. With modest global resources, adding small amounts of local connectivity yields an increasingly heterogeneous population. With substantial global resources, high-performing yet reasonably homogenous collective states emerge instead.\n: [aps,prl,epsfig]revtex[Dynamical interplay between local connectivity and global competitionin a networked population S. Gourley{{formula:856d3245-33f8-4602-a429-a8f845d68e55}} , S.C. Choe{{formula:12810f84-f94d-424c-ac3a-7788731cd848}} , P.M. Hui{{formula:f4e9794f-1cc0-481a-b232-9b42af563050}} , and N.F. Johnson{{formula:f"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper about skew polynomial rings and their connection to essential extensions, and how do the methods used contribute to these key results?",
      "expected_output": "The research paper about skew polynomial rings and their connection to essential extensions presents key findings on simple modules and their essential extensions. According to the paper by Brown, Carvalho, and Matczuk, the methods used, such as constructing skew polynomial rings and analyzing their simple modules, contribute to the understanding of essential extensions. For example, the paper 'Simple modules and their essential extensions for skew polynomial rings' provides a detailed analysis of simple modules over skew polynomial rings, which is crucial in understanding their essential extensions. The authors' approach, as seen in their 2019 paper in Math. Z., volume 291, pages 877-903, involves using technical terms like 'skew polynomial rings' and 'essential extensions', which refer to a type of ring that satisfies certain polynomial identities and a module extension that is essential, meaning it has no non-trivial submodules. The paper's findings have implications for the study of algebraic quantum groups, as discussed in the book 'Lectures on algebraic quantum groups' by Brown and Goodearl.",
      "context": [
        " journal=Glasg. Math. J., volume=52, date=2010, number=A, pages=53–59, issn=0017-0895, review=2669095, doi=10.1017/S0017089510000261,DSarticle author=Damiano, Robert F., author=Shapiro, Jay, title=Twisted polynomial rings satisfying a polynomial identity, journal=J. Algebra, volume=92, date=1985, number=1, pages=116–127, issn=0021-8693, review=0772474, doi=10.1016/0021-8693(85)90148-6,GSarticle author=Goodearl, K. R., author=Schofield, A. H., title=Non-Artinian essential extensions of simple modules, journal=Proc. Amer. Math. Soc., volume=97, date=1986, number=2, pages=233–236, issn=0002-9939, review=0835871, doi=10.2307/2046504,Iarticle author=Irving, Ronald S., title=Prime ideals of Ore",
        "BVOarticle author=Bavula, V., author=Van Oystaeyen, F., title=The simple modules of certain generalized crossed products, journal=J. Algebra, volume=194, date=1997, number=2, pages=521–566, issn=0021-8693, review=1467166, doi=10.1006/jabr.1997.7038, BCMarticle author=Brown, Ken, author=Carvalho, Paula A. A. B., author=Matczuk, Jerzy, title=Simple modules and their essential extensions for skew polynomial rings, journal=Math. Z., volume=291, date=2019, number=3-4, pages=877–903, issn=0025-5874, review=3936092, doi=10.1007/s00209-018-2128-8,BCM2article author=Brown, Ken, author=Carvalho, Paula A. A. B., author=Matczuk, Jerzy, title=Critical and injective modules over skew polynomial rings, journal=J. Pure Appl. Algebra, volume=227, date=",
        "ive modules over skew polynomial rings, journal=J. Pure Appl. Algebra, volume=227, date=2023, number=11, pages=Paper No. 107441, 26, issn=0022-4049, review=4595944, doi=10.1016/j.jpaa.2023.107441,BGbook author=Brown, Ken A., author=Goodearl, Ken R., title=Lectures on algebraic quantum groups, series=Advanced Courses in Mathematics. CRM Barcelona, publisher=Birkhäuser Verlag, Basel, date=2002, pages=x+348, isbn=3-7643-6714-8, review=1898492, doi=10.1007/978-3-0348-8205-7,PaulaChristianDilekarticle author=Carvalho, Paula A. A. B., author=Lomp, Christian, author=Pusat-Yilmaz, Dilek, title=Injective modules over down-up algebras, journal=Glasg. Math. J., volume=52, date=2010, number=A, pages=53"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between IC-SDW order and the FFLO phase, and how do the main findings of this research paper explain their connection?",
      "expected_output": "default",
      "context": [
        " the hospitality of SPSMS, CEA/Grenoble where this work was performed. The author benefited from conversations and clarifying discussions with D. Aoki, J. Flouquet, and G. Knebel who took my attention to this problem. This work is supported by a Grant-in-Aid for Specially Promoted Research (20001004) from the Ministry of Education, Culture, Sports, Science and Technology.",
        "# arXiv Paper: 0809.2192\n\nCoupled SDW and Superconducting Order in FFLO State of CeCoIn$_5$\nabstract: The mechanism of incommensurate (IC) spin-density-wave (SDW) order observed in the Flude-Ferrell-Larkin-Ovchinnikov (FFLO) phase of CeCoIn$_5$ is discussed on the basis of new mode-coupling scheme among IC-SDW order, two superconducting orders of FFLO with B$_{1{\\rm g}}$ ($d_{x^{2}-y^{2}}$) symmetry and $\\pi$-pairing of odd-parity. Unlike the mode-coupling schemes proposed by Kenzelmann et al, Sciencexpress, 21 August (2008), that proposed in the present Letter can offer a simple explanation for why the IC-SDW order is observed only in FFLO phase and the IC wave vector is rather robust against the magnetic field.\nAcknowledgements: The author acknowledges the hospitality of SPSMS, CEA/Grenoble where this work was performed. The author benefited from conversations"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of surface-ordered exponentials on non-Abelian 2-forms in this research paper?",
      "expected_output": "The research paper discusses the difficulties in defining surface-ordered exponentials for non-Abelian 2-forms, which is a major challenge in generalizing 1-form gauge theories to 2-form gauge theories. The paper introduces a new Lagrangian and a covariant derivative involving the 2-form gauge field, and derives (anti-)BRST symmetry transformations for non-Abelian 2-form gauge theories using the superunitary operator formalism.",
      "context": [
        "d2-4cbc-9391-4906fceb416d}} -forms couple to higher-dimensional objects, such as strings, membranes, etc. A generalisation of the 1-form gauge theories to {{formula:ed2ca182-1192-4d3e-ba68-57cd0cfea701}} -form gauge theories has been a long discussed problem in theoretical physics , , , , , , , . Since a 2-form couples to surface, it is natural to think of it as a gauge field for (open or closed) strings. Although this is a consistent picture for Abelian 2-form, it is problematic for its non-Abelian counterpart owing to the difficulties in defining surface-ordered exponentials, which appear when a 2-form is coupled to the world surface of a string . Nevertheless, non-Abelian 2-forms have appeared in the context of nonlinear sigma model , , in the loop space formulations of Yang-Mills theory , and gravity as a gauge theory , .Within the framework of BRST formalism,BRST is the abbreviation of the names of its founders, viz. Becchi, Rouet, Stora and Tyutin",
        "# arXiv Paper: 1612.09545\n\nSuperunitary operator and BRST transformations for non-Abelian two-form\nabstract: Using superspace unitary operator formalism, we derive various (anti-)BRST symmetry transformations explicitly for the non-Abelian 2-form gauge theories. We introduce a new Lagrangian with a coupling of matter fields not only with 1-from background field but also with a 2-form field. Moreover, the two gauge fields couple mutually as well. A new covariant derivative involving the 2-form gauge field is introduced. We also put forth a conjecture to generalise this idea to any p-form gauge theory.\nIntroduction: The {{formula:ceb29027-1e06-41d0-a815-8d8f2ef105fb}} -form objects are at the core of the higher form gauge theories. The gauge concept relies on 1-form in case of point particles. Just as 1-form (4-vector potential) couples to charged point particle, {{formula:31e51bb4-91d2-4cbc-9391-4906fceb416d}} -forms couple to higher-dimensional",
        " to their gauge-transformed counterparts. This superunitary operator is determined from the horizontality condition and gauge-invariant restrictions.Our goal in this paper is to deduce the (anti-)BRST transformation for the Kalb-Raymond {{formula:0a97dae8-8f8d-40ae-90cf-0bf0fdc8c6ba}} -field 2-form, following the superunitary operator approach. For that we consider the interacting theory where the matter fields interact with the 1-form as well as the 2-form gauge field. The two gauge fields, too, interact with each-other through the well known {{formula:dc55fbe7-e407-41c2-873f-7401c7f5ffd0}} interaction term. Our focus would be to find out the (anti-)BRST symmetry transformations for the various fields and to obtain the corresponding covariant derivatives for both the gauge fields, i.e. for 1-form and 2-form gauge connections.We start with a brief review, in Sec. , of 1-form gauge theories, (anti-)BRST transformations and the idea of superfields and superunitary"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the research paper use to optimize ABS's search path through backtracking, and what are the key findings regarding its effectiveness?",
      "expected_output": "The research paper uses beam search with backtracking and adaptive beam width to optimize ABS's search path. Backtracking allows ABS to revisit earlier decisions and optimize the search path, while adaptive beam width dynamically adjusts the number of candidate cases based on the complexity of the text being tested. The key findings show that ABS achieves higher success rates, particularly on long text datasets, and demonstrates optimal performance across all metrics when combining adaptive beam width and backtracking. For example, ABS takes less time to generate a successful test case, being faster than PWWS by 12.154 to 3441.895 seconds on average, and requires 3.608 to 218.762 fewer queries to the threat model than PWWS.",
      "context": [
        " beam search with backtracking (w/o AW) achieves higher success rates, particularly on AG's News's long text dataset. This indicates that initial errors in perturbation choices for long texts tend to worsen as the search progresses, affecting the quality of the final output. By implementing backtracking, beam search can revisit earlier decisions and optimize the search path. Regarding testing efficiency, backtracking introduces additional complexity due to the need to maintain and evaluate the historical best candidates. While this increases computational overhead, the ability to reassess early-stage decisions enables ABS to correct initial mistakes and avoid getting trapped in local optima. Beam search with adaptive beam width (w/o BT) shows lower time overhead and fewer queries, suggesting that ABS effectively adjusts beam width based on the search state. This adaptive strategy helps balance the scope and depth of testing, boosting ABS's adaptability and efficiency across various NLP software. By combining AW and BT, ABS demonstrates optimal performance across all metrics. Although backtracking increases computational demands, the overall benefits of achieving more comprehensive and higher-quality test cases justify this additional cost. This integrated approach ensures that ABS can dynamically adapt to various text lengths and complexities while maintaining high testing effectiveness and efficiency, which are crucial for testing the robustness of LLM-",
        "antically reasonable yet slightly varied text inputs. Stop-word filtering is used before each replacement as a constraint to avoid potential conflicts between the perturbation and constraint modules. Stop-word filtering involves skipping words frequently appearing in the text but contributing little to its main semantics, thereby reducing computational overhead. Based on the perturbation results, the historical optimal individual is updated (Lines 9-10), and the termination condition is checked (Lines 11-13), which refers to the presence of successful test cases in the current iteration results. Compared to standard beam search, ABS incorporates two main enhancements: adaptive beam width (Lines 14-20) and backtracking on {{formula:82c5671e-7ab9-4334-9d6d-d9e0434a1038}} (Lines 21-24). The iteration count increases until the maximum number of iterations is reached (Line 25), culminating in the output of the historically best individual as the test case (Line 27). Subsequent subsections will detail the steps involved in the adaptive beam width and backtracking strategy.\nAdaptive Beam Width: The technical challenge of adaptive beam width lies in balancing the need to extensively explore the perturbation space with the necessity of developing high-pot",
        " RQ1, for comparison with ABS across various datasets and threat models. Fig. REF displays the results for time overhead. ABS takes less time to generate a successful test case, being faster than PWWS by 12.154 to 3441.895 seconds on average, which shows its higher efficiency. Fig. REF presents the results for the query number, where ABS uses fewer queries across all datasets and threat models. For each successful test case, ABS requires 3.608 to 218.762 fewer queries to the threat model than PWWS. This efficiency is primarily attributed to beam search and adaptive strategy. ABS dynamically adjusts the number of candidate cases based on the complexity of the text being tested. A narrower beam width accelerates the search and reduces the computational load for shorter texts. A wider beam width for longer texts broadens the search range, increasing the chances of finding high-quality solutions. This dynamic adjustment not only improves the efficiency of the search process but also reduces testing costs. For example, reducing the query number lowers the cost of API calls on cloud platforms, making ABS more practical for robustness testing of LLM-based software. {{figure:18c8ac68-c9de-40f3-b9a5-150"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the sample size used in this research impact the detection of metallicity components, and what methods were employed to determine this relationship?",
      "expected_output": "The sample size used in this research significantly impacts the detection of metallicity components. A minimum sample of stars is required to detect multiple metallicity components, with the minimum number of stars per bin being lower for the APOGEE sample due to its greater metallicity precision. The researchers employed a formula to approximate the minimum sample size required for resolving two different chemical distributions, which is dependent on the standard deviation of the measurement uncertainty. For example, to resolve two populations whose metallicities differ by 0.32 dex, a sample of 500 stars would be required at the precision of the ARGOS observations, while only 35 stars would be required at the typical precision of the APOGEE bulge sample. The researchers used a three Gaussian decomposition of the metallicity distribution functions (MDFs) based on a maximum likelihood estimator and an analysis of jackknife samples to determine the relationship between sample size and metallicity component detection.",
      "context": [
        " the selected regions in Figure REF , the distribution of the metallicities contains information about such components. concluded that a minimum sample of {{formula:6f5e6565-863e-457a-90b1-351459f0e1ec}} ARGOS survey stars were required to detect multiple metallicity components. The minimum number of stars per bin must be lower for the APOGEE sample, due to its greater metallicity precision: 0.05–0.09 dex versus 0.13 dex for ARGOS. found that the minimum sample size required for resolving two different chemical distributions separated by {{formula:8b8ce4c0-9fee-4908-80ae-688dd687740c}} times the standard deviation (i.e., the measurement uncertainty) could be approximated by the expression {{formula:b9858bec-ea87-4dfb-a9dc-20a64720a2eb}} . This means that two populations whose metallicities differ by 0.32 dex could be resolved only with a sample of {{formula:6119aff6-ecae-4cf1-9dc5-65529f4d71",
        "formula:6119aff6-ecae-4cf1-9dc5-65529f4d71a2}} 500 stars measured with the precision of the ARGOS observations, while only {{formula:0df26d97-6d37-498e-bcc6-f78f5297539e}} would be required at the typical precision of the APOGEE bulge sample. In our analysis we typically have more than 100-200 stars per spatial bin, and for the most part we find a smooth variation of the distributions across neighboring regions.The metallicity distributions often exhibit multiple peaks, and vary with position. A three Gaussian (3G) decomposition of the MDFs based on a maximum likelihood estimator and an analysis of jackknife samples returned components at four different metallicities, marked with vertical lines in Figure REF : {{formula:88b529e9-5781-48bc-8cdd-02d20341fbc9}} (metal-rich), {{formula:53b680ce-b6cb-4b38-952f-b014d91dc03b}} (intermediate metallicity), and {{formula:60",
        ". This metallicity range is very similar to that reported by , but it does reach significantly lower metallicities than in the studies by , , most likely due to a smaller sample size that makes them miss the rare very low-metallicity stars. On the other hand, stars of lower metallicity are the major contributors far from the plane ({{formula:963e4bec-02c1-4ddc-81e9-582a7ebdda49}} ). Note the presence of a significant metal-poor contribution around the GC, as previously seen in APOGEE data by , and reported earlier by . {{figure:dd12fa10-858c-48bb-89f8-929bcf9f0505}}\nMDF Decomposition: The detection of different metallicity distributions in the inner Galaxy can be interpreted in terms of density variations in multiple overlapping metallicity components (e.g., disk, bar, classical bulge, and inner halo), as suggested by , rather than a bulk change in the overall population metallicity. For each of the selected regions in Figure REF , the distribution of the metallicities contains information about such components. concluded that a minimum sample"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does ARwild use to reconstruct 3D avatars and what are the key findings of this research?",
      "expected_output": "ARwild uses two sub-networks to reconstruct 3D avatars: a skinning weights network and a surface reconstruction network. The skinning weights network learns the blend skinning weights for pixel alignment, while the surface reconstruction network fuses image and geometry features to a canonical 3D shape. The key findings of this research include the introduction of the MVP-Human dataset, which contains 400 subjects with 15 scans in different poses and 8-view images for each pose, and the development of the ARwild method, which can reconstruct a 3D avatar from multiple unconstrained frames in one feed-forward pass.",
      "context": [
        " Avatar Reconstruction in the wild (ARwild) equipped with two sub-networks to achieve this goal: a skinning weights network that learns the blend skinning weights for pixel alignment, and a surface reconstruction network that fuses image and geometry features to a canonical 3D shape.\nPixel Alignment via Skinning Weights: PIFu has demonstrated that the pixel-aligned image feature is the key to reconstructing detailed 3D surfaces. However, different from single-image reconstruction , , where posed human is naturally aligned with the image space, the retrieval of 2D projected coordinates {{formula:faa548d8-d8c6-4d6e-89d6-23c7b793e01f}} for the 3D point {{formula:973ce8be-b276-441b-a341-1c31e8dffaf7}} is not straightforward due to the unknown views and poses of the input frames.To rebuild the canonical-to-image correspondence, we employ a skeleton-driven deformation method. Specifically, each 3D vertex on the body surface can be transformed to any poses using a weighted influence of its neighboring bones, defined as {{formula:e5e1e4",
        " multi-Pose 3D Human dataset (MVP-Human) with 400 subjects, each having 15 scans in different poses and 8-view images for each pose, proving {{formula:cf40055e-875c-4ac2-b176-613580a15e85}} 3D scans and {{formula:e7f04cc9-0341-4a6c-a5b0-d88cd0d7f839}} images in total. The linear blend skinning weights are also provided not only for supervising the model training, but also for reposing the captured T-pose meshes to the strict canonical pose, which is regarded as the target of 3D reconstruction models. Besides, benefit from the collected multi-view images, we enable quantitative evaluation on real-world inputs, which is more reliable than the commonly used rendered meshes , , .Based on MVP-Human, we introduce 3D Avatar Reconstruction in the wild (ARwild), a deep learning based method aiming to reconstruct a 3D avatar, including the T-pose shape and skinning weights, from multiple frames in unconstrained poses and views. Specifically, we propose a SKinning weights Network (SKNet) to predict the Linear",
        "# arXiv Paper: 2204.11184\n\nMVP-Human Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames\nabstract: In this paper, we consider a novel problem of reconstructing a 3D human avatar from multiple unconstrained frames, independent of assumptions on camera calibration, capture space, and constrained actions. The problem should be addressed by a framework that takes multiple unconstrained images as inputs, and generates a shape-with-skinning avatar in the canonical space, finished in one feed-forward pass. To this end, we present 3D Avatar Reconstruction in the wild (ARwild), which first reconstructs the implicit skinning fields in a multi-level manner, by which the image features from multiple images are aligned and integrated to estimate a pixel-aligned implicit function that represents the clothed shape. To enable the training and testing of the new framework, we contribute a large-scale dataset, MVP-Human (Multi-View and multi-Pose 3D Human), which contains 400 subjects, each of which has 15 scans in different poses and 8-view images for each pose, providing 6,000 3D scans and 48,000 images in total. Overall, benefits from the specific network architecture and"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how similar and dissimilar patterns affect graph permutations in the research paper, and what methods were used to obtain these results?",
      "expected_output": "The research paper finds that similar patterns occur when graph permutations result in natural frequencies closer to their minimum or maximum, while dissimilar patterns occur when frequencies are farther from their minimum or maximum. The paper uses the Simulated Annealing optimization method to search for these patterns. For example, in the 4-Regular graph, similar configurations have hubs with natural frequencies close to their maximum or minimum, while dissimilar configurations have nodes with frequencies far from their neighbors. The paper also analyzes other graph topologies, such as BA and CO graphs, and finds that they exhibit similar and dissimilar patterns based on their node connections and natural frequency values.",
      "context": [
        "formula:18c5073c-47fe-4dea-99ff-8dbf17309f70}} edges and 2 communities.A single choice of natural frequencies {{formula:9e4f43b3-e9d2-4830-b108-10acb5bad886}} were randomly drawn, matching network sizes {{formula:37115308-dce0-44da-9cbd-5a4231aba447}} . The term Configuration stands for a specific pair of graph and permutation of the vector {{formula:0b98b712-e060-4703-ba64-8520f496259c}} . For a fixed coupling graph, if one obtains permutations of {{formula:bf588864-2381-4bd7-a883-2c6e4473ed8f}} such that the associated {{formula:c62f1d91-5bd0-4902-8d6b-1dcf89404423}} is closer to its minimum or maximum, we have Similar (S) or Dissimilar (D) patterns, respectively. On the other hand, when no optimization process is",
        "7cd917b203}} the smallest critical coupling strength {{formula:e1fb1b10-722f-438c-9a32-0d70b5b1bd63}} inducing phase-locking, i.e. {{formula:48de5b9d-e1b2-4aeb-a80c-1ed5e8fbc19b}} and {{formula:4db5926e-01f8-490d-b59a-4e25d80261e6}} converges to a constant value, which we denote by {{formula:4e9f1118-d4d5-4515-9cd0-57e84ea8663d}} .Several complex networks topologies with {{formula:e8cb6eeb-9373-4498-961f-e00ba185bccf}} nodes and {{formula:ac3fdc81-8deb-4e19-9556-fcbdcff28e90}} (undirected) edges are considered: 4-Regular ({{formula:686b31c9-667b-4aad-",
        " graph, positive and negative natural frequency values were placed into distinct communities, with hubs close to {{formula:c445b58d-ed1c-4f0b-84fa-9e4336c09e8e}} and central nodes (in between communities) close to null {{formula:4373ac49-d128-4cea-8120-5439f467d4ec}} .About the Dissimilar configurations from Fig. REF , the opposite organization is found: each node receives natural frequency far from its neighbors. For the RE network, we clearly notice sequences of connected nodes with alternating positive and negatives values of {{formula:00d3c146-0688-4f0a-af2d-daafbfb2cfd1}} . Besides, BA and CO graphs presented connected hubs with larger natural frequencies and opposite signs. Eventually, Neutral configuration can be regarded as a blending between both previous configurations.Let us now describe a mechanism to seek Similar and Dissimilar patterns. For a given graph, we use successive iterations of the Simulated Annealing optimization method to search over the {{formula:a2705252-1ba2-43e3-8e7c-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the research paper use to simplify car control with LFBL, and what are the main findings regarding its effectiveness?",
      "expected_output": "The research paper uses Linear Feedback Linearization (LFBL) to simplify car control by learning the dynamics of a car and resulting in a linear input-output relationship. The main findings include the construction of a simple path planner using a Linear Quadratic Regulator (LQR) structure paired with linear constraints to enforce vehicle dynamics. Additionally, a novel extension to LFBL is proposed, which involves a preprocessing network to convert values from an exactly linearizing controller into suitable inputs for the system. The experiments demonstrate the effectiveness of the LFBL controller in controlling a 4-wheeled motor vehicle, exploiting the benefits of linear control in racing car algorithms.",
      "context": [
        " mind, seek to use LFBL to learn the dynamics of a car. The resulting linear input-output relationship can be used to vastly simplify its planning and control. We demonstrate this by the construction of a simple path planner that uses a Linear Quadratic Regulator (LQR) structure paired with linear constraints to enforce the dynamics of the vehicle. In addition, we posit a novel extension to the method of LFBL; a preprocessing network that is designed to convert the values calculated by an exactly linearizing controller into an appropriate set of inputs for the system we wish to control. This conversion then allows for the application of a feedback linearizing controller to systems that do not share the same inputs as the linearizing controller.The remainder of this paper is structured as follows: Section II: Theory, will briefly discuss the requisite theory for the method of FBL and how we formulate the problem of LFBL. Section III, Design Considerations, will address the specifics of the vehicle dynamics needed for this work as well as a variety of modifications and extensions made to the core methodology of LFBL to realize the desired controller. Section IV: Experementation, Results, and Discussion, will briefly address what experiments have been run to debug and validate the performance of our controller,",
        " the overall control architecture posited in this paper, we direct the reader to figure REF which is a visual representation of how the path and corresponding control signals are created and fed to the environment. In general, the entire system is guided by a path planner that calculates an optimal path from a desired starting point to a desired end-point, as-in (REF ). Then the LFBL controller can be used to calculate an input to the nonlinear dynamics of the car that will behave as-in the desired linear relationship, (REF ). The output of this controller, acceleration, is then converted into gas and brake signals that are fed directly to the racing environment in tandem with the other output of the linearizing controller, steering angle. The effect of these inputs is observed and used to further train the LFBL controller. Note that, in the case of the exact learned controller desribed below, the preprocessing network is not needed, as the dynamics being learned is strictly of the form of the LFBL controller.\nExact Learned Controller: We first sought to learn a controller that starts with a linearizing controller based on a set of nominal dynamics that can be exactly linearized relative to the true dynamics being learned. The setup for running experiments to this end involves the instantiation",
        " it. {{figure:483db6b6-de04-4e34-abf1-0ea1b2c84c7c}}Beyond briefly presenting the method of FBL and extending it to LFBL, this paper is aimed at applying the method for the task of controlling 4-wheeled motor vehicles. Specifically, we want to employ this method to exploit the benefits of linear control in the development of algorithms for racing cars. The nonlinear and nonholonomic nature of a car's dynamics, paired with their prevalence in our daily lives, makes the study of their efficient and effective control a tantalizing pursuit. A great deal of work has been done to accommodate the idiosyncrasies of path planning and control for vehicles, and autonomous racing has become a point of interest for profit, sport, and hobby alike , , , . There is a mature body of literature on the design of Model Predictive Control (MPC) algorithms that seeks to create optimal paths based on an explicitly stated sets of dynamics and system constraints. There has also been some recent work towards learning various elements of MPC , , , With this background in mind, seek to use LFBL to learn the dynamics of a car. The resulting linear input-output relationship can be used"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the role of eigenvectors in equations of motion and how do they relate to the main findings of this research paper?",
      "expected_output": "Eigenvectors play a crucial role in equations of motion as they help in solving the equations and determining the behavior of the system. In this research paper, eigenvectors such as 7f8e and 795a6134-e937-460c-bd74-aef1d6a1ab3e are used to compute important values and functions, like 7d9a26ba-276f-4093-a47d-aece283497db and 0ce4dda8-fbbb-4341-8650-4749a4018b7a, which give a8e4c1d8-3b0f-4e55-ae7f-eca66ac70580. The eigenvectors are also related to the operator c25d34b6-971b-4122-ac65-2837bc5e6ec8 and have specific eigenvalues, such as f77eb2f7-f49b-4d2b-92b2-106d5c24f1fb and 383a8807-d1e0-451e-83d8-197e19d9adf2, which are important for the main findings of the paper.",
      "context": [
        "7f8e}} and {{formula:795a6134-e937-460c-bd74-aef1d6a1ab3e}} are eigenvectors of {{formula:6cf7fc34-97b1-4c04-b558-b2dde3c2beb9}} . As a result, computing {{formula:7d9a26ba-276f-4093-a47d-aece283497db}} and {{formula:0ce4dda8-fbbb-4341-8650-4749a4018b7a}} is straightforward and gives {{formula:a8e4c1d8-3b0f-4e55-ae7f-eca66ac70580}}where we have introduced the function {{formula:d1df8489-760f-41a0-8167-e22b65cb3e98}}\nStep 3: equations of motion: For the final step, we show that (REF ) solves the equations of motion {{formula:ce4a0a3a-4737-4a9b-b40",
        "formula:d898d88a-9436-4b1f-bb03-353e4cc52806}} and {{formula:4e95d2ed-0b36-4b0e-8c18-5a6a8a44d48d}} as the eigenspaces of the operator {{formula:c25d34b6-971b-4122-ac65-2837bc5e6ec8}}which also appeared in (REF ). To be precise, elements in {{formula:d34d89e3-0642-4c23-959b-c67cf18cafaf}} and {{formula:ebf984b9-d5b5-486c-afaa-39031b81474f}} will have eigenvalues {{formula:f77eb2f7-f49b-4d2b-92b2-106d5c24f1fb}} and {{formula:383a8807-d1e0-451e-83d8-197e19d9adf2}} respectively.Now let us consider the limit {{form",
        "6970}} -matrix that furthermore satisfies the condition (REF ). Indeed, one simply demands that {{formula:7d906e50-661e-431c-9d4f-46a90d0d3227}} commutes with {{formula:a772fb05-0fe6-438a-8ea5-6621fb996e23}} and {{formula:40253cf3-3668-4a39-9eef-244ad2eec12a}} and acts as {{formula:cb4128ec-c93c-4dcb-a5ec-f4d045ec4a01}} on the eigenvectors of {{formula:7fe8bd22-ad92-49bd-9f2a-2062084b5096}} with positive/negative eigenvalues, respectively. In other words, we put {{formula:bc24dc2b-b12d-468c-b492-3b9c31c899ee}}For this particular example, these equations have a unique solution if one furthermore demands that {{formula:f7f8dfea-a369-4731"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how light pulses induce metallic states in 1T-TaS2, and what methods were used to achieve these results?",
      "expected_output": "The main findings show that light pulses induce a metallic state in 1T-TaS2 by melting the Mott gap, but the lattice retains its low-temperature symmetry. This results in polaronic transport, characterized by suppressed low-frequency conductivity, Fano phonon lineshapes, and a mid-infrared absorption band. The methods used to achieve these results involved determining the transient optical conductivity of photoexcited 1T-TaS2 over a wide frequency range, allowing for the observation of the prompt collapse and recovery of the Mott gap.",
      "context": [
        "20.kd, 71.30.+h, 78.47.jg, 78.20.CiLow-dimensional dichalcogenides exhibit a wealth of competing phenomena, ranging from Mott and Peierls transitions , to spin and charge density waves , , and superconductivity . Many such materials also display photoinduced phase transitions, wherein light pulses can create new transient states by driving ultrafast changes in the macroscopic properties , .{{formula:6aa6cb29-3985-41f2-828c-60c65eba1fd5}} -TaS{{formula:da8ae471-6d60-4e24-8d6b-f0fd0fa97f14}} is a quasi-two-dimensional transition metal dichalcogenide in which charge density wave (CDW) behaviour coexists with strong electron-electron correlations , , , , . At high temperatures {{formula:149a50aa-f76d-4d2b-9112-0cf8ff1e481b}} -TaS{{formula:00b96569-30b7-43a7-bb44-a404",
        "TaS{{formula:00b96569-30b7-43a7-bb44-a404fe095d92}} is metallic. Below 350 K, a periodic lattice distortion creates star-shaped clusters of tantalum atoms, causing the CDW to become nearly commensurate as domains of such clusters appear , . Below {{formula:ea6a7523-bb70-4bc8-8921-229c1794178e}} K, as the CDW becomes fully commensurate with the lattice and a long-range {{formula:f15a9129-25f8-40bf-baa2-cc9ddb9a1060}} superlattice structure emerges , , , the system becomes insulating. This structural distortion reduces the bandwidth below a critical value, and leads to the opening of a 100-meV correlation (Mott) gap, with electrons localized at the star centres , , . Mott localisation explains the low-temperature conductivity, which is lower than that expected from band structure .The general features of the photoinduced dynamics in {{formula:cbf94e56-cf65-4d58-b35c-",
        "# arXiv Paper: 1009.3807\n\nPolaronic conductivity in the photoinduced phase of 1T-TaS2\nabstract: The transient optical conductivity of photoexcited 1T-TaS2 is determined over a three-order-of-magnitude frequency range. Prompt collapse and recovery of the Mott gap is observed. However, we find important differences between this transient metallic state and that seen across the thermally-driven insulator-metal transition. Suppressed low-frequency conductivity, Fano phonon lineshapes, and a mid-infrared absorption band point to polaronic transport. This is explained by noting that the photo-induced metallic state of 1T-TaS2 is one in which the Mott gap is melted but the lattice retains its low-temperature symmetry, a regime only accessible by photo-doping.\n: Polaronic conductivity in the photoinduced phase of {{formula:ccfcc62a-1ceb-4171-a4e7-77dc66f1b49a}} -TaS{{formula:72520770-2659-4881-9696-617bb4bf23f2"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding quaternionic potentials and their impact on particle motion in this research paper?",
      "expected_output": "The research paper discusses the quaternionic Schrödinger equation and its application to time-independent potentials. The main findings include the introduction of purely quantum effects in particle motion due to rapid spatial variations of square potentials, which is also valid for perturbative quaternionic potentials. The solution of the quaternionic Schrödinger equation leads to a time-independent probability density, and the density current in quaternionic quantum mechanics is formally equal to that in the usual complex theory.",
      "context": [
        " potential introduce purely quantum effects in the motion of the particle. The same is valid for perturbative quaternionic potentials. Before beginning our investigation, we shall discuss some important mathematical properties of the quaternionic Schrödinger equation in the presence of time independent potentials, {{formula:83b252d4-8892-485c-91d7-e110cbd02091}}Taking into account that {{formula:ce30cd20-5e93-4937-a915-30f2d7b49536}} is a quaternionic function, we apply the method of separation of variables with the time dependent function appearing in the right hand side , {{formula:2afaf627-57d1-4c8b-bf52-87eec30a25e3}}with {{formula:27f75aa4-acec-4218-9ac1-5c83d15523e1}}This stationary solution of the Schrödinger equation leads to a time-independent probability density {{formula:d7e6d5b8-54e9-4526-a15a-46e39b35bbe4}} . Consequently",
        "3-98f6-4b16-9413-235c32826465}}\n{{formula:16721ad4-03f7-45e5-be5c-26c7121257e1}} ONE-DIMENSIONAL SQUARE POTENTIALS: Let us consider one-dimensional potentials. In the case of square shapes, the potential is a quaternionic constant in certain regions of space. In such regions, the stationary wave function {{formula:850e4ad6-e219-4f45-9353-183e9e0ad946}} is obtained by solving the following second order differential equation with (left) constant quaternionic coefficients, {{formula:c09a31a6-9c80-4f2d-9d78-cfa8e8459259}}It is not our purpose here to discuss the theory of quaternionic differential equations and we refer the interested reader to the papers cited in refs. , , where a detailed exposition of the subject is found. The solution of Eq.(REF ) is {{formula:1968aa9a-5cd7-4762-9451-d6640",
        "273d-d2bf-44b0-9b89-4886ea369ca9}}Consequently, the density current in quaternionic quantum mechanics is formally equal to that one of the usual complex theory, i.e. {{formula:40e12fe0-b740-4f71-88e3-aa8ae21979b7}}It is worth pointing out that, due to the non commutativity nature of quaternions, the position of the imaginary unit {{formula:d7b56869-62ec-44ad-bc78-5038be124ee4}} is not a choice but it is imposed by the anti-hermiticity of evolution time operator in Eq.(REF ).\n{{formula:e72fecfb-a8d9-45ea-9dbe-90e9d1036dd4}} TIME INDEPENDENT POTENTIALS: In this paper, we are going to be concerned with a particle in a time independent potential. In complex quantum mechanics, the rapid spatial variations of a square potential introduce purely quantum effects in the motion of the particle. The same is valid for perturbative quaternionic potentials"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods do the proposed algorithms use to generate MDS matrices and what are the key findings related to their performance?",
      "expected_output": "The proposed algorithms use a hybrid construction method to generate all n x n MDS and involutory MDS matrices over a finite field. The key findings include the introduction of a representative matrix form and a necessary and sufficient condition for the representative MDS matrices to ensure the resulting MDS matrix is always involutory. The approach is more effective than the exhaustive search, as it narrows down the search space to identify (n-1) x (n-1) MDS matrices, facilitating the generation of all n x n MDS and involutory MDS matrices.",
      "context": [
        "# arXiv Paper: 2403.10372\n\nConstruction of all MDS and involutory MDS matrices\nabstract: In this paper, we propose two algorithms for a hybrid construction of all $n\\times n$ MDS and involutory MDS matrices over a finite field $\\mathbb{F}_{p^m}$, respectively. The proposed algorithms effectively narrow down the search space to identify $(n-1) \\times (n-1)$ MDS matrices, facilitating the generation of all $n \\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. To the best of our knowledge, existing literature lacks methods for generating all $n\\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. In our approach, we introduce a representative matrix form for generating all $n\\times n$ MDS and involutory MDS matrices over $\\mathbb{F}_{p^m}$. The determination of these representative MDS matrices involves searching through all $(n-1)\\times (n-1)$ MDS matrices over $\\mathbb{F",
        " introduces a new matrix form called GHadamard, which can be utilized to generate new (involutory) MDS matrices from a Hadamard (involutory) MDS matrix as a representative matrix. Additionally, in , a technique was introduced to generate new n\\times n MDS matrices that are isomorphic to existing ones. In a recent study , a hybrid method is proposed to generate all 4 \\times 4 involutory MDS matrices over \\mathbb {F}_{2^m}.To the best of our knowledge, there is currently no efficient method for generating all n\\times n MDS and involutory MDS matrices over \\mathbb {F}_{p^m}. This serves as our motivation to delve into the theoretical aspects of generating all n \\times n MDS matrices, as well as involutory MDS matrices over \\mathbb {F}_{p^m}, and to propose hybrid strategies for achieving this.We introduce two algorithms designed to generate all n \\times n MDS and involutory MDS matrices over \\mathbb {F}_{p^m}, respectively. Specifically, to generate all MDS matrices over \\mathbb {F}_{p^m",
        "^m}, respectively. Specifically, to generate all MDS matrices over \\mathbb {F}_{p^m} of order n \\times n, we construct representative MDS matrices of order n and demonstrate that they can be obtained by searching through all (n-1) \\times (n-1) MDS matrices over \\mathbb {F}_{p^m}. Compared to the exhaustive search, which necessitates examining all n \\times n matrices, our approach is considerably more effective. Additionally, to generate all involutory MDS matrices of order n \\times n over \\mathbb {F}_{p^m}, we provide a necessary and sufficient condition for the representative MDS matrices to ensure that the resulting MDS matrix is always involutory.In , the authors present an explicit formula to enumerate all 3\\times 3 involutory MDS matrices over \\mathbb {F}_{2^m}. However, a precise formula to compute the number of all 3\\times 3 MDS matrices over a given finite field \\mathbb {F}_{2^m} is not available in the literature. In this paper, we establish that the count of all 3\\times 3 MDS"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does the concept of metric independence impact the parametrization of manifolds in this research paper?",
      "expected_output": "The concept of metric independence in the research paper means that the definition of the manifold's properties does not depend on the choice of the Riemannian metric. This is important because it allows the manifold to be parametrized in a way that is independent of the metric, as every point in the manifold can be represented as a set of coordinates. For example, the paper states that every point can be represented as a set of coordinates where the coordinates are independent of the metric. This independence is crucial for the parametrization of the manifold, as it ensures that the results are consistent regardless of the choice of metric.",
      "context": [
        "32c05ed339a}} ; For any {{formula:fa440187-6464-483d-bd0a-c6a81220d684}} and {{formula:98e29fd0-cb88-4d34-8289-26ff00e7c61d}} , {{formula:4c3c3cd6-ac07-49a8-af4e-5f5da0ffd925}} For any {{formula:d3f2084f-6e33-4261-8c20-343bc9c5e007}} and {{formula:bc7e48c5-41de-4f1d-9ec6-b94e1222d047}} , {{formula:f0294ed8-22bf-4521-ba5a-8401e14e6b04}}The definition is independent of the choice of the Riemannian metric {{formula:10730c3d-244c-43d9-8f6f-646e4406f128}} as the underlying manifold {{formula:59313347-269c-4db0",
        "-96e6-88f0f6e43015}} is parametrized as {{formula:56a1484b-b439-41c6-b2c6-ca4f1a61176f}} , that is every point {{formula:871ff1ec-d148-47bc-8b00-ecf3fd13ab98}} in {{formula:b2294a87-5279-49e6-9153-d9e9d786d0f6}} can be represented as {{formula:6008ed98-efba-4856-94a5-e4428292aaa3}} where {{formula:9d8b0271-fece-47ca-9ef0-086627fc06a7}} and {{formula:f52e8bfc-2cad-4698-8d3f-5336ceaeab04}} . The Riemannian metric in {{formula:14fb5205-908a-4c72-9ddd-6a6a56063ac2}} induces a Riemannian metric in {{form",
        "a-4761-8b1c-b9331dab8b9e}} such that for all {{formula:3a82b497-3f87-4982-b638-bf41c0c6fd30}} we have that {{formula:3349d595-45eb-4173-a28e-21018d88067d}}where {{formula:8effaa60-6eae-4ba0-8924-43148619931c}} is the geodesic on {{formula:92ea82d3-b5eb-47f6-b082-cb5c0b9c47ab}} connecting {{formula:e113b921-22e2-4084-95b2-fb50ad8c2917}} and {{formula:870827ff-aaf7-4ec1-8e7f-e6013c8eb886}} and {{formula:bd57db03-981c-446d-8ec4-483d35e624bc}} is the {{formula:18cfea98-c8a9-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does XPDWM use to reproduce NLVCs and what are the key findings of this process?",
      "expected_output": "The X-ray photoevaporative disc wind model (XPDWM) uses 2D emission maps to construct 3-dimensional models of line emission and calculates theoretical spectra of lines at different viewing angles, accounting for Keplerian broadening, thermal broadening, dust absorption, and limited spectral resolution. The key findings are that XPDWM can successfully reproduce simple, single-Gaussian lines with small blue shifts (2-5 km/s) and FWHM between 15 and 40 km/s, overlapping well with observations of Narrow Low-Velocity Components (NLVCs). However, it cannot reproduce NLVCs with the smallest and largest blueshifts or the narrowest line widths, and it also cannot reproduce Broad Low-Velocity Components (BLVCs) or High-Velocity Components (HVCs) on its own.",
      "context": [
        " 2D emission maps to construct 3-dimensional models of the line emission in axisymmetric disc winds and calculated theoretical spectra of the lines as they would be expected from observations at different viewing angles. In these line profile calculations we accounted for Keplerian broadening, thermal broadening, dust absorption and a limited spectral resolution. We followed the commonly used approach of decomposing our synthetic line profiles into multiple Gaussian components to assess the informative value that is held by the individual components and to explore the correlations between the component properties and the disc inclination and accretion luminosity in comparison to observations. Our main findings are: The X-ray photoevaporative disc wind model can successfully reproduce simple, single-Gaussian lines with small blue shifts (2–5 km/s) and FWHM between 15 and 40 km/s that overlap well with the observations of NLVCs. The model cannot reproduce, however, the observed NLVCs with the smallest and largest blueshift near 0 and 12 km/s and the narrowest line width near 10 km/s. Neither can it reproduce any of the observed BLVCs or HVCs and therefore needs to be coupled to another wind type in order to fully explain the line profiles observed in full/primordial",
        " value that is held by the individual components and to explore the correlations between the component properties and the disc inclination and accretion luminosity in comparison to observations. Our main findings are:The X-ray photoevaporative disc wind model can successfully reproduce simple, single-Gaussian lines with small blue shifts (2–5 km/s) and FWHM between 15 and 40 km/s that overlap well with the observations of NLVCs. The model cannot reproduce, however, the observed NLVCs with the smallest and largest blueshift near 0 and 12 km/s and the narrowest line width near 10 km/s. Neither can it reproduce any of the observed BLVCs or HVCs and therefore needs to be coupled to another wind type in order to fully explain the line profiles observed in full/primordial discs. The MHD wind model on its own can reproduce complex line profiles that can be decomposed into all component types (NLVC, BLVC and even HVC). The kinematic properties of the components approximately resemble those of observations but the profiles often show Keplerian double peaks that are rarely observed. These double peaks affect the profile decomposition (NLVCs with blue- and redshifts near 20 km/s are often",
        "). In a more extensive study that includes multiple wind models calculated with different initial conditions, we would expect the synthetic profile components to cover an even larger parameter space of observations.\nConclusions: We have calculated the photoionization structure of a detailed numerical X-ray photoevaporative disc wind model and a simple magnetocentrifugally driven wind model and created 2-dimensional emission maps of the [OI] 6300, [OI] 5577, [SII] 4068 and [SII] 6730 lines. We investigated the location and physical properties of their emission regions and studied the influence of the accretion luminosity as the dominating heating source of the wind. We used the 2D emission maps to construct 3-dimensional models of the line emission in axisymmetric disc winds and calculated theoretical spectra of the lines as they would be expected from observations at different viewing angles. In these line profile calculations we accounted for Keplerian broadening, thermal broadening, dust absorption and a limited spectral resolution. We followed the commonly used approach of decomposing our synthetic line profiles into multiple Gaussian components to assess the informative value that is held by the individual components and to explore the correlations between the component properties and the disc inclination and accretion lumin"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between CBFS parameters and rate-latency service curves, and how do they impact the main findings of this research paper?",
      "expected_output": "The research paper shows that CBFS parameters, such as idle slope and send slope, are directly related to the rate-latency service curves offered to different classes of flows. Specifically, the parameters determine the upper bound of the response time for each flow in the CBFS. For example, for class flows, the CBFS offers a rate-latency service curve with parameters, where and are the idle slope and send slope of the CBS for that class. This relationship is crucial in understanding the main findings of the paper, as it highlights how CBFS can provide guaranteed rate-latency services to different classes of flows.",
      "context": [
        "366ffa83-58bb-4ca3-9f6b-489d1c6807ad}} and {{formula:adde1b69-5bf1-4bfd-acfa-8fc143506d84}} are the sum of rates and bursts of the flows {{formula:782a557e-5d09-4da5-868a-601881f652c7}} for {{formula:46f2de0a-6e5c-4890-ac71-6a2806236eaa}} . In addition, {{formula:719015a6-5745-4e30-8f27-83886b3ef2b2}} and {{formula:20758bf9-6a35-49eb-bea6-ced7f5067aa7}} are the sum of rates and bursts of the flows that do not use the same interleaved regulator in downstream node with the previous flows. The CBFS offers a rate-latency service curve with parameters {{formula:ba1506df-bb20-46a5-984b-d5e742a",
        " 3.2.1 Assume flows of class {{formula:0b55031a-5447-4530-be9d-08f960f89f20}} , going from node {{formula:38f21771-3d43-443f-a71e-b11e6c866ff7}} to {{formula:2dc50d71-d54d-4046-95a0-06b886019503}} , and enqueued in the interleaved regulator at node {{formula:6e406ec9-33fa-45a0-9ed7-cd67552031cb}} for link {{formula:613f7a15-78c8-4010-8b40-39925191b26a}} . An upper bound, for each flow, of the combination of the response time in CBFS of node {{formula:d37b9a83-e5f0-4000-b4ab-566be6fc24f3}} (following the interleaved regulator of {{formula:6bda0cca-14c4-4f5b-8924-78a66ea10",
        "be5-f398-4952-abf5-0b2449640cf6}} , {{formula:c4a0fe72-f8f0-4526-9626-50522dd66a9d}} , and {{formula:7a99f43a-2fbf-4a27-a116-325aa6e9f026}} . Similarly for class {{formula:60ca4eb4-529a-49ab-8764-58a1a7571ef2}} flows, CBFS offers a rate-latency service curve with parameters, {{formula:4b08b56f-d779-46c8-96d6-d73d645ef241}}where {{formula:3fe9e9b9-cb88-4984-aa7a-bf8900d256b3}} and {{formula:b8fcf386-dccd-46b7-81c4-1107010ace08}} are the idle slope and send slope, correspondingly, of the CBS for class {{formula:1af77c5b-dce0-4bc8-9"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between Hawking temperature and entropy, and how do the research findings support this connection?",
      "expected_output": "The relationship between Hawking temperature and entropy is given by the formula: Hawking temperature can be written in terms of the entropy. According to the research paper, the Hawking temperature of the black hole spacetime can be calculated using a specific equation, and the black hole entropy can be obtained from the first law of black hole thermodynamics. The paper also plots the Hawking temperature with the horizon radius and analyzes the system to find the heat capacity at constant potential, which helps to determine if there are higher-order phase transitions in the black hole spacetime. For example, in the paper, the potential in a given dimension for the black hole spacetime can be calculated from the first law, supporting the connection between Hawking temperature and entropy.",
      "context": [
        "4f-5ae2730f8279}}The Hawking temperature of the black hole spacetime can be calculated using eq.(REF ) as {{formula:970840a6-1835-4c39-9f76-5bf6b4a10505}}We now use the relation {{formula:3e47c2bf-5424-4651-a223-a03e864978d0}}which is the first law of black hole thermodynamics. This form is analogous to the first law of thermodynamics {{formula:aece128a-c417-4c7d-82f8-4c7e62fb933e}}with the identification of the pressure {{formula:f59fd5e6-a89d-4f50-b401-191a2130bc19}} to the negative of the electrostatic potential {{formula:faa7c2ef-9e09-43ae-8253-51e6c9c5a83c}} , the volume to the charge {{formula:90ac3575-a971-4356-b279-661053acd4db}} and the internal",
        "ac3575-a971-4356-b279-661053acd4db}} and the internal energy {{formula:c49b33b5-ca13-4428-aa78-4fa28cede465}} to the mass of the black hole {{formula:2ba7a8d1-f515-4117-97b1-de267caeb4a1}} . From this relation, the black hole entropy {{formula:c3adf734-f7be-4c9b-baae-fb06b5fe6394}} can be obtained as {{formula:b812eb1d-733e-46ba-b777-94dc6855be97}}In principle, eq.(REF ) allows the Hawking temperature ({{formula:17c3df29-5c5f-4acf-abd4-2b955ed07f85}} ) of the black hole to be written in terms of the entropy {{formula:c62d8c73-98b9-4909-95ea-5a9bae2f03ef}} .In Fig.REF , we",
        "9-4909-95ea-5a9bae2f03ef}} .In Fig.REF , we plot the Hawking temperature of the black hole with the horizon radius. The nature of the graphs for different spacetime dimensions are as follows. {{figure:269f0aaa-1a87-4811-bb45-794116ee3d6e}}From the above plots, we observe that there is no discontinuity in these graphs hence there is no first order phase transition. Further, we analyse the system and calculate the heat capacity at constant potential. This we do to find whether there is a chance of higher order phase transitions in this black hole spacetime.The potential in {{formula:865ddf6d-62c1-4490-b8da-011da0d5db73}} dimensions for the black hole spacetime can be calculated from the first law and eq.(REF ) to be {{formula:65a78a98-a3fc-4954-8ef0-5aff968dff9e}}Considering the temperature of the black hole to be a function of entropy and charge ({{formula:9e85a8f6-fef3-4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on pose entanglement and its impact on 3D shape learning, and how do the methods used contribute to these findings?",
      "expected_output": "The research finds that pose entanglement is a major challenge in learning 3D shapes from single-view images, causing the learned shape embedding to have serious pose-entanglement problems. The methods used, such as self-supervised learning with a mesh topology and a differentiable mesh renderer, contribute to these findings by demonstrating the difficulty of learning 3D shapes without multi-view supervision. For example, the paper shows that using a mesh topology with 642 3D vertices and 1280 triangular faces, initialized as a sphere, can help drive adjacent faces to be flat and the whole mesh model to be smooth. However, the lack of multi-view observations makes it harder to relieve 3D uncertainty, and the pose-entanglement problems arise. The research highlights the importance of addressing pose entanglement in 3D shape learning from single-view images.",
      "context": [
        " {{formula:89148db8-7fcc-440a-bed5-ed9d2099101d}} . This smoothness term drives adjacent faces to be flat and the whole mesh model to be smooth. In this work, we use a mesh topology that consists of 642 3D vertices and 1280 triangular faces, which is initialized as a sphere.The above self-supervised workflow is similar to previous work , , . However, the difference is they assume having multiple-view observations {{formula:ee218af9-3688-4952-8670-92df298b8cf6}} , which makes cross-view projection-and-check possible and greatly relieves 3D uncertainty. As mentioned, our setting of single-view observations {{formula:4a56015b-6fb4-4fe6-bfbb-cfeffafb83a5}} makes it much harder, and we show the pose-entanglement problems and our novel treatment in the following sections.The Problem of Pose Entanglement. The previous self-supervised learning strategy only works properly under multi-view supervision, while under single-view condition, the learned shape embedding has serious pose-",
        "-82db-c6ab208cc530}} given no ambiguity caused.Self-supervised Learning. With no 3D shape data at hand, we employ the object silhouettes {{formula:a3a286b3-af5c-4d8d-b8ce-638b14f86bf3}} as weak supervision in a self-supervised projection-and-check manner, following recent trend , , , , , . More specifically, a 3D mesh is first inferred by {{formula:3991da50-1a8d-400b-a137-c67421fd9476}} , and then re-projected to 2D with true pose parameters by a differentiable mesh renderer resulting in a projected silhouette, i.e. {{formula:ced2156e-3c5c-4be2-a9d8-58e33888a7be}} . Here, {{formula:aa28de08-c233-48b4-b367-ea972aced4e5}} represents the re-projected silhouette of inferred shape, and {{formula:bc7fc890-02e1-4c8f-",
        " as supervision require multi-view images of each object instance, e.g. , , , , . This is because 2D supervision is much weaker compared to direct 3D supervision, and there exists infinitely many 3D shapes that can explain a given single-view image observation. Hence, researchers resort to multi-view images to constrain solution space. Yan et al. are among the first to explore multi-view supervised shape learning, where they used 24 different views of each object instance with known pose annotation. In later works, different methods are proposed using either 5 views or as least as 2 views , , with either known or estimated pose information. We argue that in practice, multi-view image datasets are still costly, since it demands extra labor of annotation and compilation. On the contrary, unstructured single-view images are more common and readily available, and they serve as a more convenient data resource for learning 3D shape models.In this work, we target the more practical yet challenging task of learning 3D shapes from only single-view images. We show that without supervision from multiple different viewpoints, this problem becomes very hard, even with the knowledge of pose information. The major challenge is found to be what we call the pose entanglement, which is"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does the seesaw mechanism play in determining the mass of active neutrinos, and how does it relate to the research paper's main findings?",
      "expected_output": "The seesaw mechanism plays a crucial role in determining the mass of active neutrinos, as it explains how the small mass of active neutrinos arises from the interaction between active and right-handed neutrinos. According to the research paper, the seesaw mechanism leads to the determination of the mass of the lightest active neutrino and the Majorana mass, which in turn determines the Yukawa coupling. The paper assumes a hierarchy between the Dirac masses for the seesaw mechanism and finds that the observational data of neutrino oscillations determines both the mass of the lightest active neutrino and the Majorana mass.",
      "context": [
        " seesaw mechanism. The mass matrix of active neutrinos {{formula:25805875-1acd-4371-8b97-f5b0cb028082}} ({{formula:658f1cd9-1c81-45be-9830-e159ccc8f7fd}} is given by {{formula:6be4914a-f1d6-4b76-8fd4-fa5767a7f2ec}}and the diagonalization of {{formula:358fb256-db9e-491e-9862-e44b0b6dc9ed}} gives the neutrino mixing matrix {{formula:00007462-9f50-49f1-a2cc-6f6226902e8d}} , called PMNS matrix, as {{formula:3b3a28a7-3904-44ed-8b1c-06fb217ed116}} where {{formula:225287cc-3d55-4266-9897-7075c531daf1}} is the mass for {{formula:af91a21f-24e7",
        "-bbf6-eb7403bc155d}} . We have shown that the observational data of {{formula:a2d714d0-3583-4744-aba6-c2cbc0130cf5}} and {{formula:2d89ea59-f9ad-4e57-ba78-214a8eef425a}} determines both the mass of the lightest active neutrino{{formula:cfa8941b-5a91-4751-8d54-7b02e36d4d57}} and the {{formula:a26e6c0a-2b14-4a90-ab1c-acd4b8848ecd}} mass {{formula:d099a942-6af8-4d41-affc-f952557abd13}} , which leads to the determination of the Yukawa coupling {{formula:da05b22b-68d3-436a-8b0e-74fad1b97481}} . It has been found that the very small value of {{formula:02e84c6a-b13b-401",
        " {{formula:9af4431c-e5db-425c-b458-098c21277542}} ({{formula:acf9967d-0548-49af-92e0-4818f15357c3}} , respectively. {{formula:125d64c1-6918-467a-ae30-6bcab7f45b68}} is the Yukawa coupling matrix and {{formula:17fbc256-a389-4421-97e8-482ecda2f905}} are the Majorana masses of right-handed neutrinos. Note that we take the basis in which the mass matrices for charged leptons and right-handed neutrinos are diagonal.We assume the hierarchy between the Dirac masses {{formula:c781c04e-4cb1-4667-b102-291d343f9e75}} and {{formula:0eea4636-6df1-484e-bf44-fbfc06e8b4f6}} for the seesaw mechanism. The mass matrix of active neutrinos {{formula:25805875-1acd-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of square root singularity on stress at cusps in this research paper?",
      "expected_output": "The research paper finds that a square root singularity at the cusps of an hypocycloidal void or rigid inclusion leads to a stress field that blows up to infinity. However, in some cases, the stress approaches a finite value, smaller than the unperturbed value, due to stress reduction. The paper also reports cases of partial stress reduction, where stress reduction occurs at some cusps but not others, as seen in rigid hypocycloidal inclusions with five and eight cusps.",
      "context": [
        "1f7a-444a-a9ef-513e46765d4b}}c(m)0(1+)] (2-{{formula:de08f7c1-9458-4748-9504-b9db2e49216d}} 2)j=0q m+1j (m+1-j n)(n-1)j, highlighting the square root singularity in the stress field at the cusp, as predicted by the asymptotics around sharp notch .\nStress Intensity Factors: Stress Intensity Factors (SIFs) for the symmetric and the anti-symmetric out-of-plane problem are defined, respectively, as follows {{formula:e75df3d3-5898-414d-9f18-810f9b10f248}}so that, considering the asymptotic stress field (), the expression of the SIFs for a {{formula:d2fa7c10-a03d-42eb-99fe-34d43d0b4c6a}} -cusped hypocycloidal inclusion can be analytically obtained in the following closed-form expression {{",
        " worth noting that solution () simplifies in some special cases, as{{formula:fb5ef320-0ecc-4ff1-88c4-363d878607a5}} (or, equivalently, {{formula:063b5780-e255-4017-a1bf-2a13f57f972d}} ) {{formula:97c719c8-e39a-40c5-ad95-35e86db171b9}} which is similar to the solution in the conformal plane for the circular and polygonal inclusions; {{formula:6703a160-bc26-421a-a484-0bfaea1278f2}} (corresponding to the case of uniform antiplane shear ) {{formula:47648e44-9d0d-4ccc-94e2-911bd316d31a}}\nStress singularities and stress reduction: As in the cases of cracks and rigid line inclusions , , , , a square root stress singularity at the cusps of an hypocycloidal void or rigid inclusion is predicted in the theory of elasticity , , , .",
        "3fc19}} -axis, for the cases considered in Fig. REF . While the stress blows up to infinity as a square root singularity in the case of {{formula:9d8e0b69-b99c-412b-84bc-dc6fa46d747c}} , the stress approaches a finite value, smaller than that unperturbed, in the case of {{formula:1c08982c-9185-4968-bc52-ff2dfeb45c64}} . {{figure:2601f9a6-7ab8-4873-a5f0-52f7c25e06af}}Cases of partial stress reduction occur when stress reduction is verified at some cusps, but not at the other, namely, eq (REF ) holds while eq (REF ) does not. Examples of such cases are reported in Fig. REF for rigid hypocycloidal inclusion with five and eight cusps. Uniform ({{formula:4bf23296-7c77-46af-b341-24903156bedb}} ) and quadratic ({{formula:2855e80a-2baa-4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main enhancements of ConvL2LFlow in calorimeter simulation and how do they improve the overall performance?",
      "expected_output": "The main enhancements of ConvL2LFlow in calorimeter simulation are the use of skip connections inspired by the U-net architecture, coupling layers instead of masked autoregressive flows, and the elimination of the need for probability density distillation. These enhancements improve the overall performance by allowing for faster training and inference, better flow of information, and more accurate modeling of particle showers in complex detectors, as demonstrated on the ILD and CaloChallenge datasets.",
      "context": [
        ") skip connections inspired by the U-net architecture to further improve the flow of information and network convergence; and iii) coupling layers instead of masked autoregressive flows for fast training and inference , , , , without the need of probability density distillation.We demonstrate the feasibility of the proposed approach on the well established electromagnetic showers in the ILD first introduced in as well as on dataset 3 of the CaloChallenge .The rest of this paper is organized as follows: Section introduces the datasets used in this work. Section describes the model architecture. Section introduces the metrics used to evaluate the model. Section presents our results. Finally, section concludes.\nGettingHigh: For comparison with earlier work, we use the dataset generated for . It consists of photon showers in the electromagnetic calorimeter of the international large detector (ILD) .Two detectors are proposed for the international linear collider (ILC), one of which is the ILD. The ILD is tailored for Particle Flow , an algorithm aiming to enhance detector resolution by reconstructing every individual particle. It combines precise tracking, good hermiticity, and highly granular calorimeters. The electromagnetic calorimeter (ECal) contains 30 silicon layers alternated with tungsten absorbers. It uses 5",
        " The electromagnetic calorimeter (ECal) contains 30 silicon layers alternated with tungsten absorbers. It uses 5\\times 5\\;\\text{mm}^2 cells. The ILD detector's simulation, reconstruction, and analysis rely on the iLCSoft ecosystem , utilizing Geant4 , , and DD4hep for realistic modeling.The dataset consists of simulated photon showers with incident energies uniformly distributed from 10 to 100 GeV directed perpendicularly into the ECal. A staggered cell geometry results in small shifts between the layers, which generative models used to generate showers have to learn. A 30x30 cells bounding box around the incident point is chosen for the datasets, resulting in a data shape of 30x30x30. It has 950,000 samples, which we split into 765,000 training, 85,000 validation, and 100,000 test samples. Further details on the dataset are provided in .\nDataset 3 of the CaloChallenge: To facilitate the development of deep generative models for calorimeter simulation, the Fast Calorimeter Simulation Challenge (“CaloChallenge”) was created. It consists of 4 calorimeter shower datasets of increasing dimensionality , , , all",
        " opens up several avenues for further study. One could investigate ways to speed up the model further, for instance, by transforming it into a latent model. Another crucial direction would be to integrate the model into a full simulation chain and assess its impact on physics analyses. Also, the model could be applied to hadronic showers, which are more complex than electromagnetic showers. Convolutional L2LFlows was submitted to the Fast Calorimeter Simulation Challenge for datasets 2 and 3 where a detailed comparison between recent models will be done.In conclusion, our work contributes to the evolution of generative modeling in high-energy physics, presenting a robust, scalable, and accurate method for modeling particle showers in complex detectors. As the field progresses towards more granular calorimeters and higher event rates, the importance of such advanced computational methods will only increase.\nCode Availability: The code for this study can be found under https://github.com/FLC-QU-hep/ConvL2LFlow.\nAcknowledgments: We thank Sascha Diefenbacher for early contributions to this work. We want to thank Henry Day-Hall for valuable comments on the manuscript.This research was supported in part by the Maxwell computational resources operated at Deutsches Ele"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how laser chirality affects angular momentum in the research paper, and what methods were used to obtain these results?",
      "expected_output": "The research paper finds that the chirality of the laser wave affects its angular momentum, with the spin angular momentum being a key component. The total angular momentum of the laser beam is proportional to the momentum of the wave. The methods used to obtain these results involve analyzing the laser wave's frequency components and their corresponding effects on the angular momentum. For example, the paper mentions that the laser wave's positive and negative frequencies correspond to specific components of the angular momentum, and that the laser's polarization and chirality play a crucial role in determining its overall angular momentum.",
      "context": [
        "aa}} , {{formula:9c2b045a-c56f-43e5-9c26-1539a14266c2}} and {{formula:88386e8d-645b-4b6a-959d-f7f9f25d1b5c}} correspond to positive and negative frequencies {{formula:57eed643-0d98-44bb-a1b0-1453b494573b}} , respectively. This form clearly shows the chirality of the laser wave and most importantly that {{formula:2f5b4fe7-3fcd-447d-912d-acb021b4c066}} is parallel to the {{formula:c1c278f7-789b-4f5e-ab04-c6c16022c1ae}} -vector which is perpendicular to the {{formula:350c8fcd-29bf-4259-8f0c-8f6f61b124a7}} field. For completeness, we also point out that for beams with a finite cross section, the laser will have an additional component of {{form",
        "29-8b5e-e33702a2be4a}} Hz, {{formula:403d19fe-c3dd-4872-ae12-c2b77de96258}} , {{formula:c71c7327-bac6-4002-865b-23b616d1c7b5}} , and {{formula:dbe9ead0-fcb4-41cc-b26d-dece6310bac8}} Oe. While the response to laser fields {{formula:f741ac3c-daaf-4cdb-a6c4-53253d1b0c61}} is negligible on a femtosecond time scale (c1), it becomes significant for {{formula:0fb76ce8-daff-4e95-894e-edc15a2e0d67}} (c2) and ultrafast reversal is found at higher fields of {{formula:83b8ec11-2fc9-4f84-8b8c-9f24c8e9f79b}} and {{formula:ac8c4edd",
        "f-4a6302255227}} W/cm{{formula:94431cca-b1aa-47e7-b64a-a5a8ad479c39}} and above.A polarized laser pulse carries angular momentum , but more importantly it also carries spin angular momentum. The total angular momentum {{formula:88536621-9e7f-47e7-96a1-dc2fcb9a0589}} of the laser beam occupying volume {{formula:8d87f4fc-ca5c-45b2-8bfb-0ba9c02c1dc0}} is proportional to the momentum {{formula:dfecb963-9bb6-425c-9679-3f9ec21d54b3}} of the wave and given by (cgs) {{formula:b4cb90da-313f-4869-932c-28205a334c04}}This vector can be decomposed into two parts according to {{formula:17e48d90-9f65-4021-b762-ba38e299393c}} , with {{formula:fda27d"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between the Min and Latest functions in milling, and how do they impact the overall process?",
      "expected_output": "The main differences between the Min and Latest functions in milling are how they handle the interaction between rings. The Min function takes the minimum value at any point, which means the deepest indentation is decisive, but it does not maintain the order of the rings. On the other hand, the Latest function considers the consecutive process of milling, where newly introduced rings overwrite older ones, leading to a dominant appearance of their rear parts. This means the Latest function prioritizes the most recent ring's effect at any point, unlike the Min function which solely looks at depth. For example, in the context of material removal, the Min function would always choose the ring that removes the most material at a point, whereas the Latest function would consider the sequence of rings and how each subsequent ring modifies the surface.",
      "context": [
        " x\\in \\mathbb {R}^2 and consider the following functions, which can be compared in Figure REF .Minimum Since material is removed during the milling process, the deepest indentation at any point should be decisive. Note that the rings' order is not maintained when using the minimum. &f_k^\\text{Min}\\left(R_1,\\dots ,R_k\\right)(x) = \\min \\left(R_1(x),\\dots ,R_k(x)\\right) Latest Here, we take into account that milling is a consecutive process. Hence, newly introduced rings overwrite older ones which leads to a dominant appearance of their rear parts. &f_{k}^\\text{Latest}\\left(R_1,\\dots ,R_k\\right)(x) = \\begin{} R_k(x) \\end{}&if x\\in P_k\\\\ f_{k-1}^\\text{Latest}\\left(R_1,\\dots ,R_{k-1}\\right)(x) &else Convex combination In the previous approaches, the value in each point is determined by a single ring.",
        "}(x) = \\begin{} \\cos \\left(\\frac{\\pi }{2}\\cdot D_k^{+i}(x)\\right)\\end{}&if x\\in P_k^{+i}\\\\ -\\cos \\left(\\frac{\\pi }{2}\\cdot D_k^-(x)\\right)&if x\\in P_k^-\\\\ \\cos \\left(\\frac{\\pi }{2}\\cdot D_k^{+o}(x)\\right)&if x\\in P_k^{+o}\\\\ 0 &else. {{figure:8542ff57-66ea-4781-8a2f-01f76783635c}}\nTilting: So far, milling shapes were assumed to be aligned to the milled surface. Tilting of the milling head introduces a depth gradient within the rings with increasing depth towards the front, i.e., in motion direction of the milling tool. For a ring R_k, this direction is given by the angle \\theta _k\\in (-\\pi ,\\pi ] between the planar tool-path at point c_k and the x-",
        "_n\\right)(x). The modeling procedure consists of three steps describing the appearance, interaction, and location of the rings as follows:the appearance of an individual ring is described by the function R_k:\\mathbb {R}^2\\rightarrow \\mathbb {R}, the interaction between n rings is described by the mapping f_n:\\left(\\mathbb {R}^2\\rightarrow \\mathbb {R}\\right)^n\\rightarrow \\left(\\mathbb {R}^2\\rightarrow \\mathbb {R}\\right):\\left(R_1, \\ldots , R_n\\right)\\mapsto f_n\\left(R_1, \\ldots , R_n\\right), the tool-path providing the position and order of the rings by defining their center points c_k\\in \\mathbb {R}^2.These sub-models are explained in more detail in Sections REF , REF and REF below. The milling process is controlled by several parameters both for the milling head, e.g. the head diameter and the blade length, and for the tool-path such as the feed rate. The known real-world"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences in side-pairing between hyperbolic equivalents, and how do these differences impact the overall structure?",
      "expected_output": "The main differences in side-pairing between hyperbolic equivalents and Euclidean cases lie in the pairing pattern. In the hyperbolic case, pairing consists of linking opposite sides, whereas in the Euclidean case, the pairing is more complex. For example, in the Euclidean hexagonal periodic boundary conditions, the pairing is noted as 1 with 4, 2 with 5, and 6 with 3. In contrast, the hyperbolic equivalents have a relatively straightforward pairing pattern, which impacts the overall structure by resulting in fundamental polygons of genus 2 with a specific number of sides.",
      "context": [
        " 2 with side 5, side 3 with side 6. Another notation, more convenient for polygons with numerous sides and complex side-pairing pattern, is based on a visual description that gives, for each side of the polygon and when moving around in a chosen direction, the number of sides between two paired sides. The above pairing can then be noted {{formula:757eed70-41c8-4f42-8f8b-ab1996af81c6}}which means, going counterclockwise around the hexagon and starting with side 1: 1 with 4, 2 with 5, ..., 6 with 3. This notation will be preferred in the following.\nElements of classification: Consider first {{formula:f43bfb0b-8214-47b1-a6e8-b9f33745f8f7}} , which corresponds to the Euclidean case. Setting {{formula:29e232c7-c7ad-43fe-bc12-8b7b5d7c51c6}} in equation eq:sides gives that the number of sides of the fundamental polygon can be 4 or 6, which confirms that the only two periodic boundary conditions with",
        "-a518-cca63f63c742}} with {{formula:bee92504-f3ea-4de7-9eb1-41c9748be50f}} . Due to the pairing of polygon edges, each edge of the graph corresponds to two edges of the polygon, i.e., {{formula:cea49a26-21a2-4221-9ad9-4731bb51cd04}} or equivalently {{formula:972eccb0-78a4-4367-a525-935098faa6ed}} . In addition, each vertex of the graph corresponds to {{formula:bf9d3181-3857-44f0-9dbf-6cd5e18fef57}} vertices of the polygon. As the fundamental polygon has as many edges as vertices, {{formula:edc254d4-0b4f-4cf9-aa49-c7151bf0469a}} . As a result, and using the fact that the coordinence {{formula:3c4c6fcc-7561-4e70-a8f0",
        "22714aa2-da8d-40d9-bf46-f1f4265a6470}} . In the case represented in fig:rep-4p-can, it is noteworthy that the metric fundamental polygon is also a standard (or canonical) fundamental polygon, which is not true for the family illustrated in fig:rep-hex-gen. However, from a practical point of view, there is no difference in the effort needed to implement these two types of periodic boundary conditions.Another simple “family\" is made up of the hyperbolic equivalents of the Euclidean hexagonal periodic boundary conditions. This family corresponds to fundamental polygons of genus {{formula:69370a82-4c02-48de-8dd2-a19ee9a2c7df}} with {{formula:b9aa9a79-71f3-44fe-a38e-9b712e6de22a}} sides (see tab:classification). The main characteristic of this family is the relatively straightforward way to pair polygon sides. Indeed, in this case, pairing consists in linking opposite sides, regardless of the value of {{formula:9d055c8f-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the implications of zero formal dimension on the emptiness of moduli space, and how do the research findings support this concept?",
      "expected_output": "According to the research paper, if the formal dimension of the moduli space is zero, it implies that the moduli space is no longer empty, generically. This is supported by the finding that the group is no longer acting by translation, which allows for the existence of solutions even when the formal dimension is zero. For example, the paper states that the dimension of the moduli space of solutions is given by a specific formula, and the morphism of complexes defined according to this formula is continuous. This suggests that the moduli space can be non-empty even when its formal dimension is zero, which is a key implication of the research findings.",
      "context": [
        "a1f9d1}} are 1-periodic orbits of {{formula:6abc032f-2c49-4da7-96e9-447b2f955f6a}} and {{formula:06199f0f-71ec-4e3b-805d-7f262759cf10}} respectively. The dimension of the moduli space {{formula:add1aa34-c74a-466d-b052-5de4d5eadae6}} of solutions of (REF - REF ) is {{formula:a8f56d7b-4b29-427d-b2fe-8e2cce8555eb}} but, to the difference of equation ( - ), the group {{formula:d80f5c43-6f64-4867-8446-877f4e1dbfd1}} is no longer acting by translation. This implies that, generically, the moduli space is no longer empty if its formal dimension is zero and the morphism of complexes defined according to (1.3{{formula:c8a386ca-ece8-4f10-b",
        "c4ccd}} , {{formula:a85a3cdb-1f01-41f3-9d3f-4ed3243f7def}} be continuous families of parameters in {{formula:06b107fe-6a6d-471d-9074-1b682686805f}} such that {{formula:c97543a7-c881-45ad-a1e2-116f8dfddb0f}} has no 1-periodic orbits with action equal to {{formula:267c3318-2f4e-45d3-bece-70a8928e87ce}} or {{formula:6c792aee-bca2-4aa8-b9ee-af672ba243d7}} . There is a canonical isomorphism {{formula:b49ea2ff-4d35-4c10-bdd9-8e7cbf8b1b9f}}obtained in the usual manner by considering solutions of the equations (REF - REF ).One should note that we suppose in particular {{formula:a96e25a0-a",
        "e7f21a-5431-4020-9778-df9b68d5fe77}} bounds comes from the fact that, for generic values of {{formula:ee184668-fff0-4a1d-9bf1-d982bc3520c2}} , there are no characteristics of period {{formula:8ee016d5-f559-480c-a1c6-f9ae13cbc921}} on {{formula:117f78e8-4ce3-45d6-9432-a7daaf27d439}} . As a consequence, the 1-periodic orbits of {{formula:c4f07867-e985-4e6c-815d-a7eb5f0277d5}} are all located in a neighbourhood of {{formula:18c5ade3-1c43-4683-b180-81a50e877683}} . The Floer trajectories that connect such 1-periodic orbits could leave the respective neighbourhood only by having an interior tangency with a certain {{formula:5fd5f05b-e206-4ea9-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the local structure of X_S points relate to the properties of pi, and what makes pi a regular or special number in this context?",
      "expected_output": "The local structure of X_S points relates to the properties of pi, where if pi is regular, it is a smooth point for X_S, and if pi is special, it is a point of multiplicity at most 2 for X_S. A pi is considered regular if it cuts out on a smooth curve C an odd theta-characteristic, and special if it is contained in a hyperplane H such that the curve C cut out by H on S has a node or pi is special for S. For example, in the context of a general K3 surface S of degree 6 and genus 4 in P^4, the variety X_S has points that are either regular or special, with the special points having a multiplicity of at most 2 due to the properties of the fibre p_2^{-1} (H) and the curve C.",
      "context": [
        " The fibre p_2^{-1} (H) consists of the points of the form (\\pi ^{\\prime },H), with \\pi ^{\\prime } cutting out on C an odd theta–characteristic. This implies that the fibre p_2^{-1} (H) consists of exactly 120 distinct points, hence p_2 is étale over H. This implies that \\mathcal {X}_S is smooth at each point of the fibre p_2^{-1} (H), in particular it is smooth at (\\pi ,H). By looking at the projection p_1: (\\pi ,H)\\in \\mathcal {X}_S\\longrightarrow \\pi \\in X_S, we see that \\mathcal {X}_S is a \\mathbb {P}^1 bundle over X_S. So that, being \\mathcal {X}_S smooth at (\\pi , H), implies that X_S is smooth at \\pi . Let us assume next that \\pi \\in X_S is special. Then we can choose a hyperplane H containing \\pi such that the curve C cut out by H on S has a node or",
        " \\pi is special for S. Local behaviour Let again S be a general K3 surface of degree 6 and genus 4 in \\mathbb {P}^4. In this section we study the local structure of the points of X_S. Theorem 3.4 If \\pi \\in X_S is regular, then it is a smooth point for X_S. If \\pi \\in X_S is special then it is a point of multiplicity at most 2 for X_S. Let us consider again the variety \\mathcal {X}_S introduced in the proof of Theorem REF , with the projection p_2: (\\pi ,H)\\in \\mathcal {X}_S\\longrightarrow H\\in (\\mathbb {P}^4)^\\vee , that is a finite morphism of degree 120. Let \\pi \\in X_S be regular. Then we can choose a hyperplane H containing \\pi such that the curve C cut out by H on S is smooth and \\pi cuts out on C an odd theta–characteristic. The fibre p_2^{-1} (H) consists of the points of the form (\\pi ^{\\prime",
        "_0, which are 28. We claim that each of these planes has to be counted with multiplicity two. Let us give this for granted for the time being. Then the fibre p_2^{-1} (H) contains the point (\\pi , H) with multiplicity 2, and this implies that (\\pi , H) is a point of multiplicity at most 2 for \\mathcal {X}_S. Then, arguing as above, we see that \\pi is a point of multiplicity at most 2 for X_S. To finish our proof we have to prove the above claim. To do this, we note that C is a limit of a general nodal hyperplane section C^{\\prime } of S. Let H^{\\prime } be the hyperplane spanned by C^{\\prime } and let q be the node of C^{\\prime }. Then the fibre p_2^{-1} (H^{\\prime }) consists of the points of the form (\\pi ^{\\prime },H^{\\prime }), with: (i') \\pi ^{\\prime } a plane that is tangent at three (proper or infinitely near) smooth points of C^{\\prime }; ("
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do FF ratios vary with multiplicity in the context of this research paper, and what methods were used to determine this relationship?",
      "expected_output": "According to the research paper, the FF ratios vary with multiplicity by comparing the FFs for different multiplicities. The methods used to determine this relationship include calculating the ratio of FFs for different multiplicities, such as the ratio of FFs for tagged and inclusive jets, and performing calculations with running coupling frozen at some value. For example, in Fig. 13, the ratio of FFs for tagged and inclusive jets is shown to illustrate the difference in magnitudes of the effect for different multiplicities.",
      "context": [
        "ula:a93a9934-7b19-4ec3-a7f3-d1ff04538a16}} dNch/d{{formula:2eed6a52-e10f-4e55-b1bf-1a82e0ee2773}} {{formula:b2b320af-e044-4e45-9c62-e541126333aa}} 20{{formula:d3ba84b3-bd85-43c0-a481-d408b8a169b7}}Since the vacuum FFs are unobservable, in practice, to observe the medium effect one should simply compare the FFs for different multiplicities. In Fig. 13 we show the ratio of the FFs for {{formula:14d2bfaa-c229-4674-9112-f76fad04dbb8}} and {{formula:4a02ba61-cabb-4470-af0b-865912e988ba}} (for inclusive jets this ratio cannot be measured, and we show it just to illustrate the difference in magnitudes of the effect for {{formula:70",
        "e-47e2-8e5c-3bacea9bb35b}} . We perform calculations with running {{formula:e8d8cf74-cdfd-4447-869c-8fe536a15341}} frozen at some value {{formula:ac8771bf-ff39-4a93-b84c-f1d89ef8d229}} at low momenta. For gluon emission in vacuum a reasonable choice is {{formula:ca510df3-17fb-482e-80bb-818f47af0d5a}} , . In plasma thermal effects can suppress {{formula:5f31159c-d50c-4b7b-8d14-49b64c870fe4}} . However, the uncertainties of jet quenching calculations are large and the extrapolation from the vacuum gluon emission to the induced radiation may be unreliable. For this reason we treat {{formula:aa5a8f4d-021b-4ba8-890d-af289891a8b2}} as a free parameter of the model. In we have observed that data on {{form",
        " cannot be measured, and we show it just to illustrate the difference in magnitudes of the effect for {{formula:70be56cf-741f-44dc-ab28-1e1370ad9b41}} -tagged and inclusive jets). As for {{formula:d396a075-2b53-4b01-8709-b002160cb946}} we have investigated the sensitivity of our results to variation of {{formula:16438767-a581-4cd2-bfea-68c3c6467dee}} , and found that {{formula:3fab108a-403f-4c9d-8393-0ef23022a51e}} is quite stable against variation of {{formula:ee94cec4-a593-429a-9b1c-5241104879b9}} .\nSummary: Assuming that a mini-QGP fireball may be created in {{formula:f7dea8cf-da5e-4561-b128-3bb8d8a925c7}} collisions, we have evaluated the medium modification of high-{{formula:8cc1e7"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between difference spectra and flare energy, and how do the research findings support this connection?",
      "expected_output": "The research paper suggests that there is a discrepancy between the expected and observed signatures of flare energy in difference spectra. According to the calculations, a broad red wing should appear in the spectra, indicating the presence of non-thermal particles. However, the observed spectra do not show this signature, even at a level of 1% of the peak irradiance of the line. This discrepancy is extreme, with the theoretical work assuming a beam intensity much smaller than that necessary to supply the flare energy. The findings support this connection by showing that the peak-to-wing intensity ratio is greater than 1% for small flares, but no signature is observed for more powerful flares. For example, the paper mentions that the energy of a 'small flare' is around 10^29 erg, and even this results in a peak-to-wing intensity ratio of greater than 1%, which conflicts with the lack of a signature at a much lower level for more powerful flares.",
      "context": [
        " difference spectra, but these caveats should generally be borne in mind. {{figure:5bba9a8a-d230-45c2-af14-8e8caaf49749}}{{figure:944e9714-24d7-41d8-b708-fdcdc6c6b340}}These figures do not show bright wings of the 30.4 nm line that could be attributed to the expected signature. In each of the plots the dashed line indicates 1% of the peak irradiance of the line itself, which is about the level expected from the calculation of . Note that this expected signature would occur mainly in the impulsive phase and (theoretically) should be much less prominent in the gradual phase. In addition the theoretical work assumed a beam intensity much smaller than that necessary to supply the flare energy, and so the discrepancy is extreme. All of these events show the presence of non-thermal particles in various ways, including line {{formula:0ac28f2b-fa61-4a7e-b51b-f982bb557969}} -radiation and SEPs (but in the case of SOL2011-09-22, only radio signatures),",
        " about 1% of the energy of a “small flare” of total energy {{formula:751bf7ff-778a-41c8-98e1-297c5acdbb22}} erg. These already result in a peak-to-wing intensity ratio of greater than 1%, which conflicts directly with our lack of a signature at a much lower level, and for much more powerful flares. Further, to interpret the flare {{formula:6e89f4f2-3264-42d4-bdb4-539e8e9432b6}} -ray observations (including those reflecting primary {{formula:76fabace-3cff-4ec7-bb36-f7ba3f9a23c0}} particles directly; see e.g. Murphy et al. 1991), a larger {{formula:7970c31e-85bb-4d11-839a-14458ccd103f}} fraction (50%) is assumed or deduced. These assumptions together suggest a discrepancy between theory and observation of several orders of magnitude. We note that the comparison with the peak irradiance of the 304 Åline is problematic, because even in the",
        " the flares are spread out over many months. The background for SOL2010-06-12 is lower than that for the 2011 events, consistent with the solar-cycle increase. The MEGS-A stability enables good use to be made of the the excellent counting statistics in the 10-s integrations, for example in constructing difference spectra with good precision as shown in Figure REF , lower-right panel. {{figure:4a30ee94-ce39-45e1-9efc-7f39e00d219c}}{{table:68184894-e385-4638-a46f-09c40fdd6248}}Figures REF and REF show a search for the beam signature in the four primary events. According to the calculations, a broad red wing should appear, with a peak wavelength depending on the {{formula:3eeb76eb-f93d-4f25-8a40-be68d0fe9414}} -particle energy. The results of for other {{formula:a5f38e37-bd03-4e96-8474-ae8742de51a7}} -ray events suggest that the particle beams may"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings on how band reduction affects the KIVC strain interval in this research paper?",
      "expected_output": "The main findings are that reducing the number of bands from six to two significantly decreases the strain interval over which KIVC order is found, as seen in the SCHF calculations. Specifically, the KIVC state is stabilized over larger strain values when using a rectangular momentum grid, but the strain interval becomes smaller when reducing the number of bands.",
      "context": [
        "-2577bd91495f}} , with {{formula:2a555007-5236-4c45-8b1a-ce785abd6b25}} the Pauli-z matrix acting on spin indices, is independent of {{formula:49bf3ccd-01e6-487c-8985-9f79686eddb9}} and retains its value {{formula:083556b3-f86f-47c5-98c2-d93f25f75dcf}} . This is because the filled active bands are completely spin polarized by the exchange interaction. As a result, we see that the SCHF calculation on the rectangular grid with only one spin component for the active bands produces results that are indistinguishable from the results obtained for the complete model with both spin flavors. This justifies doing DMRG on the model with spin polarized active bands. We also see that the transition from the KIVC to the SM on the rectangular grid with {{formula:8c58b2a1-1484-4049-8bfe-cafd4c42a11a}} happens near {{formula:f03df338-8015",
        "bc99-411a-afa6-e4d23d89950b}} momentum grid, keeping {{formula:af6a0da1-994f-440d-a620-76c7adaf1443}} bands per spin and valley. The second calculation was done on a rectangular {{formula:62e464ac-1b6e-4309-ab94-9cc6953de474}} momentum gird, also with two spin flavors and six bands. The third SCHF calculation was again done on the same rectangular grid with two spin flavors, but now keeping only {{formula:92a40662-6797-4f9d-82cd-e14e0da9180d}} bands per spin and valley. In the fourth and final SCHF calculation we reduced the number of spin components in the active bands from two to one (again working on the same rectangular grid and using {{formula:a0902325-f98c-4257-8cfb-82bbb3095c9f}} ). Importantly, even though we keep only one spin flavor for the active bands, the remote bands retain two spin flavors. This shows up in our Hamiltonian via",
        " only one spin flavor for the active bands, the remote bands retain two spin flavors. This shows up in our Hamiltonian via the HF contribution of the remote bands to the free fermion part {{formula:ff9ad294-631f-4f17-9094-c634ce60f21a}} of {{formula:e95008de-2768-4d6f-aeb1-5be3091d3603}} [Eq. (REF )].From Fig. REF we see that going from the square to the rectangular momentum grid stabilizes the KIVC state over larger strain values. On reducing the number of bands {{formula:1cc324d6-10fd-4e0d-828a-a960daae7d7f}} from six to two, however, the strain interval over which we find KIVC order becomes significantly smaller. For all SCHF calculations where we keep both spin flavors, we find that the spin polarization {{formula:2e7fdb08-44cb-437b-9543-2577bd91495f}} , with {{formula:2a555007-5236-4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What does the term alpha refer to in the context of NIST STS, and how is it used in the research paper?",
      "expected_output": "In the context of NIST STS, the term alpha refers to the chosen level of significance, denoted as 0.001 in the research paper. It is used to determine if a statistical test outputs a p-value greater than this chosen level, indicating that the number sequence is random according to that test. For example, if the p-value of a test is greater than 0.001, then the number sequence is considered random.",
      "context": [
        " it runs on the random number sequence. We say that H0 holds for a statistical test if it outputs a p-value greater than a chosen level of significance denoted as {{formula:45076f0c-2a22-4956-a139-65c6c2c39376}} . That is, if the p-value of a test is greater than {{formula:07076b90-b7d8-47f4-b062-bbc2a2e7593d}} , then the number sequence is random according to that test. We choose {{formula:9f0f3eb2-0ed5-491e-a260-e3ccea0aa4fe}} as 0.001 based on the suggested level of significance range ([0.01, 0.001]) in the NIST STS specification .We collect bitstreams from every sense amplifier (64K in one DRAM segment) in a DRAM segment following QUAC operations. We test 8K DRAM segments in every DRAM module. We observe that 1 Mbit bitstreams collected from 22 sense amplifiers can pass all NIST STS tests.Table REF presents the",
        " Mbit bitstreams collected from 22 sense amplifiers can pass all NIST STS tests.Table REF presents the average p-values for the NIST STS test results on two types of bitstreams that pass all 15 tests: (i) the output of the Von Neumann Corrector (“VNC”) and (ii) the output of the post-processing step we describe in Section REF (“SHA-256”). We conclude that QUAC generates number sequences that are indistinguishable from true random number sequences. We discuss the randomness of post-processed results (SHA-256 column) in Section REF . {{table:cf1f0f14-55bc-4d75-9840-3ce62645cc10}}\nQUAC-TRNG Evaluation: We evaluate QUAC-TRNG using real DRAM chip experiments and simulation studies to show that QUAC-TRNG (i) produces high-quality random bitstreams, and (ii) outperforms prior DRAM-based TRNG proposals.\nQUAC-TRNG Output Quality: To demonstrate that QUAC-TRNG produces high-quality bitstreams of random values, we experimentally extract nine bit",
        " segment into 1 random number sequences and test 1024 number sequences per DRAM segment using NIST STS. We find that {{formula:1cef4032-c648-4ec4-bc10-55390c70cfc4}} of the sequences pass all NIST STS tests. This pass rate is larger than the acceptable rateBased on the formula {{formula:51bb1da8-f669-4bd2-bf1b-8a25d686a3cc}} , where k is the sequence population (1024) and {{formula:85d36206-e9ce-4614-b73b-e5354473bfa8}} is the significance level ({{formula:d908e6d0-efff-4d1b-928b-2e45e0e7e523}} ) ({{formula:697e5e47-2253-4def-8dbf-d885d0d088a1}} ) that NIST specifies .Table REF , column “SHA-256” shows the average p-value for each test. We conclude that QUAC-TRNG generates high-quality unc"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do gauge boson masses play in determining the critical temperature, and how do the research findings support this relationship?",
      "expected_output": "According to the research paper, gauge boson masses play a significant role in determining the critical temperature. The findings suggest that including dynamical gauge boson masses lowers the critical temperature significantly. For example, in Fig. REF, the thermal effective potential of the ISS model including gauge mass-gap effects is plotted, showing a lower critical temperature compared to when only mass effects of other fields are considered. This is further supported by the comparison in Fig. REF, which demonstrates a notable difference in critical temperature when gauge boson masses are included versus when they are not.",
      "context": [
        " ) as {{formula:ff9c096b-0ba5-4450-869c-f30c3ceac676}}which at large {{formula:53c1b507-3a6c-49ac-9bac-c8996dadb7f3}} lies in the expected range {{formula:bb6eabe2-0fa0-4c46-b0e5-1ed1a123da91}} for {{formula:b62063d7-2c72-42a4-98bd-fac2ace64ec2}} .This analysis leads us to the following expression for the contribution of gauge degrees of freedom to the thermal effective potential (REF ) {{formula:146a671d-189b-4267-bf5a-6e9b8086cbe9}}where {{formula:ed3fdb09-c393-492a-a4a0-fbc929e5e081}}In Fig. REF we plot the thermal effective potential of the ISS model including gauge mass-gap effects (black solid line) {{formula:3d26593f-869c",
        "9fdca583dae9}} and is included to represent the contributions of massless gauge degrees of freedom. {{figure:006fd9c7-763e-487b-a337-2ce7f18235d1}}It follows from our results in Fig. REF that the inclusion of the dynamical gauge boson masses in (REF ) does lower the critical temperature significantly. This is even more apparent in Fig. REF where we compare the critical temperature with gauge boson masses included to the one obtained when we include only the mass effects of {{formula:287138c2-376b-40ea-877a-487b0e98f8aa}} and {{formula:24f03c4e-bf40-4a7d-867c-046555fe4524}} fields. {{figure:ab9fc498-9e53-439f-a5c7-c1f28351ae0d}}Indeed, performing a similar analysis as the one in section 3.2.1 of Ref. one obtains in leading order for large {{formula:52035c8a-7392-4bb1-ba90-23d6835480",
        " model including gauge mass-gap effects (black solid line) {{formula:3d26593f-869c-4819-b3e5-e645e19f24ca}}versus the expression for {{formula:74d6bbe1-afdd-4994-8349-c0bf1f20c144}} where the mass-gap for gauge degrees of freedom is not taken into account (red dashed line), {{formula:6344c26a-287f-4f03-b38f-ba751f260dfa}}The last term in the above equation, {{formula:c6677616-e31e-449d-858f-6102f84ee5d0}} is a constant shift of the potential as a whole; it does not affect the relative heights of {{formula:528e792d-d59a-4f6a-b52c-fcb073310a19}} and {{formula:13761747-5e78-4cb7-98ba-9fdca583dae9}} and is included to represent the contributions of massless gauge degrees of freedom. {{figure"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does ball-tree partitioning facilitate data point organization in the context of this research paper, and what are its key benefits?",
      "expected_output": "Ball-tree partitioning facilitates data point organization by using a divide-and-conquer approach, where the partition corresponding to each node is split into two sub-partitions. The key benefits include efficient data point assignment and reduced computational complexity, with each split having a complexity of O(n), where n is the number of data points. For example, in the research paper, the ball-tree is built by initially assigning all data points to the root node and then splitting the partition into two sub-partitions based on the farthest point from the centroid of node points.",
      "context": [
        "52-41a3-9a07-7e8ca0550bce}} is a sub-partition of {{formula:79c0c933-2dc3-41d9-92c0-db5c7404fe2c}} .\nBall-tree space partitioning: Ball-tree is built using a divide-and-conquer approach. Initially, ball-tree has only one (root) node and all data points are assigned to it. In each step, the partition corresponding to each node is split into two sub-partitions. For a partition {{formula:92e82378-e027-430a-b0a9-eb6302f20aa9}} , the splitting procedure is as follows:Select the farthest point from centroid of node points in {{formula:b543f742-9c6a-4cc8-aadb-d7e5025c5f10}} as the first (left) child pivot {{formula:88725fc1-d47c-4c9f-9420-2ad10b95cea9}} . Select the farthest point from {{formula",
        "9f-9420-2ad10b95cea9}} . Select the farthest point from {{formula:19544719-5a89-4b8a-a44d-6e92608b6ed7}} as the second (right) child pivot {{formula:17e153ed-fb8a-4b56-81ad-6d5959fdaf66}} . Assign each of the data points in {{formula:c968d8ce-a475-493b-8d6c-56107bbc29dc}} to the partition whose pivot is closer. Assign the new sub-partitions as children of {{formula:a13680c6-8a8a-4db2-b486-6682c3dedddf}} in ball-tree, i.e. {{formula:457131ec-e6ef-4a58-8739-22a6cd2505ac}} and {{formula:54ed0321-d1ae-4459-8137-b6fd2abeede7}} .The computational complexity of each split is {{formula:547c7a",
        " is evaluated for the mean value of each section.Algorithm REF illustrates the splitting method for ball*-tree. Similar to ball-tree, the computational complexity of our suggested splitting method is {{formula:c4fc2598-6c3a-4e80-95fd-cde0ab789e5d}} .Ball*-tree: Space partitioning [1] split{{formula:274df3ba-f548-4afd-a0e7-01b32275f500}}Input: {{formula:beeca4ce-25b1-4763-9b7a-02e5665722cc}} (Data points in the partition) Output: {{formula:609aa951-d09c-49b1-adb7-682931446f0c}} , {{formula:7777c613-3208-4dfd-a7ce-68ca1f09a18a}} (Data points of the right/left partition){{formula:0dde58dc-69b9-42a2-bb6f-52d30919e4cc}} Apply PCA {{formula:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research paper on extending the zero-free region and how does it affect the Euler product's sphere of influence?",
      "expected_output": "The research paper presents a novel approach to extending the zero-free region for the Riemann zeta function, which is crucial for understanding prime numbers. The main finding is that the non-vanishing of the zeta function in certain regions does not fundamentally rely on the existence of an Euler product. The paper provides evidence to the contrary, showing that for a large class of Dirichlet series, almost all of which do not have an Euler product, the zeta function can still be shown to be nonzero in certain regions. This challenges the traditional view that extending the sphere of influence of the Euler product is necessary for understanding the distribution of prime numbers.",
      "context": [
        "48e0-b42f-4cb089ebe9b1}} .It is crucial to our understanding of the primes to extend the zero-free region for {{formula:b59daf8b-3ac2-470c-a413-6046072a726c}} as far to the left of {{formula:c729729e-697c-414e-a4f0-82d7d1eccc10}} as possible.At present, the strongest result in this direction is due to Mossinghoff and Trudjian ; see also the earlier papers , , and references therein. According to Titchmarsh this means extending the “sphere of influence” of the Euler product:The problem of the zero-free region appears to be a question of extending the sphere of influence of the Euler product beyond its actual region of convergence; for examples are known of functions which are extremely like the zeta-function in their representation by Dirichlet series, functional equation, and so on, but which have no Euler product, and for which the analogue of the Riemann hypothesis is false. In fact the deepest theorems on the distribution of",
        " and for which the analogue of the Riemann hypothesis is false. In fact the deepest theorems on the distribution of the zeros of {{formula:69683902-12ce-4576-9430-fd4ff3fabb4b}} are obtained in the way suggested. But the problem of extending the sphere of influence of [the Euler product] to the left of {{formula:ce616444-eb0e-4e2a-82eb-e7b17d698a47}} in any effective way appears to be of extreme difficulty.But let's play the devil's advocate for a moment! Is it really the case that the non-vanishing of the Riemann zeta function in {{formula:31c4999c-c128-426f-b5e0-ad13ba1c9711}} (and in wider regions) fundamentally relies on the existence of an Euler product? Our aim in this paper is to provide some evidence to the contrary.\nStatement of results: For a given arithmetical function {{formula:f111b922-42a4-46ff-8965-624a005939b0",
        "447d-8a17-2863f29096af}} for a large class of Dirichlet series, almost all of which do not have an Euler product (but some do).For a Dirichlet series {{formula:a1add2c1-7f9a-4d88-ad57-9cb73c92802d}} attached to a bounded completely multiplicative function {{formula:bae4df4b-e773-42f6-8162-d11a7e211433}} (for example, the Riemann zeta function), Theorem provides a novel route to showing that {{formula:b70c6f1e-24cd-4123-b37f-a1dd4fcc3a28}} is nonzero in {{formula:feff8823-248d-4f80-bb5a-0116ff37cdb4}} . For such {{formula:dc24a8cc-ed94-486f-b078-2add94e54d1f}} , one can easily show that {{formula:c7dc0c88-0c7c-45"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding Perseus' non-thermal X-ray relation to Active Galactic Nuclei (AGN) in this research paper?",
      "expected_output": "The main findings regarding Perseus' non-thermal X-ray relation to Active Galactic Nuclei (AGN) in this research paper are that the non-thermal X-ray emission in Perseus is likely connected to the central bright AGN, NGC 1275. The paper reports that the non-thermal component is variable and exceeds the flux of NGC 1275 by a factor of 3. The BAT spectrum also shows evidence of a hard X-ray excess, which can be fit by a steep power-law. This suggests that the non-thermal emission is due to the emission of the central AGN NGC 1275.",
      "context": [
        " Perseus, NGC 1275. A hard X-ray component has been detected with HEAO 1 by . used BeppoSAX and previous RXTE measurements to prove that this non-thermal component is variable and must therefore be connected to the central bright AGN. reported, using Chandra, the presence of non-thermal X-ray emission in the core of Perseus in correspondence of the radio mini-halo , . This non-thermal emission, which displays a power-law behavior with photon index of 2.0, seems to exceed the flux of NGC 1275 by a factor {{formula:75b0d083-47bc-41be-95b1-47a02e32eab3}} 3 .The BAT spectrum shows evidences of an hard X-ray excess. Indeed, it can be fit by a steep power-law (photon index of 3.5{{formula:b553a5d5-e81f-4784-a775-857607b46b23}} and {{formula:d7a7a2b6-66da-49b3-ab4b-e4918b33098f}} ) while it rejects",
        " to the local X–ray output.\nNon-thermal hard X-ray emission: Direct evidence of the presence of relativistic electrons in the ICM arises from the existence of large radio halos , . The same electron population responsible for the synchrotron emission can in principle scatter CMB photons by IC and produce hard X-ray radiation. The intensity of this radiation relative to the synchrotron emission ultimately depends on the value of the magnetic field.A firm detection of non-thermal components in the spectra of galaxy clusters has remained elusive in the past as well as in this study. Indeed, Perseus is the only galaxy cluster in the BAT sample where a non-thermal high-energy component is revealed at high significance. Most likely this component is due to the emission of the central AGN NGC 1275. The rest of the clusters detected by BAT do not show a significant non-thermal emission. Using BAT data alone, we are able to constrain the non-thermal component below the mCrab level in the 50–100 keV energy band. The BeppoSAX detection above 50 keV of an average non-thermal component in the stacked spectrum of several clusters is consistent with the BAT upper",
        "# arXiv Paper: 0809.0006\n\nGalaxy Clusters in the Swift/BAT era: Hard X-rays in the ICM\nabstract: We report about the detection of 10 clusters of galaxies in the ongoing Swift/BAT all-sky survey. This sample, which comprises mostly merging clusters, was serendipitously detected in the 15--55 keV band. We use the BAT sample to investigate the presence of excess hard X-rays above the thermal emission. The BAT clusters do not show significant (e.g. >2 $\\sigma$) non-thermal hard X-ray emission. The only exception is represented by Perseus whose high-energy emission is likely due to NGC 1275. Using XMM-Newton, Swift/XRT, Chandra and BAT data, we are able to produce upper limits on the Inverse Compton (IC) emission mechanism which are in disagreement with most of the previously claimed hard X-ray excesses. The coupling of the X-ray upper limits on the IC mechanism to radio data shows that in some clusters the magnetic field might be larger than 0.5 $\\mu$G. We also derive the first logN-logS and luminosity function distribution of"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on Higgs portal DM's interactions and how do they affect direct detection methods?",
      "expected_output": "The research on Higgs portal DM's interactions found that introducing a new decay channel for the singlet scalar can reduce tension between successful Big Bang Nucleosynthesis (BBN) and direct detection. Alternatively, allowing for a small Majorana mass for the DM can also relax direct detection constraints. The study also showed that the non-trivial momentum dependence of scattering in direct detection can lead to distinctive recoil spectra, which can be used to indicate the existence of a light mediator if multiple experiments observe a signal.",
      "context": [
        "09-496a-a5f6-a32071aef18c}} in order for this constraint to become relevant.\nOutlook and Conclusions: Here we have explored a simple way of reducing the tension between successful BBN and direct detection in Higgs portal DM by introducing a new decay channel for {{formula:9611fddf-e180-48ff-bdaf-48858352a7fe}} so that it may decay before BBN. Another alternative to reducing the tension between BBN and direct detection is to allow for the possibility of a small Majorana mass for the DM, {{formula:ea56c3f4-0712-4d7d-b538-478f2d06fd30}} (see e.g. , ). Then with {{formula:12d4e046-f09f-4fab-813d-fac143192dc1}} direct detection constraints can be considerably relaxed or removed altogether when the mass splitting is so large that no DM in the halo has sufficient kinetic energy to inelastically scatter. {{figure:3c11158a-7622-4532-9c50-ab3159a467a4}}Light",
        "46a2-80eb-ddb2637c9a81}} , the non-trivial momentum dependence of the scattering in direct detection can lead to distinctive recoil spectra , , . If two or more direct detection experiments with disparate targets observe a signal, the combined data may yield an astrophysics-free indication of the existence of a light mediator . Regions of the parameter space with this sensitivity will have low mixing angle, {{formula:e3b7c988-8bff-4a99-a95c-b5687407e73f}}We have seen that phenomenology of self-interacting DM through the Higgs portal is quite rich. In this model a new singlet scalar provides DM with a velocity-dependent self-scattering cross section, which can well-satisfy the desire to have large self-interactions at dwarf scales while satisfying the constraints from cluster scales. In the minimal setup where the scalar only interacts with DM and the SM Higgs there is considerable tension with the combination of BBN and direct detection constraints. We focused on a simple solution to evading the BBN bounds, which is to endow the singlet scalar with interactions with sterile neutrinos.",
        "formula:96dacdcb-807e-4d06-b7df-3e7224460191}} GeV. Future low-threshold direct detection searches and improvements on the invisible Higgs width will further constrain this window at low DM masses.In the following section we explore an alternative solution to the BBN problem that allows for a much wider range of DM masses.\nCoupling {{formula:7f41abdb-a199-40e3-a71a-995baf95e04f}} to Neutrinos: A simple way for {{formula:0a01108f-574a-49bb-be82-fde696dde24d}} to decay sufficiently early in order to satisfy BBN constraints is to introduce a new coupling to neutrinos. This can proceed in one of two ways: (1) {{formula:4056d9f5-88c5-4312-ab8a-ade84e3837f3}} decays to a sufficiently light right-handed sterile neutrino via {{formula:6d6e0c20-f208-46e1-8ae4-7a"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between CR invariants and weights in the context of this research paper?",
      "expected_output": "According to the research paper, CR invariants have a specific weight associated with them. For example, a CR invariant form of weight 1 is mentioned, indicating that the weight is a key characteristic of CR invariants. The paper also discusses how combining certain equations implies that a particular form is a CR invariant of a specific weight, highlighting the relationship between CR invariants and their corresponding weights.",
      "context": [
        " {{formula:c825d077-8ef5-40c6-8e9a-e73f7f59748f}} has CR dimension {{formula:25470cbd-271f-4d14-b432-f9a36b1f9bb0}} , it holds that {{formula:3a1d8e82-5a56-4ca1-bd02-484f6e6904a8}} In particular, {{formula:3ea5702f-39f4-4ba5-be37-345cd9635fc7}} Combining eqn:Xinv-grad,eqn:Xinv-div,eqn:Xinv-trivial implies that {{formula:7ccfb8ee-4c61-4b30-b471-eb97e8230217}} is a CR invariant {{formula:b8ee731a-d326-4628-a0bb-df058594e79a}} -form of weight {{formula:1682f566-cbbe-4fb1-971d-a06de9ccda92}}",
        "-c989-48a1-ae11-eb7f63eb05f1}} be an invariant polynomial of degree {{formula:53264891-01b7-483d-b544-4fcd58932023}} , and let {{formula:09f172a3-42d0-4980-8d73-4b27b537f859}} be given by eqn:Xn. Then {{formula:b97959bc-a352-41d4-98a1-1d21c9bb7da9}} is a CR invariant {{formula:fdb7cc05-73f8-40ae-afa5-a8a46126c465}} -form of weight {{formula:0c1ed551-3af7-4057-9c1f-c6943af30fef}} ; i.e. {{formula:2aa0150e-b1de-44cc-8e58-5b921c26602a}}for all {{formula:5572f72f-3bfc-4a3d-b1d",
        "variations,eqn:mI-V that {{formula:9e72d57d-ff70-4943-88c8-5e9721acc227}} where {{formula:3d83565d-cb58-4413-8a53-7798b6c7ca41}} Combining eqn:mI-lapl,eqn:mI-mT,thm:X-invariant yields {{formula:b13937c1-62fa-493b-893d-c9f2cf4b20db}} Integrating this equation in {{formula:ab821f43-03cd-481c-ace2-1a51fcec814c}} yields the desired result.The rest of this section is devoted to the proof that the total {{formula:c7cdb9b3-840c-439d-ada1-b01bd9770f1d}} -curvature is a secondary CR invariant. The main task is to relate {{formula:a64534ea-2cb9-4924-8a53-6c5177f34cfb}}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding unitarity violation at high CM energy and its impact on scattering in this research paper?",
      "expected_output": "The research paper finds that the cross section violates tree-level unitarity at high center-of-mass (CM) energy, which can be avoided by replacing coefficients according to a specific formula. This adjustment ensures unitarity by introducing an energy-dependent factor, as shown in the formula where the exponent is chosen to guarantee unitarity. For example, in the context of scattering processes, the form factor used in Equation REF helps to maintain unitarity at high CM energies.",
      "context": [
        "f4}} -channel {{formula:f9633858-5933-4a9f-9cbb-8a8bfcb318fd}} and photon exchanges and depends on the vertex in Equation REF . The cross section violates tree-level unitarity whenever the center-of-mass (CM) energy is large enough. This can be avoided by replacing any coefficient {{formula:1bd657a3-3faa-4a24-b641-04e4eddd69d2}} in Equation REF according to {{formula:99285236-b3fa-45c3-a744-774836c13728}} where {{formula:cc0b71b0-0c30-4628-a83f-b06e0c004ed5}} is the CM energy of the scattering process and the exponent {{formula:5ef0ff80-81d9-4406-b776-8605fb76dea3}} is chosen to insure unitarity.In the experimental results we discuss, the form factor used is that of Equation REF . The value {{formula:5381987b-e9bc-",
        "a9642b}} and {{formula:53850d4f-0898-4920-99b0-e12b2b4751a6}} couplings are zero at tree level in the SM, and the {{formula:5b2da351-71b7-42c5-bdfd-4534a46212bb}} -channel diagram only contributes in the presence of anomalous couplings. As a result there is no radiation zero in {{formula:2189cdfd-1ed9-4e74-9ee8-41e877c90c86}} production. Moreover, in the SM the ratio of the {{formula:67c3bb02-86c5-4b15-bc4a-467e012f2e9a}} cross section to the {{formula:3da1d298-e772-47dd-8179-68007b36c35e}} cross section rises with increasing minimum photon {{formula:4d10ac30-a88c-4b49-a8bc-2e6a0d4d8e1b}} due to suppression of {{formula:32984",
        "fa21df9d-b49c-4e90-8e6d-40f62f524b7a}} . The leading-order helicity amplitudes and cross section for {{formula:a90d277d-d8f3-470f-a6a7-de2d53bd7bfd}} production have been evaluated . For anomalous couplings the {{formula:2f501fe6-13ce-412e-aaf7-a6a21bce5ab7}} -channel diagram contributes, resulting in events with higher average photon {{formula:3f0180de-5105-4ea4-a662-c3aac795bd1e}} , as shown in Figure b. Defining {{formula:d88d29ca-864c-407e-af26-07572fbf003f}} , it is found that the terms in the anomalous contributions to the helicity amplitudes are multiplied by factors of {{formula:f73873fd-0202-43df-8c39-774b811469cd}} for {{formula:e975e55f-fae0-4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What effect does W approaching 0 have on the formation of C_{S,R}, and how does this relate to the main findings of the research paper?",
      "expected_output": "As W approaches 0, the formation of C_{S,R} is negatively affected due to the development of compact localized states. This is a critical finding in the research paper, as it shows that disorder is necessary to bypass these compact states and enable efficient quantum state transfer (QST). In fact, the paper highlights that the increasing of W actually benefits the formation of C_{S,R} for very low disorder widths, as shown in Fig. REF. This reverse trend is understood as a reminiscent influence of the compact localized states, which restricts the localization length of the flat-band modes. The research paper concludes that disorder is needed to make the QST faster and more feasible, and that the optimal performance is achieved at intermediate disorder levels, where fidelities above the classical threshold of 2/3 can be reached.",
      "context": [
        "fcec9b5}}Things are different, however, as W\\rightarrow 0. In this regime, we observe the opposite behavior for \\Lambda . The increasing of W actually benefits the formation of C_{S,R}. The PDF for very low disorder widths is shown in Fig. REF . Such a reverse trend can be understood as a reminiscent influence of the compact localized states, to which \\Lambda =0, as we depart from W=0. As the disorder increases, the flat-band modes become strongly localized again, but are no longer restricted to each cell. Instead, their localization length scales as \\xi \\sim W^{-\\gamma } , . It is between those two regimes that the QST will occur.\nProtocol performance: Now that all the relevant quantities that govern the speed and quality of the QST have been presented, we are ready to the test its performance against W.Any disorder induces fluctuations in the energy spectrum and affects the transfer time \\tau \\propto \\delta \\epsilon ^{-1}. In Fig. REF we show how the gap \\delta \\epsilon = \\epsilon _1-\\epsilon _2",
        "ambda (Fig. REF ). Yet, it is possible to reach fidelities above the classical threshold of 2/3 , at intermediate disorder levels. Figure REF displays histograms of the maximum fidelities for N=10 and some selected values of W.We remark that the poor performances associated to the lower values of W in Fig. REF (a) is a consequence of the chosen time interval. To confirm this, Fig. REF (b) shows the behavior of correlation C_{S,R} against W. As we have seen, C_{S,R} ultimately determines the quality of the QST. Therefore, in this case higher fidelities can be achieved at times t>t_{\\mathrm {max}}. But if we were to consider disorder levels W\\ll 0.001J [inset of Fig. REF (b)], then the QST would be unfeasible because of the reverse localization trend discussed earlier (see Fig REF ). Indeed, C_{S,R} must vanish as W\\rightarrow 0 so as to conform with the development of the compact localized states.\nConcluding remarks: We studied a resonant QST through a flat band hosted by a disordered diamond",
        " show how the gap \\delta \\epsilon = \\epsilon _1-\\epsilon _2 (in units of g) responds to W. It grows roughly linear with W, not being affected by the number of cells N. Another caveat to the limit W\\rightarrow 0 is that a vanishing gap implies in a extremely slow QST, which is not a desirable feature. Therefore, disorder is needed to bypass the compact localized states of the flat band and also to make the QST faster.Because the fluctuations in the gap increase with W, we track the QST fidelity over a given time window (instead of a specific time). Let us define F_{\\mathrm {max}} = \\mathrm {max}\\lbrace F(t) \\rbrace as the maximum fidelity achieved for t \\in [0,t_{\\mathrm {max}}], where t_{\\mathrm {max}}=20\\pi /g. Given \\tau = m \\pi /\\delta \\epsilon , we remark that the value of t_{\\mathrm {max}} corresponds to m=1 and \\delta \\epsilon / g =0.05. In this way, the QST"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the significance of Proposition 5.1 in relation to the stable locus, and how does it contribute to the overall findings of the research paper?",
      "expected_output": "Proposition 5.1 is significant because it states that the complement to the completely stable locus for the given action is the fixed locus, making the completely stable locus the largest possible stable set. This contributes to the overall findings by providing insight into the structure of the stable locus and its relation to the action's invariants, as seen in the context of the morphism and its properties discussed in Lemma 4.3.",
      "context": [
        " {{formula:305e74ee-8171-4050-bcb7-d7f220e25bbc}} ).Henceforth, for notational simplicity, we consider only the morphism {{formula:69b31a7a-bf71-4470-832b-c37a36d789a7}} , and we note that the same results hold, with only minor modifications to the proofs, for {{formula:998a418d-98b8-47ec-b06a-3f9fefe5abed}} and {{formula:b4c27c0e-3a17-413c-8837-9e8d84f2f2ca}} .Lemma 4.3 The morphism {{formula:926e6857-84c8-4ed1-a3f9-23617e169442}} has the following properties.The stabiliser groups are preserved by {{formula:b1d6d7a0-d57f-4823-9502-bdf94b1de060}} . Let {{formula:0798b913-fe7b-4bc",
        "4cee28}} . Then {{formula:72d0b9e2-5f29-4b1a-832d-1d5a3b5abc4e}} . Since {{formula:e00401b2-e6f9-4ada-ba87-be89963d8afb}} , the stable set {{formula:ae5fe558-2c39-481a-b412-37a9a4f88506}} is the union of all open subsets given by the non-vanishing locus of an {{formula:ece46177-b448-4517-8c6f-2093fef3b6dd}} -invariant. Hence {{formula:19f300a6-420e-45fa-9a88-2537d2a3ce93}} is {{formula:c6c4cc5c-f1e9-4752-9770-e04622ee13af}} -stable and has trivial stabiliser group by Lemma REF . For (ii), let {{formula:f5860ea4-81d0-4ab2",
        "29-b7b9d7b3dd5e}} .Proposition 5.1 Let {{formula:4f9b8d09-ef51-4b72-82e9-62c2a0e3818d}} be as above. ThenThe complement to the completely stable locus for the {{formula:08926d0a-c6bd-4b51-ad0f-e5acd9d2f54a}} -action on {{formula:bf23a092-9d8a-47e4-87ec-7f50a4c4b9d4}} is the {{formula:76f94379-1405-42dd-8f25-71a8bfeff28c}} -fixed locus. In particular, the completely stable locus is the largest possible stable set. The {{formula:8a62b0bf-a837-46ea-8f55-6c7e0285fd4e}} -invariants on {{formula:0344a23e-c5d7-44e3-a3fe-e6c041b95"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does rotational periodicity play in High Harmonic Generation and how does it impact the resulting spectra?",
      "expected_output": "According to the research paper, rotational periodicity plays a crucial role in High Harmonic Generation (HHG) by introducing a quasi-band model, which is analogous to the band theory of solids. This periodicity leads to the observation of explicit selection rules for left-handed and right-handed harmonics under circularly polarized laser irradiation. Furthermore, in linearly polarized laser fields, rotational periodicity results in solid-like HHG, originating from intra-band oscillations and inter-band transitions. This phenomenon has the potential to optically detect the symmetry and geometry of controversial structures, establishing a connection between the high harmonics of gases and solids.",
      "context": [
        "# arXiv Paper: 2201.07100\n\nSolid-like high harmonic generation from rotationally periodic systems\nabstract: High harmonic generation (HHG) from crystals in strong laser fields has been understood by the band theory of solid, which is based on the periodic boundary condition (PBC) of translational invariant. For systems having PBC of rotational invariant, in principles an analogous Bloch theorem can be developed and applied. Taking a ring-type cluster of cyclo[18]carbon as a representative, we theoretically suggest a quasi-band model and study its HHG by solving time-dependent Liouville-von Neumann equation. Under the irradiation of circularly polarized laser, explicit selection rules for left-handed and right-handed harmonics are observed, while in linearly polarized laser field, cyclo[18]carbon exhibits solid-like HHG originated from intra-band oscillations and inter-band transitions, which in turn is promising to optically detect the symmetry and geometry of controversial structures. In a sense, this work presents a connection linking the high harmonics of gases and solids.\nAcknowledgments: This work was supported by NSF of China Grant (No. 11974185, 119040",
        "ids.\nAcknowledgments: This work was supported by NSF of China Grant (No. 11974185, 11904028, 12174195, and 11834004), the Natural Science Foundation of Jiangsu Province (Grant No. BK20170032).{{figure:eb63b524-0c73-480f-9418-8eead17ab9aa}}{{figure:0b6f28d0-cf16-4956-b66b-0d68e4661fea}}{{figure:a73662df-b483-453d-961a-28ba0a1b34c0}}{{figure:89d1c4b1-eeb5-4828-bf62-269d6f1185d5}}{{figure:915448dc-0741-4f5e-ba2b-8b391508847f}}{{figure:c27fa6dd-1a67-4f3a-abe4-3bb8d567a863}}{{figure:7f653b79-1e14-4736-9533-8acc1cedd32d}}"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does the framework use to demonstrate MapReduce-imperative equivalence, and what are the key findings of this research?",
      "expected_output": "The framework uses two methods to demonstrate MapReduce-imperative equivalence: behaviour-preserving rewrite rules and equivalence proofs using coupling predicates. The key findings of this research are that it provides a framework for proving equivalence of MapReduce and imperative programs, and it has been successfully applied to a non-trivial algorithm, the PageRank algorithm, through eight transformation steps.",
      "context": [
        " the translation into intermediate steps yields programs which are often structurally similar. We have found that in this case, techniques such as coupling predicates arise naturally and are useful for selected parts of an equivalence proof. De Angelis et al. present a further generalised approach.Radoi et al. describe an automatic translation of imperative algorithms to MapReduce algorithms based on rewrite rules. While the rewrite rules are very similar to the ones used in our approach, we complement rewrite rules by coupling predicates. Furthermore we are able to prove equivalence for algorithms for which the automatic translation from Radoi et al. is not capable of producing efficient MapReduce algorithms. The objective of verification imposes different constraints than the automated translation – in particular both programs are provided by the user, so there is less flexibility needed in the formulation of rewrite rules.Chen et al. and Radoi et al. describe languages and sequential semantics for MapReduce algorithms. Chen et al. describe an executable sequential specification in the Haskell programming language focusing on capturing non-determinism correctly. Radoi et al. use a language based on a lambda calculus as the common representation for the previously described translation from imperative to MapReduce algorithms. While this language closely resembles the language used",
        " as the common representation for the previously described translation from imperative to MapReduce algorithms. While this language closely resembles the language used in our approach, it lacks support for representing some imperative constructs such as arbitrary while-loops.Grossman et al. verify the equivalence of a restricted subset of Spark programs by reducing the problem of checking program equivalence to the validity of formulas in a decidable fragment of first-order logic. While this approach is fully automatic, it limits programs to Presburger arithmetic and requires that they are synchronized in some way.To the best of our knowledge, we are the first to propose a framework for proving equivalence of MapReduce and imperative programs.\nConclusion: In this paper we demonstrated how an imperative implementation of a relevant, non-trivial algorithm can be iteratively transformed into an equivalent efficient MapReduce implementation. The presentation bases on the formal framework described in . Equivalence within this framework is guaranteed since the individual applied transformations are either behaviour-preserving rewrite rules or equivalence proofs using coupling predicates. The example that has been used as a case study in this paper is the PageRank algorithm, a prototypical application case of the MapReduce programming model. The transformation comprises eight transformation steps.Future work"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do knot invariants play in coloring, and how do the main findings of this research paper relate to their methods and key results?",
      "expected_output": "Knot invariants, such as the determinant, play a crucial role in coloring by determining the minimum number of colors needed. The research paper presents the Kauffman-Harary conjecture, which states that a non-trivial coloring on a reduced diagram of an alternating knot of prime determinant assigns different colors to different arcs. The main findings of this paper relate to the calculation of the minimum number of colors for different moduli, with Theorem 1.1 providing a key result on the number of colors needed given positive integers. For example, the paper discusses how the determinant of a knot, denoted by the formula, is a knot invariant and how it affects the coloring of the knot.",
      "context": [
        " knot invariant and so is any minor of this matrix. The first minor of this matrix is called the determinant of the knot {{formula:d4fbef9b-e18b-44e7-bc1e-610a7c76230a}} , {{formula:666d34b8-1c21-4693-afd2-de4fe4f000c0}} .The topic of minimum number of colors was set forth in where the Kauffman-Harary conjecture was presented. Given a prime {{formula:713c3d76-1212-4ea3-983e-82dd267c286c}} , this conjecture states that a non-trivial {{formula:86bc29e9-b0b7-4177-bbba-71eb87e8218b}} -coloring on a reduced diagram of an alternating knot of prime determinant {{formula:b3b83b76-b590-431b-b7c0-1ac64388b1f0}} , assigns different colors to different arcs. At the time of the writing of this article, there is an alleged proof of this conjecture available",
        "-8e3edee82ad9}} .There is another article which addresses colorings on Turk's head knots ( ), but its goal is to show that families of these knots satisfy the Kauffman-Harary's conjecture by showing their determinants are composite numbers. In this way, the current work addresses a different matter which is the calculation of minimum number of colors for different moduli.Here are the main results in this article. We remark that given two positive integers {{formula:f7104fc8-433a-44e8-82e6-6ba036570b23}} , we let {{formula:f2eab58d-c66f-472f-9910-924804f00560}} stand for their greatest common divisor.Theorem 1.1 Given positive integers {{formula:6b212420-073e-46d6-b09b-3a481f3d6475}} and {{formula:8d81f8dd-ae77-4ff9-b4c6-0b3adbd1d2ff}} , the number of {{formula:f2e8be67-9ca"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does prior knowledge affect the relaxation of PE conditions in this study, and what methods were used to measure its impact?",
      "expected_output": "According to the study, prior knowledge of the interaction matrix relaxes the PE condition. The researchers used the assumption that the coupling functions are exactly known and the interconnection vectors satisfy a specific condition to demonstrate this relaxation. For example, they showed that without prior knowledge, the PE condition is necessary to reconstruct properties, but with prior knowledge, gaps between sets are created, allowing for relaxation of the PE condition.",
      "context": [
        ":a35d35d7-3070-4178-9a99-f027b21694c3}} shrinks the domain of a property {{formula:fdcccd2e-7c89-4aac-8688-6d8ea357e458}} , create gaps between the sets {{formula:16149d1f-4b6d-4a8c-93c4-7cc698dc100d}} in {{formula:1216dc0f-1844-484c-a47d-e4d87da8b73a}} and hence relax the PE condition.\nPrior knowledge of the interaction matrix relaxes the PE condition: For clarity, in this section we assume that the coupling functions are exactly known. The simplest prior information of {{formula:e637c7e1-2369-4bc4-a36c-511eb803fcce}} is that the interconnection vectors satisfy: {{formula:1a2c3b51-6eea-46b7-a386-f46990e24608}}where {{formula:8a6e5ee0-af7b-48",
        "4-059b6127a0d0}} . {{figure:6f32cf7b-e1a2-4b03-b6bd-841e3a60cec2}}Consequently, in a generic case, the PE condition (REF ) is necessary in order to reconstruct any property. Even if the coupling functions are exactly known, without PE we cannot generically reconstruct the sign/connectivity/adjacency patterns or degree sequence. The reason is simple: for all these properties there is no gap between the sets {{formula:a9f5098f-d820-4211-9454-410cc4a610f0}} . For example, for {{formula:e22b4b03-dc0c-42a8-a32f-f46a93f45c16}} , the vectors {{formula:b909718e-ff90-4478-8fdb-2c36636aa11b}} and {{formula:b6da4364-2ae3-44a4-a3db-eee6ca1af00e}} are infinitesimally close in {{formula:2f"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do the parameters rho_0 and rho_1 relate to each other through the variable Z in the context of this research paper?",
      "expected_output": "The parameters rho_0 and rho_1 are related to each other through the variable Z by the equation rho_1 = (e^Z) / (1 + rho_0) - 1, which implies that rho_0 and rho_1 have a one-to-one relationship and take values only in the domain [0, e^Z - 1].",
      "context": [
        "{f}(\\rho _0,\\rho _1)=\\tilde{f}(\\rho _1,\\rho _0). Using (REF ), the problem can be rewritten as a monovariable unconstrained problem\\min _{\\rho _{0}} \\tilde{f}(\\rho _0)= \\rho _0^\\alpha +\\rho _1^\\alpha = \\rho _0^\\alpha +\\left(\\frac{e^Z}{1+{\\rho _0}}-1\\right)^\\alpha .The derivative of \\tilde{f}(\\rho _0) and its limit at the bounds of its domain are given by\\tilde{f}^{\\prime }(\\rho _0)&=\\frac{\\alpha }{\\rho _0^{1-\\alpha }}-\\frac{e^Z}{(1+\\rho _0)^2}\\frac{\\alpha }{\\rho _1^{1-\\alpha }},\\ \\lim _{\\rho _0\\rightarrow 0}\\tilde{f}^{\\prime }(\\rho _0)=+\\in",
        " _{\\rho _0,\\rho _{1}} \\tilde{f}(\\rho _0,\\rho _1)=\\sum _{n=0}^{1} \\rho _n^{\\alpha } \\ \\text{s.t. } \\sum _{n=0}^{1} \\log \\left(1+\\rho _n\\right) = Zwhere Z=NR \\log 2 - \\sum _{n=2}^{N-1} \\log \\left(1+\\rho _n\\right). The constraint implies that\\rho _1&=\\frac{e^Z}{1+{\\rho _0}}-1so that \\rho _0 and \\rho _1 take values only in the domain [0,e^Z-1] and have a one-to-one relationship. As \\rho _0 \\rightarrow 0, \\rho _1 \\rightarrow e^Z-1 and vice versa. Moreover, the problem is symmetrical so that \\tilde{f}(\\rho _0,\\rho _1)=\\tilde{f}(\\rho _",
        " at the bounds of its domain are given by\\tilde{f}^{\\prime \\prime }(\\rho _0)&=-\\alpha (1-\\alpha ) \\frac{1}{\\rho _0^{2-\\alpha }}+\\frac{1}{\\rho _1^{2-\\alpha }} \\frac{\\alpha e^Z}{(1+\\rho _0)^3} \\left(-(1-\\alpha ) e^{Z}\\frac{1}{1+\\rho _0} + 2\\rho _1 \\right) \\\\ \\lim _{\\rho _0\\rightarrow 0}\\tilde{f}^{\\prime \\prime }(\\rho _0)&=-\\infty ,\\ \\lim _{\\rho _0\\rightarrow e^{Z}-1}\\tilde{f}^{\\prime \\prime }(\\rho _0)=-\\infty . {{figure:9d5a82e6-7480-405f-a37d-21cfd355782f}}As shown in Fig. REF , three cases can be distinguished. In case (a), there is a single critical"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main enhancements of the TD framework extension to Q-Learning, and how do these improvements impact the overall performance of the algorithm?",
      "expected_output": "The main enhancements of the TD framework extension to Q-Learning include partitioning the Q-function over various discount factors, allowing the agent to concurrently learn action-values for several discount factors. This multi-timescale learning method can alleviate the bias-variance tradeoff, enabling the algorithm to efficiently handle both short-term and long-term rewards. The improvements impact the overall performance of the algorithm by enhancing its capacity to learn and adapt in complex environments with long-term dependencies.",
      "context": [
        " resulting in unstable learning .Recent research in TD learning has explored the segmentation of value functions across several time scales, allowing agents to simultaneously improve value estimates at various horizons . The TD({{formula:8e39d6ac-a421-44ae-9c7d-fd9bd46be49e}} ) design improves scalability and performance in scenarios with long-term dependencies by deconstructing value functions into components that use diverse discount factors, known as TD({{formula:ad648be3-1bd3-4335-8dfe-b3bdb4e6e886}} ). The adaptability of the TD({{formula:dc813590-f5da-46c4-a539-1bac8c7ba1ce}} ) paradigm shows considerable promise in alleviating the bias-variance trade-offs intrinsic to TD learning.Expanding on this concept, we introduce Q({{formula:14d3d312-3023-4503-9581-c85151ba3242}} )-Learning, an extension of TD({{formula:853bde70-dca7-42fa-865d-",
        "cc72d}} )-learning is that the Temporal Difference (TD({{formula:d8ace902-919b-4c07-badf-73993fbc6900}} )) learning framework addresses the restrictions of employing a singular discount factor in long-horizon tasks by decomposing the Q-function across several time scales. This multi-timescale learning method can alleviate the bias-variance tradeoff by allowing the agent to concurrently learn action-values for several discount factors. Extending TD({{formula:dc13671d-19cd-4aaa-87b9-097d5c259e4c}} ) to Q-Learning enables the algorithm to partition the Q-function Q(s,a) into delta components over various time scales, hence enhancing its capacity to efficiently handle both short-term and long-term rewards.This paper's principal contributions are as follows: (i) We enhance the TD({{formula:9cb3660a-e3da-4b75-9d79-2a49cc04c15a}} ) framework for Q-Learning by partitioning the Q-function Q(s,a) over various discount factors {{form",
        "65c-cad4de13a41c}} )-Learning.Recent study has focused on the exact determination of the discount factor , , , . The authors of this study present meta-gradient methods for the adaptive selection of discount factors in RL, highlighting the importance of balancing short-term and long-term rewards, which aligns with the reasoning for analyzing action-value functions across multiple discount factors in Q({{formula:c4df6be2-7394-400c-af2b-400f64154b8d}} )-Learning.Finally, another significant area of study relevant to our research indirectly includes hierarchical RL and the bias-variance tradeoff. The referenced studies , , , , , , introduce the notion of partitioning value functions into subtasks through hierarchical RL, similar to how TD({{formula:ffb70232-1bf6-40e3-afb8-81a0537e9e5f}} ) disaggregates value functions across different time-scales. This supports the claim that value function decomposition can improve learning efficiency and stability. This paper serves as a theoretical basis for understanding the bias-variance tradeoff involved in decomposing Q-values over different"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main differences between profinite case and other cases in the research paper, and how do the methods used to study profinite case impact the key results?",
      "expected_output": "The main differences between the profinite case and other cases in the research paper lie in the connectivity and disjoint union properties of certain groups. In the profinite case, Theorem 4 states that there exists a 2-generated profinite group with the property that a specific subset is the disjoint union of connected components, and each connected component is dense. This is in contrast to the non-profinite case, where it is an open problem whether a similar subset is connected when the group is an arbitrary finite group. The methods used to study the profinite case, such as Lemma 5, provide key results, including the characterization of non-isolated points in a profinite group and the equivalence of certain conditions for open normal subgroups.",
      "context": [
        "97a9-f8c09eac5c81}} for which {{formula:312d958b-f505-4763-86ea-c355bb445f68}} is disconnected, and it is an open problem whether or not {{formula:f0bd808d-947e-4dbb-bc7c-5734a3e795a0}} is connected when {{formula:0a05e134-2194-4cbf-b4be-8f12323b23cc}} is an arbitrary finite group. The situation is different in the profinite case.Theorem 4 There exists a 2-generated profinite group {{formula:b2ec8123-fe6a-432d-81cd-062bd0f494d9}} with the property that {{formula:8043078d-87d6-4797-9299-83f5871c64af}} is the disjoint union of {{formula:78a4f691-defe-4ccc-8e88-37e044e43257}} connected components. Moreover each connected component is dense in {{formula:4b",
        "}} is a 2-generated profinite group, then {{formula:16170e57-f9f8-4a16-9670-23596a30aadc}} is a closed subgroup of {{formula:d7d2bb14-e7e9-4418-ab73-5066081bd03c}}We prove that {{formula:eb63b583-c58b-4b90-964c-9e20db1073df}} is an open subset of {{formula:ff4c1eee-f6ef-49c7-ba1b-73d307eaf7c2}} Let {{formula:6b2d966a-07ca-4747-84e6-eaa6646e83cc}} By the previous lemma, there exists {{formula:9bc89c6a-8768-44a7-8d12-4102d8d100c0}} such that {{formula:19dc2639-6657-4348-869c-d3213af8141c}} . This means that {{formula:d3b86c",
        "d3cc9394-9fb6-4377-80fa-af427510f390}} of a profinite group {{formula:b41b604b-4264-4c92-a1b8-5a23dce0ab82}} is non-isolated.Lemma 5 Let {{formula:43e08062-5d70-4961-9786-726f2d893212}} be a 2-generated profinite group. Then {{formula:e89908ed-4545-4f7c-9b76-64729b2aa73d}} if and only if {{formula:dbcdc8c3-e60d-40b4-b328-6dccf1fbe5b9}} for every open normal subgroup {{formula:2e666477-be20-48f5-a034-f3af0cb1cff7}} of {{formula:56abde76-c779-46ff-95e7-4db671847208}}Let {{formula:1bc9f630-9e4e-4055-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do quadratic subproblems play in affecting the scalability of networks, and how do the researchers address this issue in their findings?",
      "expected_output": "Quadratic subproblems play a significant role in affecting the scalability of networks, as they pose computational challenges due to the large network dimension. The researchers address this issue by leveraging the ADMM method and component-based decomposition, which enables parallel computation of the SQP subproblems on GPU platforms, resulting in a significant reduction in solution time, particularly as the network size increases.",
      "context": [
        " techniques, we effectively address the scalability and computational challenges posed by a large quadratic subproblems due to network dimension. Leveraging the ADMM method and the idea of component-based decomposition, we achieve parallel computation of the SQP subproblems on GPU platforms. Through our numerical study, we observed that the GPU-based SQP-ADMM method demonstrates significant reduction in solution time, as compared with the SQP method with the interior-point method QP solver, as well as that with the CPU-based ADMM solver, particularly as the network size increases. As future work, we will focus on techniques to further accelerate the SQP algorithm and improve overall solution accuracy such as addressing the inexact ADMM steps for larger instances.",
        "68rte, we observe an almost 10 times advantages from SQP-GPU over Ipopt. We also observe that the dual infeasibilities are very small even when we are not able to reduce the primal infeasibility below 10^{-3}.\nLimitations: Although we have achieved computational advantages over CPU-based ADMM solvers and Ipopt, there are still limitations to the current SQP-GPU implementation. First, we use only a basic trust-region SQP structure (i.e., Algorithm REF ) without supporting features such as feasibility restoration and second-order correction , which may further improve the solution accuracy and computational performance as well as resolving the QP infeasibility. Second, our tuning for ADMM is heuristic and aims to achieve the best performance, which encourages early termination or high tolerance for larger instances. In this sense, we would like to present our work as a preliminary work to this topic of accelerating SQP algorithm using parallel computation.\nConclusion: This paper introduces a GPU-accelerated SQP algorithm for solving the ACOPF problem. By combining decomposition and parallel computing techniques, we effectively address the scalability and computational challenges posed by a large quadratic subproblems due to network dimension",
        "468 buses and observe that as the network size increases, the GPU-based SQP-ADMM method exhibits greater computational advantages over centralized SQP methods and CPU-based SQP-ADMM methods.The rest of the paper is organized as follows. A preliminary introduction to the trust-region SQP algorithm is first given in Section . In Section we introduce the ACOPF formulation and formulate the corresponding QP subproblem for ACOPF with modifications. In Section we discuss the ADMM algorithm by component decomposition to solve the QP subproblem as well as the GPU implementation to achieve parallel computing. Section demonstrates our numerical results by comparing with SQP-ADMM methods on CPUs and GPUs with centralized SQP methods with Ipopt. We also discuss the limitations of the current work. Section briefly summarizes our conclusions and possible future work.\nPreliminary: Trust-Region SQP: This section serves as a preliminary introduction to the trust-region SQP algorithm considered in this paper. For the algorithm description, we consider the following NLP formulation:\\min _x \\quad & f(x) \\\\ \\text{s.t.} \\quad & Ax \\ge b, \\\\ & c_i(x) = 0,\\quad \\forall"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the conditions under which a linear CP map qualifies as a quantum channel, and how does this relate to the main findings of the research paper?",
      "expected_output": "A linear CP map qualifies as a quantum channel when it is trace-preserving (TP). According to the research paper, if the map is TP, then it is a quantum channel and can be implemented deterministically. For example, the paper states that when the map is linear and TP, the requirement in Definition REF is equivalent to the weaker condition. This highlights the importance of TP in determining whether a linear CP map is a quantum channel.",
      "context": [
        "062-45d0c165d4a8}} and yields {{formula:737dd6ff-9a4f-4a44-a214-324601978b21}}where {{formula:810f7ba0-a514-4b23-90f4-de261c6f63ac}} is a linear CP map. Map {{formula:1c63cc87-eb3a-4726-b23e-66cd3ea15811}} can be linear, when {{formula:c7302063-6524-4084-b7db-00d60fe8b2d8}} is TP (so {{formula:af95162e-ba62-45f3-9e87-4d250d6aeb63}} is a quantum channel), thus conditional quantum maps include all quantum channels. Map {{formula:aaf7360c-a965-4715-94b7-e41893c5b045}} can also be nonlinear, which occurs when {{formula:752eef0a-43dd-4fd4-9040-7be9358e1d",
        "-9f45-f0e012c94ade}} Note that if in Eq. (REF ) {{formula:8be5c475-b890-489d-ad1b-fc08deb3b40e}} , then the original linear CP map {{formula:8ff88c46-737c-46b1-83ed-93bfbdb205b7}} is also Gaussian. And if {{formula:87fad9ae-55fc-48a2-8672-01362c92cf73}} is linear, the requirement in Definition REF is equivalent to the weaker condition: {{formula:f5eea66e-aecc-4d1c-b389-af610a317226}} , . Since on Gaussian inputs, Gaussian measurements can also be transformed to TP operations by post-processing , we are particularly interested in the set of Gaussian channels {{formula:1edc6dc0-76be-4c95-81f1-9d38a3d02e26}} . Any quantum operation outside {{formula:d0f87f5b-d6cf-4554-",
        " states, i.e. {{formula:f62b4744-cf04-4bb5-826b-8ec6ba22d291}} , and similarly {{formula:f1ab5de0-70de-4850-a5cc-8ac0f235fc19}} .When {{formula:59877822-a79d-451f-9eab-396abfaa723a}} is also trace-preserving (TP), it is a quantum channel and can be implemented deterministically. {{formula:d3f7fb54-d4d0-4b89-ab67-bb4a5dd72052}} can also be non-trace-preserving. In that case, {{formula:113d83ea-23b3-404e-804e-52422dd1aab3}} is implemented probabilistically. The probability of the map {{formula:2935dd08-1af2-4bd1-9804-43b83f41c74a}} successfully happening is given by {{formula:989a6440-554e-4755-84a6-ca7"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the definition of a homogeneous tree and how is it applied in research?",
      "expected_output": "According to the research paper, a homogeneous tree is defined as a tree that is homogeneous in a specific set. For example, a finite tree determined by a compact open set can be identified with a subset of another tree. In Lemma 2.8, it is stated that a compact set is homogeneous if and only if a finite set has a specific property. This concept is applied in research to prove that certain trees are homogeneous, such as the tree in Theorem REF. A homogeneous tree is also called a finite homogeneous tree, and it depends on the values taken by specific parameters.",
      "context": [
        "dc}} is considered as a subset of {{formula:9d37be9b-21bd-4590-b75d-48c34a4877e6}} , the tree {{formula:dc37ee5d-1bbc-48e9-b264-d7c2b5755150}} could be identified with the finite tree determined by the compact open set {{formula:3be294e4-7350-488d-9b83-54ea58fba91e}} . By definition, we immediately have the following lemma.Lemma 2.8 The above compact set {{formula:b6e5e7a3-477c-4af2-b608-7d302b9c45a8}} is {{formula:8a5eed91-2cd6-423b-a64b-96878927b1fd}} -homogeneous in {{formula:bebe9f61-b62a-42f7-9a95-aaa872bd0de9}} if and only if the finite set {{formula:5dd9ffce-0380-4fef-baa",
        "d-bf8a-e3bbdd7155f4}} : Observe that {{formula:9e9e1a3d-f317-4339-826c-c89b514fd1d6}} means {{formula:93344abe-5fb1-4c88-b9a0-b8c60b0c078d}} which is exactly the condition in Theorem REF . Therefore we can prove that {{formula:8384bb91-76a1-4dce-9640-bfaf54eaf7ca}} and {{formula:4a9f62f1-fcfd-42c9-b15b-609e3c1e263f}} is a {{formula:fe0a07c2-6af0-4249-af2e-8154d73cf3c8}} -homogeneous tree. {{formula:d43048ee-6390-4b61-9a46-519513029e6f}} : Assume that {{formula:f83a5429-90a6-4981-84ad-8392c",
        "-4370-4df6-9a21-1a3976f46c99}} .Remark that such a subtree depends not only on {{formula:6cff36b6-9d89-451c-a142-0fe12cf245a7}} and {{formula:fdccb9f7-04ca-4dd5-a02b-236bc18ac467}} but also on the values taken by {{formula:a78ff06c-b26a-4ace-b7c1-413c37c51308}} 's with {{formula:4d39800b-022d-478c-a222-874be06fc333}} . A {{formula:58900a4c-2c77-4b48-a512-dc1a032cb997}} -form tree is called a finite {{formula:966d5520-e509-4786-8497-d5b3b3e28acd}} -homogeneous tree. A special subtree {{formula:00320ad9-9d1a-4f20-a59a-f5"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does tilt-invariance play in the Kelvin-wave cascade, and how does it affect the main findings of this research paper?",
      "expected_output": "Tilt-invariance plays a crucial role in the Kelvin-wave cascade as it ensures the mechanism of the cascade is invariant with respect to rotation of the axis along which the unperturbed vortex line is located. This means that any credible mechanism of the Kelvin-wave cascade must respect this symmetry. The research paper finds that the assumption of weak non-linearity is not always valid, and the value of the cascade exponent can depend on the energy input into the cascade. The paper also suggests a simple scenario of the crossover between the classical Kolmogorov and the quantum Kelvin-wave cascades, which does not encounter any problems of mismatch or bottleneck at the crossover. For example, the paper mentions that the Bio-Savart law satisfies the condition of tilting symmetry, which is important for numerical simulations of the Kelvin-wave cascade.",
      "context": [
        "a475-09e1d6c49810}} with initial conditions of their simulation and considered it as a transient law, which must eventually transform at longer times to the law {{formula:63d2a858-298d-48ad-a292-8a5d59ad6f95}} . Meanwhile, the present analysis suggests that the law {{formula:1793e27a-4e47-48db-af62-4293246f1172}} is not transient but would persist at longer times since at low {{formula:c5582d6f-6261-4d66-a510-7940f6d7aa77}} the Kelvin-wave amplitudes grow and become non-linear. Additional simulations are needed in order to resolve, which interpretation is true.All these laws are within the expected interval. In the light of our discussion of symmetry it is important that in numerical simulations of the Kelvin-wave cascade one used the equations of motion respecting tilting symmetry. The Bio–Savart law satisfies this condition. On the other hand, received their power law {{formula:75ca797e-398b-4a10-9b87-29161fab33",
        ", in which average vortex displacement and tilt are absent. This would be a mystery if the mechanism revived in any other frame.Both competing approaches assumed weak non-linearity and considered kelvon interactions of the lowest possible order. There were a number of numerical simulations for the Kelvin-wave cascade but they could not resolve the controversy supporting either this or that stance or being in disagreement with the both. The paper suggests that a variety of exponents revealed in simulations is because the assumption of weak non-linearity is not valid everywhere in the Kelvin-wave cascade interval, and the value of the exponent can depend on the energy input into the cascade. The scaling-based derivation of the exponent was generalized on the case of {{formula:407bd38c-5335-42bc-b3f5-c342405412ed}} -kelvon interaction, which yielded an interval of possible exponents. Most of exponents obtained in numerical simulations lie in this interval.The last topic of the present paper is the crossover from the classical Kolmogorov cascade to the quantum Kelvin-wave cascade, which was also vividly argued in the literature. The paper revises the conditions for the crossover and suggests a scenario using only a single scale: t",
        " why very high {{formula:65cd629c-b4f3-41b9-804f-56376e83e45e}} must affect the processes in the cascade either.\nConclusions: In summary, the conclusion of the present analysis is that although the spectrum of Kelvin waves does depend on the choice of the coordinate frame in which the Kelvin wave is investigated (the effect similar to the Doppler effect in moving media), the mechanism of the Kelvin-wave cascade must be tilt-invariant, i.e., invariant with respect to rotation of the axis along which the unperturbed vortex line presumably located. Therefore, the mechanism, which violates this invariance, is not credible and apparently does not collect all symmetry-breaking terms in the series expansion, which eventually must cancel. The paper attracts attention to the fact that the assumption of weak non-linearity used in previous theoretical studies is not always valid and estimates how it can change the values of the cascade exponent. Finally, the paper suggests a simple scenario of the crossover between the classical Kolmogorov and the quantum Kelvin-wave cascades, which does not encounter with any problem of mismatch or bottleneck at the crossover and does not require introduction of a broad intermediate interval"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of contaminations on Gadolinium data analysis in this research paper?",
      "expected_output": "The main findings regarding the impact of contaminations on Gadolinium data analysis in this research paper are that Gadolinium data are much more prone to various contaminations due to its smaller absolute concentration compared to Samarium, and that natural element contamination and uncertainties in isotopic analysis measurements can affect the reliability of the data. To make a meaningful analysis, it is recommended to restrict the analysis to samples that were exposed to rather mild fluences.",
      "context": [
        "q. ()) is too small to be measured reliably. The case of Gd{{formula:356e844b-9af4-4b9e-bad9-e8b1e77ecd5a}} is more favorable, its effective cross section being comparable to that of Sm{{formula:64971dd0-4087-41f0-bcc7-202bd05c4410}} . However, its fission yield is much smaller ({{formula:0a619111-e632-4fe0-81e3-25495118643c}} instead of {{formula:fafecb29-f58b-4d15-b33a-f06470da910f}} ). The absolute concentration of all isotopes of Gadolinium is about ten times smaller than that of Samarium (see, e.g., ). This implies that Gadolinium data are much more prone to various contaminations (natural element contamination due to a post-reaction remobilization, and uncertainties in the isotopic analysis measurements). To make a meaningful analysis of Gadolinium data, one should probably restrict oneself to samples that were exposed to rather mild fluences.",
        " {{formula:a71ac05a-129f-4173-88c3-4055be6c5f2d}} eV from combining Samarium and Europium data.We tried to make use of Oklo Gadolinium data to restrict further the Samarium-derived range (REF ). A priori, one could think of making use of both Gd{{formula:bbdb4b94-1531-42b4-82f7-9b16423338cd}} and Gd{{formula:beb2744d-7ad8-4cf9-80d9-a285c50fc711}} which are strong absorbers of neutrons. In fact, Gd{{formula:af9053fe-90f3-4e58-a05d-db8a856dae1b}} is such a strong absorber that its final concentration {{formula:f3a09ebd-ef84-4a44-abf9-78f7c9115fef}} (generalizing Eq. ()) is too small to be measured reliably. The case of Gd{{formula:356e844b",
        " the temperature of the water-moderated neutrons during the fission reactions has been evaluated by a study of the Lu{{formula:d27caf1a-1f53-4c97-ab41-c2406efbd02b}} /Lu{{formula:1a97c2ec-eae4-499b-b3e7-3fa53700d70e}} and Gd{{formula:c79fa757-a871-4e65-bf86-e9e070838c7e}} /Gd{{formula:2d8b7c02-0a3e-401f-a27a-6ddb17b87c70}} isotope ratios in several samples. The values obtained range between 250{{formula:b202a137-9d49-4f6f-926d-5a3d89eee0e9}} 40{{formula:2a1e2293-2458-4310-800a-8df8fa1c1e76}} C and 450{{formula:9889ed9f-6e5a-4f21-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between height function and conformal classes, and how do the methods used impact the key results?",
      "expected_output": "The research paper analyzes the extremal properties of the height function in each conformal class of a surface. Osgood, Phillips, and Sarnak's work shows that the height function has a meromorphic extension with no pole at a specific point. The methods used, such as harmonic extension and involution, allow for the definition of a compact surface without boundary. For example, the paper mentions that the near boundary part of a surface is mapped to the left side of the half upper plane, illustrating the relationship between the height function and conformal classes.",
      "context": [
        ":8afeea68-edd7-4179-83a1-4929cfc12da7}} is apriori defined for {{formula:85e6420b-3ae6-4118-8192-33ecb8bb1302}} but has a meromorphic extension to {{formula:3a27da6d-7aca-42bf-a4e8-b9a0620d9d6a}} with no pole at {{formula:bdf37756-b8b4-4268-b015-b51aff2e5681}} . So we can define the height function {{formula:88c4ba94-a9aa-4bc9-a0e0-008dd5785b36}} as {{formula:c6a5a02a-f02d-4260-b68b-90baf21ad504}}Osgood, Phillips, and Sarnak first analyzed the extremal properties of the height function {{formula:064e831a-459e-43c6-9159-2e7368ac64c4}} in each conformal class of a",
        "-0b5a-49fe-9023-18e8b3116894}} . {{formula:b2a8249d-f204-43fb-a5e7-93a7b96bdfc1}} is the harmonic extension with {{formula:19a92109-fff8-4880-ba4c-94caf94adbf2}} on {{formula:d1edcd2a-13ff-4600-9a96-7935dc1bb658}} and {{formula:e6a11706-dcef-48f7-ac0b-7b413f91b475}} , here {{formula:334566c6-579a-4688-88d6-e5b0a08983f8}} , {{formula:e1334fe1-6d21-42b8-9d8d-1de4a0d6eee0}} , and {{formula:81e6a7f9-40ef-42a1-bb23-743887a4e109}} are all independent.First, we know {{formula",
        ":5017d5fb-78b2-477c-9ce6-abdea1bc2cb7}} . Here {{formula:ed71c4de-f0f2-4ae1-a3f2-ffeac0d519cd}} is the involution, i.e. {{formula:615e773c-c799-4ca6-b88b-094be93b8441}} , {{formula:409626f8-4ddc-4ba6-979b-a1bf0553feeb}} , and we call the resulting compact surface without boundary {{formula:d07425e4-cfa1-4d13-a7ab-4a63c8a95e4b}} . Locally, the near boundary part of {{formula:ec24e525-e6bd-4105-860b-823dfe08e7aa}} is mapped to the left side of half upper plane {{formula:b350b5e2-9605-40f3-bd9f-b50f11ec3546}} and the corresponding part of {{formula"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used to compress control qubits onto wave functions and what are the main findings of this process in the research paper?",
      "expected_output": "The research paper describes a method to compress control qubits onto wave functions using a controlled unitary operation, denoted as CU. This operation encodes the appropriate information on the wave function, allowing for the compression of multiple qubits. For example, with a single control qubit, two qubits can be compressed onto the target qubit, resulting in a wave function of the form Ψ_tot^T = [a b c d]. The paper also explores the use of multiple control qubits, showing that the number of unentangled qubits that can be compressed increases exponentially with the number of control qubits, specifically 2^n. A key finding is that compressing the space required for multi-qubit operations can have a substantial impact, and the paper proposes a method to achieve this by involving two copies of the wave function Ψ_tot, resulting in Ψ_ult^T = [aa ab ac ad ba bb bc bd ca cb cc cd da db dc dd].",
      "context": [
        " is a rather simple process involving a set of control qubits as well as a single target qubit. If there is a single control qubit, then there are two `compressed' qubits that can be stored on the target qubit; with these two qubits being \\Psi _1^T=[a\\ b] and \\Psi _2^T=[c\\ d]. The transpose is used as a method to save space. If a single control qubit is involved along with the target qubit, then the wave function then becomes:\\Psi _{\\mathrm {tot}}^T = \\mathrm {CU}(\\mathrm {C}_1\\otimes \\mathrm {T}_1) =[\\Psi _1^T\\ \\Psi _2^T] = [a\\ b\\ c\\ d].where \\mathrm {C}_1 is the control qubit, \\mathrm {T}_1 is the target qubit, and \\mathrm {CU} is a controlled unitary that encodes the appropriate information on the wave function.Meanwhile, if two control qubits are involved, the corresponding wave function becomes:\\Psi _{\\mathrm",
        " control qubits, then the number of unentangled qubits that can be `compressed' onto the wave function becomes 2^n. This allows for the space required for single qubit operations to be reduced logarithmically.However, it would be substantially more impactful to compress the space required for multi-qubit operations . The ket \\Psi _\\mathrm {tot} = [a\\ b\\ c\\ d] is not easy to express entanglement with because it does not seem readily apparent to have the results a and b depend on c and d or vice versa. So instead, the attempt is to involve two copies of \\Psi _{\\mathrm {tot}}, such that \\Psi _{\\mathrm {ult}}^T = \\Psi _{\\mathrm {tot}}\\otimes \\Psi _{\\mathrm {tot}}, which becomes:\\Psi _\\mathrm {ult}^T = [aa\\ ab\\ ac\\ ad\\ ba\\ bb\\ bc\\ bd\\ ca\\ cb\\ cc\\ cd\\ da\\ db\\ dc\\ dd]Now, the entanglement operation is performed such that if the outcome of any",
        " wave function.Meanwhile, if two control qubits are involved, the corresponding wave function becomes:\\Psi _{\\mathrm {tot}}^T = \\mathrm {CU}(\\mathrm {C}_1\\otimes \\mathrm {C}_2\\otimes \\mathrm {T}_1) = [\\Psi _1^T\\ \\Psi _2^T\\ \\Psi _3^T\\ \\Psi _4^T] =\\\\ [a\\ b\\ c\\ d\\ e\\ f\\ g\\ h].where \\Psi _1^T = [a\\ b], \\Psi _2^T = [c\\ d], \\Psi _3^T = [e\\ f], \\Psi _4^T = [g\\ h], \\mathrm {C}_1 is the first control qubit, \\mathrm {C}_2 is the second control qubit, and \\mathrm {T}_1 is the target qubit. As can be seen, if n describes the number of control qubits, then the number of unentangled qubits that can be `compressed' onto the wave function becomes"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does device density impact STAR-RIS outage probability, and what are the main findings of this research regarding the relationship between device density and outage probability?",
      "expected_output": "According to the research, increasing the density of devices impacts the STAR-RIS outage probability. The study utilizes stochastic geometry to analyze the outage probability of users in the transmit zone and reflect zone of the STAR-RIS, finding closed-form expressions in terms of Meijer G-functions. The main finding is that the outage probability is affected by the density of devices, and the research provides insights into the relationship between device density and outage probability, highlighting the importance of considering device density in STAR-RIS system design.",
      "context": [
        ", depending on whether the private message or the common message are in outage. The invalid SINR thresholds for the common message are highlighted.An active STAR-RIS is considered in with perfect and imperfect SIC. They derive the outage expressions for users and find the diversity order. The diversity order is zero with imperfect SIC due to residual interference, and it is related to the number of elements of the STAR-RIS when the SIC is perfect. In , the EE of the system with active STAR-RIS is compared to that with passive STAR-RIS in the delay-limited mode. The system throughput for a fixed data rate can be related to the outage probability in that delay-limited mode. They report that, despite the extra power requirement of the active STAR-RIS, it can still achieve overall better EE for the system.In , the users are assumed to be distributed according to a Poisson point process (PPP), and stochastic geometry is utilized to study the outage probability of users in the transmit zone as well as the reflect zone of the STAR-RIS. Closed-form expressions are found for the outage probability in terms of the Meijer G-functions. Increasing the density of the devices, the RS power",
        " The DEP shows a convex behaviour for increasing total transmission power, and an optimal transmission power should be used to get the lowest DEP. When the total power is low, the probability of mis-detection is the dominant term and it decreases with increasing power, resulting in a decrease of DEP. Then, the probability of false alarm becomes the dominant term and increases with increased total power, which results in an increase in DEP again.\nSTAR-RIS: Several works have considered a STAR-RIS for the outage analysis , , , . In , the authors consider the ES and MS modes with continuous phase for their analysis where two users are served by each STAR-RIS. They also consider that Rician fading are spatially correlated due to the proximity of cells of the STAR-RIS. They derive the outage probability for the IBL transmission. In addition, they derive the BLER and goodput for the FBL system. Capacity analysis and asymptotic outage probability are also presented. Conclusions include a preference for alternating elements in the MS mode. Furthermore, the outage probability follows a convex-like shape, depending on whether the private message or the common message are in outage. The invalid SINR thresholds for the common message",
        " the same logic in separating users into groups as done from the model `f) BS-LC-RIS-D&S' to the model `g) BS-L-RIS-D&S', this model assumes that the BS serves a group of users that is separate from the groups served by each RIS. Hence, each user receives a single message, either through the direct link from the BS, or through the secondary link from the RIS. This model is used by , , . The authors in utilize an on-off scheme for the RIS phase shifts and study the 2L-HRS. They derive the outage probability for cell-edge and near users. In , the closed-form expressions of the OP for NU and CEU are derived, and the effect of the number of the RIS elements on the outage behaviour for CEU is explored, and they show performance gains over a DF-RS scheme and RIS-NOMA scheme. In addition, in , the rate region for RIS-assisted RSMA is shown and compared to that of RIS-assisted NOMA.\nBS-L-RIS-S: This model is another simplification from the previous model `h) BS-L-RIS-DorS"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods do encoder layers use to generate input sequence representations, and what are the key findings related to their effectiveness in this research paper?",
      "expected_output": "The encoder layers use multiheaded attention, normalization layers, and position-wise feedforward networks to generate input sequence representations. The key findings related to their effectiveness in this research paper are that the encoder-only variant of the transformer model, consisting of 6 encoder layers with 8 multiheaded attention blocks, is effective in extracting features from the input sequence and generating latent vectors. The use of residual connections and normalization layers helps to prevent the vanishing or exploding gradient problem, and the model dimensionality of 256 is found to maximize efficiency while keeping computational complexity relatively low.",
      "context": [
        " summed up to be fed into the encoder layers. The token embeddings contain information about the type of token that each sequence base consists of. The segment embeddings divide the input sequence into two segments for the pair of drug SMILES in each data point. These two embeddings are then combined to create the non-positional embedding vector. The final encoder is responsible for generating the positional embeddings, which also consist of learnable weights containing information about the position of each token in the input sequence.The encoder module is responsible for generating the representation of the input sequence in the form of a latent vector. It consists of attention heads which are the principal computational blocks in the transformer architecture followed by a normalization layer and a position-wise feedforward network before ending with another normalization layer. These layers make sure that the information flow does not become too large or too small which could potentially lead to the vanishing or exploding gradient problem. There is also a residual connection that bypasses the attention and feedforward network in each layer to make sure that the input information also reaches the later layers in the module. Multiple encoder layers are stacked on top of one another to create the overall architecture of the encoder module.\nMultiheaded Attention:",
        " extracts the information relating to the interaction between the two drug embeddings.On the other side, the SMILE representation of the two Drug compounds in the DDI event is randomized, tokenized, and concatenated into a single one-dimensional sequence which is then padded or concatenated to a fixed predefined dimension of 500 tokens. The sequence then goes into the encoder module, made up of 6 encoder layers with 8 multiheaded attention blocks. A model dimensionality of 256 is picked which seems to maximize efficiency while keeping the computational complexity relatively low. The length of the feedforward network is also kept equal to the model dimensionality at 256 to ensure stability. The output from the encoder layers are latent vectors of each token in the input sequence that are gathered into a multidimensional vector of shape 500 by 256. This latent vector then goes through a convolutional block to extract the features and shrink the dimension. {{figure:68ca00e9-e4b1-4aa2-8ced-7103c0e3e1b0}}The structure of the convolutional module is illustrated in Fig. REF . A single repeating block consists of a convolutional layer followed by a batch normalization layer,",
        " The decoder, on the other hand, takes the previous token of the sequence along with the latent vectors coming from the encoder to generate the output sequence. Depending on the context of the application, several variants of the original transformer model were later proposed which could be divided into three major categories :Encoder Only: This type of architecture only consists of the encoder block and the decoder is absent , , , . These are typically applied to tasks like sequence labeling, image classification, sequence ranking etc. Encoder-Decoder: The complete transformer architecture consists of both an encoder and a decoder module , . This is commonly used in applications like neural machine translation and sequence-to-sequence tasks. Decoder Only: These models only consist of a decoder module , , . Applications of these types of models include sequence generation, language modeling, etc.The transformer module used in this work is of the encoder-only variant. It consists of three encoders that take the tokenized input sequence and convert it into latent vectors. All the encoder modules consist of trainable weights which generate various features of the input sequence, that are then summed up to be fed into the encoder layers. The token embeddings contain information about the type of token that each"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Theorem REF and how does it imply uniqueness and existence in the research paper?",
      "expected_output": "The main findings of Theorem REF in the research paper imply uniqueness and existence. The uniqueness part is immediately implied when a solution is found in a specific space. For existence, the method uses a continuation argument and a priori estimates to ensure robustness and allow passing to the limit in a sequence of solutions. This leads to the existence of a unique solution that satisfies expected bounds, making it a classical solution of the given problem.",
      "context": [
        "7fad-80fd-4fab-9cd6-9adf2bfa968f}} , then {{formula:fb08aa1a-7e7f-4b41-b8c7-672e3a3599cd}} in {{formula:4cc895c0-ebc1-4e36-b2d3-44b90936e33a}} . In particular, if {{formula:f384f5d0-a9dc-450c-89cc-3bd2a4bc9bf0}} is a solution in {{formula:364549a5-0cfb-47ff-b6e0-3836166d4faf}} , then {{formula:d47c0169-8f23-42a3-b009-8494733bf756}} . This immediately implies the uniqueness part of Theorem REF .The method for existence is based on the one hand on a continuation argument, described in Section , and on the other hand on a priori estimates. The latter must be robust enough to allow us to pass to the limit in a sequence of solutions. To ensure the",
        "-4309-b185-a11bf75c03d7}} if {{formula:d9f9f20d-7f91-468d-a169-33b9f143364c}}admits a fixed point {{formula:c4b5a430-d542-48cd-b70e-39ce9a001e12}} , which is a classical solution of (REF ,REF ). It satisfies the expected bounds {{formula:0de4cc4c-8627-4b82-ac6f-26d80d7d85a3}}These bounds ensure that {{formula:06b7eb70-27de-4a98-a6a6-df8f187cb8ba}} is actually a solution of (REF ,REF ). We point out that {{formula:5cd17c55-89f5-4704-970f-770d155d9d64}} is unique.\nPassage to the limit: We now prove that the {{formula:1c5fef7e-72bf-4a14-90f3-4ef898781239}} 's",
        "c-a5dba97eb167}} of (REF ) vanishing at 0 and {{formula:99a93567-e614-4840-b1a7-847b13312c59}} , where {{formula:66f5c59d-b947-492b-8281-06101e037c03}} ranges from 0 to {{formula:35904fc9-7595-4115-a638-40b3573d9795}} when {{formula:5b5358ab-e50d-4b1c-b014-6b103c951488}} . Therefore, there exists a unique {{formula:818eb197-2d07-43b6-ab2b-7049ffb5f92f}} for which {{formula:80c2a08d-ea10-46ec-9363-8eb9c66c8695}} . Then {{formula:bafcdc00-472c-4a58-ba2b-60108abb7fe4}} is the announced solution.\nExistence proof: So far, we have proved that if"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How do estimators based on nonlinear increments relate to jump diffusion analysis in the context of this research paper?",
      "expected_output": "Estimators based on nonlinear increments are central to the analysis of jump diffusion processes in this research paper. They are used to estimate the drift and jump activity of the process. The paper derives expressions for the conditional moments of nonlinearly transformed increments, which are used to construct asymptotically normal estimators. For example, the estimators are based on smoothly truncated increments of the form Δ_n Y_t, where Y_t is the jump diffusion process and Δ_n is a sequence of truncation levels. The paper also employs a jump-filtering technique to derive the asymptotic distribution of the estimators, allowing for symmetric jumps of infinite variation and infinite variance as a nuisance.",
      "context": [
        "0-ec01-470b-ae82-b1c023b71ce8}} .\nConditional moments of jump diffusions: The estimators proposed in the sequel will be based on nonlinearly transformed increments of the semimartingale {{formula:f01fe83f-4160-4429-8a0b-0b1de176fb7e}} given by (REF ). Central to our analysis are expressions for the corresponding conditional moments, i.e. quantities of the form {{formula:1c4436d4-4348-46d4-9ca6-507cc509eb65}} . This includes the rescaled increments {{formula:8ab25988-0aa8-46e0-9b7b-5ad512960fa4}} by setting {{formula:54059b9b-d68b-4fc0-a392-41810e6ec218}} . The expressions derived in this section allow us to build suitable nonparametric estimators in subsequent sections, as well as enabling the study of their asymptotic behavior. To this end, let {{formula:41404a04-3549-42",
        " {{formula:125d5878-7312-4d8e-a775-568124bcf7c6}} to the state-dependent situation is thus of interest for future work.\nOutline: The remainder of this paper is structured as follows. In section , we present the methodology to bound the conditional moments of nonlinearly transformed increments of jump diffusion process by an analytical investigation of the Markov transition semigroup. The derived bounds on the infinitesimal generator might be of independent interest. These approximations are used to construct an asymptotically normal estimator of the drift in section . Inference for the small jumps, i.e. jump activity estimation, is treated in section . We demonstrate the applicability of the results by a simulation study in section . All technical proofs are postponed to section .\nNotation: To abbreviate some notation, we introduce {{formula:b51e268a-cc46-4813-972a-8dbc094414d3}} and {{formula:7d554a3d-9542-493d-bdf4-b2f5c9d717c8}} for {{formula:fa1c2",
        " for {{formula:5b5be55e-b37d-4315-afb8-8155f8fc8e94}} and derive its asymptotic distribution. By employing a jump-filtering technique similar to , we are able to derive the same asymptotic variance as in the diffusion case, while allowing for symmetric jumps of infinite variation and infinite variance as a nuisance. Our estimator is similar to the nonparametric thresholded drift estimator of , who require the jumps to be of finite activity.The estimators proposed in this paper are based on smoothly truncated increments of the form {{formula:fd5288f2-c405-42ee-a327-23e5c00d01aa}} for a bounded smooth function {{formula:de2466c2-c35e-45e9-8488-f33cf4982712}} and a sequence {{formula:b5d6ae0a-26d1-4f2b-a12e-c155726b834c}} as {{formula:8e79b923-ca06-4afa-8fcb-9f9918c63"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the geometric relationship between Phi and gamma_0 in the context of this research paper?",
      "expected_output": "default",
      "context": [
        "bracket 1_{[0,1]\\times \\operatorname{spt}Q}\\rrbracket = \\psi _\\sharp Q -Q - \\Phi ^{\\prime }_\\sharp \\llbracket 1_{[0,1]\\times \\operatorname{spt}P}\\rrbracket .Then\\partial W=P\\quad \\text{and}\\quad Q=W+\\partial V.By elementary spherical geometry, and since \\psi :\\operatorname{spt}Q\\rightarrow \\gamma _0 was chosen to be 1-Lipschitz, there is a positive constant \\bar{c} depending only on the dimension n such that\\mathbf {M}(W ) \\le \\bar{c} \\mathbf {M}(P).As in Section REF , let h\\in H_n(\\Gamma ;\\mathbb {Z}), let C_i \\in {C}(h) be a minimizing sequence of cycles converging to a Plateau solutionC_\\infty = (X_\\infty ,d_\\infty ,S_\\infty )in the intrinsic",
        "]\\times \\operatorname{spt}Q \\rightarrow S^\\infty /\\Gamma ,namely we replace, for each x\\in \\operatorname{spt}Q, the curve \\lbrace \\Phi (t,x)\\rbrace _{t\\in [0,1]} by the unique length minimizing geodesic segment joining x to \\Phi (1,x)=\\psi (x) inside S^\\infty /\\Gamma (which has length less than \\frac{2\\pi }{3} by choice of \\gamma _0). This straightening yields a new Lipschitz map\\Phi ^{\\prime } : [0,1]\\times \\operatorname{spt}Q \\rightarrow S^\\infty /\\Gammasuch that \\Phi ^{\\prime }(0,.)=\\operatorname{Id}, \\Phi ^{\\prime }(1,.)=\\psi . SetW:=-\\Phi ^{\\prime }_\\sharp \\llbracket 1_{[0,1]\\times \\operatorname{spt}P",
        " homotopic to a length minimizing geodesic segment joining x to \\Phi (1,x)=\\psi (x) inside S^\\infty /\\Gamma of length less than \\frac{2\\pi }{3}. This length condition ensures that the length minimizing geodesic segment is unique (for fixed endpoints and fixed homotopy class) and varies in a Lipschitz way with respect to x\\in \\operatorname{spt}Q.Next, we can suppose that the map \\psi is simplicial and “linear” on \\operatorname{spt}Q, meaning that it maps 0-dimensional faces in \\operatorname{spt}Q to one point p_0\\in \\Gamma _0, and it maps geodesics segments in \\operatorname{spt}Q to geodesic segments inside the curve \\gamma _0 with respect to the metric induced on \\gamma _0, in a linear way. We straighten the map\\Phi : [0,1]\\times \\operatorname{spt}Q \\rightarrow S^\\infty /\\Gamma"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does SpinView use to simplify magnetic data analysis and what are the main findings of this approach?",
      "expected_output": "SpinView simplifies magnetic data analysis by using methods such as dynamic filters, including advanced FFT denoising, projection, rescale warping, and clipping, as well as powerful data postprocessing abilities from libraries like Numpy, Scipy, and Pandas. The main findings of this approach are that it enables the generation of publication-quality figures, shareable interactive HTML, and high-quality trajectory videos in seconds, promoting equality and accessibility in computational magnetism research. For example, SpinView can be used to analyze data from micromagnetic simulations, such as those generated by Mumax3 and OOMMF, and atomistic spin dynamic data from tools like UppASD, Spirit, and Vampire.",
      "context": [
        " theoretical physicists and experimentalist who want to understand and embed computational analysis in their workflows. {{figure:6b0cb6cb-6731-4ebf-b703-68f490b6d315}}To handle the challenge of offering accessibility to researchers at various coding proficiency levels with interactive visual investigating and interpretation of magnetic simulation data. In this work, we use extendable Python programming language and leverage well-develop visualization packages Pyvista ( based on the Visualization Toolkit, VTK) combined with the GUI platform Trame and CLI (command-line interface) platform Typer to develop the general visualization software: SpinView. Besides offering a variety of glyph support (including defined glyph), with powerful data postprocessing ability from Numpy , Scipy , Pandas , and Pyvista, we offer easy-to-use mesh rendering and dynamic filters, e.g., advanced FFT (fast Fourier transform) denoising , projection, rescale warping, and clipping. To promote processed data (with prior knowledge from professional computational scientists) sharing, we embedded a built-in SQL database that allows storing all filter parameters and provides a shareable interactive HTML file that can directly show post-processed data in any modern web",
        " can simplify magnetic data postprocessing and exploration. It can generate publication-quality figures, shareable interactive HTML, and high-quality trajectory video in seconds with a stored profile in the built-in database. Thereby, SpinView is expected to enable novel findings in computational magnetism and promote equality and accessibility for everyone to enjoy new achievements in computational science.\nImplementation concept: SpinView was written in Python and has three tiers in its architecture, i.e., presentation, data processing, and data tier.The presentation tier, which includes CLI, Web UI, desktop UI, and local view, is based on the Typer, Trame, and Pyvista frameworks. All of them are cross-platform, easy to extend, and make SpinView user-friendly. The core components in the data processing tier include design filters constructed by Numpy, SciPy, Pandas, and Pyvista. A python-built-in SQLite3 database is used to store user's profiles and interact with UI through CLI.\nMultiscale computational magnetic simulation: Micromagnetic demos shown in this work are simulated by Mumax3 and OOMMF and atomistic spin dynamic data are generated by UppASD, Spirit, and Vampire.\nData availability",
        "# arXiv Paper: 2309.17367\n\nSpinView: General Interactive Visual Analysis Tool for Multiscale Computational Magnetism\nabstract: Multiscale magnetic simulations, including micromagnetic and atomistic spin dynamics simulations, are widely used in the study of complex magnetic systems over a wide range of spatial and temporal scales. The advances in these simulation technologies have generated considerable amounts of data. However, a versatile and general tool for visualization, filtering, and denoising this data is largely lacking. To overcome these limitations, we have developed SpinView, a general interactive visual analysis tool for graphical exploration and data distillation. Combined with dynamic filters and a built-in database, it is possible to generate reproducible publication-quality images, videos, or portable interactive webpages within seconds. Since the basic input to SpinView is a vector field, it can be directly integrated with any spin dynamics simulation tool. With minimal effort on the part of the user, SpinView delivers a simplified workflow, speeds up analysis of complex datasets and trajectories, and enables new types of analysis and insight. SpinView is available from https://mxjk851.github.io/SpinView/\nIntroduction: Interactive postprocessing and visual exploration are essential"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do supernova asymmetries play in the formation of black hole pairs, and how do the methods used in this research paper support these findings?",
      "expected_output": "Supernova asymmetries play a crucial role in the formation of black hole pairs, as they can cause kicks that affect the orbit of the black holes. According to the research paper, kicks from supernova asymmetries are necessary to form close black hole pairs, as the formation process itself may prevent black holes from forming in close orbits. The methods used in this paper, such as analyzing the velocities of black hole X-ray binaries and the masses of their companions, support these findings. For example, the paper mentions that the high velocity of Cyg X-1 can be explained by a delayed black hole formation, which is consistent with the idea that supernova asymmetries play a key role in shaping the orbits of black hole pairs.",
      "context": [
        " happens twice during the formation of a black hole pair, possibly preventing black holes to form in a close orbit at all. Only kicks from an asymmetry in the SN could then form close pairs. But as shown above, there is not much evidence for kicks and the magnitude of any kicks is severely limited to {{formula:a224f089-9b60-42f8-bb3b-4e36c54c50d1}} 40 km s{{formula:58036396-8227-46a6-ab26-e29674ba3895}} by the black hole X-ray binaries with low mass donors, unless the black holes in these system formed in a direct collapse. We thank Philipp Podsiadlowski and the referee for comments that improved this article and Lex Kaper who made us aware of the proper motion of Cyg X-1. This work was supported by NWO Spinoza grant 08-0 to E.P.J. van den Heuvel.",
        "96 can be explained by scattering at molecular clouds and density waves, since these binaries could be an old population (Podsiadlowski, private communication; see also Brandt et al. bps95).With the new discovery of the relatively high velocity of Cyg X-1, we think the above is unlikely, because now the two systems with highest mass companions must have formed through a delayed black hole formation, while the systems with low mass companions form in a direct collapse. This would mean that the success of the SN in which the black hole is formed is related to the nature of its binary companion, for which we see no reasonTutukov & Cherepahshchuk tc97 discuss the system velocities of the X-ray binaries containing black holes and conclude that all velocities can be explained with mass ejection alone. However, they only consider the maximum velocity that can be obtained with the observed limits on the masses of both stars, assuming the shortest possible period at the moment of the SN and the maximum amount of mass that can be ejected without disrupting the binary. In that case, the pre-SN mass ratio is not independent of the final (observed) mass ratio and it would be better to use the current (ob",
        "93371-aa84-4205-8180-0cc5750d5ef2}} must have been ejected.\nThe remaining black hole X-ray transients: Table REF shows that all the black hole X-ray binaries with low mass donors have low velocities. As derived by White & van Paradijs wp96, the expected additional velocity component of these X-ray binaries is of the order of 20 – 40 km s{{formula:26139f76-dfd4-4f2d-b91b-39130d5c8f6c}} . In Cyg X-1 and Nova Sco at least 28 and 48% of the mass of the progenitor must have been ejected in the SN. Therefore we computed the velocities for these systems assuming a constant fraction of 35% of the helium star mass to be ejected and show the obtained range in velocities given the range in black hole masses in Fig. REF . The last five systems are all expected to have evolved during mass transfer to smaller periods, and all seem to be compatible with an initial systems close to ({{formula:733ee115-9fd6-4cf7-b835-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between formation energy and material stability, and how do researchers use this concept to predict material properties?",
      "expected_output": "Formation energy is a fundamental property that quantifies the energy change during the formation of a compound from its constituent elements, indicating whether energy is absorbed or released in the process. It exhibits an underlying connection to material stability, where a compound is typically considered thermodynamically stable if its formation energy lies on or below the convex hull. Researchers use this concept to predict material properties by leveraging deep learning algorithms, such as neural networks, to directly predict formation energy and identify the stability of materials. For example, the study uses a deep neural network model that incorporates elemental fractions and symmetry classifications to predict a crystal's formation energy, achieving the highest accuracy when space group classification is incorporated.",
      "context": [
        " also includes symmetry information, with a column for the designated symmetry classification under a specific crystallographic category. The symmetry classifications are then converted into binary format using one-hot encoding, resulting in 7 columns for crystal system classification, 32 columns for point group classification, and 228 columns for space group classification (while there are 230 space groups, two do not exist in the Materials Project data.) The final dataframe contains both elemental columns and one-hot encoded columns for crystal system, point group, or space group classfications as input features, with formation energy as the target feature. The data is then split into an 80 percent training set and a 20 percent testing set.\nDeep Learning: Complementary to theoretical and experimental approaches, machine learning algorithms such as deep learning can be used to directly predict formation energy. Formation energy is a crucial indicator of material stability, and deep learning offers a robust method for making such predictions. Deep learning leverages neural networks , which are sophisticated artificial systems inspired by the structure and function of biological neural networks. A visualization of an artificial neural network is shown in FIG. REF . Within these networks, information traverses through multiple hidden layers. Each layer processes its inputs using weights and biases before generating an output. This computational process can be represented mat",
        "# arXiv Paper: 2412.00819\n\nFormation Energy Prediction of Material Crystal Structures using Deep Learning\nabstract: Determining the stability of chemical compounds is essential for advancing material discovery. In this study, we introduce a novel deep neural network model designed to predict a crystal's formation energy, which identifies its stability property. Our model leverages elemental fractions derived from material composition and incorporates the symmetry classification as an additional input feature. The materials' symmetry classifications represent the crystal polymorphs and are crucial for understanding phase transitions in materials. Our findings demonstrate that the integration of crystal system, point group, or space group symmetry information significantly enhances the predictive performance of the developed deep learning architecture, where the highest accuracy was achieved when space group classification was incorporated. In addition, we use the same model architecture to predict the energy above hull, an indicator to material stability, with formation energy as an additional input feature.\nIntroduction: Formation energy is a fundamental property that quantifies the energy change during the formation of a compound from its constituent elements, indicating whether energy is absorbed or released in the process . This property exhibits an underlying connection to material stability. A compound is typically considered thermodynamically stable if its formation energy lies on or below the convex hull,",
        " phases, also known as the crystal polymorphs. Our aim is to tackle this multi-class problem by adding symmetry labels to the various chemical systems. These descriptors serve as our input features in a deep learning model that predicts formation energy. Furthermore, we use the same model architecture, but with the addition of each material's formation energy as an input feature, to predict the energy above the hull and identify if a material is stable. Ultimately, our goal is to predict the formation energy of chemical compounds across various space groups, allowing us to identify the specific space group in which a compound can potentially form its most stable structure.This paper is organized as follows. Section covers data mining, which includes details on the dataset, structure of the input features, and specifications of the deep learning model. The results and discussion section is found in Sec. which shows comparisons of the formation energy prediction models with and without symmetry classifications in the input features. Additionally, this section presents the results of energy above hull predictions for Manganese-Nickel-Oxygen chemical systems. Finally, Sec. shows the conclusions.\nData Mining: Using the Materials Project database , we extracted a comprehensive dataset comprising 153,232 material entries. Of these, 104,351 are theoretically derived data, calculated"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on Burau relations and how do they contribute to our understanding of the subject?",
      "expected_output": "The research on Burau relations provides a presentation for the braid group of a sphere with generators and relations, including the sphere relation. For example, the pure braid group on a sphere has generators denoted by a specific formula, which simplifies the understanding of the subject. The Burau relations contribute to our understanding of the braid group by defining the relations between the generators, such as the braid relations and the sphere relation, which helps in studying the properties of the braid group.",
      "context": [
        "f2-4b1e-9d6c-31a6c698ae30}} for the pure braid group {{formula:c42c6555-3023-43cd-a107-4f882848ea7b}} (of a disc) can be defined (as elements of the the braid group {{formula:00c09419-8b40-49f2-a3f5-6b07e6a559e1}} ) by the formula: {{formula:bd50c70a-631c-492d-a6fe-ec463ec210f8}}Then the defining relations, which are called the Burau relations , are as follows: {{formula:22f11aec-d315-41a4-9866-19e5603c0454}}It was proved by O. Zariski and then rediscovered by E. Fadell and J. Van Buskirk that a presentation for the braid group of a sphere can be given with the generators {{formula:bea6c9c9-30e5-4002-b509-2ce86481ef21}} , {{",
        "6c9c9-30e5-4002-b509-2ce86481ef21}} , {{formula:66a9a1b0-caa5-4520-aa21-cdbf83dc445f}} , the same as for the classical braid group, satisfying the braid relations (REF ) and the following sphere relation: {{formula:13490c94-1b65-4c62-877c-e3a5cb0e8df5}}For the pure braid group on a sphere let us introduce the elements {{formula:b4606d5c-3940-4f2d-888a-1b0711a957b4}} for all {{formula:714eaeee-a2c8-4619-b866-a0d4942ef00f}} by the formulae: {{formula:1d3dc556-bfc7-4058-8d65-eb608daadc76}}The pure braid group for the sphere has the generators {{formula:7efb5636-ef3b-41f5-b401-b3931",
        "5eaf}} , which we denote for simplicity by {{formula:6cef588e-3d20-4626-a63b-cbd22fb12a4d}}Usually the braid group {{formula:82f9e060-49a4-4261-aabd-742cf6d99435}} is given by the following Artin presentation . It has the generators {{formula:6ad753be-c7b9-431c-8ed8-ef8ef789630d}} , {{formula:ce2c2814-04d9-4627-8fb7-9ed14d6e8d0d}} , and the two types of relations: {{formula:e7fa2ddb-6c9a-42aa-8ef1-839c2cbd59aa}}The generators {{formula:e2ef3633-38c8-480a-ab85-bcd5ddb83607}} , {{formula:3f768297-06f2-4b1e-9d6c-31a6c698ae30}} for the pure b"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "How does COCO enhance code generation robustness?",
      "expected_output": "COCO enhances code generation robustness by concretizing the original programming instruction, which helps reduce robustness inconsistencies by 18.35% to 53.91% through fine-tuning, making code generation systems more reliable and trustworthy.",
      "context": [
        " commercial tools such as Copilot and ChatGPT, using two widely-used datasets. Our results demonstrate the effectiveness of COCO in testing the robustness of code generation systems, outperforming two techniques adopted from general text-to-text software testing by 466.66% and 104.02%, respectively. Furthermore, concretized instructions generated by COCO can help reduce robustness inconsistencies by 18.35% to 53.91% through fine-tuning.\nIntroduction: Automated code generation is to automatically generate source code conforming to a given programming instruction . This area has garnered significant attention over the years, with considerable effort invested in its development , , . In particular, Large Language Models (LLMs) have made substantial progress in this field recently, such as CodeX , Copilot , and ChatGPT , which take code generation to new heights of practicality. For example, according to a recent report , Copilot has written nearly 40% of code in files where it is enabled, thereby allowing developers to focus on tackling more complex software development challenges.It is indeed encouraging about the breakthrough brought by LLM-based code generation systems in terms of prediction performance, but their robustness, an equally critical property, is often overlooked. As",
        "# arXiv Paper: 2308.13319\n\nCOCO: Testing Code Generation Systems via Concretized Instructions\nabstract: Code generation systems have been extensively developed in recent years to generate source code based on natural language instructions. However, despite their advancements, these systems still face robustness issues where even slightly different instructions can result in significantly different code semantics. Robustness is critical for code generation systems, as it can have significant impacts on software development, software quality, and trust in the generated code. Although existing testing techniques for general text-to-text software can detect some robustness issues, they are limited in effectiveness due to ignoring the characteristics of code generation systems. In this work, we propose a novel technique COCO to test the robustness of code generation systems. It exploits the usage scenario of code generation systems to make the original programming instruction more concrete by incorporating features known to be contained in the original code. A robust system should maintain code semantics for the concretized instruction, and COCO detects robustness inconsistencies when it does not. We evaluated COCO on eight advanced code generation systems, including commercial tools such as Copilot and ChatGPT, using two widely-used datasets. Our results demonstrate the effectiveness of C",
        " code generation with a reduction of 18.35% to 53.91% robustness inconsistencies, without damaging the code-generation performance. {{figure:cf5c6142-f07e-46b0-b65e-a315524e265a}}To sum up, this paper makes the following contributions:We propose a novel technique (COCO) to test robustness of code generation systems. COCO concretizes the original programming instruction by incorporating features known to be contained in the code generated by the original instruction. To facilitate experimental replication and practical use, we have implemented and released COCO, which includes code feature extraction, concretized instruction construction, and code consistency checking. The project homepage can be found at . We extensively evaluated COCO on eight advanced code generation systems (including two commercial systems: Copilot and ChatGPT) and two widely-used datasets. The results showed COCO detected more robustness inconsistencies than the state-of-the-art baselines and can also help to enhance the robustness of code generation systems.\nMotivation: Figure REF shows a simplified example from the HumanEval dataset. The original instruction describes the programming requirement. When applying PEGASUS to it"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the key steps involved in transforming the sinh-Gordon equation into the sine-Gordon equation, and how do the main findings of this research relate to these transformations?",
      "expected_output": "The key steps involved in transforming the sinh-Gordon equation into the sine-Gordon equation include setting the sinh-Gordon coupling constant to be purely imaginary. This is achieved by letting the coupling constant become 'i' times a real number. As a result, the sinh-Gordon Hamiltonian turns into the sine-Gordon Hamiltonian. The main findings of this research relate to these transformations by providing a framework for understanding the relationship between the two models and how the results from one model can be applied to the other. For example, the research shows that the amplitude can be obtained from the corresponding breather reflection amplitude in the sine-Gordon model, and that the Hamiltonian for the oscillator can be transformed into a simpler form using rescaled variables.",
      "context": [
        " amplitude can again be obtained from the corresponding breather reflection amplitude in the sine-Gordon model . Because we expect that the results for our model can be obtained similarly we now turn our attention to the sine-Gordon model.When we let the sinh-Gordon coupling constant {{formula:22e0dc25-8276-4b09-a618-4ff1b970f167}} become purely imaginary, i.e., if we set {{formula:d0b1dfeb-a8cd-4aee-966c-1050534cf058}} with {{formula:77a9c1d8-262e-43e9-b050-2d6e0092770b}} purely real, then the sinh-Gordon Hamiltonian (REF ) turns into the sine-Gordon Hamiltonian {{formula:e8cc266a-2509-401f-9eaf-24fde3443e80}}Under the same replacement the Hamiltonian for the oscillator becomes {{formula:b3a49c8a-75e9-4f3d-b7c7-6bbc43d570eb}}Notice that",
        " at the boundary. The exact form of the higher non-linear terms in the Hamiltonian of the oscillator are fixed by integrability.To shorten the formulas we introduce the rescaled variables {{formula:acd9664d-3956-436d-86ac-ff0c9ed13ec8}}which have Poisson bracket {{formula:1502f1a3-12f2-480b-b93f-9dd7fe2aa46e}} . In terms of these the boundary equations of motion are {{formula:7a21b7da-bd37-4701-bee4-214e49b7460c}}To determine the equations of motion of the sinh-Gordon field {{formula:c5ccc69a-76c7-4178-8bea-fdfa9480d573}} one needs to use the full Hamiltonian {{formula:07c714a3-d350-4384-b84c-e4d54f0b539d}}One finds {{formula:49384cce-ca7c-4c00-8916-7b24ae449682}}Note the term",
        " calculating the Poisson brackets of the entries of the matrix {{formula:09556db5-5427-437c-b61f-2f08efd52b37}} it gives a unique solution to eq. (REF ): {{formula:43149fc6-e747-4b15-bd19-41ae1f2b95b9}}To construct the generating function (REF ) of the integrals of motion for the sine-Gordon model with dynamical boundaries, we need {{formula:2c25db85-2d91-43bc-a1f2-83c1e96418a7}} -dependent solutions {{formula:421e0eba-7e07-45c7-ae53-7a8ef2a632e2}} to the classical reflection equation (REF ). Let us introduce the rescaled variables {{formula:2a483320-a430-4ec9-b7d9-c8bd5abe25a4}} and {{formula:ce3aab46-80c4-40b6-828e-7a424ee43730}} . Then the matrix {{form"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods does GPTEval3D use to assess the quality of edited images, and what are its key findings on this topic?",
      "expected_output": "GPTEval3D uses GPT-4V with customizable 3D-aware prompts to assess the quality of edited images. Its key findings include evaluating the naturalness and fidelity of edited images across several dimensions, such as text-to-asset alignment, 3D plausibility, texture-geometry coherence, texture details, and geometry details. GPTEval3D prompts GPT-4V to compare two 3D assets generated by different methods using four rendered images and normal maps, and then calculates Elo ratings to reflect each method's performance.",
      "context": [
        " makes DAI a flexible tool for examining various aspects of editing quality. Given that the image resolution is 256\\times 256, we set \\gamma ={1,3,5,7,10}.GPTEval3D. While DAI effectively measures drag accuracy, it is not sufficient on its own because the editing process can introduce distortions or artifacts, leading to unrealistic or unnatural results. Therefore, evaluating the naturalness and fidelity of the edited images is crucial for a comprehensive quality assessment. This task is particularly challenging due to the absence of ground-truth edited 3D objects for reference. To address this, we utilize GPTEval3D, which leverages GPT-4V with customizable 3D-aware prompts. GPTEval3D aligns well with human judgment across several dimensions, including text-to-asset alignment, 3D plausibility, texture-–geometry coherence, texture details, and geometry details. Specifically, GPTEval3D prompts GPT-4V to compare two 3D assets generated by different methods using four rendered images and normal maps. The pairwise comparisons are then used to calculate Elo ratings, which reflect each method's performance. For more details, please refer to .Fig. REF presents",
        "ians from . We collect data that are representative to demonstrate drag editing but do not cherry-pick based on any results. The 3D drag points are manually specified using MeshLab, following .Metrics. In this work, we employ two assessment metrics for quantitative evaluation: Dragging Accuracy Index (DAI) and GPTEval3D . DAI measures the effectiveness of a method in transferring source content to a target point. While DAI effectively measures drag accuracy, it is insufficient because the editing process sometimes introduce overall distortions or artifacts, resulting in unrealistic or unnatural results. To address this, we use GPTEval3D, which leverages GPT-4V and customizable 3D-aware prompts to offer flexible comparisons between two 3D assets based on a set of specific evaluation criteria. For more details about these metrics, please refer to Sec. REF .\nResults: Baselines. One baseline comparison involves leveraging a 2D drag method to edit each view independently. In this setup, we use DiffEditor to drag the four rendered views, followed by the same reconstruction and optimization steps as ours to produce the final 3D results. During our initial experiments, we observed that when editing much more than four views, such as 120, DiffEditor introduced significant 2D",
        " visual comparisons, we conducted a quantitative evaluation to assess the effectiveness of all compared methods in terms of dragging accuracy (DAI) and overall editing quality (GPTEval3D). Table REF reports different methods' DAI across varying patch radius values \\gamma . As \\gamma increases from 1 to 10, our method, both with and without SDS, shows consistently lower error against other approaches like APAP, PhysGaussian, and DiffEditor. In Table REF , the GPTEval3D evaluation reveals that the “Ours + deformation + SDS” method performs almost the best across all criteria on both meshes and 3D Gaussians. Notably, we observed that while the SDS version of our method may not always achieve the highest DAI score, this is understandable. The SDS tends to sharpen visual details, which can lead to minor numerical decreases, but it ultimately results in more visually pleasing outputs. This is further supported by the GPTEval3D results, where the SDS version achieves the highest score in texture details.\nAbalation and Discussion: Abalation. We start with the initial reconstruction from as a baseline (Ours (LGM)) and progressively integrate our two-step optimizations:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do axion-gluon couplings play in enhancing the sensitivity of NMR axion detection, and how do the research findings contribute to our understanding of axion physics?",
      "expected_output": "Axion-gluon couplings play a crucial role in enhancing the sensitivity of NMR axion detection by allowing for the detection of axions with masses less than 85c6ad74-be2f-4e9b-954d-2cb68022ed7c eV. The research findings contribute to our understanding of axion physics by setting the first laboratory constraints for axions with masses ranging from d2adb03a-00a1-4cf6-a96f-cf0876dd3378 eV to d8c88b89-48ff-4466-8806-d73ed4cd4c2f eV and obtaining constraints on the axion-nucleon coupling strength for axion masses ranging from 80ec0ff1-d781-47ce-9601-fa211d4fe915 eV to c01752cf-b942-418 eV.",
      "context": [
        "6aafd30a-5b01-413d-8b75-223463d93b4a}} eV .It has been proposed that the axion-gluon and axion-fermion couplings could be used to detect axions with masses less than {{formula:85c6ad74-be2f-4e9b-954d-2cb68022ed7c}} eV by utilizing nuclear magnetic resonance (NMR) techniques , , . Similar ideas have been applied to analyze experimental data from a search for the neutron EDM, setting the first laboratory constraints for axions with masses ranging from {{formula:d2adb03a-00a1-4cf6-a96f-cf0876dd3378}} eV to {{formula:d8c88b89-48ff-4466-8806-d73ed4cd4c2f}} eV . The precession frequencies of overlapping ensembles of ultracold neutrons and {{formula:9f7ebcb7-fc36-4754-bb77-4759b1e239d5}} Hg atoms were simultaneously measured to distinguish",
        "36-4754-bb77-4759b1e239d5}} Hg atoms were simultaneously measured to distinguish the signals from axions and from magnetic field. Because of the spatial separation between the ensemble-averaged vertical position of the warm {{formula:8c447c51-8219-4dce-adea-84d220e51b5b}} Hg and the ultracold neutrons due to Earth's gravitational field, magnetic-field gradients lead to differential effects in the measured spin-precession frequencies of the two species , .Here, we report an experimental search for axions using a new comagnetometer configuration. Different nuclear spins are simultaneously probed within the same molecule in this comagnetometer, which has highly suppressed systematic effects from magnetic-field gradients . We use the comagnetometer to perform a month-long search for frequency shifts in nuclear spins induced by axions, and obtain constraints on the axion-nucleon coupling strength for axion masses ranging from {{formula:80ec0ff1-d781-47ce-9601-fa211d4fe915}} eV to {{formula:c01752cf-b942-418",
        ", which can induce spin-dependent energy shifts and spin precession in fermions , , , , . The axion-photon coupling has been searched for in numerous experiments, many of which give constraints for axions with masses heavier than {{formula:4203adce-a56a-4784-a15e-52da87ecbf5d}} eV , , , , , , . An optical-cavity experiment was proposed to search for axions with masses of {{formula:85241f9f-c193-45c1-b585-4c12b04f2028}} eV and up to {{formula:04f2f48c-93ba-48bd-81b7-0597048d50dc}} eV . Recently, the first results from ABRACADABRA-10 cm set upper limits on the axion-photon coupling over the mass range {{formula:594a34bb-9012-4145-860a-2c7cf328c518}} eV to {{formula:6aafd30a-5b01-413d-8b75-223463d93b4"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does intrinsic scatter play in estimating black hole mass, and how do the methods used in this research paper account for its impact on the accuracy of these estimates?",
      "expected_output": "Intrinsic scatter plays a crucial role in estimating black hole mass as it measures the fundamental link between physical quantities. The research paper accounts for its impact by recognizing that the magnitude of intrinsic scatter affects the accuracy of black hole mass estimates. For instance, knowing the intrinsic scatter is essential for estimating the space density of the most massive black holes in the local universe. The paper uses the relation between velocity dispersion and black hole mass, allowing for the estimation of a galaxy's central black hole mass from the more easily measured velocity dispersion. The methods used also consider the distribution of intrinsic scatter, such as normal or log-normal in mass, to understand its effects on the evolution of black hole mass estimates.",
      "context": [
        ".3 dex. The total scatter of the relations (intrinsic combined with statistical and systematic measurement errors) is what makes the relation useful as a secondary tool for BH mass estimation. The intrinsic scatter, however, is the measure of the fundamental link between the physical quantities in question.The magnitude of the intrinsic scatter is extremely important for several reasons. First, the range of BH masses in galaxies of a given velocity dispersion or bulge luminosity constrains BH formation and evolution theories. Many theories of BH formation and galaxy evolution have used the {{formula:a93f8b17-397a-4ab5-b1a9-83db90cc42e1}} –{{formula:fc29b7c6-790c-44a8-a13c-ae07075a9d63}} relation either as a starting point for further work or as a prediction of the theory , , ; for a review, see . A further test of such theories is whether they can reproduce the observed cosmic scatter in the relation. For example, there may be an increased intrinsic scatter in low-mass galaxies because BHs are ejected by asymmetric gravitational wave emission and low-mass spheroids have",
        " in low-mass galaxies because BHs are ejected by asymmetric gravitational wave emission and low-mass spheroids have lower escape velocities , .Understanding the scatter in the {{formula:69c9f159-b550-47ad-ba96-d688986e157e}} –{{formula:44a33ad5-bc13-4c00-9ae8-999e0703f032}} relation is also essential for estimating the space density of the most massive BHs in the local universe. One of the most useful aspects of the {{formula:cdb07798-d6e0-4d07-b0a5-f56855518b72}} –{{formula:84453ad1-44c2-4009-a996-dc46d863ad78}} relation is that it allows one to estimate a galaxy's central BH mass from the more easily measured velocity dispersion. Because of the steep decline in number density of galaxies having high velocity dispersion , , , the majority of the extremely large BHs will reside in galaxies with moderate velocity dispersions that happen to contain BHs that are overmassive for the given velocity dispersion",
        " reside in galaxies with moderate velocity dispersions that happen to contain BHs that are overmassive for the given velocity dispersion , , . Knowing the magnitude of the intrinsic scatter is thus required to find the density of the most massive BHs. For example, the number density of BHs with {{formula:0545a3ca-af8a-48a2-a010-d16a4652c8da}} is {{formula:07adb627-f8bd-4e0a-91f0-22b0420e6d4e}} if the intrinsic scatter is 0.15 dex and {{formula:d928e95b-6869-4c59-a674-be213faf44f2}} if the intrinsic scatter is 0.30 dex .Both the magnitude of the intrinsic scatter and its distribution (e.g., normal or log-normal in mass) are also important to know for studies of the evolution of the {{formula:3839ff50-7cb3-44cb-b41b-eb79f1aca4de}} –{{formula:3d83b404-2b49-4055-a6"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings in the research paper regarding the relationship between GHZ-nonlocality and time-local realism in quantum mechanics?",
      "expected_output": "The main findings in the research paper are that GHZ-nonlocality can be applied to time, establishing a temporal version of Bell's theorem without inequalities. This implies that quantum mechanics cannot be completely reproduced by a time-local hidden variable model, accentuating that it is incompatible with either space-local realism or time-local realism. The discovery of GHZ-nonlocality in time can be experimentally verified using current superconducting quantum technology.",
      "context": [
        " a temporal GHZ version of Bell's theorem without inequalities, which implies that the results of quantum mechanics cannot be completely reproduced by a time-local hidden variable model. Our result combined with the GHZ-analysis thereby accentuate that quantum mechanics is inconsistent either with space-local realism or with time-local realism. The GHZ-nonlocality in time, discovered in this work can be experimentally verified with the present superconducting quantum technology .Md. Manirul Ali was supported by the Centre for Quantum Science and Technology, Chennai Institute of Technology, India, vide funding number CIT/CQST/2021/RD-007.",
        " Nonlocality in TimeMd. Manirul Ali manirul@citchennai.net Centre for Quantum Science and Technology, Chennai Institute of Technology, Chennai 600069, IndiaBell's theorem showed that a local hidden variable model cannot completely reproduce the predictions of quantum mechanics. Bell's inequality provides an upper bound under the spatial locality and reality assumptions that can be violated for correlated measurement statistics of quantum mechanics. A more compelling demonstration of Bell's theorem was given by Greenberger, Horne and Zeilinger (GHZ) by considering perfect correlations rather than statistical correlations. GHZ-nonlocality tests Bell's local-realism hypothesis directly without inequalities. In this work, I present a temporal analogue of GHZ-nonlocality which is unexplored in the literature. This result establishes a temporal version of Bell's theorem without inequalities, which implies that the results of quantum mechanics cannot be completely reproduced by a time-local hidden variable model. Our result combined with the GHZ-nonlocality thereby accentuate that quantum mechanics is incompatible either with space-local realism or with time-local realism.03.67.-a,03.67.Hk,03.65.UdEinstein, Podolski and Rosen in 1935 remarked that the",
        "# arXiv Paper: 2205.15729\n\nGreenberger-Horne-Zeilinger Nonlocality in Time\nabstract: Bell's theorem showed that a local hidden variable model cannot completely reproduce the predictions of quantum mechanics. Bell's inequality provides an upper bound under the spatial locality and reality assumptions that can be violated for correlated measurement statistics of quantum mechanics. A more compelling demonstration of Bell's theorem was given by Greenberger, Horne, and Zeilinger (GHZ) by considering perfect correlations rather than statistical correlations. GHZ-nonlocality tests Bell's local-realism hypothesis directly without inequalities. In this work, I present a temporal analogue of GHZ-nonlocality which is unexplored in the literature. This result establishes a temporal version of Bell's theorem {\\it without inequalities}, which implies that the results of quantum mechanics cannot be completely reproduced by a time-local hidden variable model. Our result combined with the GHZ-nonlocality thereby accentuates that quantum mechanics is incompatible either with space-local realism or with time-local realism.\n: Greenberger-Horne-Zeilinger Nonlocality in TimeMd. Manirul Ali manirul@citchennai.net Centre for Quantum Science"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of the research on tensor-pentagon diagrams' role in amplitude, and how do the methods used contribute to the key results?",
      "expected_output": "The research paper explores the role of tensor-pentagon diagrams in amplitude, specifically focusing on three topologies contributing to the amplitude. The main findings include the development of differential operators that lower the loop-level by one, making them valid for higher-loop higher-point diagrams. The methods used, such as extending previous results and utilizing iterative structures, contribute to the key results by relating higher-loop orders to one-loop hexagons, allowing for analytical results from one-loop information. For example, the double-pentagon integral is presented in detail, and its iterative structure is shown to relate to one-loop hexagon integrals.",
      "context": [
        " by the compact expression (REF ). Specifically, there are three topologies contributing to the amplitude, which are collected in Fig. REF . As already mentioned, they are composed by tensor-pentagon diagrams. Some differential equations for these diagrams have been already found in ; here we want to extend those results in such a way to be possible to use them for higher-loop higher-point diagrams. We reviewed in the previous section that the number of independent cross-ratios is {{formula:78fb4192-3f0b-4724-8584-9fd2045c564d}} ; since the integrals do not depend on some dual points, the respective cross-ratios are not present and their number is actually reduced diagram by diagram. The goal is to find differential operators for each of these diagrams such that the loop-level is lowered by one: the operators will be therefore valid for the {{formula:d19bcdaf-0162-4c1d-a68f-c0bcf5b75db3}} -loop double pentaladder diagrams. We will show that the iterative structure relates them to one-loop hexagon integrals.\nDouble-pentagon integral {{form",
        " show that the iterative structure relates them to one-loop hexagon integrals.\nDouble-pentagon integral {{formula:33d9df94-4821-40cd-bc0c-1defb6ca0a19}}: We will present the {{formula:485a182d-8b6c-4c5e-b13c-192732ce4051}} case in detail here, and resume the results for the others in the next subsection. Since this diagram is independent of the dual point {{formula:fdd98bc4-01b0-4d12-a543-a65bcada0e87}} , the following cross-ratios will be present only: {{formula:b32e956f-34fe-4a01-b1c0-7e493842e350}}where they are defined as in (REF ). Going to two-dimensional kinematics by using the fixing (REF ), they become {{formula:79337619-a11c-4e98-840c-76b951dc9dc9}}Since there are two legs attached to one of the gluing points, we can use",
        "-af8f-e58257eb90dc}} we mean the double pentaladder integrals of Fig. REF , where the pentagon subintegrals correspond case by case to the ones at two loops. which are present in loop amplitudes. This iterative structure, relating higher-loop orders to one-loop hexagons, is very powerful since it allows to obtain analytical results from one-loop information.\nDouble-pentagon integrals {{formula:6189e051-d752-4e76-91a4-82d8225059d9}} and {{formula:ff4cd5a3-ba01-47ee-b543-b5bb57934baa}}: We now consider the other two topologies appearing in the two-loop eight-point MHV amplitude. The case {{formula:37d38460-ca64-4e0f-b148-53e1535de643}} is the simplest, as it turns out to depend on one cross-ratio only when projected to the two-dimensional subspace. Considering the massive pentagon, we have in four dimensions {{formula:f0a6520d-e451-"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings of Theorem REF in relation to Hermitian groups and how do the methods used in the research lead to these key results?",
      "expected_output": "The main findings of Theorem REF relate to Hermitian locally compact groups, including connected nilpotent Lie groups and the Euclidean rigid motion groups. The theorem states that the conjecture is true for these groups, establishing a bijective correspondence between weak*-closed maximal left ideals of certain weighted measure algebras and the maximal modular left ideals of the weighted algebra. The methods used involve classification results for representations of these groups, such as the Fell groups, and applying Lemma REF to show that certain functions vanish at infinity. This leads to key results, including the application of the theorem to Hermitian, connected, real or complex algebraic groups, and the establishment of a bijective correspondence similar to the compact case.",
      "context": [
        "}} . Now {{formula:393790e6-1811-41e7-adc1-13acf8a5592d}} must vanish at infinity by Lemma REF .Remark. By the remarks on page 221 of every irreducible representation of a connected nilpotent Lie group vanishes at infinity modulo its kernel, and as such the previous theorem applies. By it also applies to Hermitian, connected, real or complex algebraic groups. The theorem can also be seen to apply to the Fell groups {{formula:1e8b7017-5acf-4fb4-a5eb-2e6967b0dcf5}} and {{formula:94f6cba0-5cb0-4624-94f5-839cb206147b}} , since the representations of these groups are classified (see for instance ).We conclude this section with a classification result for certain weighted measure algebras. In the case of a compact group {{formula:8cea0add-8bf2-4552-af3c-0bc8985a5214}} there is a bijective correspondence between the maximal modular left",
        "ff4b5}} and {{formula:9d0725fc-6fc7-443a-8038-64cb6fa515bd}} . Now {{formula:393790e6-1811-41e7-adc1-13acf8a5592d}} must vanish at infinity by Lemma REF . Remark. By the remarks on page 221 of every irreducible representation of a connected nilpotent Lie group vanishes at infinity modulo its kernel, and as such the previous theorem applies. By it also applies to Hermitian, connected, real or complex algebraic groups. The theorem can also be seen to apply to the Fell groups {{formula:1e8b7017-5acf-4fb4-a5eb-2e6967b0dcf5}} and {{formula:94f6cba0-5cb0-4624-94f5-839cb206147b}} , since the representations of these groups are classified (see for instance ). We conclude this section with a classification result for certain weighted measure algebras. In the case of a compact group {{formula:8cea0add",
        "formula:a7abdf7a-e7a0-4206-9857-959a484ca998}} .Our main result is Theorem REF , which states that our conjecture is true for a large class of Hermitian locally compact groups, which includes connected nilpotent Lie groups, the Euclidean rigid motion groups {{formula:3ab86f30-f547-40c5-8273-1616ee8285ca}} , and the Fell groups {{formula:c9408390-8902-46a0-b8f4-54f9fe60c822}} . We shall also prove in Theorem REF that certain weighted measure algebras enjoy a bijective correspondence between their weak*-closed maximal left ideals, and the maximal modular left ideals of the weighted {{formula:422b1b88-a8e0-423a-87e7-a541f9a90fbc}} -algebra (without any constraint on the representation), similar to the compact case.We shall now explain some of the applications of our classification result. In his seminal work of 1980 , Barnes established a correspondence between integrable"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between wavelength and polarization transformation in the context of this research paper?",
      "expected_output": "The wavelength of the photon represents a spatial period in which the polarisation of the photon is transformed from pure electric to pure magnetic. For example, in the context of the research paper, the photon's path is divided into odd and even sections where it loses and gains electric and magnetic polarisation, respectively.",
      "context": [
        " of the photon represents a spatial period in which the polarisation of the photon is transformed from pure electric to pure magnetic. Having {{formula:0570d4d9-ab7f-4fe8-81fd-c62ba02fd034}} and knowing the velocity {{formula:f18b2d12-c804-434a-8f46-5ca25dc04f91}} of a free photon we can calculate the photon frequency, which features the frequency of transformation of magnetic and electric polarisations: {{formula:adfe9e20-7592-4dab-ad63-c7663f8b346f}} .\n3. Quantum theory of diffraction: Epstein and Ehrenfest [14,15] following Compton considered a three dimensional infinite triclinic lattice with the spacings {{formula:4e50bc8c-dae4-4a13-9890-308e056d267c}} , {{formula:c4f3f573-b547-46db-b128-75d44e162c14}} and {{formula:e46a223f-16fb-4161-8205",
        "abatic processes, for example, a collision of the charged particle's photon cloud with an obstacle, free photons are released from the photon cloud that surrounds the charged particle.A free photon migrates in the tessel-lattice by hopping from cell to cell. During such a motion the state of its surface periodically changes between the state of normal needles (electric polarisation) and the state of combed needles (magnetic polarisation). The photon in each odd section {{formula:a775c354-eab3-4397-b840-b9c711856cd2}} of its path looses the electric polarisation, which is going to zero, and acquires the magnetic polarisation; in even sections {{formula:a46a9287-107c-4aa5-8225-e0edc1693b68}} of the photon's path it looses the magnetic polarisation but restores its electric polarisation. Thus the wavelength {{formula:5be8284a-41b4-4b85-8c67-7115eccb701d}} of the photon represents a spatial period in which the polarisation of the photon is transformed from pure electric to pure magnetic. Having",
        " photon-photon crossing. {{figure:45920b1d-af7b-4585-8993-2de4e8e85b69}}\n2. Structure of the photon: The inerton is a basic excitation of the real space, which transfers fragments of mass (i.e. local deformation of a cell) and fractality. The photon is the second basic excitation of the space.The photon appears [11-13] as a polarisation state of the surface of the inerton. These two fundamental quasi-particles of space can exist only in the state of motion. We can draw the appropriate picture of the photon as follows: the mass (local deformation) of the migrating photon oscillates, periodically transforming to the state that can be described as the tension of the cell. The geometry of the surface of the photon oscillates between the state of normal needles (electric polarisation) and the state of combed needles (magnetic polarisation).Since we compare the size of an elementary cell of the tessel-lattice with the Planck's fundamental length {{formula:8f2706e7-1d2a-4a26-b876-73302048"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the main findings regarding the impact of 3-body interactions on the phases of spin-1 Bose gases, and how do the methods used in this research paper support these findings?",
      "expected_output": "The main findings of this research paper indicate that 3-body interactions significantly impact the phases of spin-1 Bose gases, leading to the formation of spin singlet and nematic Mott insulating phases. The methods used, including Mean Field Approximation (MFA) and Density Matrix Renormalization Group (DMRG) techniques, support these findings by showing an asymmetry in the Mott insulating phase in the presence of both two and three body interactions. However, the DMRG studies suggest that this asymmetry is destroyed when only three body interactions are present. The MFA phase diagrams are in good agreement with the analytical phase diagrams obtained using perturbative expansion, confirming the validity of the results. For example, the paper mentions that at higher values of the three body strength, a first and second order transition is observed at different locations of the chemical potential, corresponding to the first and third Mott insulating lobes.",
      "context": [
        " the spin singlet and nematic MI phases respectively. At a considerable higher values of the three body strength, we have found a first as well as a second order transition at different location of the chemical potential corresponding to the first and the third MI lobes. This was confirmed by carefully scrutinizing the variational ground state energy and the spin eigenvalue and spin nematic order parameter.The region in the vicinity of the first and third, as well as the third and the fifth MI lobes also show a first order phase transition which were subsequently confirmed from the behaviour of the variational energy and order parameters. Finally, all these MFA phase diagrams have splendid match with the analytical phase diagrams obtained using perturbative expansion (PMFA). Both the phase diagrams are in good agreement with each other however a small discrepancy is observed near the tip of the MI lobes for smaller values of the two body spin dependent interaction potential. In the ferromagnetic case, the MFA phase diagrams are similar to that of the spin-0 Bose gas for different values of the three body interaction strength and they are in complete agreement with the analytical PMFA phase diagrams.",
        " It was found that the ground state shows a {{formula:8e67c3d2-3f22-42b7-93b7-f1a829391b37}} symmetry in the AF case and a {{formula:dcb459f1-aa19-4791-a8c6-3db3e19a29ee}} symmetry in the ferromagnetic case in contrast to the usual {{formula:7c620323-b151-434f-9d8e-e9ab7c0a953b}} symmetry of a standard spin-0 BHM , .To understand the origin of various phases in a spinor gas, several numerical techniques, such as mean field approximation (MFA) , , , quantum Monte Carlo (QMC) , , , and strong coupling perturbative expansion (PMFA) , , have been developed to gain an in-depth knowledge on how different extent of the Mott insulating and superfluid phases will affect the ground state phase diagram. In the AF case, the MI state with even occupation densities form a spin singlet state with vanishing spin nematic order, while the odd occupation densities show a spin nematic state with",
        "-diagonal order in the latter. Moreover, the effects of magnetic fields , , , , , , , disorder , , , dipolar interaction , and spin dynamics etc have now been studied extensively in the context of a spin-1 Bose gas.Apart from studying such properties in presence of two body interactions cited above, recently, the consequences of higher body interaction, such as a repulsive three body interaction on the ground state phase diagrams have been explored on a spin-1 ultracold Bose gas using MFA in chapter 6 and DMRG , techniques. Interestingly, the mean field phase diagram still shows a spin nematic-singlet formation and hence an asymmetry in the MI phase in presence of both the two and three body interactions, while for a purely three body interaction such asymmetry is destroyed . On the contrary, the DMRG studies show that there is neither any asymmetry, nor any spin singlet-nematic formation in the MI phase and there is a possible phase transition involved in the SF phase in presence of both the two and three body interactions , .Besides the presence of such repulsive three body interaction, another form of three body interaction which is attractive in nature is proposed by Safavi-Naini et al. ."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does electron influence play in the functional impact of weak localization, and how do the researchers measure this effect in their study?",
      "expected_output": "The electron influence functional plays a crucial role in the functional impact of weak localization, as it contains the electron density matrix, which makes it necessary to keep integrals over both coordinate and momentum variables. Researchers measure this effect by employing the standard picture of time-reversed paths, allowing for a semiclassical evaluation of path integrals that do not contain the electron density matrix, while keeping the latter in its exact form in the corresponding matrix elements.",
      "context": [
        "ula:97e18919-91fa-4dfa-af1c-8896ce5b4832}}is the electron influence functional. It is important that the term {{formula:d33720e9-9361-4360-85a0-97f0207061ae}} (REF ) contains the electron density matrix {{formula:96d5dc58-0dff-422f-bbdc-be7e9ee2570e}} which makes it necessary to keep integrals over both coordinate and momentum variables because the corresponding Hamiltonians turn out to be nonlocal. It is also important to observe that both dissipative term {{formula:3744d8c9-2475-497e-be10-ace799e1de08}} and noise term {{formula:601657fd-5231-41e8-b683-6b635c7bfb6c}} are purely real functionals.In order to describe weak localization in the limit {{formula:d6d22dc3-56d7-451a-ab91-903ab01c9097}} it is sufficient to employ the standard picture of time-reversed paths",
        " form allowing to semiclassically evaluate only those path integrals which do not contain the electron density matrix {{formula:c17595fa-f2b4-4882-81a4-5dfd1f75686d}} while the latter was kept in its exact form in the corresponding matrix elements which cannot grow with time. This analysis was performed within the accuracy of the definition of the weak localization correction to conductivity and fully confirmed our earlier results and conclusions.Unfortunately this latter development was completely ignored by JvD who attempted to treat the path integral yet “somewhat more accurately” than we do. JvD's key idea is to effectively integrate out the momentum variables in Eq. (REF ) which would yield the “position-only” representation of the influence functional. Instead of (REF ) JvD obtained: {{formula:8c06abed-6194-47a7-8249-41314f0a4d8a}}where {{formula:c0ee5c91-9406-4232-83c1-7f22b5ee63d4}} is the action for non-interacting electrons and {{formula:3d8",
        "22b5ee63d4}} is the action for non-interacting electrons and {{formula:3d846c44-970b-42f4-b5c0-e0bd7d83f8e5}}where {{formula:aff37c89-db1a-454a-8747-ffc1f3e8494d}} is defined in Eq. (REF ).Below we will demonstrate that the functional (REF ) is entirely different from the correct one (REF ) and it cannot be obtained from the latter by performing momentum integrations or by any other correct means.A quick glance already at the first JvD's formula (Eq. (1a,b) in Ref. vD1) demonstrates that this and subsequent equations violate the energy-time uncertainty relation. Indeed, on p. 5 of Ref. vD1 JvD writes that the double path integral entering his Eq. (1b) “gives the amplitude for an electron with energy {{formula:ae6b0ef3-d3b9-4e36-8b89-c10e2d7c30bf}} to propagate from {{formula:"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What methods are used to capture high-resolution images of hot Jupiter flow features, and what are the main findings of this research?",
      "expected_output": "The research uses high-resolution models, such as PlaSim-Gen, to capture the flow features of hot Jupiter atmospheres. The main findings suggest that high horizontal resolutions are not required to capture the key features of hot Jupiter atmospheric flows, and that the stronger flow variability achieved at high resolution does not translate into noticeably stronger dayside infrared flux variability. For example, the study found that the breakdown of north-south symmetry of the equatorial wind, which is absent from lower resolution models, does not significantly impact the dayside-averaged emission pattern of these atmospheres.",
      "context": [
        " additional flow variability on long timescales (of order 100 planet days or more), which is absent from the lower resolution models. It manifests as a breakdown of north-south symmetry of the equatorial wind. From post-processing the atmospheric flows at various resolutions (assuming a cloud-free situation), we show that the stronger flow variability achieved at high resolution does not translate into noticeably stronger dayside infrared flux variability. More generally, our results suggest that high horizontal resolutions are not required to capture the key features of hot Jupiter atmospheric flows.\nIntroduction: Hot Jupiters atmospheres have been extensively characterized by observational campaigns and, as a result, they provide some of the best laboratories available to us for studying the physics of exoplanet atmospheres , , , , . The atmospheric flows that develop on these planets, with permanent day and night sides, are the subject of ongoing modelling efforts. While we know and have an understanding of what drives winds in these atmospheres , , the dominant form of dissipation for wind kinetic energy is still the subject of intense debate. Indeed new physics is at play in these atmospheres: shocks , , MHD effects , , , and vertical transport could all play a role in limiting the wind speeds on these planets.In their study of shocks",
        " , , , and vertical transport could all play a role in limiting the wind speeds on these planets.In their study of shocks in hot Jupiter atmospheres, found that a barotropic horizontal shear instability develops in a deep model of the specific hot Jupiter HD209458b. The phenomenology of this instability, and the time variable flow that results from it, is reminiscent of the barotropic instability discussed by in their shallow hot Jupiter model . To this day, however, a similar shear instability has not manifested in any of the deep global hot Jupiter flow models published in the literature. results suggest that this could be due to insufficient resolution in typical deep global model since the instability does not occur in their simulations until a demanding latitudinal resolution threshold is met (see their Figure 4). The existence of such an instability is important in (i) offering a novel avenue to limit wind speeds in hot Jupiter atmospheres and (ii) in the possibility that it could manifest observationally in the form of atmospheric photometric and/or spectroscopic variability. Recently, have reviewed the observational evidence on hot Jupiter variability and presented a detailed exploration of the level of variability expected from global circulation models.Here we reconsider the horizontal shear instability problem with PlaSim-Gen, an adaptation of the",
        " for all three models. This is consistent with our earlier observation that the larger equatorial wind variability at late times in the high-resolution models (Fig. 3) is largely caused by limited north-south shifts in the off-equatorial latitudinal location of the equatorial wind, which have limited impact on the dayside-averaged emission pattern of these atmospheres.While not disproving the possibility of greater variability, e.g. when clouds are accounted for, our results illustrate well the point that atmospheric flow variability does not necessarily translate into a more readily observable photometric/spectroscopic variability. {{table:076b7c70-cf95-4464-af0d-5db3f5ba9747}}\nDiscussion and Conclusions: Superrotating equatorial jets like those realized in most simulations of hot Jupiter atmospheric flows are potentially subject to a barotropic (horizontal shear) instability , . While the barotropic instability was triggered in the shallow hot Jupiter model of , with {{formula:cda0200f-80c8-4ae4-9cee-b41421105107}} bar, none of the many deep hot Jupiter models ({{formula:9a3503a7-ef"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between stellar flux ratio and dust scattering, and how does it impact our understanding of the research findings?",
      "expected_output": "The research paper suggests that the stellar flux ratio is related to dust scattering, with the scattered light fluxes obtained by summing the AARPs covering the entire image over all angles. The estimated uncertainty in the stellar fluxes due to the psf fitting is a factor of 3eab7a0f-d545-4b71-b2ac-0d00f60e962b. The results show that on average, the scattering process is very optically thin, with the circumstellar to stellar flux ratios given in Table REF. The relationship between stellar flux ratio and dust scattering is quantified by the formula 636b093b-9933-45fd-aed1-4341c4b87791, where the number of scatterers, scattering cross section, and other factors contribute to the expected circumstellar to stellar flux ratio. This understanding is crucial in analyzing the observed scattered emission and estimating dust shell masses.",
      "context": [
        "606 and f814 filters, respectively). Thus, within the uncertainty, the flux densities are the same in the narrow and the broad filters, suggesting that even in the narrow filters the contribution from atomic scattering (by Na and K in the 590 nm and 770 nm filter, respectively) is limited, and the broad filters used on the HST are completely dominated by dust scattering.Of a more quantitative importance is the ratio between the integrated surface brightness and the stellar flux, i.e., the fraction of the stellar light that is scattered. The integrated scattered light fluxes are obtained by summing the AARPs covering the entire image over all angles. The estimated uncertainty in the stellar fluxes due to the psf fitting is estimated to be a factor of {{formula:3eab7a0f-d545-4b71-b2ac-0d00f60e962b}} . The circumstellar and stellar fluxes as well as the resulting circumstellar to stellar flux ratios are given in Table REF . The results show that on average the scattering process is very optically thin.\nDust shell masses: We proceed with a simple analysis of the observed scattered emission based on the assumption of optically thin dust scattering in a",
        " masses: We proceed with a simple analysis of the observed scattered emission based on the assumption of optically thin dust scattering in a geometrically thin shell following the work of . The circumstellar scattered flux (at the distance of the object), integrated over the total scattering wavelength range (defined by filter f) in the optically thin regime is given by {{formula:a09befa6-bfbd-4869-b821-a48b46316ba1}}where {{formula:33a7fab2-0df2-4555-8ec1-3fbcd4139e47}} is the stellar surface flux. Hence, the expected circumstellar to stellar flux ratio is given by {{formula:636b093b-9933-45fd-aed1-4341c4b87791}}where {{formula:5a7ae268-725d-4b4a-bd3c-f6c6b4970742}} is the number of scatterers, {{formula:65416dd6-b420-43b7-9dd3-fb7e7107dd7e}} the scattering cross section, and {{formula:",
        " this process the artifacts produced by the 1{{formula:4630adc4-9333-45d5-a46e-9682d00a6ac7}} 8 coronographic spot and the coronographic `finger' were eliminated since these features appear in different regions of the rotated images. Finally, all images were median filtered. The resulting images are shown in Figs REF and REF .The stellar fluxes were obtained from 0.1 s exposure images. In the f475 filter the stellar flux density was obtained directly from the image. For the f606 and f814 filters this procedure was not possible, since the area on the CCD which contains most of the stellar flux was saturated even with 0.1 s of exposure time. The stellar fluxes therefore were estimated through psf fitting. The publicly available program `Tiny Tim' was used to produce model HST psfs, which were rotated and resampled to have the same rotation and pixel scale as the observed images. This model psf image was fitted to the target image, excluding the central saturated region, which made it possible to estimate the stellar flux densities in both filters.The presence of a circumstellar medium is apparent in all images. A comparison with"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role do van der Waals interactions play in determining the force between molecules, and how do researchers measure their effects?",
      "expected_output": "Van der Waals interactions play a crucial role in determining the force between molecules, as seen in the context where the force described by a specific formula arises due to these interactions between carbon atoms of the sheet and the substrate. Researchers measure their effects by modeling the substrate as a rigid sheet of graphene and computing an improper integral to find the force per unit area exerted by the substrate on the sheet, which is given by a specific formula. For example, in the case of a graphene sheet and a HOPG substrate, the attractive force is derived to be of the form of a specific formula, where the distance between the graphene sheet and the substrate is a key factor.",
      "context": [
        "}} nm, which is consistent with some experimental data , . The graph of {{formula:bfdd53fe-2501-4d57-ae8c-c05ed471a69a}} is depicted in Figure REF (a). {{figure:d66f188d-bfd3-4c57-83ed-80d95bf35807}}We also consider the case in which the substrate is HOPG. The force described by {{formula:4e9c0181-81da-4050-bd85-95e1b30bbb39}} in {{formula:b39e50e4-a001-4591-934a-8c2a1881f1b8}} arises because of van der Waals interactions between the carbon atoms of the sheet and the carbon atoms of the substrate. We consider the interactions between the sheet and only the top layer of the atoms on the HOPG substrate. Hence we model the substrate as a second, rigid sheet of graphene. (Because the van der Waals force between carbon atoms decays rapidly as the spacing between the atoms increases, including interactions with additional layers of the HOPG substrate does not significantly change the",
        " rapidly as the spacing between the atoms increases, including interactions with additional layers of the HOPG substrate does not significantly change the interaction force between the sheet and the substrate.) To define {{formula:809f2173-9062-4fbe-ab17-184d7970bdd4}} appropriately for a continuum model, we assume the atoms are distributed on the substrate and on the sheet with a uniform atomic density {{formula:0783ff66-5007-4baa-be88-477d94a3f427}} , a value computed from the geometry of graphene. Also, we assume the substrate is infinite in extent. By computing an appropriate improper integral, we find that the force per unit area exerted by the substrate on the sheet is {{formula:1e2805e5-7cc9-46d4-bfb2-add222654806}} , where {{formula:eb37c861-ac43-4e7a-a1bb-822ba6e0d19d}}with {{formula:d2cc5dfd-596c-406d-b829-998b94c5d8fe}} and {{form",
        "e40062c-3fff-4a07-a19a-e3f0a3600ba7}} is Planck's constant, and {{formula:2bc9d628-1b57-4d85-83ec-565a5ad3bde7}} is the distance between the graphene sheet and the substrate. From (REF ), we derive an attractive force of the form {{formula:f60ac16a-334d-4dda-bbe1-db2e0f65563d}} , where {{formula:ff235bb3-006a-48b8-aaee-edd3ee0cf7b0}} nN nm. Now we define {{formula:b5680df3-ef73-4146-b0ea-2312cf4a4461}} , where {{formula:6b5a902d-6a87-4d97-b287-7494748b8c47}} has the form {{formula:1537bef8-32bd-473b-9472-25c44ee05da4}}with {{formula:d8e4301e"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What is the relationship between the signal-to-noise ratio and the R coefficient in this study, and how do the researchers measure its impact on the results?",
      "expected_output": "The study finds that the R coefficient is biased at low signal-to-noise (S/N) ratios, which is typically observed in the blue part of the spectrum. The researchers measure the impact of S/N on the R coefficient by analyzing the distribution of R values as a function of wavelength and the RV scatter of each line. They find that the average value of |R| increases from short to long wavelengths, peaking for lines between 7000 and 8000 Å, and decreasing again for redder wavelengths. The RV scatter of each line decreases from the bluest wavelengths up to 7500 Å and remains constant for longer wavelengths. This suggests that the low S/N in the blue part of the spectrum results in large uncertainties in the RV measurements, leading to a decrease in the precision of the R coefficient.",
      "context": [
        ", but we observe a similar behaviour for the other five stars.The increase of |R| with wavelength in the blue part of the spectrum seems to be related to the decrease in RV scatter. In the blue, the spectrum has lower S/N than in the red. Due to this, the RV of the bluest lines has large uncertainties and is not as precise as the RV of lines at longer wavelengths. Hence, this decrease in precision due to low S/N results in an increase of the RV scatter. The low S/N also drives the decrease in |R|. If the line RV measurements are not precise enough, it is not possible to detect any correlation with activity, resulting in |R| values close to 0. Indeed, we see that blue lines with large RV scatter have |R| \\sim 0. We conclude that these |R| values close to 0 do not mean that these lines are not sensitive to activity, but instead reflect the lack of precision in their RV measurement. In other words, the R coefficient seems to be biased at low S/N (which is what we observe in the blue part of the spectrum).Regarding the red part of the spectrum, for wavelengths longer than \\sim 7500\\,Å, we do",
        "Correlation strength as a function of the line wavelength: Next, we study the distribution of R values as a function of wavelength. In Fig. REF , we show the distribution of R values of all the lines obtained from the correlations with the three selected indicators (CCF RV, BIS, and CRX) as a function of the line wavelength, for J07446+035. In the same figure, we also show the correlation of the R values with the RV scatter of each line, measured as the weighted standard deviation of the line RV in all the observations (wstd RV, as in Fig. REF ).For the three indicators, the distribution of R values is not constant in wavelength. The average value of |R| increases from short to long wavelengths, peaking for lines between \\lambda \\sim 7000 and 8000 \\mathrm {Å}, and decreasing again for redder wavelengths. Regarding the RV scatter of each line, its average value decreases from the bluest wavelengths up to \\sim 7500\\,Å, and remains constant for longer wavelengths. Figure REF only shows data of J07446+035, but we observe a similar behaviour for the other five stars.The increase of |R| with wavelength in the blue part",
        "rm {R}|, we are effectively reducing the activity signal present in the RV measurements. If all the lines were equally affected by activity, we would expect the RV scatter to become larger as we restrict the R of the averaged lines. This is because, as we decrease the range of allowed R values, we are decreasing the number of lines used, and hence, we are degrading the precision of the RV measurement. Despite that, since the RV scatter decreases as we reduce the number of lines used, it seems that, for this star, reducing the red noise caused by activity has a larger effect than the increase in white photon noise due to the reduction in RV content, as also found by for \\alpha Cen B. We note that some of the smallest RV scatters are obtained using datasets with only \\sim 200 lines, about 10% of the original \\sim 2000. With more restrictive cuts, the RV scatter starts to increase. This probably occurs because the number of lines is too small to obtain sufficient precision in the RV measurements, and hence the photon noise dominates. For these sets of lines, the periodogram peak at P_{\\mathrm {rot}} also becomes less significant, and the highest peak in the periodogram is not related to P_{\\math"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What are the limitations of the Rayleigh approximation when applied to conducting needles, and how do these limitations affect the accuracy of the results?",
      "expected_output": "The Rayleigh approximation has limitations when applied to conducting needles, as it is not applicable to slender needles capable of supplying high far-IR and microwave opacities. This limitation affects the accuracy of the results, as the approximation underestimates the absorption properties of the needles. For example, the antenna theory predicts a much larger absorption cross section per unit dust mass for exceedingly elongated grains compared to the infinite cylinder approximation, as seen in Figure REF. This discrepancy highlights the need for a more accurate solution to model the absorption properties of slender needles.",
      "context": [
        "/or {{formula:0a859df6-7122-467e-a978-d95cc312469e}} would have little effects on the infinite cylinder model. Increasing the cylinder thickness ({{formula:868cc5a6-c356-44b6-b767-bf78cc802ca9}} ) either does not help (see Figure REF ).However, if appealing to the antenna theory, it appears that {{formula:8f811abe-579d-42a7-aae8-118ebf645169}} is attainable by models with a relatively flat distribution of needle elongation ({{formula:85612f93-6bca-49dd-a7a8-9d2454a46ac2}} ) since the absorption cross sections per unit dust mass for exceedingly elongated grains ({{formula:55a03aab-8b3e-4120-bb60-f4891a61af5c}} ) calculated from the antenna theory are much larger than those from the infinite cylinder approximation (see Figure REF ). However, it is not clear whether the antenna theory applies to thin needles with a thickness of nan",
        " we also present in Figure REF results obtained from the Rayleigh approximation (Eq.[REF ]) since it is widely used in literature. It is seen in Figure REF that the infinite cylinder model predicts a much larger 5{{formula:6453cf93-e91a-4dc5-a481-e3d780b74f80}} opacity for {{formula:75977683-ad36-45d7-8a20-6c9a209887a1}} and a much smaller one for {{formula:c1301265-d65c-4b20-a512-4058e4c4da62}} . The antenna model shows a rapid drop at {{formula:5426dba6-60a5-4eec-bdb3-514b6670a01b}} which corresponds to a cutoff wavelength {{formula:0d1dd3f5-a919-4abb-ab23-9793dfad70aa}} (see Eq.[REF ]). This is because the long-wavelength cutoff becomes smaller as {{formula:a475eb84-9c43-478b-bc01-dd193543",
        " needles, of the radiation of Population III objects. It is pointed out here that in many of these results insufficient attention was given to the limits of applicability of the small particle (Rayleigh) approximation. It is shown that the widely adopted Rayleigh approximation is not applicable to conducting needles capable of supplying high far-IR and microwave opacities. Due to lack of an accurate solution to the absorption properties of slender needles, we model them either in terms of infinite cylinders or the antenna theory. It is found that the available intergalactic iron dust, if modelled as infinite cylinders, is not sufficient to produce a sufficiently large optical depth at long wavelengths required by the observed isotropy and Planckian nature of the CMB. In the context of the antenna theory, conducting needles with exceedingly large elongations ({{formula:d7dd6762-0255-4269-9894-2d981bdc6652}} ) appear to be capable of satisfying the optical depth requirement without violating the iron density limit. But the applicability of the antenna theory to exceedingly thin needles of nanometer/micrometer in thickness needs to be justified.I am grateful to A. Burrows, A.G.W. Cameron, B.T."
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    },
    {
      "input": "What role does SU(1,1) symmetry play in the analysis of two-photon effects, and how do the researchers utilize this symmetry in their findings?",
      "expected_output": "SU(1,1) symmetry plays a major role in connection with two-photon effects. The researchers utilize this symmetry by providing a practical framework to represent SU(1,1) states in an appropriate way, using a physics-based approach and eliminating spurious degrees of freedom to get a description on the upper sheet of a two-sheeted hyperboloid, which is the natural arena for these phenomena.",
      "context": [
        "-7da1d9b55d6c}} ) .. Probably, the most widespread example beyond the harmonic oscillator is that of SU(2), with the Bloch sphere as associated phase space , , ; this case is of paramount importance in dealing with spinlike systems , , , , , . Suitable results have also been found for the Euclidean group E(2), this time with the cylinder as phase space , , , ; this is of primary importance in treating the orbital angular momentum of twisted photons , . Additional applications to more general dynamical groups have also appeared in the literature , , , . Moreover, the basic notions have been successfully extended to discrete qudits, where the phase space is a finite grid , , , , , , .Surprisingly, the phase-space description of systems having SU(1,1) symmetry has received comparatively little attention , , in part because the representation theory of this group is not as familiar as SU(2) or even E(2). However, SU(1,1) plays a major role in connection with what can be called two-photon effects , , , . The topic is experiencing a revival in popularity due to the recent realization of a nonlinear SU(1,1) inter",
        " unanimously recognized for continuous variables (with the complex plane as phase space), for spinlike systems (with the Bloch sphere as the underlying manifold), for orbital angular momentum (represented in the cylinder), and for other systems. Surprisingly, the physics related to the SU(1,1) symmetry is not displayed on the hyperboloid, the natural arena for these phenomena.What we have accomplished here is to provide a practical framework to represent SU(1,1) states in an appropriate way. Apart from the intrinsic beauty of the formalism, our compelling arguments should convince the community of the benefits that arise using the proper phase-space tools to deal with these systems.\nAcknowledgments: We acknowledge financial support from the Mexican Consejo Nacional de Ciencia y Tecnologia (CONACYT) (Grant 254127), the Spanish Ministerio de Ciencia e Innovación (MICINN) (Grant PGC2018-099183-B-I00) and the Natural Sciences and Engineering Research Council (NSERC) of Canada",
        " . The topic is experiencing a revival in popularity due to the recent realization of a nonlinear SU(1,1) interferometer , . According to the pioneering proposal of Yurke et al. , this device would allow one to improve the phase measurement sensitivity in a stunning manner .In spite of the importance of these systems, the mathematical complexity of the group SU(1,1) has prevented a proper phase-space description. In this paper we approach this question resorting to a physics-based approach. For SU(2), one can model the description in terms of a superposition of two harmonic modes. In technical terms, this corresponds to the Jordan-Schwinger bosonic realization of the algebra {{formula:0ffcfc49-3f3e-491f-a0ec-c5d084491099}} . Here, we propose a similar way to deal with {{formula:ba48d874-387a-4bd1-aced-61958c5f0e53}} : starting with two orthonormal modes, and using the standard tools for continuous variables, we eliminate the spurious degrees of freedom and we get a description on the upper sheet of a two-sheeted hyperbol"
      ],
      "additional_metadata": {
        "evolutions": [
          "Reasoning"
        ],
        "synthetic_input_quality": 0.8
      }
    }
  ],
  "metadata": {
    "total_goldens": 506,
    "generated_with": "University LLM",
    "timestamp": 1749963252.5917287
  }
}